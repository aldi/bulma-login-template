Speech and Language Processing
An Introduction to Natural Language Processing,
Computational Linguistics, and Speech Recognition
Third Edition draft
Daniel Jurafsky
Stanford University
James H. Martin
University of Colorado at Boulder
Copyright ©2023. All rights reserved.
Draft of January 7, 2023. Comments and typos welcome!Summary of Contents
I FundamentalAlgorithmsforNLP 1
1 Introduction................................................... 3
2 RegularExpressions,TextNormalization,EditDistance......... 4
3 N-gramLanguageModels ..................................... 31
4 NaiveBayes,TextClassification,andSentiment................. 58
5 LogisticRegression............................................ 79
6 VectorSemanticsandEmbeddings.............................103
7 NeuralNetworksandNeuralLanguageModels.................134
8 SequenceLabelingforPartsofSpeechandNamedEntities......160
9 RNNsandLSTMs.............................................185
10 TransformersandPretrainedLanguageModels.................211
11 Fine-TuningandMaskedLanguageModels.....................228
12 Prompting,In-ContextLearning,andInstructTuning...........244
II NLPApplications 245
13 MachineTranslation...........................................247
14 QuestionAnsweringandInformationRetrieval.................269
15 Chatbots&DialogueSystems..................................296
16 AutomaticSpeechRecognitionandText-to-Speech..............329
III AnnotatingLinguisticStructure 355
17 Context-FreeGrammarsandConstituencyParsing.............357
18 DependencyParsing...........................................381
19 LogicalRepresentationsofSentenceMeaning...................405
20 ComputationalSemanticsandSemanticParsing................428
21 RelationandEventExtraction................................. 429
22 TimeandTemporalReasoning.................................446
23 WordSensesandWordNet.....................................457
24 SemanticRoleLabeling........................................476
25 LexiconsforSentiment,Affect,andConnotation................496
26 CoreferenceResolution........................................516
27 DiscourseCoherence...........................................543
28 Phonetics......................................................565
Bibliography......................................................587
SubjectIndex.....................................................621
2Contents
I FundamentalAlgorithmsforNLP 1
1 Introduction 3
2 RegularExpressions,TextNormalization,EditDistance 4
2.1 RegularExpressions . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2 Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.3 Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.4 TextNormalization . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.5 MinimumEditDistance . . . . . . . . . . . . . . . . . . . . . . . 24
2.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 29
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
3 N-gramLanguageModels 31
3.1 N-Grams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
3.2 EvaluatingLanguageModels . . . . . . . . . . . . . . . . . . . . 37
3.3 Samplingsentencesfromalanguagemodel. . . . . . . . . . . . . 40
3.4 GeneralizationandZeros . . . . . . . . . . . . . . . . . . . . . . 40
3.5 Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
3.6 HugeLanguageModelsandStupidBackoff . . . . . . . . . . . . 48
3.7 Advanced: Kneser-NeySmoothing . . . . . . . . . . . . . . . . . 49
3.8 Advanced: Perplexity’sRelationtoEntropy . . . . . . . . . . . . 52
3.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 55
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4 NaiveBayes,TextClassification,andSentiment 58
4.1 NaiveBayesClassifiers . . . . . . . . . . . . . . . . . . . . . . . 59
4.2 TrainingtheNaiveBayesClassifier . . . . . . . . . . . . . . . . . 62
4.3 Workedexample . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.4 OptimizingforSentimentAnalysis . . . . . . . . . . . . . . . . . 64
4.5 NaiveBayesforothertextclassificationtasks . . . . . . . . . . . 66
4.6 NaiveBayesasaLanguageModel . . . . . . . . . . . . . . . . . 67
4.7 Evaluation: Precision,Recall,F-measure . . . . . . . . . . . . . . 68
4.8 TestsetsandCross-validation . . . . . . . . . . . . . . . . . . . . 70
4.9 StatisticalSignificanceTesting . . . . . . . . . . . . . . . . . . . 71
4.10 AvoidingHarmsinClassification . . . . . . . . . . . . . . . . . . 75
4.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 76
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
5 LogisticRegression 79
5.1 Thesigmoidfunction . . . . . . . . . . . . . . . . . . . . . . . . 80
5.2 ClassificationwithLogisticRegression . . . . . . . . . . . . . . . 82
5.3 Multinomiallogisticregression . . . . . . . . . . . . . . . . . . . 86
5.4 LearninginLogisticRegression . . . . . . . . . . . . . . . . . . . 89
5.5 Thecross-entropylossfunction . . . . . . . . . . . . . . . . . . . 90
5.6 GradientDescent . . . . . . . . . . . . . . . . . . . . . . . . . . 91
34 CONTENTS
5.7 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
5.8 LearninginMultinomialLogisticRegression . . . . . . . . . . . . 98
5.9 Interpretingmodels . . . . . . . . . . . . . . . . . . . . . . . . . 99
5.10 Advanced: DerivingtheGradientEquation . . . . . . . . . . . . . 100
5.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 101
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
6 VectorSemanticsandEmbeddings 103
6.1 LexicalSemantics . . . . . . . . . . . . . . . . . . . . . . . . . . 104
6.2 VectorSemantics . . . . . . . . . . . . . . . . . . . . . . . . . . 107
6.3 WordsandVectors . . . . . . . . . . . . . . . . . . . . . . . . . . 108
6.4 Cosineformeasuringsimilarity . . . . . . . . . . . . . . . . . . . 112
6.5 TF-IDF:Weighingtermsinthevector . . . . . . . . . . . . . . . 113
6.6 PointwiseMutualInformation(PMI) . . . . . . . . . . . . . . . . 116
6.7 Applicationsofthetf-idforPPMIvectormodels . . . . . . . . . . 118
6.8 Word2vec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
6.9 VisualizingEmbeddings . . . . . . . . . . . . . . . . . . . . . . . 125
6.10 Semanticpropertiesofembeddings . . . . . . . . . . . . . . . . . 126
6.11 BiasandEmbeddings . . . . . . . . . . . . . . . . . . . . . . . . 128
6.12 EvaluatingVectorModels . . . . . . . . . . . . . . . . . . . . . . 129
6.13 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 131
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
7 NeuralNetworksandNeuralLanguageModels 134
7.1 Units . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
7.2 TheXORproblem . . . . . . . . . . . . . . . . . . . . . . . . . . 137
7.3 FeedforwardNeuralNetworks. . . . . . . . . . . . . . . . . . . . 140
7.4 FeedforwardnetworksforNLP:Classification . . . . . . . . . . . 144
7.5 FeedforwardNeuralLanguageModeling . . . . . . . . . . . . . . 147
7.6 TrainingNeuralNets . . . . . . . . . . . . . . . . . . . . . . . . 150
7.7 Trainingtheneurallanguagemodel . . . . . . . . . . . . . . . . . 156
7.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 158
8 SequenceLabelingforPartsofSpeechandNamedEntities 160
8.1 (Mostly)EnglishWordClasses . . . . . . . . . . . . . . . . . . . 161
8.2 Part-of-SpeechTagging . . . . . . . . . . . . . . . . . . . . . . . 163
8.3 NamedEntitiesandNamedEntityTagging . . . . . . . . . . . . . 165
8.4 HMMPart-of-SpeechTagging . . . . . . . . . . . . . . . . . . . 167
8.5 ConditionalRandomFields(CRFs) . . . . . . . . . . . . . . . . . 174
8.6 EvaluationofNamedEntityRecognition . . . . . . . . . . . . . . 179
8.7 FurtherDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
8.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 182
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
9 RNNsandLSTMs 185
9.1 RecurrentNeuralNetworks . . . . . . . . . . . . . . . . . . . . . 185
9.2 RNNsasLanguageModels . . . . . . . . . . . . . . . . . . . . . 189
9.3 RNNsforotherNLPtasks . . . . . . . . . . . . . . . . . . . . . . 192CONTENTS 5
9.4 StackedandBidirectionalRNNarchitectures . . . . . . . . . . . . 195
9.5 TheLSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
9.6 Summary: CommonRNNNLPArchitectures . . . . . . . . . . . 201
9.7 TheEncoder-DecoderModelwithRNNs . . . . . . . . . . . . . . 201
9.8 Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
9.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 209
10 TransformersandPretrainedLanguageModels 211
10.1 Self-AttentionNetworks: Transformers . . . . . . . . . . . . . . . 212
10.2 TransformersasLanguageModels . . . . . . . . . . . . . . . . . 220
10.3 Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
10.4 BeamSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
10.5 PretrainingLargeLanguageModels . . . . . . . . . . . . . . . . 225
10.6 LanguageModelsforZero-shotLearning . . . . . . . . . . . . . . 225
10.7 PotentialHarmsfromLanguageModels . . . . . . . . . . . . . . 226
10.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 227
11 Fine-TuningandMaskedLanguageModels 228
11.1 BidirectionalTransformerEncoders. . . . . . . . . . . . . . . . . 228
11.2 TrainingBidirectionalEncoders . . . . . . . . . . . . . . . . . . . 232
11.3 TransferLearningthroughFine-Tuning . . . . . . . . . . . . . . . 237
11.4 TrainingCorpora. . . . . . . . . . . . . . . . . . . . . . . . . . . 242
11.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 243
12 Prompting,In-ContextLearning,andInstructTuning 244
II NLPApplications 245
13 MachineTranslation 247
13.1 LanguageDivergencesandTypology . . . . . . . . . . . . . . . . 248
13.2 MachineTranslationusingEncoder-Decoder . . . . . . . . . . . . 252
13.3 DetailsoftheEncoder-DecoderModel . . . . . . . . . . . . . . . 256
13.4 Translatinginlow-resourcesituations . . . . . . . . . . . . . . . . 258
13.5 MTEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
13.6 BiasandEthicalIssues . . . . . . . . . . . . . . . . . . . . . . . 263
13.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 265
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
14 QuestionAnsweringandInformationRetrieval 269
14.1 InformationRetrieval . . . . . . . . . . . . . . . . . . . . . . . . 270
14.2 IR-basedFactoidQuestionAnswering . . . . . . . . . . . . . . . 278
14.3 EntityLinking . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
14.4 Knowledge-basedQuestionAnswering . . . . . . . . . . . . . . . 286
14.5 UsingLanguageModelstodoQA . . . . . . . . . . . . . . . . . 289
14.6 ClassicQAModels . . . . . . . . . . . . . . . . . . . . . . . . . 290
14.7 EvaluationofFactoidAnswers . . . . . . . . . . . . . . . . . . . 293
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 294
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2956 CONTENTS
15 Chatbots&DialogueSystems 296
15.1 PropertiesofHumanConversation . . . . . . . . . . . . . . . . . 297
15.2 Chatbots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
15.3 GUS:SimpleFrame-basedDialogueSystems . . . . . . . . . . . 308
15.4 TheDialogue-StateArchitecture . . . . . . . . . . . . . . . . . . 312
15.5 EvaluatingDialogueSystems . . . . . . . . . . . . . . . . . . . . 321
15.6 DialogueSystemDesign. . . . . . . . . . . . . . . . . . . . . . . 324
15.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 326
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328
16 AutomaticSpeechRecognitionandText-to-Speech 329
16.1 TheAutomaticSpeechRecognitionTask . . . . . . . . . . . . . . 330
16.2 FeatureExtractionforASR:LogMelSpectrum . . . . . . . . . . 332
16.3 SpeechRecognitionArchitecture . . . . . . . . . . . . . . . . . . 336
16.4 CTC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
16.5 ASREvaluation: WordErrorRate . . . . . . . . . . . . . . . . . 343
16.6 TTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
16.7 OtherSpeechTasks . . . . . . . . . . . . . . . . . . . . . . . . . 350
16.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 351
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354
III AnnotatingLinguisticStructure 355
17 Context-FreeGrammarsandConstituencyParsing 357
17.1 Constituency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
17.2 Context-FreeGrammars . . . . . . . . . . . . . . . . . . . . . . . 358
17.3 Treebanks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
17.4 GrammarEquivalenceandNormalForm . . . . . . . . . . . . . . 364
17.5 Ambiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
17.6 CKYParsing: ADynamicProgrammingApproach . . . . . . . . 367
17.7 Span-BasedNeuralConstituencyParsing . . . . . . . . . . . . . . 373
17.8 EvaluatingParsers . . . . . . . . . . . . . . . . . . . . . . . . . . 375
17.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 378
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
18 DependencyParsing 381
18.1 DependencyRelations . . . . . . . . . . . . . . . . . . . . . . . . 382
18.2 Transition-BasedDependencyParsing . . . . . . . . . . . . . . . 386
18.3 Graph-BasedDependencyParsing . . . . . . . . . . . . . . . . . 395
18.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
18.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 402
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
19 LogicalRepresentationsofSentenceMeaning 405
19.1 ComputationalDesiderataforRepresentations . . . . . . . . . . . 406
19.2 Model-TheoreticSemantics . . . . . . . . . . . . . . . . . . . . . 408
19.3 First-OrderLogic . . . . . . . . . . . . . . . . . . . . . . . . . . 411
19.4 EventandStateRepresentations. . . . . . . . . . . . . . . . . . . 418CONTENTS 7
19.5 DescriptionLogics . . . . . . . . . . . . . . . . . . . . . . . . . . 419
19.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 425
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
20 ComputationalSemanticsandSemanticParsing 428
21 RelationandEventExtraction 429
21.1 RelationExtraction . . . . . . . . . . . . . . . . . . . . . . . . . 430
21.2 RelationExtractionAlgorithms . . . . . . . . . . . . . . . . . . . 432
21.3 ExtractingEvents . . . . . . . . . . . . . . . . . . . . . . . . . . 441
21.4 TemplateFilling . . . . . . . . . . . . . . . . . . . . . . . . . . . 442
21.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 444
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445
22 TimeandTemporalReasoning 446
22.1 RepresentingTime . . . . . . . . . . . . . . . . . . . . . . . . . . 446
22.2 RepresentingAspect . . . . . . . . . . . . . . . . . . . . . . . . . 449
22.3 TemporallyAnnotatedDatasets: TimeBank. . . . . . . . . . . . . 451
22.4 AutomaticTemporalAnalysis . . . . . . . . . . . . . . . . . . . . 452
22.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 456
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456
23 WordSensesandWordNet 457
23.1 WordSenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
23.2 RelationsBetweenSenses . . . . . . . . . . . . . . . . . . . . . . 460
23.3 WordNet: ADatabaseofLexicalRelations . . . . . . . . . . . . . 462
23.4 WordSenseDisambiguation. . . . . . . . . . . . . . . . . . . . . 465
23.5 AlternateWSDalgorithmsandTasks . . . . . . . . . . . . . . . . 468
23.6 UsingThesaurusestoImproveEmbeddings . . . . . . . . . . . . 471
23.7 WordSenseInduction . . . . . . . . . . . . . . . . . . . . . . . . 471
23.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 473
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474
24 SemanticRoleLabeling 476
24.1 SemanticRoles . . . . . . . . . . . . . . . . . . . . . . . . . . . 477
24.2 DiathesisAlternations . . . . . . . . . . . . . . . . . . . . . . . . 478
24.3 SemanticRoles: ProblemswithThematicRoles . . . . . . . . . . 479
24.4 ThePropositionBank . . . . . . . . . . . . . . . . . . . . . . . . 480
24.5 FrameNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481
24.6 SemanticRoleLabeling . . . . . . . . . . . . . . . . . . . . . . . 483
24.7 SelectionalRestrictions . . . . . . . . . . . . . . . . . . . . . . . 487
24.8 PrimitiveDecompositionofPredicates . . . . . . . . . . . . . . . 491
24.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 493
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495
25 LexiconsforSentiment,Affect,andConnotation 496
25.1 DefiningEmotion . . . . . . . . . . . . . . . . . . . . . . . . . . 4978 CONTENTS
25.2 AvailableSentimentandAffectLexicons . . . . . . . . . . . . . . 499
25.3 CreatingAffectLexiconsbyHumanLabeling . . . . . . . . . . . 500
25.4 Semi-supervisedInductionofAffectLexicons . . . . . . . . . . . 502
25.5 SupervisedLearningofWordSentiment . . . . . . . . . . . . . . 505
25.6 UsingLexiconsforSentimentRecognition . . . . . . . . . . . . . 510
25.7 UsingLexiconsforAffectRecognition . . . . . . . . . . . . . . . 511
25.8 Lexicon-basedmethodsforEntity-CentricAffect. . . . . . . . . . 512
25.9 ConnotationFrames . . . . . . . . . . . . . . . . . . . . . . . . . 512
25.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 515
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
26 CoreferenceResolution 516
26.1 CoreferencePhenomena: LinguisticBackground . . . . . . . . . . 519
26.2 CoreferenceTasksandDatasets . . . . . . . . . . . . . . . . . . . 524
26.3 MentionDetection . . . . . . . . . . . . . . . . . . . . . . . . . . 525
26.4 ArchitecturesforCoreferenceAlgorithms . . . . . . . . . . . . . 528
26.5 Classifiersusinghand-builtfeatures . . . . . . . . . . . . . . . . . 530
26.6 Aneuralmention-rankingalgorithm . . . . . . . . . . . . . . . . 532
26.7 EvaluationofCoreferenceResolution . . . . . . . . . . . . . . . . 535
26.8 WinogradSchemaproblems . . . . . . . . . . . . . . . . . . . . . 536
26.9 GenderBiasinCoreference . . . . . . . . . . . . . . . . . . . . . 537
26.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 539
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 539
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 542
27 DiscourseCoherence 543
27.1 CoherenceRelations . . . . . . . . . . . . . . . . . . . . . . . . . 545
27.2 DiscourseStructureParsing . . . . . . . . . . . . . . . . . . . . . 548
27.3 CenteringandEntity-BasedCoherence . . . . . . . . . . . . . . . 552
27.4 Representationlearningmodelsforlocalcoherence . . . . . . . . 556
27.5 GlobalCoherence . . . . . . . . . . . . . . . . . . . . . . . . . . 558
27.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 562
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564
28 Phonetics 565
28.1 SpeechSoundsandPhoneticTranscription . . . . . . . . . . . . . 565
28.2 ArticulatoryPhonetics . . . . . . . . . . . . . . . . . . . . . . . . 566
28.3 Prosody . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 571
28.4 AcousticPhoneticsandSignals . . . . . . . . . . . . . . . . . . . 573
28.5 PhoneticResources . . . . . . . . . . . . . . . . . . . . . . . . . 584
28.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585
BibliographicalandHistoricalNotes . . . . . . . . . . . . . . . . . . . . 585
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 586
Bibliography 587
SubjectIndex 621Part I
FUNDAMENTAL ALGORITHMS FOR
NLP
In the first part of the book we introduce the fundamental suite of algorithmic
toolsthatmakeupthemodernneurallanguagemodelthatistheheartofend-to-end
NLPsystems.Webeginwithtokenizationandpreprocessing,aswellasusefulalgo-
rithmslikecomputingeditdistance,andthenproceedtothetasksofclassification,
logisticregression,neuralnetworks,proceedingthroughfeedforwardnetworks,re-
currentnetworks,andthentransformers. We’llalsoseetheroleofembeddingsasa
modelofwordmeaning.CHAPTER
1 Introduction
La dernie`re chose qu’on trouve en faisant un ouvrage est de savoir celle qu’il faut
mettrelapremie`re.
[Thelastthingyoufigureoutinwritingabookiswhattoputfirst.]
Pascal
34 CHAPTER2 • REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCE
CHAPTER
2 Regular Expressions, Text
Normalization, Edit Distance
User: I am unhappy.
ELIZA: DO YOU THINK COMING HERE WILL HELP YOU NOT TO BE UNHAPPY
User: I need some help, that much seems certain.
ELIZA: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP
User: Perhaps I could learn to get along with my mother.
ELIZA: TELL ME MORE ABOUT YOUR FAMILY
User: My mother takes care of me.
ELIZA: WHO ELSE IN YOU FAMILY TAKES CARE OF YOU
User: My father.
ELIZA: YOUR FATHER
User: You are like my father in some ways.
Weizenbaum(1966)
ELIZA ThedialogueaboveisfromELIZA,anearlynaturallanguageprocessingsystem
thatcouldcarryonalimitedconversationwithauserbyimitatingtheresponsesof
a Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple
programthatusespatternmatchingtorecognizephraseslike“IneedX”andtranslate
them into suitable outputs like “What would it mean to you if you got X?”. This
simpletechniquesucceedsinthisdomainbecauseELIZAdoesn’tactuallyneedto
knowanythingtomimicaRogerianpsychotherapist. AsWeizenbaumnotes,thisis
oneofthefewdialoguegenreswherelistenerscanactasiftheyknownothingofthe
world. ELIZA’smimicryofhumanconversationwasremarkablysuccessful: many
people who interacted with ELIZA came to believe that it really understood them
and their problems, many continued to believe in ELIZA’s abilities even after the
program’s operation was explained to them (Weizenbaum, 1976), and even today
chatbots suchchatbotsareafundiversion.
Of course modern conversational agents are much more than a diversion; they
cananswerquestions,bookflights,orfindrestaurants,functionsforwhichtheyrely
on a much more sophisticated understanding of the user’s intent, as we will see in
Chapter 15. Nonetheless, the simple pattern-based methods that powered ELIZA
andotherchatbotsplayacrucialroleinnaturallanguageprocessing.
We’llbeginwiththemostimportanttoolfordescribingtextpatterns:theregular
expression. Regular expressions can be used to specify strings we might want to
extractfromadocument,fromtransforming“IneedX”inELIZAabove,todefining
stringslike$199or$24.99forextractingtablesofpricesfromadocument.
text We’llthenturntoasetoftaskscollectivelycalledtextnormalization,inwhich
normalization
regular expressions play an important part. Normalizing text means converting it
to a more convenient, standard form. For example, most of what we are going to
do with language relies on first separating out or tokenizing words from running
tokenization text, the task of tokenization. English words are often separated from each other
by whitespace, butwhitespace is not always sufficient. New York and rock ’n’ roll
aresometimestreatedaslargewordsdespitethefactthattheycontainspaces,while
sometimeswe’llneedtoseparateI’mintothetwowordsIandam. Forprocessing
tweetsortextswe’llneedtotokenizeemoticonslike:) orhashtagslike#nlproc.2.1 • REGULAREXPRESSIONS 5
Somelanguages,likeJapanese,don’thavespacesbetweenwords,sowordtokeniza-
tionbecomesmoredifficult.
lemmatization Another part of text normalization is lemmatization, the task of determining
that two words have the same root, despite their surface differences. For example,
the words sang, sung, and sings are forms of the verb sing. The word sing is the
common lemma of these words, and a lemmatizer maps from all of these to sing.
Lemmatizationisessentialforprocessingmorphologicallycomplexlanguageslike
stemming Arabic. Stemmingreferstoasimplerversionoflemmatizationinwhichwemainly
just strip suffixes from the end of the word. Text normalization also includes sen-
sentence tence segmentation: breaking up a text into individual sentences, using cues like
segmentation
periodsorexclamationpoints.
Finally,we’llneedtocomparewordsandotherstrings. We’llintroduceametric
callededitdistancethatmeasureshowsimilartwostringsarebasedonthenumber
of edits (insertions, deletions, substitutions) it takes to change one string into the
other. Editdistanceisanalgorithmwithapplicationsthroughoutlanguageprocess-
ing,fromspellingcorrectiontospeechrecognitiontocoreferenceresolution.
2.1 Regular Expressions
One of the unsung successes in standardization in computer science has been the
regular regularexpression(oftenshortenedtoregex),alanguageforspecifyingtextsearch
expression
strings.Thispracticallanguageisusedineverycomputerlanguage,wordprocessor,
andtextprocessingtoolsliketheUnixtoolsgreporEmacs. Formally,aregularex-
pressionisanalgebraicnotationforcharacterizingasetofstrings. Regularexpres-
sionsareparticularlyusefulforsearchingintexts,whenwehaveapatterntosearch
corpus for and a corpus of texts to search through. A regular expression search function
willsearchthroughthecorpus,returningalltextsthatmatchthepattern. Thecorpus
canbeasingledocumentoracollection. Forexample,theUnixcommand-linetool
grep takes a regular expression and returns every line of the input document that
matchestheexpression.
Asearchcanbedesignedtoreturneverymatchonaline,iftherearemorethan
one, or just the first match. In the following examples we generally underline the
exactpartofthepatternthatmatchestheregularexpressionandshowonlythefirst
match. We’llshowregularexpressionsdelimitedbyslashesbutnotethatslashesare
notpartoftheregularexpressions.
Regularexpressionscomeinmanyvariants.We’llbedescribingextendedregu-
larexpressions;differentregularexpressionparsersmayonlyrecognizesubsetsof
these,ortreatsomeexpressionsslightlydifferently. Usinganonlineregularexpres-
siontesterisahandywaytotestoutyourexpressionsandexplorethesevariations.
2.1.1 BasicRegularExpressionPatterns
Thesimplestkindofregularexpressionisasequenceofsimplecharacters;putting
concatenation charactersinsequenceiscalledconcatenation. Tosearchforwoodchuck,wetype
/woodchuck/. The expression /Buttercup/ matches any string containing the
substringButtercup;grepwiththatexpressionwouldreturnthelineI’mcalledlit-
tle Buttercup. The search string can consist of a single character (like /!/) or a
sequenceofcharacters(like/urgl/)(seeFig.2.1).
Regular expressions are case sensitive; lower case /s/ is distinct from upper6 CHAPTER2 • REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCE
Regex ExamplePatternsMatched
/woodchucks/ “interestinglinkstowoodchucksandlemurs”
/a/ “MaryAnnstoppedbyMona’s”
/!/ “You’velefttheburglarbehindagain!”saidNori
Figure2.1 Somesimpleregexsearches.
case /S/ (/s/ matches a lower case s but not an upper case S). This means that
thepattern/woodchucks/willnotmatchthestringWoodchucks. Wecansolvethis
problemwiththeuseofthesquarebraces[and].Thestringofcharactersinsidethe
bracesspecifiesadisjunctionofcharacterstomatch. Forexample,Fig.2.2shows
thatthepattern/[wW]/matchespatternscontainingeitherworW.
Regex Match ExamplePatterns
/[wW]oodchuck/ Woodchuckorwoodchuck “Woodchuck”
/[abc]/ ‘a’,‘b’,or‘c’ “Inuomini,insoldati”
/[1234567890]/ anydigit “plentyof7to5”
Figure2.2 Theuseofthebrackets[]tospecifyadisjunctionofcharacters.
Theregularexpression/[1234567890]/specifiesanysingledigit. Whilesuch
classesofcharactersasdigitsorlettersareimportantbuildingblocksinexpressions,
theycangetawkward(e.g.,it’sinconvenienttospecify
/[ABCDEFGHIJKLMNOPQRSTUVWXYZ]/
tomean“anycapitalletter”). Incaseswherethereisawell-definedsequenceasso-
ciatedwithasetofcharacters,thebracketscanbeusedwiththedash(-)tospecify
range anyonecharacterinarange. Thepattern/[2-5]/specifiesanyoneofthecharac-
ters2,3,4,or5. Thepattern/[b-g]/specifiesoneofthecharactersb,c,d,e,f,or
g. SomeotherexamplesareshowninFig.2.3.
Regex Match ExamplePatternsMatched
/[A-Z]/ anuppercaseletter “weshouldcallit‘DrenchedBlossoms’”
/[a-z]/ alowercaseletter “mybeanswereimpatienttobehoed!”
/[0-9]/ asingledigit “Chapter1: DowntheRabbitHole”
Figure2.3 Theuseofthebrackets[]plusthedash-tospecifyarange.
Thesquarebracescanalsobeusedtospecifywhatasinglecharactercannotbe,
byuseofthecaretˆ. Ifthecaretˆisthefirstsymbolaftertheopensquarebrace[,
theresultingpatternisnegated.Forexample,thepattern/[ˆa]/matchesanysingle
character (including special characters) except a. This is only true when the caret
isthefirstsymbolaftertheopensquarebrace. Ifitoccursanywhereelse,itusually
standsforacaret;Fig.2.4showssomeexamples.
Regex Match(singlecharacters) ExamplePatternsMatched
/[ˆA-Z]/ notanuppercaseletter “Oyfnpripetchik”
/[ˆSs]/ neither‘S’nor‘s’ “Ihavenoexquisitereasonfor’t”
/[ˆ.]/ notaperiod “ourresidentDjinn”
/[eˆ]/ either‘e’or‘ˆ’ “lookupˆ now”
/aˆb/ thepattern‘aˆb’ “lookupaˆbnow”
Figure2.4 Thecaretˆfornegationorjusttomeanˆ.Seebelowre:thebackslashforescapingtheperiod.
Howcanwetalkaboutoptionalelements, likeanoptionalsinwoodchuckand
woodchucks? Wecan’tusethesquarebrackets,becausewhiletheyallowustosay2.1 • REGULAREXPRESSIONS 7
“sorS”,theydon’tallowustosay“sornothing”.Forthisweusethequestionmark
/?/,whichmeans“theprecedingcharacterornothing”,asshowninFig.2.5.
Regex Match ExamplePatternsMatched
/woodchucks?/ woodchuckorwoodchucks “woodchuck”
/colou?r/ colororcolour “color”
Figure2.5 Thequestionmark?marksoptionalityofthepreviousexpression.
We can think of the question mark as meaning “zero or one instances of the
previous character”. That is, it’s a way of specifying how many of something that
we want, something that is very important in regular expressions. For example,
consider the language of certain sheep, which consists of strings that look like the
following:
baa!
baaa!
baaaa!
baaaaa!
...
Thislanguageconsistsofstringswithab,followedbyatleasttwoa’s,followed
byanexclamationpoint. Thesetofoperatorsthatallowsustosaythingslike“some
Kleene* numberofas”arebasedontheasteriskor*, commonlycalledtheKleene*(gen-
erallypronounced“cleanystar”). TheKleenestarmeans“zeroormoreoccurrences
oftheimmediatelypreviouscharacterorregularexpression”. So/a*/means“any
stringofzeroormoreas”. Thiswillmatchaoraaaaaa, butitwillalsomatchthe
emptystringatthestartofOffMinorsincethestringOffMinorstartswithzeroa’s.
Sotheregularexpressionformatchingoneormoreais/aa*/,meaningoneafol-
lowedbyzeroormoreas.Morecomplexpatternscanalsoberepeated.So/[ab]*/
means“zeroormorea’sorb’s”(not“zeroormorerightsquarebraces”). Thiswill
matchstringslikeaaaaorabababorbbbb.
Forspecifyingmultipledigits(usefulforfindingprices)wecanextend/[0-9]/,
the regular expression for a single digit. An integer (a string of digits) is thus
/[0-9][0-9]*/. (Whyisn’titjust/[0-9]*/?)
Sometimesit’sannoyingtohavetowritetheregularexpressionfordigitstwice,
so there is a shorter way to specify “at least one” of some character. This is the
Kleene+ Kleene +, which means “one or more occurrences of the immediately preceding
characterorregularexpression”. Thus,theexpression/[0-9]+/isthenormalway
to specify “a sequence of digits”. There are thus two ways to specify the sheep
language: /baaa*!/or/baa+!/.
Oneveryimportantspecialcharacteristheperiod(/./),awildcardexpression
thatmatchesanysinglecharacter(exceptacarriagereturn),asshowninFig.2.6.
Regex Match ExampleMatches
/beg.n/ anycharacterbetweenbegandn begin,beg’n,begun
Figure2.6 Theuseoftheperiod.tospecifyanycharacter.
ThewildcardisoftenusedtogetherwiththeKleenestartomean“anystringof
characters”. For example, suppose we want to find any line in which a particular
word, for example, aardvark, appears twice. We can specify this with the regular
expression/aardvark.*aardvark/.
anchors Anchorsarespecialcharactersthatanchorregularexpressionstoparticularplaces8 CHAPTER2 • REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCE
inastring.Themostcommonanchorsarethecaretˆandthedollarsign$.Thecaret
ˆmatchesthestartofaline. Thepattern/ˆThe/matchesthewordTheonlyatthe
start of a line. Thus, the caret ˆ has three uses: to match the start of a line, to in-
dicateanegationinsideofsquarebrackets,andjusttomeanacaret. (Whatarethe
contextsthatallowgreporPythontoknowwhichfunctionagivencaretissupposed
tohave?) Thedollarsign$matchestheendofaline. Sothepattern(cid:32)$isauseful
pattern for matching a space at the end of a line, and /ˆThe dog\.$/ matches a
linethatcontainsonlythephraseThedog. (Wehavetousethebackslashheresince
wewantthe.tomean“period”andnotthewildcard.)
Regex Match
ˆ startofline
$ endofline
\b wordboundary
\B non-wordboundary
Figure2.7 Anchorsinregularexpressions.
Therearealsotwootheranchors: \bmatchesawordboundary,and\Bmatches
a non-boundary. Thus, /\bthe\b/ matches the word the but not the word other.
Moretechnically,a“word”forthepurposesofaregularexpressionisdefinedasany
sequenceofdigits,underscores,orletters;thisisbasedonthedefinitionof“words”
in programming languages. For example, /\b99\b/ will match the string 99 in
Thereare99bottlesofbeeronthewall(because99followsaspace)butnot99in
There are 299 bottles of beer on the wall (since 99 follows a number). But it will
match99in$99(since99followsadollarsign($),whichisnotadigit,underscore,
orletter).
2.1.2 Disjunction,Grouping,andPrecedence
Supposeweneedtosearchfortextsaboutpets;perhapsweareparticularlyinterested
incatsanddogs. Insuchacase,wemightwanttosearchforeitherthestringcator
thestringdog.Sincewecan’tusethesquarebracketstosearchfor“catordog”(why
disjunction can’twesay/[catdog]/?),weneedanewoperator,thedisjunctionoperator,also
calledthepipesymbol|. Thepattern/cat|dog/matcheseitherthestringcator
thestringdog.
Sometimesweneedtousethisdisjunctionoperatorinthemidstofalargerse-
quence. For example, suppose I want to search for information about pet fish for
my cousin David. How can I specify both guppy and guppies? We cannot simply
say/guppy|ies/, becausethatwouldmatchonlythestringsguppyandies. This
precedence is because sequences like guppy take precedence over the disjunction operator |.
Tomakethedisjunctionoperatorapplyonlytoaspecificpattern,weneedtousethe
parenthesisoperators(and). Enclosingapatterninparenthesesmakesitactlike
asinglecharacterforthepurposesofneighboringoperatorslikethepipe|andthe
Kleene*. Sothepattern/gupp(y|ies)/wouldspecifythatwemeantthedisjunc-
tiononlytoapplytothesuffixesyandies.
The parenthesis operator ( is also useful when we are using counters like the
Kleene*. Unlike the | operator, the Kleene* operator applies by default only to
a single character, not to a whole sequence. Suppose we want to match repeated
instances of a string. Perhaps we have a line that has column labels of the form
Column 1 Column 2 Column 3. The expression /Column(cid:32)[0-9]+(cid:32)*/ will not
match any number of columns; instead, it will match a single column followed by2.1 • REGULAREXPRESSIONS 9
any number of spaces! The star here applies only to the space (cid:32) that precedes it,
not to the whole sequence. With the parentheses, we could write the expression
/(Column(cid:32)[0-9]+(cid:32)*)*/ to match the word Column, followed by a number and
optionalspaces,thewholepatternrepeatedzeroormoretimes.
This idea that one operator may take precedence over another, requiring us to
sometimesuseparenthesestospecifywhatwemean,isformalizedbytheoperator
operator precedencehierarchyforregularexpressions. Thefollowingtablegivestheorder
precedence
ofREoperatorprecedence,fromhighestprecedencetolowestprecedence.
Parenthesis ()
Counters * + ? {}
Sequencesandanchors the ˆmy end$
Disjunction |
Thus, because counters have a higher precedence than sequences,
/the*/ matches theeeee but not thethe. Because sequences have a higher prece-
dencethandisjunction,/the|any/matchestheoranybutnotthanyortheny.
Patternscanbeambiguousinanotherway. Considertheexpression/[a-z]*/
whenmatchingagainstthetextonceuponatime. Since/[a-z]*/matcheszeroor
moreletters, thisexpressioncouldmatchnothing, orjustthefirstlettero, on, onc,
oronce. Inthesecasesregularexpressionsalwaysmatchthelargeststringtheycan;
greedy wesaythatpatternsaregreedy,expandingtocoverasmuchofastringastheycan.
non-greedy Thereare,however,waystoenforcenon-greedymatching,usinganothermean-
*? ingofthe?qualifier. Theoperator*? isaKleenestarthatmatchesaslittletextas
+? possible. Theoperator+? isaKleeneplusthatmatchesaslittletextaspossible.
2.1.3 ASimpleExample
SupposewewantedtowriteaREtofindcasesoftheEnglisharticlethe. Asimple
(butincorrect)patternmightbe:
/the/
One problem is that this pattern will miss the word when it begins a sentence and
henceiscapitalized(i.e.,The). Thismightleadustothefollowingpattern:
/[tT]he/
Butwewillstillincorrectlyreturntextswiththeembeddedinotherwords(e.g.,
otherortheology).Soweneedtospecifythatwewantinstanceswithawordbound-
aryonbothsides:
/\b[tT]he\b/
Supposewewantedtodothiswithouttheuseof/\b/.Wemightwantthissince
/\b/won’ttreatunderscoresandnumbersaswordboundaries;butwemightwant
tofindtheinsomecontextwhereitmightalsohaveunderlinesornumbersnearby
(the or the25). We need to specify that we want instances in which there are no
alphabeticlettersoneithersideofthethe:
/[ˆa-zA-Z][tT]he[ˆa-zA-Z]/
Butthereisstillonemoreproblemwiththispattern: itwon’tfindthewordthe
when it begins a line. This is because the regular expression [ˆa-zA-Z], which10 CHAPTER2 • REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCE
weusedtoavoidembeddedinstancesofthe,impliesthattheremustbesomesingle
(although non-alphabetic) character before the the. We can avoid this by specify-
ing that before the the we require either the beginning-of-line or a non-alphabetic
character,andthesameattheendoftheline:
/(ˆ|[ˆa-zA-Z])[tT]he([ˆa-zA-Z]|$)/
Theprocesswejustwentthroughwasbasedonfixingtwokindsoferrors: false
falsepositives positives, strings that we incorrectly matched like other or there, and false nega-
falsenegatives tives, strings that we incorrectly missed, like The. Addressing these two kinds of
errors comes up again and again in implementing speech and language processing
systems. Reducingtheoverallerrorrateforanapplicationthusinvolvestwoantag-
onisticefforts:
• Increasingprecision(minimizingfalsepositives)
• Increasingrecall(minimizingfalsenegatives)
We’llcomebacktoprecisionandrecallwithmoreprecisedefinitionsinChapter4.
2.1.4 MoreOperators
Figure 2.8 shows some aliases for common ranges, which can be used mainly to
savetyping. BesidestheKleene*andKleene+wecanalsouseexplicitnumbersas
counters,byenclosingthemincurlybrackets. Theregularexpression/{3}/means
“exactly 3 occurrences of the previous character or expression”. So /a\.{24}z/
willmatchafollowedby24dotsfollowedbyz(butnotafollowedby23or25dots
followedbyaz).
Regex Expansion Match FirstMatches
\d [0-9] anydigit Party(cid:32)of(cid:32)5
\D [ˆ0-9] anynon-digit Blue(cid:32)moon
\w [a-zA-Z0-9_] anyalphanumeric/underscore Daiyu
\W [ˆ\w] anon-alphanumeric !!!!
\s [(cid:32)\r\t\n\f] whitespace(space,tab) inConcord
\S [ˆ\s] Non-whitespace in(cid:32)Concord
Figure2.8 Aliasesforcommonsetsofcharacters.
A range of numbers can also be specified. So /{n,m}/ specifies from n to m
occurrencesofthepreviouscharorexpression,and/{n,}/meansatleastnoccur-
rencesofthepreviousexpression. REsforcountingaresummarizedinFig.2.9.
Regex Match
* zeroormoreoccurrencesofthepreviouscharorexpression
+ oneormoreoccurrencesofthepreviouscharorexpression
? zerooroneoccurrenceofthepreviouscharorexpression
{n} exactlynoccurrencesofthepreviouscharorexpression
{n,m} fromntomoccurrencesofthepreviouscharorexpression
{n,} atleastnoccurrencesofthepreviouscharorexpression
{,m} uptomoccurrencesofthepreviouscharorexpression
Figure2.9 Regularexpressionoperatorsforcounting.
Finally,certainspecialcharactersarereferredtobyspecialnotationbasedonthe
newline backslash(\)(seeFig.2.10). Themostcommonofthesearethenewlinecharacter2.1 • REGULAREXPRESSIONS 11
\nandthetabcharacter\t. Torefertocharactersthatarespecialthemselves(like
.,*,[,and\),precedethemwithabackslash,(i.e.,/\./,/\*/,/\[/,and/\\/).
Regex Match FirstPatternsMatched
\* anasterisk“*” “K*A*P*L*A*N”
\. aperiod“.” “Dr.Livingston,Ipresume”
\? aquestionmark “Whydon’ttheycomeandlendahand?”
\n anewline
\t atab
Figure2.10 Somecharactersthatneedtobebackslashed.
2.1.5 AMoreComplexExample
Let’stryoutamoresignificantexampleofthepowerofREs. Supposewewantto
buildanapplicationtohelpauserbuyacomputerontheWeb. Theusermightwant
“anymachinewithatleast6GHzand500GBofdiskspaceforlessthan$1000”.
To do this kind of retrieval, we first need to be able to look for expressions like 6
GHzor500GBorMacor$999.99. Intherestofthissectionwe’llworkoutsome
simpleregularexpressionsforthistask.
First, let’s complete ourregular expressionfor prices. Here’s a regular expres-
sionforadollarsignfollowedbyastringofdigits:
/$[0-9]+/
Notethatthe$characterhasadifferentfunctionherethantheend-of-linefunction
we discussed earlier. Most regular expression parsers are smart enough to realize
that $ here doesn’t mean end-of-line. (As a thought experiment, think about how
regexparsersmightfigureoutthefunctionof$fromthecontext.)
Now we just need to deal with fractions of dollars. We’ll add a decimal point
andtwodigitsafterwards:
/$[0-9]+\.[0-9][0-9]/
This pattern only allows $199.99 but not $199. We need to make the cents
optionalandtomakesurewe’reatawordboundary:
/(ˆ|\W)$[0-9]+(\.[0-9][0-9])?\b/
Onelastcatch! Thispatternallowspriceslike$199999.99whichwouldbefar
tooexpensive! Weneedtolimitthedollars:
/(ˆ|\W)$[0-9]{0,3}(\.[0-9][0-9])?\b/
Howaboutdiskspace?We’llneedtoallowforoptionalfractionsagain(5.5GB);
notetheuseof?formakingthefinalsoptional,andtheuseof/(cid:32)*/tomean“zero
ormorespaces”sincetheremightalwaysbeextraspaceslyingaround:
/\b[0-9]+(\.[0-9]+)?(cid:32)*(GB|[Gg]igabytes?)\b/
Modifyingthisregularexpressionsothatitonlymatchesmorethan500GBis
leftasanexerciseforthereader.12 CHAPTER2 • REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCE
2.1.6 Substitution,CaptureGroups,andELIZA
substitution Animportantuseofregularexpressionsisinsubstitutions.Forexample,thesubsti-
tutionoperators/regexp1/pattern/usedinPythonandinUnixcommandslike
vimorsedallowsastringcharacterizedbyaregularexpressiontobereplacedby
anotherstring:
s/colour/color/
Itisoftenusefultobeabletorefertoaparticularsubpartofthestringmatching
thefirstpattern. Forexample, supposewewantedtoputanglebracketsaroundall
integers in a text, for example, changing the 35 boxes to the <35> boxes. We’d
likeawaytorefertotheintegerwe’vefoundsothatwecaneasilyaddthebrackets.
Todothis,weputparentheses(and)aroundthefirstpatternandusethenumber
operator\1inthesecondpatterntoreferback. Here’showitlooks:
s/([0-9]+)/<\1>/
The parenthesis and number operators can also specify that a certain string or
expression must occur twice in the text. For example, suppose we are looking for
the pattern “the Xer they were, the Xer they will be”, where we want to constrain
the two X’s to be the same string. We do this by surrounding the first X with the
parenthesis operator, and replacing the second X with the number operator \1, as
follows:
/the (.*)er they were, the \1er they will be/
Herethe\1willbereplacedbywhateverstringmatchedthefirstiteminparen-
theses. Sothiswillmatchthebiggertheywere, thebiggertheywillbebutnotthe
biggertheywere,thefastertheywillbe.
capturegroup Thisuseofparenthesestostoreapatterninmemoryiscalledacapturegroup.
Every time a capture group is used (i.e., parentheses surround a pattern), the re-
register sulting match is stored in a numbered register. If you match two different sets of
parentheses,\2meanswhatevermatchedthesecondcapturegroup. Thus
/the (.*)er they (.*), the \1er we \2/
willmatchthefastertheyran,thefasterweranbutnotthefastertheyran,thefaster
weate. Similarly,thethirdcapturegroupisstoredin\3,thefourthis\4,andsoon.
Parentheses thus have a double function in regular expressions; they are used
to group terms for specifying the order in which operators should apply, and they
are used to capture something in a register. Occasionally we might want to use
parenthesesforgrouping,butdon’twanttocapturetheresultingpatterninaregister.
non-capturing Inthatcaseweuseanon-capturinggroup,whichisspecifiedbyputtingthespecial
group
commands?:aftertheopenparenthesis,intheform(?: pattern ).
/(?:some|a few) (people|cats) like some \1/
willmatchsomecatslikesomecatsbutnotsomecatslikesomesome.
Substitutions and capture groups are very useful in implementing simple chat-
bots like ELIZA (Weizenbaum, 1966). Recall that ELIZA simulates a Rogerian
psychologistbycarryingonconversationslikethefollowing:2.2 • WORDS 13
User : Menareallalike.
1
ELIZA : INWHATWAY
1
User : They’realwaysbuggingusaboutsomethingorother.
2
ELIZA : CANYOUTHINKOFASPECIFICEXAMPLE
2
User : Well,myboyfriendmademecomehere.
3
ELIZA : YOURBOYFRIENDMADEYOUCOMEHERE
3
User : HesaysI’mdepressedmuchofthetime.
4
ELIZA : IAMSORRYTOHEARYOUAREDEPRESSED
4
ELIZAworksbyhavingaseriesorcascadeofregularexpressionsubstitutions
each of which matches and changes some part of the input lines. Input lines are
first uppercased. The first substitutions then change all instances of MY to YOUR,
andI’MtoYOUARE,andsoon. Thenextsetofsubstitutionsmatchesandreplaces
otherpatternsintheinput. Herearesomeexamples:
s/.* I’M (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \1/
s/.* I AM (depressed|sad) .*/WHY DO YOU THINK YOU ARE \1/
s/.* all .*/IN WHAT WAY/
s/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE/
Sincemultiplesubstitutionscanapplytoagiveninput,substitutionsareassigned
a rank and applied in order. Creating patterns is the topic of Exercise 2.3, and we
returntothedetailsoftheELIZAarchitectureinChapter15.
2.1.7 LookaheadAssertions
Finally, there will be times when we need to predict the future: look ahead in the
texttoseeifsomepatternmatches,butnotadvancethematchcursor,sothatwecan
thendealwiththepatternifitoccurs.
lookahead
Theselookaheadassertionsmakeuseofthe(?syntaxthatwesawintheprevi-
oussectionfornon-capturegroups. Theoperator(?= pattern)istrueifpattern
zero-width occurs, but is zero-width, i.e. the match pointer doesn’t advance. The operator
(?! pattern)onlyreturnstrueifapatterndoesnotmatch,butagainiszero-width
and doesn’t advance the cursor. Negative lookahead is commonly used when we
areparsingsomecomplexpatternbutwanttoruleoutaspecialcase. Forexample
supposewewanttomatch,atthebeginningofaline,anysinglewordthatdoesn’t
startwith“Volcano”. Wecanusenegativelookaheadtodothis:
/ˆ(?!Volcano)[A-Za-z]+/
2.2 Words
Before we talk about processing words, we need to decide what counts as a word.
corpus Let’sstartbylookingatoneparticularcorpus(pluralcorpora),acomputer-readable
corpora collection of text or speech. For example the Brown corpus is a million-word col-
lection of samples from 500 written English texts from different genres (newspa-
per,fiction,non-fiction,academic,etc.),assembledatBrownUniversityin1963–64
(KucˇeraandFrancis,1967). HowmanywordsareinthefollowingBrownsentence?
Hesteppedoutintothehall,wasdelightedtoencounterawaterbrother.14 CHAPTER2 • REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCE
This sentence has 13 words if we don’t count punctuation marks as words, 15
ifwecountpunctuation. Whetherwetreatperiod(“.”), comma(“,”), andsoonas
wordsdependsonthetask. Punctuationiscriticalforfindingboundariesofthings
(commas, periods, colons) and for identifying some aspects of meaning (question
marks, exclamation marks, quotation marks). For some tasks, like part-of-speech
taggingorparsingorspeechsynthesis,wesometimestreatpunctuationmarksasif
theywereseparatewords.
TheSwitchboardcorpusofAmericanEnglishtelephoneconversationsbetween
strangerswascollectedintheearly1990s;itcontains2430conversationsaveraging
6 minutes each, totaling 240 hours of speech and about 3 million words (Godfrey
etal.,1992). Suchcorporaofspokenlanguagedon’thavepunctuationbutdointro-
duceothercomplicationswithregardtodefiningwords. Let’slookatoneutterance
utterance fromSwitchboard;anutteranceisthespokencorrelateofasentence:
Idouhmain-mainlybusinessdataprocessing
disfluency This utterance has two kinds of disfluencies. The broken-off word main- is
fragment calledafragment. Wordslikeuhandumarecalledfillersorfilledpauses. Should
filledpause we consider these to be words? Again, it depends on the application. If we are
building a speech transcription system, we might want to eventually strip out the
disfluencies.
But we also sometimes keep disfluencies around. Disfluencies like uh or um
areactuallyhelpfulinspeechrecognitioninpredictingtheupcomingword,because
they may signal that the speaker is restarting the clause or idea, and so for speech
recognition they are treated as regular words. Because people use different disflu-
encies they can also be a cue to speaker identification. In fact Clark and Fox Tree
(2002)showedthatuhandumhavedifferentmeanings.Whatdoyouthinktheyare?
Are capitalized tokens like They and uncapitalized tokens like they the same
word?Thesearelumpedtogetherinsometasks(speechrecognition),whileforpart-
of-speechornamed-entitytagging,capitalizationisausefulfeatureandisretained.
Howaboutinflectedformslikecatsversuscat? Thesetwowordshavethesame
lemma lemma cat but are different wordforms. A lemma is a set of lexical forms having
thesamestem,thesamemajorpart-of-speech,andthesamewordsense. Theword-
wordform formisthefullinflectedorderivedformoftheword. Formorphologicallycomplex
languageslikeArabic,weoftenneedtodealwithlemmatization. Formanytasksin
English,however,wordformsaresufficient.
How many words are there in English? To answer this question we need to
wordtype distinguishtwowaysoftalkingaboutwords.Typesarethenumberofdistinctwords
in a corpus; if the set of words in the vocabulary isV, the number of types is the
wordtoken vocabularysize V . TokensarethetotalnumberN ofrunningwords. Ifweignore
| |
punctuation,thefollowingBrownsentencehas16tokensand14types:
Theypicnickedbythepool,thenlaybackonthegrassandlookedatthestars.
When we speak about the number of words in the language, we are generally
referringtowordtypes.
Fig. 2.11 shows the rough numbers of types and tokens computed from some
popular English corpora. The larger the corpora we look at, the more word types
we find, and in fact this relationship between the number of types V and number
| |
Herdan’sLaw oftokensN iscalledHerdan’sLaw(Herdan,1960)orHeaps’Law(Heaps,1978)
Heaps’Law afteritsdiscoverers(inlinguisticsandinformationretrievalrespectively).Itisshown
inEq.2.1,wherekandβ arepositiveconstants,and0<β <1.
V = kNβ (2.1)
| |2.3 • CORPORA 15
Corpus Tokens=N Types= V
| |
Shakespeare 884thousand 31thousand
Browncorpus 1million 38thousand
Switchboardtelephoneconversations 2.4million 20thousand
COCA 440million 2million
Googlen-grams 1trillion 13million
Figure2.11 RoughnumbersoftypesandtokensforsomeEnglishlanguagecorpora. The
largest, theGooglen-gramscorpus, contains13milliontypes, butthiscountonlyincludes
typesappearing40ormoretimes,sothetruenumberwouldbemuchlarger.
The value of β depends on the corpus size and the genre, but at least for the large
corpora in Fig. 2.11, β ranges from .67 to .75. Roughly then we can say that the
vocabulary size for a text goes up significantly faster than the square root of its
lengthinwords.
Anothermeasureofthenumberofwordsinthelanguageisthenumberoflem-
masinsteadofwordformtypes. Dictionariescanhelpingivinglemmacounts;dic-
tionaryentriesorboldfaceformsareaveryroughupperboundonthenumberof
lemmas(sincesomelemmashavemultipleboldfaceforms). The1989editionofthe
OxfordEnglishDictionaryhad615,000entries.
2.3 Corpora
Words don’t appear out of nowhere. Any particular piece of text that we study
is produced by one or more specific speakers or writers, in a specific dialect of a
specificlanguage,ataspecifictime,inaspecificplace,foraspecificfunction.
Perhaps themost importantdimension ofvariation isthe language. NLPalgo-
rithmsaremostusefulwhentheyapplyacrossmanylanguages. Theworldhas7097
languages at the time of this writing, according to the online Ethnologue catalog
(SimonsandFennig,2018). Itisimportanttotestalgorithmsonmorethanonelan-
guage, and particularly on languages with different properties; by contrast there is
an unfortunate current tendency for NLP algorithms to be developed or tested just
on English (Bender, 2019). Even when algorithms are developed beyond English,
they tend to be developed for the official languages of large industrialized nations
(Chinese, Spanish, Japanese, Germanetc.), butwedon’twanttolimittoolstojust
thesefewlanguages. Furthermore,mostlanguagesalsohavemultiplevarieties,of-
ten spoken in different regions or by different social groups. Thus, for example,
AAE if we’re processing text that uses features of African American English (AAE) or
African American Vernacular English (AAVE)—the variations of English used by
millions of people in African American communities (King 2020)—we must use
NLPtoolsthatfunctionwithfeaturesofthosevarieties. Twitterpostsmightusefea-
turesoftenusedbyspeakersofAfricanAmericanEnglish,suchasconstructionslike
MAE iont(Idon’tinMainstreamAmericanEnglish(MAE)),ortalmboutcorresponding
to MAE talking about, both examples that influence word segmentation (Blodgett
etal.2016,Jones2015).
It’s also quite common for speakers or writers to use multiple languages in a
codeswitching single communicative act, a phenomenon called code switching. Code switching
is enormously common across the world; here are examples showing Spanish and
(transliterated)HindicodeswitchingwithEnglish(Solorioetal.2014,Jurgensetal.
2017):16 CHAPTER2 • REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCE
(2.2) Porprimeravezveoa@usernameactuallybeinghateful! itwasbeautiful:)
[ForthefirsttimeIgettosee@usernameactuallybeinghateful! itwas
beautiful:) ]
(2.3) dostthaorra-hega... dontwory... butdheryarakhe
[“hewasandwillremainafriend... don’tworry... buthavefaith”]
Anotherdimensionofvariationisthegenre. Thetextthatouralgorithmsmust
processmightcomefromnewswire,fictionornon-fictionbooks,scientificarticles,
Wikipedia, or religious texts. It might come from spoken genres like telephone
conversations, business meetings, police body-worn cameras, medical interviews,
or transcripts of television shows or movies. It might come from work situations
likedoctors’notes,legaltext,orparliamentaryorcongressionalproceedings.
Textalsoreflectsthedemographiccharacteristicsofthewriter(orspeaker):their
age,gender,race,socioeconomicclasscanallinfluencethelinguisticpropertiesof
thetextweareprocessing.
And finally, time matters too. Language changes over time, and for some lan-
guageswehavegoodcorporaoftextsfromdifferenthistoricalperiods.
Becauselanguageissosituated,whendevelopingcomputationalmodelsforlan-
guage processing from a corpus, it’s important to consider who produced the lan-
guage,inwhatcontext,forwhatpurpose.Howcanauserofadatasetknowallthese
datasheet details? The best way is for the corpus creator to build a datasheet (Gebru et al.,
2020)ordatastatement(BenderandFriedman,2018)foreachcorpus.Adatasheet
specifiespropertiesofadatasetlike:
Motivation: Whywasthecorpuscollected,bywhom,andwhofundedit?
Situation: Whenandinwhatsituationwasthetextwritten/spoken? Forexample,
was there a task? Was the language originally spoken conversation, edited
text,socialmediacommunication,monologuevs. dialogue?
Languagevariety: Whatlanguage(includingdialect/region)wasthecorpusin?
Speakerdemographics: Whatwas,e.g.,theageorgenderofthetext’sauthors?
Collectionprocess: Howbigisthedata? Ifitisasubsamplehowwasitsampled?
Was the data collected with consent? How was the data pre-processed, and
whatmetadataisavailable?
Annotationprocess: Whataretheannotations,whatarethedemographicsofthe
annotators,howweretheytrained,howwasthedataannotated?
Distribution: Aretherecopyrightorotherintellectualpropertyrestrictions?
2.4 Text Normalization
Beforealmostanynaturallanguageprocessingofatext,thetexthastobenormal-
ized.Atleastthreetasksarecommonlyappliedaspartofanynormalizationprocess:
1. Tokenizing(segmenting)words
2. Normalizingwordformats
3. Segmentingsentences
Inthenextsectionswewalkthrougheachofthesetasks.2.4 • TEXTNORMALIZATION 17
2.4.1 UnixToolsforCrudeTokenizationandNormalization
Let’s begin withan easy, if somewhatnaive version of word tokenization and nor-
malization(andfrequencycomputation)thatcanbeaccomplishedforEnglishsolely
inasingleUNIXcommand-line,inspiredbyChurch(1994).We’llmakeuseofsome
Unixcommands: tr, usedtosystematicallychangeparticularcharactersinthein-
put;sort,whichsortsinputlinesinalphabeticalorder;anduniq,whichcollapses
andcountsadjacentidenticallines.
For example let’s begin with the ‘complete words’ of Shakespeare in one file,
sh.txt. Wecanusetrtotokenizethewordsbychangingeverysequenceofnon-
alphabetic characters to a newline (’A-Za-z’ means alphabetic, the -c option com-
plements to non-alphabet, and the -s option squeezes all sequences into a single
character):
tr -sc ’A-Za-z’ ’\n’ < sh.txt
Theoutputofthiscommandwillbe:
THE
SONNETS
by
William
Shakespeare
From
fairest
creatures
We
...
Nowthatthereisonewordperline,wecansortthelines,andpassthemtouniq
-cwhichwillcollapseandcountthem:
tr -sc ’A-Za-z’ ’\n’ < sh.txt | sort | uniq -c
withthefollowingoutput:
1945 A
72 AARON
19 ABBESS
25 Aaron
6 Abate
1 Abates
5 Abbess
6 Abbey
3 Abbot
...
Alternatively,wecancollapsealltheuppercasetolowercase:
tr -sc ’A-Za-z’ ’\n’ < sh.txt | tr A-Z a-z | sort | uniq -c
whoseoutputis
14725 a
97 aaron
1 abaissiez
10 abandon
2 abandoned18 CHAPTER2 • REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCE
2 abase
1 abash
14 abate
3 abated
3 abatement
...
Nowwecansortagaintofindthefrequentwords. The-noptiontosortmeans
to sort numerically rather than alphabetically, and the -r option means to sort in
reverseorder(highest-to-lowest):
tr -sc ’A-Za-z’ ’\n’ < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r
The results show that the most frequent words in Shakespeare, as in any other
corpus,aretheshortfunctionwordslikearticles,pronouns,prepositions:
27378 the
26084 and
22538 i
19771 to
17481 of
14725 a
13826 you
12489 my
11318 that
11112 in
...
Unixtoolsofthissortcanbeveryhandyinbuildingquickwordcountstatistics
foranycorpusinEnglish.WhileinsomeversionsofUnixthesecommand-linetools
also correctly handle Unicode characters and so can be used for many languages,
in general for handling most languages outside English we use more sophisticated
tokenizationalgorithms.
2.4.2 WordTokenization
ThesimpleUNIXtoolsabovewerefineforgettingroughwordstatisticsbutmore
tokenization sophisticated algorithms are generally necessary for tokenization, the task of seg-
mentingrunningtextintowords.
While the Unix command sequence just removed all the numbers and punctu-
ation, for mostNLP applicationswe’ll needto keepthese inour tokenization. We
oftenwanttobreakoffpunctuationasaseparatetoken;commasareausefulpieceof
informationforparsers,periodshelpindicatesentenceboundaries. Butwe’lloften
want to keep the punctuation that occurs word internally, in examples like m.p.h.,
Ph.D., AT&T, and cap’n. Special characters and numbers will need to be kept in
prices($45.55)anddates(01/02/06);wedon’twanttosegmentthatpriceintosepa-
ratetokensof“45”and“55”.AndthereareURLs(https://www.stanford.edu),
Twitterhashtags(#nlproc),oremailaddresses(someone@cs.colorado.edu).
Numberexpressionsintroduceothercomplicationsaswell;whilecommasnor-
mallyappearatwordboundaries,commasareusedinsidenumbersinEnglish,every
threedigits: 555,500.50. Languages,andhencetokenizationrequirements,differ
onthis;manycontinentalEuropeanlanguageslikeSpanish,French,andGerman,by
contrast,useacommatomarkthedecimalpoint,andspaces(orsometimesperiods)
whereEnglishputscommas,forexample,555 500,50.2.4 • TEXTNORMALIZATION 19
clitic A tokenizer can also be used to expand clitic contractions that are marked by
apostrophes, for example, converting what’re to the two tokens what are, and
we’retowe are.Acliticisapartofawordthatcan’tstandonitsown,andcanonly
occur when it is attached to another word. Some such contractions occur in other
alphabeticlanguages,includingarticlesandpronounsinFrench(j’ai,l’homme).
Depending on the application, tokenization algorithms may also tokenize mul-
tiwordexpressionslikeNew Yorkorrock ’n’ rollasasingletoken,whichre-
quires a multiword expression dictionary of some sort. Tokenization is thus inti-
matelytiedupwithnamedentityrecognition, thetaskofdetectingnames, dates,
andorganizations(Chapter8).
OnecommonlyusedtokenizationstandardisknownasthePennTreebankto-
PennTreebank kenization standard, used for the parsed corpora (treebanks) released by the Lin-
tokenization
guisticDataConsortium(LDC),thesourceofmanyusefuldatasets. Thisstandard
separates out clitics (doesn’t becomes does plus n’t), keeps hyphenated words to-
gether,andseparatesoutallpunctuation(tosavespacewe’reshowingvisiblespaces
‘ ’betweentokens,althoughnewlinesisamorecommonoutput):
Input: "The San Francisco-based restaurant," they said,
"doesn’t charge $10".
Output: " The San Francisco-based restaurant , " they said ,
" does n’t charge $ 10 " .
Inpractice,sincetokenizationneedstoberunbeforeanyotherlanguageprocess-
ing,itneedstobeveryfast. Thestandardmethodfortokenizationisthereforetouse
deterministic algorithms based on regular expressions compiled into very efficient
finite state automata. For example, Fig. 2.12 shows an example of a basic regular
expressionthatcanbeusedtotokenizeEnglishwiththenltk.regexp tokenize
function of the Python-based Natural Language Toolkit (NLTK) (Bird et al. 2009;
https://www.nltk.org).
>>> text = ’That U.S.A. poster-print costs $12.40...’
>>> pattern = r’’’(?x) # set flag to allow verbose regexps
... (?:[A-Z]\.)+ # abbreviations, e.g. U.S.A.
... | \w+?:(-\w+)* # words with optional internal hyphens
... | \$?\d+(?:\.\d+)?%? # currency, percentages, e.g. $12.40, 82%
... | \.\.\. # ellipsis
... | [][.,;"’?():_‘-] # these are separate tokens; includes ], [
... ’’’
>>> nltk.regexp_tokenize(text, pattern)
[’That’, ’U.S.A.’, ’poster-print’, ’costs’, ’$12.40’, ’...’]
Figure2.12 APythontraceofregularexpressiontokenizationintheNLTKPython-based
naturallanguageprocessingtoolkit(Birdetal.,2009),commentedforreadability;the(?x)
verboseflagtellsPythontostripcommentsandwhitespace. FigurefromChapter3ofBird
etal.(2009).
Carefully designed deterministic algorithms can deal with the ambiguities that
arise,suchasthefactthattheapostropheneedstobetokenizeddifferentlywhenused
asagenitivemarker(asinthebook’scover),aquotativeasin‘Theotherclass’,she
said,orincliticslikethey’re.
WordtokenizationismorecomplexinlanguageslikewrittenChinese,Japanese,
andThai,whichdonotusespacestomarkpotentialword-boundaries. InChinese,
hanzi for example, words are composed of characters (called hanzi in Chinese). Each20 CHAPTER2 • REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCE
charactergenerallyrepresentsasingleunitofmeaning(calledamorpheme)andis
pronounceableasasinglesyllable. Wordsareabout2.4characterslongonaverage.
ButdecidingwhatcountsasawordinChineseiscomplex. Forexample,consider
thefollowingsentence:
(2.4) 姚明进入总决赛
“YaoMingreachesthefinals”
As Chen et al. (2017b) point out, this could be treated as 3 words (‘Chinese Tree-
bank’segmentation):
(2.5) 姚明 进入 总决赛
YaoMingreachesfinals
oras5words(‘PekingUniversity’segmentation):
(2.6) 姚 明 进入 总 决赛
YaoMingreachesoverallfinals
Finally,itispossibleinChinesesimplytoignorewordsaltogetherandusecharacters
asthebasicelements,treatingthesentenceasaseriesof7characters:
(2.7) 姚 明 进 入 总 决 赛
YaoMingenterenteroveralldecisiongame
In fact, for most Chinese NLP tasks it turns out to work better to take characters
rather than words as input, since characters are at a reasonable semantic level for
mostapplications,andsincemostwordstandards,bycontrast,resultinahugevo-
cabularywithlargenumbersofveryrarewords(Lietal.,2019b).
However, for Japanese and Thai the character is too small a unit, and so algo-
word rithms for word segmentation are required. These can also be useful for Chinese
segmentation
intheraresituationswherewordratherthancharacterboundariesarerequired. The
standard segmentation algorithms for these languages use neural sequence mod-
elstrainedviasupervisedmachinelearningonhand-segmentedtrainingsets; we’ll
introducesequencemodelsinChapter8andChapter9.
2.4.3 Byte-PairEncodingforTokenization
There is a third option to tokenizing text. Instead of defining tokens as words
(whether delimited by spaces or more complex algorithms), or as characters (as in
Chinese), we can use our data to automatically tell us what the tokens should be.
This is especially useful in dealing with unknown words, an important problem in
languageprocessing. Aswewillseeinthenextchapter,NLPalgorithmsoftenlearn
somefactsaboutlanguagefromonecorpus(atrainingcorpus)andthenusethese
facts to make decisions about a separate test corpus and its language. Thus if our
trainingcorpuscontains, saythewordslow, new, newer, butnotlower, thenifthe
wordlowerappearsinourtestcorpus,oursystemwillnotknowwhattodowithit.
To deal with this unknown word problem, modern tokenizers often automati-
subwords callyinducesetsoftokensthatincludetokenssmallerthanwords,calledsubwords.
Subwordscanbearbitrarysubstrings,ortheycanbemeaning-bearingunitslikethe
morphemes-estor-er. (Amorphemeisthesmallestmeaning-bearingunitofalan-
guage; for example the word unlikeliest has the morphemes un-, likely, and -est.)
In modern tokenization schemes, most tokens are words, but some tokens are fre-
quently occurring morphemes or other subwords like -er. Every unseen word like
lower can thus be represented by some sequenceof known subword units, such as
lowander,orevenasasequenceofindividuallettersifnecessary.2.4 • TEXTNORMALIZATION 21
Most tokenization schemes have two parts: a token learner, and a token seg-
menter. The token learner takes a raw training corpus (sometimes roughly pre-
separated into words, for example by whitespace) and induces a vocabulary, a set
of tokens. The token segmenter takes a raw test sentence and segments it into the
tokens in the vocabulary. Three algorithms are widely used: byte-pair encoding
(Sennrichetal.,2016),unigramlanguagemodeling(Kudo,2018),andWordPiece
(SchusterandNakajima,2012); thereisalsoaSentencePiecelibrarythatincludes
implementationsofthefirsttwoofthethree(KudoandRichardson,2018a).
Inthissectionweintroducethesimplestofthethree,thebyte-pairencodingor
BPE BPEalgorithm(Sennrichetal.,2016);seeFig.2.13. TheBPEtokenlearnerbegins
withavocabularythatisjustthesetofallindividualcharacters.Itthenexaminesthe
trainingcorpus,choosesthetwosymbolsthataremostfrequentlyadjacent(say‘A’,
‘B’),addsanewmergedsymbol‘AB’tothevocabulary,andreplaceseveryadjacent
’A’’B’inthecorpuswiththenew‘AB’.Itcontinuestocountandmerge, creating
new longer and longer character strings, until k merges have been done creating
k novel tokens; k is thus a parameter of the algorithm. The resulting vocabulary
consistsoftheoriginalsetofcharactersplusknewsymbols.
Thealgorithmisusuallyruninsidewords(notmergingacrosswordboundaries),
sotheinputcorpusisfirstwhite-space-separatedtogiveasetofstrings,eachcorre-
spondingtothecharactersofaword,plusaspecialend-of-wordsymbol ,andits
counts. Let’sseeitsoperationonthefollowingtinyinputcorpusof18wordtokens
withcountsforeachword(thewordlowappears5times,thewordnewer6times,
andsoon),whichwouldhaveastartingvocabularyof11letters:
corpus vocabulary
5 l o w , d, e, i, l, n, o, r, s, t, w
2 l o w e s t
6 n e w e r
3 w i d e r
2 n e w
TheBPEalgorithmfirstcountsallpairsofadjacentsymbols: themostfrequent
isthepaire rbecauseitoccursinnewer(frequencyof6)andwider(frequencyof
3) for a total of 9 occurrences.1 We then merge these symbols, treating er as one
symbol,andcountagain:
corpus vocabulary
5 l o w , d, e, i, l, n, o, r, s, t, w, er
2 l o w e s t
6 n e w er
3 w i d er
2 n e w
Now the most frequent pair is er , which we merge; our system has learned
thatthereshouldbeatokenforword-finaler,representedaser :
corpus vocabulary
5 l o w ,d,e,i,l,n,o,r,s,t,w,er,er
2 l o w e s t
6 n e w er
3 w i d er
2 n e w
1 Notethattherecanbeties; wecouldhaveinsteadchosentomerger first,sincethatalsohasa
frequencyof9.22 CHAPTER2 • REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCE
Nextn e(totalcountof8)getmergedtone:
corpus vocabulary
5 l o w ,d,e,i,l,n,o,r,s,t,w,er,er ,ne
2 l o w e s t
6 ne w er
3 w i d er
2 ne w
Ifwecontinue,thenextmergesare:
merge currentvocabulary
(ne, w) ,d,e,i,l,n,o,r,s,t,w,er,er ,ne,new
(l, o) ,d,e,i,l,n,o,r,s,t,w,er,er ,ne,new,lo
(lo, w) ,d,e,i,l,n,o,r,s,t,w,er,er ,ne,new,lo,low
(new, er ) ,d,e,i,l,n,o,r,s,t,w,er,er ,ne,new,lo,low,newer
(low, ) ,d,e,i,l,n,o,r,s,t,w,er,er ,ne,new,lo,low,newer ,low
functionBYTE-PAIRENCODING(stringsC,numberofmergesk)returnsvocabV
V alluniquecharactersinC #initialsetoftokensischaracters
←
fori=1tok do #mergetokensktimes
tL,tR MostfrequentpairofadjacenttokensinC
←
t NEW←tL+tR #makenewtokenbyconcatenating
V V+t #updatethevocabulary
← NEW
ReplaceeachoccurrenceoftL,tRinCwitht
NEW
#andupdatethecorpus
returnV
Figure2.13 The token learner part of the BPE algorithm for taking a corpus broken up
intoindividualcharactersorbytes,andlearningavocabularybyiterativelymergingtokens.
FigureadaptedfromBostromandDurrett(2020).
Once we’ve learned our vocabulary, the token segmenter is used to tokenize a
test sentence. The token segmenter just runs on the test data the merges we have
learned from the training data, greedily, in the order we learned them. (Thus the
frequencies in the test data don’t play a role, just the frequencies in the training
data). Sofirstwesegmenteachtestsentencewordintocharacters. Thenweapply
thefirstrule: replaceeveryinstanceofe rinthetestcorpuswither,andthenthe
secondrule: replaceeveryinstanceofer inthetestcorpuswither ,andsoon.
By the end, if the test corpus contained the character sequence n e w e r , it
wouldbetokenizedasafullword. Butthecharactersofanew(unknown)wordlike
l o w e r wouldbemergedintothetwotokenslower .
OfcourseinrealsettingsBPEisrunwithmanythousandsofmergesonavery
largeinputcorpus. Theresultisthatmostwordswillberepresentedasfullsymbols,
and only the very rare words (and unknown words) will have to be represented by
theirparts.
2.4.4 WordNormalization,LemmatizationandStemming
normalization Wordnormalizationisthetaskofputtingwords/tokensinastandardformat,choos-
ingasinglenormalformforwordswithmultipleformslikeUSAandUSoruh-huh
anduhhuh. Thisstandardizationmaybevaluable,despitethespellinginformation
that is lost in the normalization process. For information retrieval or information2.4 • TEXTNORMALIZATION 23
extractionabouttheUS,wemightwanttoseeinformationfromdocumentswhether
theymentiontheUSortheUSA.
casefolding Case folding is another kind of normalization. Mapping everything to lower
case means that Woodchuck and woodchuck are represented identically, which is
veryhelpfulforgeneralizationinmanytasks,suchasinformationretrievalorspeech
recognition. For sentiment analysis and other text classification tasks, information
extraction,andmachinetranslation,bycontrast,casecanbequitehelpfulandcase
foldingisgenerallynotdone. Thisisbecausemaintainingthedifferencebetween,
for example, US the country and us the pronoun can outweigh the advantage in
generalizationthatcasefoldingwouldhaveprovidedforotherwords.
Formanynaturallanguageprocessingsituationswealsowanttwomorpholog-
ically different forms of a word to behave similarly. For example in web search,
someone may type the string woodchucks but a useful system might want to also
returnpagesthatmentionwoodchuckwithnos. Thisisespeciallycommoninmor-
phologically complex languages like Polish, where for example the word Warsaw
has different endings when it is the subject (Warszawa), or after a preposition like
“inWarsaw”(wWarszawie),or“toWarsaw”(doWarszawy),andsoon.
Lemmatization is the task of determining that two words have the same root,
despitetheirsurfacedifferences. Thewordsam,are,andishavethesharedlemma
be; the words dinner and dinners both have the lemma dinner. Lemmatizing each
of these forms to the same lemma will let us find all mentions of words in Polish
likeWarsaw. ThelemmatizedformofasentencelikeHeisreadingdetectivestories
wouldthusbeHebereaddetectivestory.
Howislemmatizationdone? Themostsophisticatedmethodsforlemmatization
involvecompletemorphologicalparsingoftheword. Morphologyisthestudyof
morpheme thewaywordsarebuiltupfromsmallermeaning-bearingunitscalledmorphemes.
stem Two broad classes of morphemes can be distinguished: stems—the central mor-
affix phemeoftheword,supplyingthemainmeaning—andaffixes—adding“additional”
meaningsofvariouskinds. So,forexample,thewordfoxconsistsofonemorpheme
(the morpheme fox) and the word cats consists of two: the morpheme cat and the
morpheme-s. Amorphologicalparsertakesawordlikecatsandparsesitintothe
two morphemes cat and s, or parses a Spanish word like amaren (‘if in the future
theywouldlove’)intothemorphemeamar‘tolove’,andthemorphologicalfeatures
3PLandfuturesubjunctive.
ThePorterStemmer
Lemmatization algorithms can be complex. For this reason we sometimes make
use of a simpler but cruder method, which mainly consists of chopping off word-
stemming finalaffixes. Thisnaiveversionofmorphologicalanalysisiscalledstemming. For
Porterstemmer example, the Porter stemmer, a widely used stemming algorithm (Porter, 1980),
whenappliedtothefollowingparagraph:
This was not the map we found in Billy Bones’s chest, but
an accurate copy, complete in all things-names and heights
and soundings-with the single exception of the red crosses
and the written notes.
producesthefollowingstemmedoutput:
Thi wa not the map we found in Billi Bone s chest but an
accur copi complet in all thing name and height and sound
with the singl except of the red cross and the written note24 CHAPTER2 • REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCE
Thealgorithmisbasedonseriesofrewriterulesruninseries:theoutputofeach
passisfedasinputtothenextpass. Herearesomesamplerules(moredetailscan
befoundathttps://tartarus.org/martin/PorterStemmer/):
ATIONAL ATE (e.g.,relational relate)
→ →
ING (cid:15) ifthestemcontainsavowel(e.g.,motoring motor)
→ →
SSES SS (e.g.,grasses grass)
→ →
Simplestemmerscanbeusefulincaseswhereweneedtocollapseacrossdiffer-
entvariantsofthesamelemma. Nonetheless,theydotendtocommiterrorsofboth
over-andunder-generalizing,asshowninthetablebelow(Krovetz,1993):
ErrorsofCommission ErrorsofOmission
organization organ European Europe
doing doe analyzes analysis
numerical numerous noisy noise
policy police sparsity sparse
2.4.5 SentenceSegmentation
sentence Sentencesegmentationisanotherimportantstepintextprocessing. Themostuse-
segmentation
fulcuesforsegmentingatextintosentencesarepunctuation,likeperiods,question
marks, and exclamation points. Question marks and exclamation points are rela-
tivelyunambiguousmarkersofsentenceboundaries. Periods,ontheotherhand,are
moreambiguous. Theperiodcharacter“.” isambiguousbetweenasentencebound-
arymarkerandamarkerofabbreviationslikeMr. orInc. Theprevioussentencethat
youjustreadshowedanevenmorecomplexcaseofthisambiguity,inwhichthefinal
periodofInc. markedbothanabbreviationandthesentenceboundarymarker. For
thisreason,sentencetokenizationandwordtokenizationmaybeaddressedjointly.
Ingeneral,sentencetokenizationmethodsworkbyfirstdeciding(basedonrules
ormachinelearning)whetheraperiodispartofthewordorisasentence-boundary
marker. An abbreviation dictionary can help determine whether the period is part
of a commonly used abbreviation; the dictionaries can be hand-built or machine-
learned (Kiss and Strunk, 2006), as can the final sentence splitter. In the Stanford
CoreNLPtoolkit(Manningetal.,2014),forexamplesentencesplittingisrule-based,
adeterministicconsequenceoftokenization;asentenceendswhenasentence-ending
punctuation(.,!,or?)isnotalreadygroupedwithothercharactersintoatoken(such
asforanabbreviationornumber),optionallyfollowedbyadditionalfinalquotesor
brackets.
2.5 Minimum Edit Distance
Muchofnaturallanguageprocessingisconcernedwithmeasuringhowsimilartwo
strings are. For example in spelling correction, the user typed some erroneous
string—let’ssaygraffe–andwewanttoknowwhattheusermeant. Theuserprob-
ably intended a word that is similar to graffe. Among candidate similar words,
thewordgiraffe,whichdiffersbyonlyoneletterfromgraffe,seemsintuitively
tobemoresimilarthan, saygrailorgraf, whichdifferinmoreletters. Another2.5 • MINIMUMEDITDISTANCE 25
examplecomesfromcoreference,thetaskofdecidingwhethertwostringssuchas
thefollowingrefertothesameentity:
Stanford President Marc Tessier-Lavigne
Stanford University President Marc Tessier-Lavigne
Again, thefactthatthesetwostringsareverysimilar(differingbyonlyoneword)
seemslikeusefulevidencefordecidingthattheymightbecoreferent.
Editdistancegivesusawaytoquantifybothoftheseintuitionsaboutstringsim-
minimumedit ilarity. Moreformally, theminimumeditdistancebetweentwostringsisdefined
distance
as the minimum number of editing operations (operations like insertion, deletion,
substitution)neededtotransformonestringintoanother.
Thegapbetweenintentionandexecution,forexample,is5(deleteani,substi-
tute e for n, substitute x for t, insert c, substitute u for n). It’s much easier to see
alignment thisbylookingatthemostimportantvisualizationforstringdistances,analignment
betweenthetwostrings,showninFig.2.14. Giventwosequences,analignmentis
acorrespondencebetweensubstringsofthetwosequences. Thus, wesayIaligns
with the empty string, N with E, and so on. Beneath the aligned strings is another
representation; a series of symbols expressing an operation list for converting the
topstringintothebottomstring: dfordeletion,sforsubstitution,iforinsertion.
INTE*NTION
| | | | | | | | | |
*EXECUTION
d s s i s
Figure2.14 Representingtheminimumeditdistancebetweentwostringsasanalignment.
Thefinalrowgivestheoperationlistforconvertingthetopstringintothebottomstring:dfor
deletion,sforsubstitution,iforinsertion.
Wecanalsoassignaparticularcostorweighttoeachoftheseoperations. The
Levenshtein distance between two sequences is the simplest weighting factor in
whicheachofthethreeoperationshasacostof1(Levenshtein,1966)—weassume
thatthesubstitutionofaletterforitself,forexample,tfort,haszerocost.TheLev-
enshteindistancebetweenintentionandexecutionis5. Levenshteinalsoproposed
analternativeversionofhismetricinwhicheachinsertionordeletionhasacostof
1andsubstitutionsarenotallowed. (Thisisequivalenttoallowingsubstitution,but
givingeachsubstitutionacostof2sinceanysubstitutioncanberepresentedbyone
insertion and one deletion). Using this version, the Levenshtein distance between
intentionandexecutionis8.
2.5.1 TheMinimumEditDistanceAlgorithm
Howdowefindtheminimumeditdistance? Wecanthinkofthisasasearchtask,in
whichwearesearchingfortheshortestpath—asequenceofedits—fromonestring
toanother.
Thespaceofallpossibleeditsisenormous,sowecan’tsearchnaively.However,
lotsofdistincteditpathswillendupinthesamestate(string),soratherthanrecom-
putingallthosepaths,wecouldjustremembertheshortestpathtoastateeachtime
dynamic we saw it. We can do this by using dynamic programming. Dynamic program-
programming
mingisthenameforaclassofalgorithms,firstintroducedbyBellman(1957),that26 CHAPTER2 • REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCE
i n t e n t i o n
del ins subst
n t e n t i o n i n t e c n t i o n i n x e n t i o n
Figure2.15 Findingtheeditdistanceviewedasasearchproblem
applyatable-drivenmethodtosolveproblemsbycombiningsolutionstosubprob-
lems. Someofthemostcommonlyusedalgorithmsinnaturallanguageprocessing
makeuseofdynamicprogramming,suchastheViterbialgorithm(Chapter8)and
theCKYalgorithmforparsing(Chapter17).
The intuition of a dynamic programming problem is that a large problem can
be solved by properly combining the solutions to various subproblems. Consider
the shortest path of transformed words that represents the minimum edit distance
betweenthestringsintentionandexecutionshowninFig.2.16.
i n t e n t i o n
delete i
n t e n t i o n
substitute n by e
e t e n t i o n
substitute t by x
e x e n t i o n
insert u
e x e n u t i o n
substitute n by c
e x e c u t i o n
Figure2.16 Pathfromintentiontoexecution.
Imaginesomestring(perhapsitisexention)thatisinthisoptimalpath(whatever
it is). The intuition of dynamic programming is that if exention is in the optimal
operation list, then the optimal sequence must also include the optimal path from
intentiontoexention. Why? Iftherewereashorterpathfromintentiontoexention,
then we could use it instead, resulting in a shorter overall path, and the optimal
sequencewouldn’tbeoptimal,thusleadingtoacontradiction.
minimumedit
distance The minimum edit distance algorithm was named by Wagner and Fischer
algorithm
(1974)butindependentlydiscoveredbymanypeople(seetheHistoricalNotessec-
tionofChapter8).
Let’s first define the minimum edit distance between two strings. Given two
strings,thesourcestringX oflengthn,andtargetstringY oflengthm,we’lldefine
D[i,j]astheeditdistancebetweenX[1..i]andY[1..j],i.e.,thefirsticharactersofX
andthefirst jcharactersofY. TheeditdistancebetweenX andY isthusD[n,m].
We’llusedynamicprogrammingtocomputeD[n,m]bottomup,combiningso-
lutionstosubproblems. Inthebasecase,withasourcesubstringoflengthibutan
empty target string, going from i characters to 0 requires i deletes. With a target
substring of length j but an empty source going from 0 characters to j characters
requires j inserts. Having computed D[i,j] for small i,j we then compute larger
D[i,j] based on previously computed smaller values. The value of D[i,j] is com-
putedbytakingtheminimumofthethreepossiblepathsthroughthematrixwhich
arrivethere:
D[i 1,j]+del-cost(source[i])
−
D[i,j]=min D[i,j 1]+ins-cost(target[j]) (2.8)
 −
D[i 1,j 1]+sub-cost(source[i],target[j])

− −
2.5 • MINIMUMEDITDISTANCE 27
IfweassumetheversionofLevenshteindistanceinwhichtheinsertionsanddele-
tions each have a cost of 1 (ins-cost() = del-cost() = 1), and substitutions have a
· ·
costof2(exceptsubstitutionofidenticallettershavezerocost),thecomputationfor
D[i,j]becomes:
D[i 1,j]+1
−
D[i,j 1]+1
D[i,j]=min − (2.9)
2; if source[i]=target[j]
D[i −1,j −1]+
0; if
source[i]=(cid:54)
target[j]
(cid:26)
The algorithm is
summarized
in Fig. 2.17; Fig. 2.18 shows the results of applying
the algorithm to the distance between intention and execution with the version of
LevenshteininEq.2.9.
functionMIN-EDIT-DISTANCE(source,target)returnsmin-distance
n LENGTH(source)
←
m LENGTH(target)
←
CreateadistancematrixD[n+1,m+1]
# Initialization:thezerothrowandcolumnisthedistancefromtheemptystring
D[0,0]=0
foreachrowifrom1tondo
D[i,0] D[i-1,0]+del-cost(source[i])
←
foreachcolumn jfrom1tomdo
D[0,j] D[0,j-1]+ins-cost(target[j])
←
# Recurrencerelation:
foreachrowifrom1tondo
foreachcolumn jfrom1tomdo
D[i,j] MIN(D[i 1,j]+del-cost(source[i]),
← −
D[i 1,j 1]+sub-cost(source[i],target[j]),
− −
D[i,j 1]+ins-cost(target[j]))
−
# Termination
returnD[n,m]
Figure2.17 The minimum edit distance algorithm, an example of the class of dynamic
programming algorithms. The various costs can either be fixed (e.g., x,ins-cost(x)=1)
∀
orcanbespecifictotheletter(tomodelthefactthatsomelettersaremorelikelytobein-
serted than others). We assume that there is no cost for substituting a letter for itself (i.e.,
sub-cost(x,x)=0).
Alignment Knowingtheminimumeditdistanceisusefulforalgorithmslikefind-
ingpotentialspellingerrorcorrections. Buttheeditdistancealgorithmisimportant
in another way; with a small change, it can also provide the minimum cost align-
ment between two strings. Aligning two strings is useful throughout speech and
language processing. In speech recognition, minimum edit distance alignment is
used to compute the word error rate (Chapter 16). Alignment plays a role in ma-
chinetranslation,inwhichsentencesinaparallelcorpus(acorpuswithatextintwo
languages)needtobematchedtoeachother.
Toextendtheeditdistancealgorithmtoproduceanalignment, wecanstartby
visualizing an alignment as a path through the edit distance matrix. Figure 2.19
shows this path with boldfaced cells. Each boldfaced cell represents an alignment28 CHAPTER2 • REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCE
Src Tar # e x e c u t i o n
\
# 0 1 2 3 4 5 6 7 8 9
i 1 2 3 4 5 6 7 6 7 8
n 2 3 4 5 6 7 8 7 8 7
t 3 4 5 6 7 8 7 8 9 8
e 4 3 4 5 6 7 8 9 10 9
n 5 4 5 6 7 8 9 10 11 10
t 6 5 6 7 8 9 8 9 10 11
i 7 6 7 8 9 10 9 8 9 10
o 8 7 8 9 10 11 10 9 8 9
n 9 8 9 10 11 12 11 10 9 8
Figure2.18 Computationofminimumeditdistancebetweenintentionandexecutionwith
thealgorithmofFig.2.17,usingLevenshteindistancewithcostof1forinsertionsordele-
tions,2forsubstitutions.
ofapairoflettersinthetwostrings. Iftwoboldfacedcellsoccurinthesamerow,
therewillbeaninsertioningoingfromthesourcetothetarget;twoboldfacedcells
inthesamecolumnindicateadeletion.
Figure2.19alsoshowstheintuitionofhowtocomputethisalignmentpath. The
computationproceedsintwosteps. Inthefirststep,weaugmenttheminimumedit
distance algorithm to store backpointers in each cell. The backpointer from a cell
pointstothepreviouscell(orcells)thatwecamefrominenteringthecurrentcell.
We’veshownaschematicofthesebackpointersinFig.2.19. Somecellshavemul-
tiplebackpointersbecausetheminimumextensioncouldhavecomefrommultiple
backtrace previouscells. Inthesecondstep,weperformabacktrace. Inabacktrace,westart
fromthelastcell(atthefinalrowandcolumn),andfollowthepointersbackthrough
thedynamicprogrammingmatrix.Eachcompletepathbetweenthefinalcellandthe
initial cell is a minimum distance alignment. Exercise 2.7 asks you to modify the
minimumeditdistancealgorithmtostorethepointersandcomputethebacktraceto
outputanalignment.
# e x e c u t i o n
# 0 1 2 3 4 5 6 7 8 9
← ← ← ← ← ← ← ← ←
i 1 2 3 4 5 6 7 6 7 8
↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45) ← ←
n 2 3 4 5 6 7 8 7 8 7
↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ ↑ (cid:45)←↑ (cid:45)
t 3 4 5 6 7 8 7 8 9 8
↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45) ←↑ (cid:45)←↑ ↑
e 4 3 4 5 6 7 8 9 10 9
↑ (cid:45) ← (cid:45)← ← ← ←↑ (cid:45)←↑ (cid:45)←↑ ↑
n 5 4 5 6 7 8 9 10 11 10
↑ ↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45)↑
t 6 5 6 7 8 9 8 9 10 11
↑ ↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45) ← ← ←↑
i 7 6 7 8 9 10 9 8 9 10
↑ ↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ ↑ (cid:45) ← ←
o 8 7 8 9 10 11 10 9 8 9
↑ ↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ ↑ ↑ (cid:45) ←
n 9 8 9 10 11 12 11 10 9 8
↑ ↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ (cid:45)←↑ ↑ ↑ ↑ (cid:45)
Figure2.19 Whenenteringavalueineachcell,wemarkwhichofthethreeneighboring
cellswecamefromwithuptothreearrows.Afterthetableisfullwecomputeanalignment
(minimum edit path) by using a backtrace, starting at the 8 in the lower-right corner and
followingthearrowsback.Thesequenceofboldcellsrepresentsonepossibleminimumcost
alignmentbetweenthetwostrings.DiagramdesignafterGusfield(1997).
WhileweworkedourexamplewithsimpleLevenshteindistance,thealgorithm
inFig.2.17allowsarbitraryweightsontheoperations. Forspellingcorrection,for
example, substitutions are more likely to happen between letters that are next to2.6 • SUMMARY 29
each other on the keyboard. The Viterbi algorithm is a probabilistic extension of
minimumeditdistance. Insteadofcomputingthe“minimumeditdistance”between
two strings, Viterbi computes the “maximum probability alignment” of one string
withanother. We’lldiscussthismoreinChapter8.
2.6 Summary
Thischapterintroducedafundamentaltoolinlanguageprocessing,theregularex-
pression, and showed how to perform basic text normalization tasks including
word segmentation and normalization, sentence segmentation, and stemming.
Wealsointroducedtheimportantminimumeditdistancealgorithmforcomparing
strings. Here’sasummaryofthemainpointswecoveredabouttheseideas:
• Theregularexpressionlanguageisapowerfultoolforpattern-matching.
• Basic operations in regular expressions include concatenation of symbols,
disjunctionofsymbols([],|,and.),counters(*,+,and{n,m}),anchors
(ˆ,$)andprecedenceoperators((,)).
• Word tokenization and normalization are generally done by cascades of
simpleregularexpressionsubstitutionsorfiniteautomata.
• ThePorteralgorithmisasimpleandefficientwaytodostemming,stripping
offaffixes. Itdoesnothavehighaccuracybutmaybeusefulforsometasks.
• Theminimumeditdistancebetweentwostringsistheminimumnumberof
operations it takes to edit one into the other. Minimum edit distance can be
computedbydynamicprogramming,whichalsoresultsinanalignmentof
thetwostrings.
Bibliographical and Historical Notes
Kleene1951;1956firstdefinedregularexpressionsandthefiniteautomaton,based
ontheMcCulloch-Pittsneuron. KenThompsonwasoneofthefirsttobuildregular
expressions compilers into editors for text searching (Thompson, 1968). His edi-
toredincludedacommand“g/regularexpression/p”,orGlobalRegularExpression
Print,whichlaterbecametheUnixgreputility.
Text normalization algorithms have been applied since the beginning of the
field. One of the earliest widely used stemmers was Lovins (1968). Stemming
was also applied early to the digital humanities, by Packard (1973), who built an
affix-stripping morphological parser for Ancient Greek. Currently a wide vari-
ety of code for tokenization and normalization is available, such as the Stanford
Tokenizer(https://nlp.stanford.edu/software/tokenizer.shtml)orspe-
cialized tokenizers for Twitter (O’Connor et al., 2010), or for sentiment (http:
//sentiment.christopherpotts.net/tokenizing.html).SeePalmer(2012)
forasurveyoftextpreprocessing. NLTKisanessentialtoolthatoffersbothuseful
Pythonlibraries(https://www.nltk.org)andtextbookdescriptions(Birdetal.,
2009)ofmanyalgorithmsincludingtextnormalizationandcorpusinterfaces.
For more on Herdan’s law and Heaps’ Law, see Herdan (1960, p. 28), Heaps
(1978),Egghe(2007)andBaayen(2001);Yasserietal.(2012)discusstherelation-
shipwithothermeasuresoflinguisticcomplexity. Formoreoneditdistance,seethe
excellentGusfield(1997).Ourexamplemeasuringtheeditdistancefrom‘intention’30 CHAPTER2 • REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCE
to‘execution’wasadaptedfromKruskal(1983). Therearevariouspubliclyavail-
ablepackagestocomputeeditdistance,includingUnixdiffandtheNISTsclite
program(NIST,2005).
In his autobiography Bellman (1984) explains how he originally came up with
thetermdynamicprogramming:
“...The1950swerenotgoodyearsformathematicalresearch. [the]
SecretaryofDefense...hadapathologicalfearandhatredoftheword,
research... I decided therefore to use the word, “programming”. I
wanted to get across the idea that this was dynamic, this was multi-
stage... I thought, let’s ... take a word that has an absolutely precise
meaning,namelydynamic... it’simpossibletousetheword,dynamic,
inapejorativesense. Trythinkingofsomecombinationthatwillpos-
sibly give it a pejorative meaning. It’s impossible. Thus, I thought
dynamicprogrammingwasagoodname. Itwassomethingnotevena
Congressmancouldobjectto.”
Exercises
2.1 Writeregularexpressionsforthefollowinglanguages.
1. thesetofallalphabeticstrings;
2. thesetofalllowercasealphabeticstringsendinginab;
3. the set of all strings from the alphabet a,b such that each a is immedi-
atelyprecededbyandimmediatelyfollowedbyab;
2.2 Writeregularexpressionsforthefollowinglanguages. By“word”,wemean
an alphabetic string separated from other words by whitespace, any relevant
punctuation,linebreaks,andsoforth.
1. thesetofallstringswithtwoconsecutiverepeatedwords(e.g., “Hum-
bertHumbert”and“thethe”butnot“thebug”or“thebigbug”);
2. allstringsthatstartatthebeginningofthelinewithanintegerandthat
endattheendofthelinewithaword;
3. all strings that have both the word grotto and the word raven in them
(butnot,e.g.,wordslikegrottosthatmerelycontainthewordgrotto);
4. write a pattern that places the first word of an English sentence in a
register. Dealwithpunctuation.
2.3 ImplementanELIZA-likeprogram,usingsubstitutionssuchasthosedescribed
onpage13.YoumightwanttochooseadifferentdomainthanaRogerianpsy-
chologist,althoughkeepinmindthatyouwouldneedadomaininwhichyour
programcanlegitimatelyengageinalotofsimplerepetition.
2.4 Computetheeditdistance(usinginsertioncost1,deletioncost1,substitution
cost1)of“leda”to“deal”. Showyourwork(usingtheeditdistancegrid).
2.5 Figureoutwhetherdriveisclosertobriefortodiversandwhattheeditdis-
tanceistoeach. Youmayuseanyversionofdistancethatyoulike.
2.6 Nowimplementaminimumeditdistancealgorithmanduseyourhand-computed
resultstocheckyourcode.
2.7 Augment the minimum edit distance algorithm to output an alignment; you
willneedtostorepointersandaddastagetocomputethebacktrace.CHAPTER
3 N-gram Language Models
“Youareuniformlycharming!”criedhe,withasmileofassociatingandnow
andthenIbowedandtheyperceivedachaiseandfourtowishfor.
RandomsentencegeneratedfromaJaneAustentrigrammodel
Predicting is difficult—especially about the future, as the old quip goes. But how
aboutpredictingsomethingthatseemsmucheasier,likethenextfewwordssomeone
isgoingtosay? Whatword,forexample,islikelytofollow
Please turn your homework ...
Hopefully, most of you concluded that a very likely word is in, or possibly over,
but probably not refrigerator or the. In the following sections we will formalize
thisintuitionbyintroducingmodelsthatassignaprobabilitytoeachpossiblenext
word. Thesamemodelswillalsoservetoassignaprobabilitytoanentiresentence.
Such a model, for example, could predict that the following sequence has a much
higherprobabilityofappearinginatext:
all of a sudden I notice three guys standing on the sidewalk
thandoesthissamesetofwordsinadifferentorder:
on guys all I of notice sidewalk three a sudden standing the
Whywouldyouwanttopredictupcomingwords,orassignprobabilitiestosen-
tences? Probabilitiesareessentialinanytaskinwhichwehavetoidentifywordsin
noisy,ambiguousinput,likespeechrecognition. Foraspeechrecognizertorealize
thatyousaidIwillbebacksoonishandnotIwillbebassoondish,ithelpstoknow
thatbacksoonishisamuchmoreprobablesequencethanbassoondish. Forwriting
toolslikespellingcorrectionorgrammaticalerrorcorrection,weneedtofindand
correcterrorsinwritinglikeTheiraretwomidterms,inwhichTherewasmistyped
asTheir,orEverythinghasimprove,inwhichimproveshouldhavebeenimproved.
ThephraseTherearewillbemuchmoreprobablethanTheirare,andhasimproved
thanhasimprove,allowingustohelpusersbydetectingandcorrectingtheseerrors.
Assigningprobabilitiestosequencesofwordsisalsoessentialinmachinetrans-
lation. SupposewearetranslatingaChinesesourcesentence:
他 向 记者 介绍了 主要 内容
He to reporters introduced main content
Aspartoftheprocesswemighthavebuiltthefollowingsetofpotentialrough
Englishtranslations:
heintroducedreporterstothemaincontentsofthestatement
hebriefedtoreportersthemaincontentsofthestatement
hebriefedreportersonthemaincontentsofthestatement32 CHAPTER3 • N-GRAMLANGUAGEMODELS
Aprobabilisticmodelofwordsequencescouldsuggestthatbriefedreporterson
isamoreprobableEnglishphrasethanbriefedtoreporters(whichhasanawkward
to after briefed) or introduced reporters to (which uses a verb that is less fluent
Englishinthiscontext),allowingustocorrectlyselecttheboldfacedsentenceabove.
Probabilities are also important for augmentative and alternative communi-
AAC cation systems (Trnka et al. 2007, Kane et al. 2017). People often use such AAC
devicesiftheyarephysicallyunabletospeakorsignbutcaninsteaduseeyegazeor
otherspecificmovementstoselectwordsfromamenutobespokenbythesystem.
Wordpredictioncanbeusedtosuggestlikelywordsforthemenu.
Modelsthatassignprobabilitiestosequencesofwordsarecalledlanguagemod-
languagemodel elsorLMs. Inthischapterweintroducethesimplestmodelthatassignsprobabil-
LM ities to sentences and sequences of words, the n-gram. An n-gram is a sequence
n-gram of n words: a 2-gram (which we’ll call bigram) is a two-word sequence of words
like “please turn”, “turn your”, or ”your homework”, and a 3-gram (a trigram) is
athree-wordsequenceofwordslike“pleaseturnyour”,or“turnyourhomework”.
We’llseehowtousen-grammodelstoestimatetheprobabilityofthelastwordof
an n-gram given the previous words, and also to assign probabilities to entire se-
quences. In a bit of terminological ambiguity, we usually drop the word “model”,
andusethetermn-gram(andbigram,etc.) tomeaneitherthewordsequenceitself
orthepredictivemodelthatassignsitaprobability. Whilen-grammodelsaremuch
simplerthanstate-of-theartneurallanguagemodelsbasedontheRNNsandtrans-
formerswewillintroduceinChapter9,theyareanimportantfoundationaltoolfor
understandingthefundamentalconceptsoflanguagemodeling.
3.1 N-Grams
Let’s begin with the task of computing P(wh), the probability of a word w given
|
some history h. Suppose the history h is “its water is so transparent that” and we
wanttoknowtheprobabilitythatthenextwordisthe:
P(theitswaterissotransparentthat). (3.1)
|
One way to estimate this probability is from relative frequency counts: take a
verylargecorpus,countthenumberoftimesweseeitswaterissotransparentthat,
andcountthenumberoftimesthisisfollowedbythe. Thiswouldbeansweringthe
question“Outofthetimeswesawthehistoryh,howmanytimeswasitfollowedby
thewordw”,asfollows:
P(theitswaterissotransparentthat)=
|
C(itswaterissotransparentthatthe)
(3.2)
C(itswaterissotransparentthat)
Withalargeenoughcorpus,suchastheweb,wecancomputethesecountsand
estimate the probability from Eq. 3.2. You should pause now, go to the web, and
computethisestimateforyourself.
Whilethismethodofestimatingprobabilitiesdirectlyfromcountsworksfinein
manycases,itturnsoutthateventhewebisn’tbigenoughtogiveusgoodestimates
inmostcases. Thisisbecauselanguageiscreative;newsentencesarecreatedallthe
time,andwewon’talwaysbeabletocountentiresentences.Evensimpleextensions3.1 • N-GRAMS 33
of the example sentence may have counts of zero on the web (such as “Walden
Pond’swaterissotransparentthatthe”;well,usedtohavecountsofzero).
Similarly, if we wanted to know the joint probability of an entire sequence of
wordslikeitswaterissotransparent,wecoulddoitbyasking“outofallpossible
sequences of five words, how many of them are its water is so transparent?” We
wouldhavetogetthecountofitswaterissotransparentanddividebythesumof
thecountsofallpossiblefivewordsequences. Thatseemsratheralottoestimate!
Forthisreason,we’llneedtointroducemorecleverwaysofestimatingtheprob-
abilityofawordwgivenahistoryh,ortheprobabilityofanentirewordsequence
W. Let’sstartwithalittleformalizingofnotation. Torepresenttheprobabilityofa
particularrandomvariableX takingonthevalue“the”,orP(X =“the”),wewilluse
i i
thesimplificationP(the). We’llrepresentasequenceofnwordseitherasw ...w
1 n
or w (so the expression w means the string w ,w ,...,w ). For the joint
1:n 1:n 1 1 2 n 1
− −
probability of each word in a sequence having a particular value P(X =w ,X =
1 1 2
w ,X =w ,...,X =w )we’lluseP(w ,w ,...,w ).
2 3 3 n n 1 2 n
Now,howcanwecomputeprobabilitiesofentiresequenceslikeP(w ,w ,...,w )?
1 2 n
Onethingwecandoisdecomposethisprobabilityusingthechainruleofproba-
bility:
P(X ...X ) = P(X )P(X X )P(X X )...P(X X )
1 n 1 2 1 3 1:2 n 1:n 1
| | | −
n
= P(X k X 1:k 1) (3.3)
| −
k=1
(cid:89)
Applyingthechainruletowords,weget
P(w ) = P(w )P(w w )P(w w )...P(w w )
1:n 1 2 1 3 1:2 n 1:n 1
| | | −
n
= P(w k w 1:k 1) (3.4)
| −
k=1
(cid:89)
Thechainruleshowsthelinkbetweencomputingthejointprobabilityofasequence
and computing the conditional probability of a word given previous words. Equa-
tion3.4suggeststhatwecouldestimatethejointprobabilityofanentiresequenceof
wordsbymultiplyingtogetheranumberofconditionalprobabilities. Butusingthe
chainruledoesn’treallyseemtohelpus! Wedon’tknowanywaytocomputethe
exactprobabilityofawordgivenalongsequenceofprecedingwords,P(w w ).
n 1:n 1
| −
Aswesaidabove,wecan’tjustestimatebycountingthenumberoftimeseveryword
occursfollowingeverylongstring,becauselanguageiscreativeandanyparticular
contextmighthaveneveroccurredbefore!
Theintuitionofthen-grammodelisthatinsteadofcomputingtheprobabilityof
awordgivenitsentirehistory,wecanapproximatethehistorybyjustthelastfew
words.
bigram Thebigrammodel,forexample,approximatestheprobabilityofawordgiven
allthepreviouswordsP(w w )byusingonlytheconditionalprobabilityofthe
n 1:n 1
| −
precedingwordP(w w ). Inotherwords,insteadofcomputingtheprobability
n n 1
| −
P(theWaldenPond’swaterissotransparentthat) (3.5)
|
weapproximateitwiththeprobability
P(thethat) (3.6)
|34 CHAPTER3 • N-GRAMLANGUAGEMODELS
Whenweuseabigrammodeltopredicttheconditionalprobabilityofthenextword,
wearethusmakingthefollowingapproximation:
P(w w ) P(w w ) (3.7)
n 1:n 1 n n 1
| − ≈ | −
Theassumptionthattheprobabilityofaworddependsonlyonthepreviouswordis
Markov calledaMarkovassumption. Markovmodelsaretheclassofprobabilisticmodels
thatassumewecanpredicttheprobabilityofsomefutureunitwithoutlookingtoo
farintothepast. Wecangeneralizethebigram(whichlooksonewordintothepast)
n-gram tothetrigram(whichlookstwowordsintothepast)andthustothen-gram(which
looksn 1wordsintothepast).
−
Let’s see a general equation for this n-gram approximation to the conditional
probability of the next word in a sequence. We’ll use N here to mean the n-gram
size,soN=2meansbigramsandN=3meanstrigrams. Thenweapproximatethe
probabilityofawordgivenitsentirecontextasfollows:
P(w n w 1:n 1) P(w n w n N+1:n 1) (3.8)
| − ≈ | − −
Giventhebigramassumptionfortheprobabilityofanindividualword,wecancom-
putetheprobabilityofacompletewordsequencebysubstitutingEq.3.7intoEq.3.4:
n
P(w ) P(w w ) (3.9)
1:n k k 1
≈ | −
k=1
(cid:89)
Howdoweestimatethesebigramorn-gramprobabilities? Anintuitivewayto
maximum
likelihood estimate probabilities is called maximum likelihood estimation or MLE. We get
estimation
theMLEestimatefortheparametersofann-grammodelbygettingcountsfroma
normalize
corpus,andnormalizingthecountssothattheyliebetween0and1.1
For example, to compute a particular bigram probability of a word w given a
n
previouswordw ,we’llcomputethecountofthebigramC(w w )andnormal-
n 1 n 1 n
− −
izebythesumofallthebigramsthatsharethesamefirstwordw :
n 1
−
C(w w )
n 1 n
P(w n w n 1)= − (3.10)
| − wC(w n 1w)
−
Wecansimplifythisequation,sincethesumofallbigramcountsthatstartwith
(cid:80)
agivenwordw mustbeequaltotheunigramcountforthatwordw (thereader
n 1 n 1
− −
shouldtakeamomenttobeconvincedofthis):
C(w w )
n 1 n
P(w n w n 1)= − (3.11)
| − C(w n 1)
−
Let’s work through an example using a mini-corpus of three sentences. We’ll
first need to augment each sentence with a special symbol <s> at the beginning
of the sentence, to give us the bigram context of the first word. We’ll also need a
specialend-symbol. </s>2
<s> I am Sam </s>
<s> Sam I am </s>
<s> I do not like green eggs and ham </s>
1 Forprobabilisticmodels,normalizingmeansdividingbysometotalcountsothattheresultingproba-
bilitiesfallbetween0and1.
2 Weneedtheend-symboltomakethebigramgrammaratrueprobabilitydistribution.Withoutanend-
symbol,insteadofthesentenceprobabilitiesofallsentencessummingtoone,thesentenceprobabilities
forallsentencesofagivenlengthwouldsumtoone.Thismodelwoulddefineaninfinitesetofprobability
distributions,withonedistributionpersentencelength.SeeExercise3.5.3.1 • N-GRAMS 35
Herearethecalculationsforsomeofthebigramprobabilitiesfromthiscorpus
P(I|<s>)= 2 =.67 P(Sam|<s>)= 1 =.33 P(am|I)= 2 =.67
3 3 3
P(</s>|Sam)= 1 =0.5 P(Sam|am)= 1 =.5 P(do|I)= 1 =.33
2 2 3
ForthegeneralcaseofMLEn-gramparameterestimation:
C(w w )
n N+1:n 1 n
P(w n w n N+1:n 1)= − − (3.12)
| − − C(w n N+1:n 1)
− −
Equation 3.12 (like Eq. 3.11) estimates the n-gram probability by dividing the
observedfrequencyofaparticularsequencebytheobservedfrequencyofaprefix.
relative This ratio is called a relative frequency. We said above that this use of relative
frequency
frequenciesasawaytoestimateprobabilitiesisanexampleofmaximumlikelihood
estimation or MLE. In MLE, the resulting parameter set maximizes the likelihood
of the training set T given the model M (i.e., P(T M)). For example, suppose the
|
wordChineseoccurs400timesinacorpusofamillionwordsliketheBrowncorpus.
What is the probability that a random word selected from some other text of, say,
a million words will be the word Chinese? The MLE of its probability is 400
1000000
or.0004. Now.0004isnotthebestpossibleestimateoftheprobabilityofChinese
occurring in all situations; it might turn out that in some other corpus or context
Chinese is a very unlikely word. But it is the probability that makes it most likely
that Chinese will occur 400 times in a million-word corpus. We present ways to
modifytheMLEestimatesslightlytogetbetterprobabilityestimatesinSection3.5.
Let’smoveontosomeexamplesfromaslightlylargercorpusthanour14-word
exampleabove. We’llusedatafromthenow-defunctBerkeleyRestaurantProject,
a dialogue system from the last century that answered questions about a database
of restaurants in Berkeley, California (Jurafsky et al., 1994). Here are some text-
normalizedsampleuserqueries(asampleof9332sentencesisonthewebsite):
canyoutellmeaboutanygoodcantoneserestaurantscloseby
midpricedthaifoodiswhati’mlookingfor
tellmeaboutchezpanisse
canyougivemealistingofthekindsoffoodthatareavailable
i’mlookingforagoodplacetoeatbreakfast
wheniscaffeveneziaopenduringtheday
Figure3.1showsthebigramcountsfromapieceofabigramgrammarfromthe
BerkeleyRestaurantProject. Notethatthemajorityofthevaluesarezero. Infact,
wehavechosenthesamplewordstocoherewitheachother;amatrixselectedfrom
arandomsetofeightwordswouldbeevenmoresparse.
i want to eat chinese food lunch spend
i 5 827 0 9 0 0 0 2
want 2 0 608 1 6 6 5 1
to 2 0 4 686 2 0 6 211
eat 0 0 2 0 16 2 42 0
chinese 1 0 0 0 0 82 1 0
food 15 0 15 0 1 4 0 0
lunch 2 0 0 0 0 1 0 0
spend 1 0 1 0 0 0 0 0
Figure3.1 Bigramcountsforeightofthewords(outofV=1446)intheBerkeleyRestau-
rantProjectcorpusof9332sentences.Zerocountsareingray.36 CHAPTER3 • N-GRAMLANGUAGEMODELS
Figure3.2showsthebigramprobabilitiesafternormalization(dividingeachcell
in Fig. 3.1 by the appropriate unigram for its row, taken from the following set of
unigramprobabilities):
i want to eat chinese food lunch spend
2533 927 2417 746 158 1093 341 278
i want to eat chinese food lunch spend
i 0.002 0.33 0 0.0036 0 0 0 0.00079
want 0.0022 0 0.66 0.0011 0.0065 0.0065 0.0054 0.0011
to 0.00083 0 0.0017 0.28 0.00083 0 0.0025 0.087
eat 0 0 0.0027 0 0.021 0.0027 0.056 0
chinese 0.0063 0 0 0 0 0.52 0.0063 0
food 0.014 0 0.014 0 0.00092 0.0037 0 0
lunch 0.0059 0 0 0 0 0.0029 0 0
spend 0.0036 0 0.0036 0 0 0 0 0
Figure3.2 BigramprobabilitiesforeightwordsintheBerkeleyRestaurantProjectcorpus
of9332sentences.Zeroprobabilitiesareingray.
Hereareafewotherusefulprobabilities:
P(i|<s>)=0.25 P(english|want)=0.0011
P(food|english)=0.5 P(</s>|food)=0.68
Now we can compute the probability of sentences like I want English food or
IwantChinesefoodbysimplymultiplyingtheappropriatebigramprobabilitiesto-
gether,asfollows:
P(<s> i want english food </s>)
= P(i|<s>)P(want|i)P(english|want)
P(food|english)P(</s>|food)
= .25 .33 .0011 0.5 0.68
× × × ×
= .000031
WeleaveitasExercise3.2tocomputetheprobabilityofiwantchinesefood.
What kinds of linguistic phenomena are captured in these bigram statistics?
Someofthebigramprobabilitiesaboveencodesomefactsthatwethinkofasstrictly
syntactic in nature, like the fact that what comes after eat is usually a noun or an
adjective,orthatwhatcomesaftertoisusuallyaverb. Othersmightbeafactabout
the personal assistant task, like the high probability of sentences beginning with
thewordsI.Andsomemightevenbeculturalratherthanlinguistic,likethehigher
probabilitythatpeoplearelookingforChineseversusEnglishfood.
Somepracticalissues: Althoughforpedagogicalpurposeswehaveonlydescribed
trigram bigram models, in practice it’s more common to use trigram models, which con-
4-gram ditionontheprevioustwowordsratherthanthepreviousword,or4-gramoreven
5-gram 5-grammodels,whenthereissufficienttrainingdata. Notethatfortheselargern-
grams,we’llneedtoassumeextracontextstotheleftandrightofthesentenceend.
Forexample,tocomputetrigramprobabilitiesattheverybeginningofthesentence,
weusetwopseudo-wordsforthefirsttrigram(i.e.,P(I|<s><s>).
We always represent and compute language model probabilities in log format
log as log probabilities. Since probabilities are (by definition) less than or equal to
probabilities3.2 • EVALUATINGLANGUAGEMODELS 37
1, the more probabilities we multiply together, the smaller the product becomes.
Multiplyingenoughn-gramstogetherwouldresultinnumericalunderflow.Byusing
logprobabilitiesinsteadofrawprobabilities,wegetnumbersthatarenotassmall.
Addinginlogspaceisequivalenttomultiplyinginlinearspace,sowecombinelog
probabilitiesbyaddingthem. Theresultofdoingallcomputationandstorageinlog
space is that we only need to convert back into probabilities if we need to report
themattheend;thenwecanjusttaketheexpofthelogprob:
p p p p =exp(logp +logp +logp +logp ) (3.13)
1 2 3 4 1 2 3 4
× × ×
3.2 Evaluating Language Models
The best way to evaluate the performance of a language model is to embed it in
an application and measure how much the application improves. Such end-to-end
extrinsic evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to
evaluation
know if a particular improvement in a component is really going to help the task
at hand. Thus, for speech recognition, we can compare the performance of two
languagemodelsbyrunningthespeechrecognizertwice,oncewitheachlanguage
model,andseeingwhichgivesthemoreaccuratetranscription.
Unfortunately,runningbigNLPsystemsend-to-endisoftenveryexpensive. In-
stead,itwouldbenicetohaveametricthatcanbeusedtoquicklyevaluatepotential
intrinsic improvementsinalanguagemodel. Anintrinsicevaluationmetricisonethatmea-
evaluation
suresthequalityofamodelindependentofanyapplication.
Foranintrinsicevaluationofalanguagemodelweneedatestset.Aswithmany
ofthestatisticalmodelsinourfield,theprobabilitiesofann-grammodelcomefrom
trainingset thecorpusitistrainedon,thetrainingsetortrainingcorpus.Wecanthenmeasure
thequalityofann-grammodelbyitsperformanceonsomeunseendatacalledthe
testset testsetortestcorpus.
So if we are given a corpus of text and want to compare two different n-gram
models, we divide the data into training and test sets, train the parameters of both
modelsonthetrainingset,andthencomparehowwellthetwotrainedmodelsfitthe
testset.
But what does it mean to “fit the test set”? The answer is simple: whichever
model assigns a higher probability to the test set—meaning it more accurately
predicts the test set—is a better model. Given two probabilistic models, the better
modelistheonethathasatighterfittothetestdataorthatbetterpredictsthedetails
ofthetestdata,andhencewillassignahigherprobabilitytothetestdata.
Sinceourevaluationmetricisbasedontestsetprobability,it’simportantnotto
let the test sentences into the training set. Suppose we are trying to compute the
probabilityofaparticular“test”sentence. Ifourtestsentenceispartofthetraining
corpus, we will mistakenly assign it an artificially high probability when it occurs
in the test set. We call this situation training on the test set. Training on the test
setintroducesabiasthatmakestheprobabilitiesalllooktoohigh,andcauseshuge
inaccuraciesinperplexity,theprobability-basedmetricweintroducebelow.
Sometimes we use a particular test set so often that we implicitly tune to its
characteristics. Wethenneedafreshtestsetthatistrulyunseen. Insuchcases,we
development call the initial test set the development test set or, devset. How do we divide our
test
data into training, development, and test sets? We want our test set to be as large
aspossible,sinceasmalltestsetmaybeaccidentallyunrepresentative,butwealso38 CHAPTER3 • N-GRAMLANGUAGEMODELS
want as much training data as possible. At the minimum, we would want to pick
thesmallesttestsetthatgivesusenoughstatisticalpowertomeasureastatistically
significantdifferencebetweentwopotentialmodels.Inpractice,weoftenjustdivide
ourdatainto80%training, 10%development, and10%test. Givenalargecorpus
thatwewanttodivideintotrainingandtest,testdatacaneitherbetakenfromsome
continuous sequence of text inside the corpus, or we can remove smaller “stripes”
oftextfromrandomlyselectedpartsofourcorpusandcombinethemintoatestset.
3.2.1 Perplexity
Inpracticewedon’tuserawprobabilityasourmetricforevaluatinglanguagemod-
perplexity els,butavariantcalledperplexity.Theperplexity(sometimescalledPPLforshort)
ofalanguagemodelonatestsetistheinverseprobabilityofthetestset,normalized
bythenumberofwords. ForatestsetW =w w ...w ,:
1 2 N
1
perplexity(W) = P(w 1w 2...w N) −N (3.14)
1
= N
(cid:115)P(w 1w 2...w N)
WecanusethechainruletoexpandtheprobabilityofW:
N
1
perplexity(W) = N (3.15)
(cid:118) P(w w ...w )
(cid:117) (cid:117)(cid:89)i=1 i | 1 i −1
(cid:116)
TheperplexityofatestsetW dependsonwhichlanguagemodelweuse. Here’s
theperplexityofW withaunigramlanguagemodel(justthegeometricmeanofthe
unigramprobabilities):
N
1
perplexity(W) = N (3.16)
(cid:118) P(w)
(cid:117)i=1 i
(cid:117)(cid:89)
(cid:116)
The perplexity ofW computed with a bigram language model is still a geometric
mean,butnowofthebigramprobabilities:
N
1
perplexity(W) = N (3.17)
(cid:118) P(w w )
(cid:117) (cid:117)(cid:89)i=1 i | i −1
(cid:116)
NotethatbecauseoftheinverseinEq.3.15,thehighertheconditionalprobabil-
ity of the word sequence, the lower the perplexity. Thus, minimizing perplexity is
equivalent to maximizing the test set probability according to the language model.
What we generally use for word sequence in Eq. 3.15 or Eq. 3.17 is the entire se-
quence of words in some test set. Since this sequence will cross many sentence
boundaries,weneedtoincludethebegin-andend-sentencemarkers<s>and</s>
intheprobabilitycomputation. Wealsoneedtoincludetheend-of-sentencemarker
</s>(butnotthebeginning-of-sentencemarker<s>)inthetotalcountofwordto-
kensN.
Thereisanotherwaytothinkaboutperplexity:astheweightedaveragebranch-
ingfactorofalanguage. Thebranchingfactorofalanguageisthenumberofpossi-
blenextwordsthatcanfollowanyword. Considerthetaskofrecognizingthedigits3.2 • EVALUATINGLANGUAGEMODELS 39
inEnglish(zero,one,two,...,nine),giventhat(bothinsometrainingsetandinsome
testset)eachofthe10digitsoccurswithequalprobabilityP= 1 .Theperplexityof
10
thismini-languageisinfact10. Toseethat,imagineateststringofdigitsoflength
N,andassumethatinthetrainingsetallthedigitsoccurredwithequalprobability.
ByEq.3.15,theperplexitywillbe
1
perplexity(W) = P(w 1w 2...w N) −N
1 N 1
= ( ) −N
10
1 1
−
=
10
= 10 (3.18)
But suppose that the number zero is really frequent and occurs far more often
thanothernumbers.Let’ssaythat0occur91timesinthetrainingset,andeachofthe
otherdigitsoccurred1timeeach.Nowweseethefollowingtestset:000003000
0. Weshouldexpecttheperplexityofthistestsettobelowersincemostofthetime
thenextnumberwillbezero,whichisverypredictable,i.e. hasahighprobability.
Thus,althoughthebranchingfactorisstill10,theperplexityorweightedbranching
factorissmaller. Weleavethisexactcalculationasexercise3.12.
WeseeinSection3.8thatperplexityisalsocloselyrelatedtotheinformation-
theoreticnotionofentropy.
We mentioned above that perplexity is a function of both the text and the lan-
guagemodel: givenatextW,differentlanguagemodelswillhavedifferentperplex-
ities. Becauseofthis, perplexitycanbeusedtocomparedifferentn-grammodels.
Let’slookatanexample,inwhichwetrainedunigram,bigram,andtrigramgram-
marson38millionwords(includingstart-of-sentencetokens)fromtheWallStreet
Journal,usinga19,979wordvocabulary. Wethencomputedtheperplexityofeach
of these models on a test set of 1.5 million words, using Eq. 3.16 for unigrams,
Eq.3.17forbigrams,andthecorrespondingequationfortrigrams. Thetablebelow
showstheperplexityofa1.5millionwordWSJtestsetaccordingtoeachofthese
grammars.
Unigram Bigram Trigram
Perplexity 962 170 109
As we see above, the more information the n-gram gives us about the word
sequence,thehighertheprobabilitythen-gramwillassigntothestring. Atrigram
model is less surprised than a unigram model because it has a better idea of what
wordsmightcomenext,andsoitassignsthemahigherprobability. Andthehigher
the probability, the lower the perplexity (since as Eq. 3.15 showed, perplexity is
relatedinverselytothelikelihoodofthetestsequenceaccordingtothemodel). Soa
lowerperplexitycantellusthatalanguagemodelisabetterpredictorofthewords
inthetestset.
Note that in computing perplexities, the n-gram model P must be constructed
withoutanyknowledgeofthetestsetoranypriorknowledgeofthevocabularyof
the test set. Any kind of knowledge of the test set can cause the perplexity to be
artificially low. The perplexity of two language models is only comparable if they
useidenticalvocabularies.
An (intrinsic) improvement in perplexity does not guarantee an (extrinsic) im-
provementintheperformanceofalanguageprocessingtasklikespeechrecognition40 CHAPTER3 • N-GRAMLANGUAGEMODELS
ormachinetranslation. Nonetheless, becauseperplexityoftencorrelateswithsuch
improvements,itiscommonlyusedasaquickcheckonanalgorithm.Butamodel’s
improvementinperplexityshouldalwaysbeconfirmedbyanend-to-endevaluation
ofarealtaskbeforeconcludingtheevaluationofthemodel.
3.3 Sampling sentences from a language model
Oneimportantwaytovisualizewhatkindofknowledgealanguagemodelembodies
sampling istosamplefromit. Samplingfromadistributionmeanstochooserandompoints
according to their likelihood. Thus sampling from a language model—which rep-
resentsadistributionoversentences—meanstogeneratesomesentences,choosing
eachsentenceaccordingtoitslikelihoodasdefinedbythemodel. Thuswearemore
likely to generate sentences that the model thinks have a high probability and less
likelytogeneratesentencesthatthemodelthinkshavealowprobability.
Thistechniqueofvisualizingalanguagemodelbysamplingwasfirstsuggested
very early on by Shannon (1951) and Miller and Selfridge (1950). It’s simplest to
visualizehowthisworksfortheunigramcase. ImagineallthewordsoftheEnglish
language covering the probability space between 0 and 1, each word covering an
intervalproportionaltoitsfrequency.Fig.3.3showsavisualization,usingaunigram
LMcomputedfromthetextofthisbook. Wechoosearandomvaluebetween0and
1,findthatpointontheprobabilityline,andprintthewordwhoseintervalincludes
this chosen value. We continue choosing random numbers and generating words
untilwerandomlygeneratethesentence-finaltoken</s>.
polyphonic
however p=.0000018
the of a to in … (p=.0003)
0.06 0.03 0.02 0.020.02
… …
.06 .09.11.13.15 .66 .99
0 1
Figure3.3 Avisualizationofthesamplingdistributionforsamplingsentencesbyrepeat-
edlysamplingunigrams. Thebluebarrepresentstherelativefrequencyofeachword(we’ve
orderedthemfrommostfrequenttoleastfrequent,butthechoiceoforderisarbitrary). The
numberlineshowsthecumulativeprobabilities. Ifwechoosearandomnumberbetween0
and1,itwillfallinanintervalcorrespondingtosomeword. Theexpectationfortherandom
numbertofallinthelargerintervalsofoneofthefrequentwords(the,of,a)ismuchhigher
thaninthesmallerintervalofoneoftherarewords(polyphonic).
We can use the same technique to generate bigrams by first generating a ran-
dombigramthatstartswith<s>(accordingtoitsbigramprobability). Let’ssaythe
secondwordofthatbigramisw. Wenextchoosearandombigramstartingwithw
(again,drawnaccordingtoitsbigramprobability),andsoon.
3.4 Generalization and Zeros
Then-grammodel,likemanystatisticalmodels,isdependentonthetrainingcorpus.
One implication of this is that the probabilities often encode specific facts about a3.4 • GENERALIZATIONANDZEROS 41
giventrainingcorpus. Anotherimplicationisthatn-gramsdoabetterandbetterjob
ofmodelingthetrainingcorpusasweincreasethevalueofN.
We can use the sampling method from the prior section to visualize both of
these facts! To give an intuition for the increasing power of higher-order n-grams,
Fig.3.4showsrandomsentencesgeneratedfromunigram, bigram, trigram, and4-
grammodelstrainedonShakespeare’sworks.
–To him swallowed confess hear both. Which. Of save on trail for are ay device and
1
rotelifehave
gram –Hillhelatespeaks;or! amoretoleglessfirstyouenter
–Whydoststandforththycanopy,forsooth;heisthispalpablehittheKingHenry. Live
2
king. Follow.
gram –Whatmeans,sir. Iconfessshe? thenallsorts,heistrim,captain.
–Fly,andwillridmethesenewsofprice. Thereforethesadnessofparting,astheysay,
3
’tisdone.
gram –Thisshallforbiditshouldbebranded,ifrenownmadeitempty.
–KingHenry. What! IwillgoseekthetraitorGloucester. Exeuntsomeofthewatch. A
4
greatbanquetserv’din;
gram –Itcannotbebutso.
Figure3.4 Eightsentencesrandomlygeneratedfromfourn-gramscomputedfromShakespeare’sworks.All
charactersweremappedtolower-caseandpunctuationmarksweretreatedaswords. Outputishand-corrected
forcapitalizationtoimprovereadability.
Thelongerthecontextonwhichwetrainthemodel,themorecoherentthesen-
tences. Intheunigramsentences,thereisnocoherentrelationbetweenwordsorany
sentence-final punctuation. The bigram sentences have some local word-to-word
coherence (especially if we consider that punctuation counts as a word). The tri-
gramand4-gramsentencesarebeginningtolookalotlikeShakespeare. Indeed,a
carefulinvestigationofthe4-gramsentencesshowsthattheylookalittletoomuch
likeShakespeare.ThewordsItcannotbebutsoaredirectlyfromKingJohn.Thisis
because,nottoputtheknockonShakespeare,hisoeuvreisnotverylargeascorpora
go(N=884,647,V =29,066),andourn-gramprobabilitymatricesareridiculously
sparse.ThereareV2=844,000,000possiblebigramsalone,andthenumberofpos-
sible4-gramsisV4=7 1017. Thus,oncethegeneratorhaschosenthefirst4-gram
×
(It cannot be but), there are only five possible continuations (that, I, he, thou, and
so);indeed,formany4-grams,thereisonlyonecontinuation.
Togetanideaofthedependenceofagrammaronitstrainingset,let’slookatan
n-gram grammar trained on a completely different corpus: the Wall Street Journal
(WSJ) newspaper. Shakespeare and the Wall Street Journal are both English, so
we might expect some overlap between our n-grams for the two genres. Fig. 3.5
shows sentences generated by unigram, bigram, and trigram grammars trained on
40millionwordsfromWSJ.
Comparetheseexamplestothepseudo-ShakespeareinFig.3.4.Whiletheyboth
model“English-likesentences”, thereisclearlynooverlapingeneratedsentences,
andlittleoverlapeveninsmallphrases.Statisticalmodelsarelikelytobeprettyuse-
lessaspredictorsifthetrainingsetsandthetestsetsareasdifferentasShakespeare
andWSJ.
Howshouldwedealwiththisproblemwhenwebuildn-grammodels? Onestep
istobesuretouseatrainingcorpusthathasasimilargenretowhatevertaskweare
trying to accomplish. To build a language model for translating legal documents,42 CHAPTER3 • N-GRAMLANGUAGEMODELS
1 Months the my and issue of year foreign new exchange’s september
wererecessionexchangenewendorsedaacquiretosixexecutives
gram
LastDecemberthroughthewaytopreservetheHudsoncorporationN.
2
B.E.C.Taylorwouldseemtocompletethemajorcentralplannersone
gram pointfivepercentofU.S.E.hasalreadyoldM.X.corporationofliving
oninformationsuchasmorefrequentlyfishingtokeepher
Theyalsopointtoninetyninepointsixbilliondollarsfromtwohundred
3
fourohsixthreepercentoftheratesofintereststoresasMexicoand
gram Brazilonmarketconditions
Figure3.5 Threesentencesrandomlygeneratedfromthreen-grammodelscomputedfrom
40millionwordsoftheWallStreetJournal,lower-casingallcharactersandtreatingpunctua-
tionaswords.Outputwasthenhand-correctedforcapitalizationtoimprovereadability.
we need a training corpus of legal documents. To build a language model for a
question-answeringsystem,weneedatrainingcorpusofquestions.
Itisequallyimportanttogettrainingdataintheappropriatedialectorvariety,
especially when processing social media posts or spoken transcripts. For example
some tweets will use features of African American Language (AAL)— the name
forthemanyvariationsoflanguageusedinAfricanAmericancommunities(King,
2020). Suchfeaturesincludewordslikefinna—anauxiliaryverbthatmarksimme-
diatefuturetense—thatdon’toccurinothervarieties,orspellingslikedenforthen,
intweetslikethisone(BlodgettandO’Connor,2017):
(3.19) Boredafdenmyphonefinnadie!!!
while tweets from varieties like Nigerian English have markedly different vocabu-
laryandn-grampatternsfromAmericanEnglish(Jurgensetal.,2017):
(3.20) @usernameRuawizardorwatgansef: indmornin-utweet,afternoon-u
tweet,nytganudeytweet. betageturITplacementwivtwitter
Matching genres and dialects is still not sufficient. Our models may still be
subjecttotheproblemofsparsity.Foranyn-gramthatoccurredasufficientnumber
oftimes,wemighthaveagoodestimateofitsprobability.Butbecauseanycorpusis
limited,someperfectlyacceptableEnglishwordsequencesareboundtobemissing
fromit. Thatis,we’llhavemanycasesofputative“zeroprobabilityn-grams”that
should really have some non-zero probability. Consider the words that follow the
bigramdeniedtheintheWSJTreebank3corpus,togetherwiththeircounts:
deniedtheallegations: 5
deniedthespeculation: 2
deniedtherumors: 1
deniedthereport: 1
Butsupposeourtestsethasphraseslike:
deniedtheoffer
deniedtheloan
OurmodelwillincorrectlyestimatethattheP(offerdeniedthe)is0!
|
zeros These zeros—things that don’t ever occur in the training set but do occur in
the test set—are a problem for two reasons. First, their presence means we are
underestimating the probability of all sorts of words that might occur, which will
hurttheperformanceofanyapplicationwewanttorunonthisdata.
Second, iftheprobabilityofanywordinthetestsetis0, theentireprobability
ofthetestsetis0. Bydefinition,perplexityisbasedontheinverseprobabilityofthe3.5 • SMOOTHING 43
testset. Thusifsomewordshavezeroprobability, wecan’tcomputeperplexityat
all,sincewecan’tdivideby0!
Whatdowedoaboutzeros? Therearetwosolutions,dependingonthekindof
zero. Forwordswhosen-gramprobabilityiszerobecausetheyoccurinanoveltest
setcontext,liketheexampleofdeniedtheofferabove,we’llintroduceinSection3.5
algorithmscalledsmoothingordiscounting. Smoothingalgorithmsshaveoffabit
of probability mass from some more frequent events and give it to these unseen
events. Butfirst,let’stalkaboutanevenmoreinsidiousformofzero:wordsthatthe
modelhasneverseenbelowatall(inanycontext): unknownwords!
3.4.1 UnknownWords
Whatdowedoaboutwordswehaveneverseenbefore? PerhapsthewordJurafsky
simplydidnotoccurinourtrainingset,butpopsupinthetestset!
Wecanchoosetodisallowthissituationfromoccurring,bystipulatingthatwe
closed already know all the words that can occur. In such a closed vocabulary system
vocabulary
the test set can only contain words from this known lexicon, and there will be no
unknownwords.
In most real situations, however, we have to deal with words we haven’t seen
OOV before,whichwe’llcallunknownwords,oroutofvocabulary(OOV)words. The
percentageofOOVwordsthatappearinthetestsetiscalledtheOOVrate.Oneway
open tocreateanopenvocabularysystemistomodelthesepotentialunknownwordsin
vocabulary
thetestsetbyaddingapseudo-wordcalled<UNK>.
There are two common ways to train the probabilities of the unknown word
model<UNK>. Thefirstoneistoturntheproblembackintoaclosedvocabularyone
bychoosingafixedvocabularyinadvance:
1. Chooseavocabulary(wordlist)thatisfixedinadvance.
2. Convertinthetrainingsetanywordthatisnotinthisset(anyOOVword)to
theunknownwordtoken<UNK>inatextnormalizationstep.
3. Estimatetheprobabilitiesfor<UNK>fromitscountsjustlikeanyotherregular
wordinthetrainingset.
Thesecondalternative,insituationswherewedon’thaveapriorvocabularyinad-
vance,istocreatesuchavocabularyimplicitly,replacingwordsinthetrainingdata
by<UNK>basedontheirfrequency. Forexamplewecanreplaceby<UNK>allwords
thatoccurfewerthanntimesinthetrainingset,wherenissomesmallnumber,or
equivalentlyselectavocabularysizeVinadvance(say50,000)andchoosethetop
Vwordsbyfrequencyandreplacetherestby<UNK>. Ineithercasewethenproceed
totrainthelanguagemodelasbefore,treating<UNK>likearegularword.
Theexactchoiceof<UNK>hasaneffectonmetricslikeperplexity. Alanguage
modelcanachievelowperplexitybychoosingasmallvocabularyandassigningthe
unknown word a high probability. Thus perplexities can only be compared across
languagemodelswiththesamevocabularies(Bucketal.,2014).
3.5 Smoothing
Whatdowedowithwordsthatareinourvocabulary(theyarenotunknownwords)
butappearinatestsetinanunseencontext(forexampletheyappearafteraword
they never appeared after in training)? To keep a language model from assigning44 CHAPTER3 • N-GRAMLANGUAGEMODELS
zeroprobabilitytotheseunseenevents, we’llhavetoshaveoffabitofprobability
mass from some more frequent events and give it to the events we’ve never seen.
smoothing This modification is called smoothing or discounting. In this section and the fol-
discounting lowingoneswe’llintroduceavarietyofwaystodosmoothing: Laplace(add-one)
smoothing,add-ksmoothing,stupidbackoff,andKneser-Neysmoothing.
3.5.1 LaplaceSmoothing
The simplest way to do smoothing is to add one to all the n-gram counts, before
wenormalizethemintoprobabilities. Allthecountsthatusedtobezerowillnow
have a count of 1, the counts of 1 will be 2, and so on. This algorithm is called
Laplace Laplacesmoothing. Laplacesmoothingdoesnotperformwellenoughtobeused
smoothing
in modern n-grammodels, but it usefullyintroduces many of the conceptsthat we
see in other smoothing algorithms, gives a useful baseline, and is also a practical
smoothingalgorithmforothertasksliketextclassification(Chapter4).
Let’s start with the application of Laplace smoothing to unigram probabilities.
Recallthattheunsmoothedmaximumlikelihoodestimateoftheunigramprobability
ofthewordw isitscountc normalizedbythetotalnumberofwordtokensN:
i i
c
i
P(w)=
i
N
Laplacesmoothingmerelyaddsonetoeachcount(henceitsalternatenameadd-
add-one onesmoothing). SincethereareV wordsinthevocabularyandeachonewasincre-
mented, we also need to adjust the denominator to take into account the extra V
observations. (WhathappenstoourPvaluesifwedon’tincreasethedenominator?)
c +1
i
P Laplace(w i)= (3.21)
N+V
Instead of changing both the numerator and denominator, it is convenient to
describehowasmoothingalgorithmaffectsthenumerator,bydefininganadjusted
countc ∗.ThisadjustedcountiseasiertocomparedirectlywiththeMLEcountsand
canbeturnedintoaprobabilitylikeanMLEcountbynormalizingbyN. Todefine
thiscount, sinceweareonlychangingthenumeratorinadditiontoadding1we’ll
alsoneedtomultiplybyanormalizationfactor N :
N+V
N
c∗i =(c i+1)
N+V
(3.22)
Wecannowturnc intoaprobabilityP bynormalizingbyN.
∗i i∗
discounting A related way to view smoothing is as discounting (lowering) some non-zero
countsinordertogettheprobabilitymassthatwillbeassignedtothezerocounts.
Thus,insteadofreferringtothediscountedcountsc ,wemightdescribeasmooth-
∗
discount ingalgorithmintermsofarelativediscountd c,theratioofthediscountedcountsto
theoriginalcounts:
c
∗
d =
c
c
Nowthatwehavetheintuitionfortheunigramcase,let’ssmoothourBerkeley
RestaurantProjectbigrams. Figure3.6showstheadd-onesmoothedcountsforthe
bigramsinFig.3.1.3.5 • SMOOTHING 45
i want to eat chinese food lunch spend
i 6 828 1 10 1 1 1 3
want 3 1 609 2 7 7 6 2
to 3 1 5 687 3 1 7 212
eat 1 1 3 1 17 3 43 1
chinese 2 1 1 1 1 83 2 1
food 16 1 16 1 2 5 1 1
lunch 3 1 1 1 1 2 1 1
spend 2 1 2 1 1 1 1 1
Figure3.6 Add-onesmoothedbigramcountsforeightofthewords(outofV =1446)in
theBerkeleyRestaurantProjectcorpusof9332sentences.Previously-zerocountsareingray.
Figure3.7showstheadd-onesmoothedprobabilitiesforthebigramsinFig.3.2.
Recall that normal bigram probabilities are computed by normalizing each row of
countsbytheunigramcount:
C(w w )
n 1 n
P(w n w n 1)= − (3.23)
| − C(w n 1)
−
Foradd-onesmoothedbigramcounts,weneedtoaugmenttheunigramcountby
thenumberoftotalwordtypesinthevocabularyV:
C(w w )+1 C(w w )+1
n 1 n n 1 n
P Laplace(w n w n 1)= − = − (3.24)
| − w(C(w n 1w)+1) C(w n 1)+V
− −
Thus, each of the unigram c(cid:80)ounts given in the previous section will need to be
augmentedbyV =1446.TheresultisthesmoothedbigramprobabilitiesinFig.3.7.
i want to eat chinese food lunch spend
i 0.0015 0.21 0.00025 0.0025 0.00025 0.00025 0.00025 0.00075
want 0.0013 0.00042 0.26 0.00084 0.0029 0.0029 0.0025 0.00084
to 0.00078 0.00026 0.0013 0.18 0.00078 0.00026 0.0018 0.055
eat 0.00046 0.00046 0.0014 0.00046 0.0078 0.0014 0.02 0.00046
chinese 0.0012 0.00062 0.00062 0.00062 0.00062 0.052 0.0012 0.00062
food 0.0063 0.00039 0.0063 0.00039 0.00079 0.002 0.00039 0.00039
lunch 0.0017 0.00056 0.00056 0.00056 0.00056 0.0011 0.00056 0.00056
spend 0.0012 0.00058 0.0012 0.00058 0.00058 0.00058 0.00058 0.00058
Figure3.7 Add-one smoothed bigram probabilities for eight of the words (out ofV =1446) in the BeRP
corpusof9332sentences.Previously-zeroprobabilitiesareingray.
Itisoftenconvenienttoreconstructthecountmatrixsowecanseehowmucha
smoothingalgorithmhaschangedtheoriginalcounts. Theseadjustedcountscanbe
computedbyEq.3.25. Figure3.8showsthereconstructedcounts.
[C(w w )+1] C(w )
n 1 n n 1
c∗(w n 1w n)= − × − (3.25)
− C(w n 1)+V
−
Note thatadd-one smoothinghas madea verybig changeto thecounts. Com-
paringFig.3.8totheoriginalcountsinFig.3.1,wecanseethatC(wantto)changed
from608to238! Wecanseethisinprobabilityspaceaswell: P(towant)decreases
|
from .66 in the unsmoothed case to .26 in the smoothed case. Looking at the dis-
countd (theratiobetweennewandoldcounts)showsushowstrikinglythecounts46 CHAPTER3 • N-GRAMLANGUAGEMODELS
i want to eat chinese food lunch spend
i 3.8 527 0.64 6.4 0.64 0.64 0.64 1.9
want 1.2 0.39 238 0.78 2.7 2.7 2.3 0.78
to 1.9 0.63 3.1 430 1.9 0.63 4.4 133
eat 0.34 0.34 1 0.34 5.8 1 15 0.34
chinese 0.2 0.098 0.098 0.098 0.098 8.2 0.2 0.098
food 6.9 0.43 6.9 0.43 0.86 2.2 0.43 0.43
lunch 0.57 0.19 0.19 0.19 0.19 0.38 0.19 0.19
spend 0.32 0.16 0.32 0.16 0.16 0.16 0.16 0.16
Figure3.8 Add-onereconstitutedcountsforeightwords(ofV=1446)intheBeRPcorpus
of9332sentences.Previously-zerocountsareingray.
foreachprefixwordhavebeenreduced;thediscountforthebigramwanttois.39,
whilethediscountforChinesefoodis.10,afactorof10!
Thesharpchangeincountsandprobabilitiesoccursbecausetoomuchprobabil-
itymassismovedtoallthezeros.
3.5.2 Add-ksmoothing
One alternative to add-one smoothing is to move a bit less of the probability mass
fromtheseentotheunseenevents. Insteadofadding1toeachcount,weaddafrac-
add-k tionalcountk(.5? .05? .01?). Thisalgorithmisthereforecalledadd-ksmoothing.
C(w w )+k
n 1 n
P A∗dd-k(w n |w n −1)=
C(w
n−
1)+kV
(3.26)
−
Add-k smoothing requires that we have a method for choosing k; this can be
done, for example, by optimizing on a devset. Although add-k is useful for some
tasks (including text classification), it turns out that it still doesn’t work well for
language modeling, generating counts with poor variances and often inappropriate
discounts(GaleandChurch,1994).
3.5.3 BackoffandInterpolation
Thediscountingwehavebeendiscussingsofarcanhelpsolvetheproblemofzero
frequencyn-grams. Butthereisanadditionalsourceofknowledgewecandrawon.
IfwearetryingtocomputeP(w w w )butwehavenoexamplesofaparticular
n n 2 n 1
| − −
trigram w w w , we can instead estimate its probability by using the bigram
n 2 n 1 n
− −
probabilityP(w w ). Similarly,ifwedon’thavecountstocomputeP(w w ),
n n 1 n n 1
| − | −
wecanlooktotheunigramP(w ).
n
Inotherwords,sometimesusinglesscontextisagoodthing,helpingtogeneral-
izemoreforcontextsthatthemodelhasn’tlearnedmuchabout. Therearetwoways
backoff to use this n-gram “hierarchy”. In backoff, we use the trigram if the evidence is
sufficient,otherwiseweusethebigram,otherwisetheunigram. Inotherwords,we
only“backoff”toalower-ordern-gramifwehavezeroevidenceforahigher-order
interpolation n-gram. By contrast, in interpolation, we always mix the probability estimates
fromallthen-gramestimators, weightingandcombiningthetrigram, bigram, and
unigramcounts.
In simple linear interpolation, we combine different order n-grams by linearly
interpolating them. Thus, we estimate the trigram probability P(w w w ) by
n n 2 n 1
| − −
mixingtogethertheunigram,bigram,andtrigramprobabilities,eachweightedbya3.5 • SMOOTHING 47
λ:
Pˆ(w w w ) = λ P(w )
n n 2 n 1 1 n
| − −
+λ P(w w )
2 n n 1
| −
+λ 3P(w n w n 2w n 1) (3.27)
| − −
Theλsmustsumto1,makingEq.3.27equivalenttoaweightedaverage:
λ i=1 (3.28)
i
(cid:88)
In a slightly more sophisticated version of linear interpolation, each λ weight is
computedbyconditioningonthecontext. Thisway,ifwehaveparticularlyaccurate
countsforaparticularbigram, weassumethatthecountsofthetrigramsbasedon
this bigram will be more trustworthy, so we can make the λs for those trigrams
higher and thus give that trigram more weight in the interpolation. Equation 3.29
showstheequationforinterpolationwithcontext-conditionedweights:
Pˆ(w w w ) = λ (w )P(w )
n n 2 n 1 1 n 2:n 1 n
| − − − −
+λ (w )P(w w )
2 n 2:n 1 n n 1
− − | −
+λ 3(w n 2:n 1)P(w n w n 2w n 1) (3.29)
− − | − −
Howaretheseλ valuesset? Boththesimpleinterpolationandconditionalinterpo-
held-out lation λs are learned from a held-out corpus. A held-out corpus is an additional
trainingcorpus,so-calledbecauseweholditoutfromthetrainingdata,thatweuse
tosethyperparametersliketheseλ values. Wedosobychoosingtheλ valuesthat
maximizethelikelihoodoftheheld-outcorpus. Thatis,wefixthen-gramprobabil-
itiesandthensearchfortheλ valuesthat—whenpluggedintoEq.3.27—giveusthe
highestprobabilityoftheheld-outset. Therearevariouswaystofindthisoptimal
setofλs. OnewayistousetheEMalgorithm,aniterativelearningalgorithmthat
convergesonlocallyoptimalλs(JelinekandMercer,1980).
Inabackoffn-grammodel,ifthen-gramweneedhaszerocounts,weapprox-
imateitbybackingofftothe(n-1)-gram. Wecontinuebackingoffuntilwereacha
historythathassomecounts.
Inorderforabackoffmodeltogiveacorrectprobabilitydistribution, wehave
discount to discount the higher-order n-grams to save some probability mass for the lower
order n-grams. Just as with add-one smoothing, if the higher-order n-grams aren’t
discountedandwejustusedtheundiscountedMLEprobability,thenassoonaswe
replacedann-gramwhichhaszeroprobabilitywithalower-ordern-gram,wewould
beaddingprobabilitymass,andthetotalprobabilityassignedtoallpossiblestrings
bythelanguagemodelwouldbegreaterthan1! Inadditiontothisexplicitdiscount
factor,we’llneedafunctionα todistributethisprobabilitymasstothelowerorder
n-grams.
Katzbackoff ThiskindofbackoffwithdiscountingisalsocalledKatzbackoff.InKatzback-
offwerelyonadiscountedprobabilityP ifwe’veseenthisn-grambefore(i.e.,if
∗
wehavenon-zerocounts). Otherwise,werecursivelybackofftotheKatzprobabil-
ityfortheshorter-history(n-1)-gram. Theprobabilityforabackoffn-gramP is
BO
thuscomputedasfollows:
P (w w ), ifC(w )>0
∗ n n N+1:n 1 n N+1:n
P BO(w n |w n −N+1:n −1)= α(w
n
| N+−
1:n
1)P− BO(w
n
w
n N+2:n
1), otherwi− se. (3.30)
 − − | − −
48 CHAPTER3 • N-GRAMLANGUAGEMODELS
Good-Turing KatzbackoffisoftencombinedwithasmoothingmethodcalledGood-Turing.
ThecombinedGood-Turingbackoffalgorithminvolvesquitedetailedcomputation
forestimatingtheGood-TuringsmoothingandtheP andα values.
∗
3.6 Huge Language Models and Stupid Backoff
By using text from the web or other enormous collections, it is possible to build
extremely large language models. The Web 1 Trillion 5-gram corpus released by
Googleincludesvariouslargesetsofn-grams, including1-gramsthrough5-grams
from all the five-word sequences that appear in at least 40 distinct books from
1,024,908,267,229 words of text from publicly accessible Web pages in English
(FranzandBrants,2006). GooglehasalsoreleasedGoogleBooksNgramscorpora
with n-grams drawn from their book collections, including another 800 billion to-
kensofn-gramsfromChinese,English,French,German,Hebrew,Italian,Russian,
and Spanish (Lin et al., 2012a). Smaller but more carefully curated n-gram cor-
poraforEnglishincludethemillionmostfrequentn-gramsdrawnfromtheCOCA
(Corpus of Contemporary American English) 1 billion word corpus of American
English (Davies, 2020). COCA is a balanced corpus, meaning that it has roughly
equalnumbersofwordsfromdifferentgenres: web,newspapers,spokenconversa-
tion transcripts, fiction, and so on, drawn from the period 1990-2019, and has the
contextofeachn-gramaswellaslabelsforgenreandprovenance.
Someexample4-gramsfromtheGoogleWebcorpus:
4-gram Count
serveastheincoming 92
serveastheincubator 99
serveastheindependent 794
serveastheindex 223
serveastheindication 72
serveastheindicator 120
serveastheindicators 45
Efficiencyconsiderationsareimportantwhenbuildinglanguagemodelsthatuse
such large sets of n-grams. Rather than store each word as a string, it is generally
represented in memoryas a 64-bit hashnumber, with the wordsthemselves stored
ondisk. Probabilitiesaregenerallyquantizedusingonly4-8bits(insteadof8-byte
floats),andn-gramsarestoredinreversetries.
An n-gram language model can also be shrunk by pruning, for example only
storingn-gramswithcountsgreaterthansomethreshold(suchasthecountthreshold
of40usedfortheGooglen-gramrelease)orusingentropytopruneless-important
n-grams (Stolcke,1998). Another option isto build approximatelanguage models
Bloomfilters usingtechniqueslikeBloomfilters(TalbotandOsborne2007,Churchetal.2007).
Finally,efficientlanguagemodeltoolkitslikeKenLM(Heafield2011,Heafieldetal.
2013) use sorted arrays, efficiently combine probabilities and backoffs in a single
value, and use merge sorts to efficiently build the probability tables in a minimal
numberofpassesthroughalargecorpus.
Although with these toolkits it is possible to build web-scale language models
usingadvancedsmoothingalgorithmsliketheKneser-Neyalgorithmwewillseein
Section3.7,Brantsetal.(2007)showthatwithverylargelanguagemodelsamuch
stupidbackoff simpleralgorithmmaybesufficient. Thealgorithmiscalledstupidbackoff. Stupid
backoffgivesuptheideaoftryingtomakethelanguagemodelatrueprobabilitydis-3.7 • ADVANCED: KNESER-NEYSMOOTHING 49
tribution. Thereisnodiscountingofthehigher-orderprobabilities. Ifahigher-order
n-gramhasazerocount,wesimplybackofftoalowerordern-gram,weighedbya
fixed(context-independent)weight. Thisalgorithmdoesnotproduceaprobability
distribution,sowe’llfollowBrantsetal.(2007)inreferringtoitasS:
count(w )
i N+1:i
S(w i |w i −N+1:i −1)=  count(w i −− N+1:i −1) ifcount(w i −N+1:i)>0 (3.31)
λS(w w ) otherwise
 i i N+2:i 1
| − −
Thebackoffterminates intheunigram,whichhasscoreS(w)= count(w) .Brantsetal.
N
(2007)findthatavalueof0.4workedwellforλ.
3.7 Advanced: Kneser-Ney Smoothing
Kneser-Ney A popular advanced n-gram smoothing method is the interpolated Kneser-Ney al-
gorithm(KneserandNey1995,ChenandGoodman1998).
3.7.1 AbsoluteDiscounting
Kneser-Neyhasitsrootsinamethodcalledabsolutediscounting. Recallthatdis-
counting of the counts for frequent n-grams is necessary to save some probability
massforthesmoothingalgorithmtodistributetotheunseenn-grams.
To see this, we can use a clever idea from Church and Gale (1991). Consider
an n-gram that has count 4. We need to discount this count by some amount. But
how much should we discount it? Church and Gale’s clever idea was to look at a
held-out corpus and just see what the count is for all those bigrams that had count
4 in the training set. They computed a bigram grammar from 22 million words of
AP newswire and then checked the counts of each of these bigrams in another 22
million words. On average, a bigram that occurred 4 times in the first 22 million
wordsoccurred3.23timesinthenext22millionwords. Fig.3.9fromChurchand
Gale(1991)showsthesecountsforbigramswithcfrom0to9.
Bigramcountin Bigramcountin
trainingset heldoutset
0 0.0000270
1 0.448
2 1.25
3 2.24
4 3.23
5 4.21
6 5.23
7 6.21
8 7.21
9 8.26
Figure3.9 Forallbigramsin22millionwordsofAPnewswireofcount0,1,2,...,9,the
countsofthesebigramsinaheld-outcorpusalsoof22millionwords.
Notice in Fig. 3.9 that except for the held-out counts for 0 and 1, all the other
bigramcountsintheheld-outsetcouldbeestimatedprettywellbyjustsubtracting50 CHAPTER3 • N-GRAMLANGUAGEMODELS
absolute 0.75fromthecountinthetrainingset! Absolutediscountingformalizesthisintu-
discounting
itionbysubtractingafixed(absolute)discountd fromeachcount. Theintuitionis
thatsincewehavegoodestimatesalreadyfortheveryhighcounts,asmalldiscount
d won’taffectthemmuch. Itwillmainlymodifythesmallercounts,forwhichwe
don’tnecessarilytrusttheestimateanyway,andFig.3.9suggeststhatinpracticethis
discountisactuallyagoodoneforbigramswithcounts2through9. Theequation
forinterpolatedabsolutediscountingappliedtobigrams:
C(w w) d
P AbsoluteDiscounting(w i |w i −1)= vi C− (1
w
ii 1−
v)
+λ(w i −1)P(w i) (3.32)
−
Thefirsttermisthediscountedbigram,(cid:80)with0 d 1,andthesecondtermisthe
≤ ≤
unigramwithaninterpolationweightλ. ByinspectionofFig.3.9,itlookslikejust
settingallthed valuesto.75wouldworkverywell,orperhapskeepingaseparate
seconddiscountvalueof0.5forthebigramswithcountsof1. Thereareprincipled
methodsforsettingd; forexample, Neyetal.(1994)setd asafunctionofn and
1
n ,thenumberofunigramsthathaveacountof1andacountof2,respectively:
2
n
1
d= (3.33)
n +2n
1 2
3.7.2 Kneser-NeyDiscounting
Kneser-Ney discounting (Kneser and Ney, 1995) augments absolute discounting
withamoresophisticatedwaytohandlethelower-orderunigramdistribution. Con-
siderthejobofpredictingthenextwordinthissentence,assumingweareinterpo-
latingabigramandaunigrammodel.
Ican’tseewithoutmyreading .
The word glasses seems much more likely to follow here than, say, the word
Kong,sowe’dlikeourunigrammodeltopreferglasses. Butinfactit’sKongthatis
morecommon,sinceHongKongisaveryfrequentword.Astandardunigrammodel
will assign Kong a higher probability than glasses. We would like to capture the
intuitionthatalthoughKongisfrequent,itismainlyonlyfrequentinthephraseHong
Kong,thatis,afterthewordHong. Thewordglasseshasamuchwiderdistribution.
In other words, instead of P(w), which answers the question “How likely is
w?”,we’dliketocreateaunigrammodelthatwemightcallP ,which
CONTINUATION
answersthequestion“Howlikelyiswtoappearasanovelcontinuation?”. Howcan
weestimatethisprobabilityofseeingthewordwasanovelcontinuation,inanew
unseencontext?TheKneser-NeyintuitionistobaseourestimateofP
CONTINUATION
onthenumberofdifferentcontextswordwhasappearedin, thatis, thenumberof
bigramtypesitcompletes.Everybigramtypewasanovelcontinuationthefirsttime
itwasseen. Wehypothesizethatwordsthathaveappearedinmorecontextsinthe
pastaremorelikelytoappearinsomenewcontextaswell. Thenumberoftimesa
wordwappearsasanovelcontinuationcanbeexpressedas:
P CONTINUATION(w)∝ v:C(vw)>0 (3.34)
|{ }|
To turn this count into a probability, we normalize by the total number of word
bigramtypes. Insummary:
v:C(vw)>0
P CONTINUATION(w)= |{ }| (3.35)
(u,w):C(uw)>0
|{ (cid:48) (cid:48) (cid:48) (cid:48) }|3.7 • ADVANCED: KNESER-NEYSMOOTHING 51
An equivalent formulation based on a different metaphor is to use the number of
wordtypesseentoprecedew(Eq.3.34repeated):
P CONTINUATION(w)∝ v:C(vw)>0 (3.36)
|{ }|
normalizedbythenumberofwordsprecedingallwords,asfollows:
v:C(vw)>0
P CONTINUATION(w)= |{ }| (3.37)
v:C(vw)>0
w (cid:48)|{ (cid:48) }|
Afrequentword(Kong)occurringinon(cid:80)lyonecontext(Hong)willhavealowcon-
tinuationprobability.
Interpolated ThefinalequationforInterpolatedKneser-Neysmoothingforbigramsisthen:
Kneser-Ney
max(C(w w) d,0)
P KN(w i |w i −1)= C(i w−1
i
1i )− +λ(w i −1)P CONTINUATION(w i) (3.38)
−
Theλ isanormalizingconstantthatisusedtodistributetheprobabilitymasswe’ve
discounted:
d
λ(w i 1)= w:C(w i 1w)>0 (3.39)
− vC(w i 1v)|{ − }|
−
d (cid:80)
The first term, , is the normalized discount (the discount d, 0 d
C(w v) ≤ ≤
v i 1
1, was introduced in the−absolute discounting section above). The second term,
(cid:80)
w:C(w w)>0 ,isthenumberofwordtypesthatcanfolloww or,equiva-
i 1 i 1
|{ − }| −
lently,thenumberofwordtypesthatwediscounted;inotherwords,thenumberof
timesweappliedthenormalizeddiscount.
Thegeneralrecursiveformulationisasfollows:
max(c (w ) d,0)
P KN(w i |w i −n+1:i −1)= vcK KN N(wi − in n+ +1 1:i
:i
−
1v)
+λ(w i −n+1:i −1)P KN(w i |w i −n+2:i −1) (3.40)
− −
where the de(cid:80)finition of the count c
KN
depends on whether we are counting the
highest-ordern-grambeinginterpolated(forexampletrigramifweareinterpolating
trigram,bigram,andunigram)oroneofthelower-ordern-grams(bigramorunigram
ifweareinterpolatingtrigram,bigram,andunigram):
count() forthehighestorder
c KN( ·)= continu·
ationcount() forlowerorders
(3.41)
(cid:26) ·
Thecontinuationcountofastring isthenumberofuniquesinglewordcontextsfor
·
thatstring .
·
Attheterminationoftherecursion,unigramsareinterpolatedwiththeuniform
distribution,wheretheparameter(cid:15)istheemptystring:
max(c (w) d,0) 1
KN
P KN(w)=
c
(−
w)
+λ((cid:15))
V
(3.42)
w KN (cid:48)
(cid:48)
If we want to include an unknown(cid:80)word <UNK>, it’s just included as a regular vo-
cabularyentrywithcountzero,andhenceitsprobabilitywillbealambda-weighted
uniformdistribution
λ((cid:15))
.
V
ThebestperformingversionofKneser-NeysmoothingiscalledmodifiedKneser-
modified Neysmoothing,andisduetoChenandGoodman(1998). Ratherthanuseasingle
Kneser-Ney
fixed discount d, modified Kneser-Ney uses three different discounts d , d , and
1 2
d forn-gramswithcountsof1,2andthreeormore,respectively. SeeChenand
3+
Goodman(1998,p. 19)orHeafieldetal.(2013)forthedetails.52 CHAPTER3 • N-GRAMLANGUAGEMODELS
3.8 Advanced: Perplexity’s Relation to Entropy
We introduced perplexity in Section 3.2.1 as a way to evaluate n-gram models on
a test set. A better n-gram model is one that assigns a higher probability to the
test data, and perplexity is a normalized version of the probability of the test set.
The perplexity measure actually arises from the information-theoretic concept of
cross-entropy, which explains otherwise mysterious properties of perplexity (why
Entropy theinverseprobability,forexample?) anditsrelationshiptoentropy. Entropyisa
measureofinformation. GivenarandomvariableX rangingoverwhateverweare
predicting(words,letters,partsofspeech,thesetofwhichwe’llcallχ)andwitha
particularprobabilityfunction,callit p(x),theentropyoftherandomvariableX is:
H(X)= p(x)log p(x) (3.43)
− 2
x χ
(cid:88)∈
The log can, in principle, be computed in any base. If we use log base 2, the
resultingvalueofentropywillbemeasuredinbits.
Oneintuitivewaytothinkaboutentropyisasalowerboundonthenumberof
bitsitwouldtaketoencodeacertaindecisionorpieceofinformationintheoptimal
codingscheme.
ConsideranexamplefromthestandardinformationtheorytextbookCoverand
Thomas (1991). Imagine that we want to place a bet on a horse race but it is too
fartogoallthewaytoYonkersRacetrack,sowe’dliketosendashortmessageto
thebookietotellhimwhichoftheeighthorsestobeton. Onewaytoencodethis
messageisjusttousethebinaryrepresentationofthehorse’snumberasthecode;
thus,horse1wouldbe001,horse2010,horse3011,andsoon,withhorse8coded
as000. Ifwespendthewholedaybettingandeachhorseiscodedwith3bits, on
averagewewouldbesending3bitsperrace.
Canwedobetter? Supposethatthespreadistheactualdistributionofthebets
placedandthatwerepresentitasthepriorprobabilityofeachhorseasfollows:
Horse1 1 Horse5 1
2 64
Horse2 1 Horse6 1
4 64
Horse3 1 Horse7 1
8 64
Horse4 1 Horse8 1
16 64
The entropy of the random variable X that ranges over horses gives us a lower
boundonthenumberofbitsandis
i=8
H(X) = p(i)logp(i)
−
i=1
(cid:88)
= 1log1 1log1 1log1 1 log 1 4(1 log 1)
−2 2−4 4−8 8−16 16− 64 64
= 2bits (3.44)
Acodethataverages2bitsperracecanbebuiltwithshortencodingsformore
probable horses, and longer encodings for less probable horses. For example, we
couldencodethemostlikelyhorsewiththecode0,andtheremaininghorsesas10,
then110,1110,111100,111101,111110,and111111.3.8 • ADVANCED: PERPLEXITY’SRELATIONTOENTROPY 53
What if the horses are equally likely? We saw above that if we used an equal-
length binary code for the horse numbers, each horse took 3 bits to code, so the
average was 3. Is the entropy the same? In this case each horse would have a
probabilityof 1. Theentropyofthechoiceofhorsesisthen
8
i=8
1 1 1
H(X)= log = log =3bits (3.45)
− 8 8 − 8
i=1
(cid:88)
Until now we have been computing the entropy of a single variable. But most
of what we will use entropy for involves sequences. For a grammar, for example,
wewillbecomputingtheentropyofsomesequenceofwordsW = w ,w ,...,w .
1 2 n
{ }
One way to do this is to have a variable that ranges over sequences of words. For
examplewecancomputetheentropyofarandomvariablethatrangesoverallfinite
sequencesofwordsoflengthninsomelanguageLasfollows:
H(w ,w ,...,w )= p(w )logp(w ) (3.46)
1 2 n 1:n 1:n
−
w (cid:88)1:n∈L
entropyrate Wecoulddefinetheentropyrate(wecouldalsothinkofthisastheper-word
entropy)astheentropyofthissequencedividedbythenumberofwords:
1 1
H(w )= p(w )logp(w ) (3.47)
1:n 1:n 1:n
n −n
w (cid:88)1:n∈L
Buttomeasurethetrueentropyofalanguage,weneedtoconsidersequencesof
infinite length. If we think of a language as a stochastic process L that produces a
sequenceofwords,andallowW torepresentthesequenceofwordsw ,...,w ,then
1 n
L’sentropyrateH(L)isdefinedas
1
H(L) = lim H(w ,w ,...,w )
1 2 n
n ∞n
→
1
= lim p(w 1,...,w n)logp(w 1,...,w n) (3.48)
−n ∞n
→ W L
(cid:88)∈
The Shannon-McMillan-Breiman theorem (Algoet and Cover 1988, Cover and
Thomas1991)statesthatifthelanguageisregularincertainways(tobeexact,ifit
isbothstationaryandergodic),
1
H(L)= lim logp(w w ...w ) (3.49)
1 2 n
n ∞−n
→
Thatis, wecantakeasinglesequencethatislongenoughinsteadofsumming
over all possible sequences. The intuition of the Shannon-McMillan-Breiman the-
oremisthatalong-enoughsequenceofwordswillcontaininitmanyothershorter
sequences and that each of these shorter sequences will reoccur in the longer se-
quenceaccordingtotheirprobabilities.
Stationary A stochastic process is said to be stationary if the probabilities it assigns to a
sequence are invariant with respect to shifts in the time index. In other words, the
probabilitydistributionforwordsattimet isthesameastheprobabilitydistribution
at timet+1. Markov models, and hence n-grams, are stationary. For example, in
a bigram, P is dependent only on P . So if we shift our time index by x, P is
i i 1 i+x
−
stilldependentonP . Butnaturallanguageisnotstationary,sinceasweshow
i+x 1
−54 CHAPTER3 • N-GRAMLANGUAGEMODELS
inAppendixD,theprobabilityofupcomingwordscanbedependentoneventsthat
were arbitrarily distant and time dependent. Thus, our statistical models only give
anapproximationtothecorrectdistributionsandentropiesofnaturallanguage.
To summarize, by making some incorrect but convenient simplifying assump-
tions,wecancomputetheentropyofsomestochasticprocessbytakingaverylong
sampleoftheoutputandcomputingitsaveragelogprobability.
cross-entropy Nowwearereadytointroducecross-entropy.Thecross-entropyisusefulwhen
we don’t know the actual probability distribution p that generated some data. It
allowsustousesomem, whichisamodelof p(i.e., anapproximationto p). The
cross-entropyofmon pisdefinedby
1
H(p,m)= lim p(w ,...,w )logm(w ,...,w ) (3.50)
1 n 1 n
n ∞−n
→ W L
(cid:88)∈
Thatis,wedrawsequencesaccordingtotheprobabilitydistribution p,butsum
thelogoftheirprobabilitiesaccordingtom.
Again, followingtheShannon-McMillan-Breimantheorem, forastationaryer-
godicprocess:
1
H(p,m)= lim logm(w w ...w ) (3.51)
1 2 n
n ∞−n
→
This means that, as for entropy, we can estimate the cross-entropy of a model
monsomedistribution pbytakingasinglesequencethatislongenoughinsteadof
summingoverallpossiblesequences.
Whatmakesthecross-entropyusefulisthatthecross-entropyH(p,m)isanup-
perboundontheentropyH(p). Foranymodelm:
H(p) H(p,m) (3.52)
≤
Thismeansthatwecanusesomesimplifiedmodelmtohelpestimatethetrueen-
tropyofasequenceofsymbolsdrawnaccordingtoprobabilityp.Themoreaccurate
m is, the closer the cross-entropy H(p,m) will be to the true entropy H(p). Thus,
thedifferencebetweenH(p,m)andH(p)isameasureofhowaccurateamodelis.
Betweentwomodelsm andm ,themoreaccuratemodelwillbetheonewiththe
1 2
lowercross-entropy. (Thecross-entropycanneverbelowerthanthetrueentropy,so
amodelcannoterrbyunderestimatingthetrueentropy.)
We are finally ready to see the relation between perplexity and cross-entropy
as we saw it in Eq. 3.51. Cross-entropy is defined in the limit as the length of the
observedwordsequencegoestoinfinity. Wewillneedanapproximationtocross-
entropy, relyingona(sufficientlylong)sequenceoffixedlength. Thisapproxima-
tiontothecross-entropyofamodelM=P(w w )onasequenceofwords
i i N+1:i 1
| − −
W is
1
H(W)= logP(w w ...w ) (3.53)
1 2 N
−N
perplexity TheperplexityofamodelPonasequenceofwordsW isnowformallydefinedas
2raisedtothepowerofthiscross-entropy:3.9 • SUMMARY 55
Perplexity(W) = 2H(W)
1
= P(w 1w 2...w N) −N
1
= N
(cid:115)P(w 1w 2...w N)
N
1
= N (3.54)
(cid:118) P(w w ...w )
(cid:117) (cid:117)(cid:89)i=1 i | 1 i −1
(cid:116)
3.9 Summary
Thischapterintroducedlanguagemodelingandthen-gram,oneofthemostwidely
usedtoolsinlanguageprocessing.
• Language models offer a way to assign a probability to a sentence or other
sequenceofwords,andtopredictawordfromprecedingwords.
• n-gramsareMarkovmodelsthatestimatewordsfromafixedwindowofpre-
vious words. n-gram probabilities can be estimated by counting in a corpus
andnormalizing(themaximumlikelihoodestimate).
• n-gramlanguagemodelsareevaluatedextrinsicallyinsometask,orintrinsi-
callyusingperplexity.
• The perplexity of a test set according to a language model is the geometric
meanoftheinversetestsetprobabilitycomputedbythemodel.
• Smoothingalgorithmsprovideamoresophisticatedwaytoestimatetheprob-
abilityofn-grams.Commonlyusedsmoothingalgorithmsforn-gramsrelyon
lower-ordern-gramcountsthroughbackofforinterpolation.
• Bothbackoffandinterpolationrequirediscountingtocreateaprobabilitydis-
tribution.
• Kneser-Neysmoothingmakesuseoftheprobabilityofawordbeinganovel
continuation. The interpolated Kneser-Ney smoothing algorithm mixes a
discountedprobabilitywithalower-ordercontinuationprobability.
Bibliographical and Historical Notes
The underlying mathematics of the n-gram was first proposed by Markov (1913),
who used what are now called Markov chains (bigrams and trigrams) to predict
whetheranupcomingletterinPushkin’sEugeneOneginwouldbeavoweloracon-
sonant. Markov classified 20,000 letters as V or C and computed the bigram and
trigram probability that a given letter would be a vowel given the previous one or
twoletters. Shannon(1948)appliedn-gramstocomputeapproximationstoEnglish
wordsequences.BasedonShannon’swork,Markovmodelswerecommonlyusedin
engineering,linguistic,andpsychologicalworkonmodelingwordsequencesbythe
1950s. InaseriesofextremelyinfluentialpapersstartingwithChomsky(1956)and
includingChomsky(1957)andMillerandChomsky(1963),NoamChomskyargued
that “finite-state Markov processes”, while a possibly useful engineering heuristic,56 CHAPTER3 • N-GRAMLANGUAGEMODELS
wereincapableofbeingacompletecognitivemodelofhumangrammaticalknowl-
edge. These arguments led many linguists and computational linguists to ignore
workinstatisticalmodelingfordecades.
Theresurgenceofn-grammodelscamefromJelinekandcolleaguesattheIBM
Thomas J. Watson Research Center, who were influenced by Shannon, and Baker
atCMU,whowasinfluencedbytheworkofBaumandcolleagues. Independently
thesetwolabssuccessfullyusedn-gramsintheirspeechrecognitionsystems(Baker
1975b,Jelinek1976,Baker1975a,Bahletal.1983,Jelinek1990).
Add-onesmoothingderivesfromLaplace’s1812lawofsuccessionandwasfirst
appliedasanengineeringsolutiontothezerofrequencyproblembyJeffreys(1948)
based on an earlier Add-K suggestion by Johnson (1932). Problems with the add-
onealgorithmaresummarizedinGaleandChurch(1994).
Awidevarietyofdifferentlanguagemodelingandsmoothingtechniqueswere
proposed in the 80s and 90s, including Good-Turing discounting—first applied to
the n-gram smoothing at IBM by Katz (Na´das 1984, Church and Gale 1991)—
Witten-Bell discounting (Witten and Bell, 1991), and varieties of class-based n-
class-based grammodelsthatusedinformationaboutwordclasses.
n-gram
Startinginthelate1990s,ChenandGoodmanperformedanumberofcarefully
controlled experiments comparing different discounting algorithms, cache models,
class-based models, and other language model parameters (Chen and Goodman
1999, Goodman 2006, inter alia). They showed the advantages of Modified In-
terpolated Kneser-Ney, which became the standard baseline for n-gram language
modeling,especiallybecausetheyshowedthatcachesandclass-basedmodelspro-
videdonlyminoradditionalimprovement. Thesepapersarerecommendedforany
reader with further interest in n-gram language modeling. SRILM (Stolcke, 2002)
andKenLM(Heafield2011,Heafieldetal.2013)arepubliclyavailabletoolkitsfor
buildingn-gramlanguagemodels.
Modernlanguagemodelingismorecommonlydonewithneuralnetworklan-
guagemodels,whichsolvethemajorproblemswithn-grams:thenumberofparam-
eters increases exponentially as the n-gram order increases, and n-grams have no
waytogeneralizefromtrainingtotestset. Neurallanguagemodelsinsteadproject
words into a continuous space in which words with similar contexts have similar
representations. We’llintroducebothfeedforwardlanguagemodels(Bengioetal.
2006,Schwenk2007)inChapter7,andrecurrentlanguagemodels(Mikolov,2012)
inChapter9.
Exercises
3.1 Writeouttheequationfortrigramprobabilityestimation(modifyingEq.3.11).
Nowwriteoutallthenon-zerotrigramprobabilitiesfortheI am Samcorpus
onpage34.
3.2 Calculatetheprobabilityofthesentencei want chinese food. Givetwo
probabilities,oneusingFig.3.2andthe‘usefulprobabilities’justbelowiton
page36,andanotherusingtheadd-1smoothedtableinFig.3.7. Assumethe
additionaladd-1smoothedprobabilitiesP(i|<s>)=0.19andP(</s>|food)=
0.40.
3.3 Whichofthetwoprobabilitiesyoucomputedinthepreviousexerciseishigher,
unsmoothedorsmoothed? Explainwhy.
3.4 Wearegiventhefollowingcorpus,modifiedfromtheoneinthechapter:EXERCISES 57
<s> I am Sam </s>
<s> Sam I am </s>
<s> I am Sam </s>
<s> I do not like green eggs and Sam </s>
Using a bigram language model with add-one smoothing, what is P(Sam
|
am)? Include<s>and</s>inyourcountsjustlikeanyothertoken.
3.5 Suppose we didn’t use the end-symbol </s>. Train an unsmoothed bigram
grammaronthefollowingtrainingcorpuswithoutusingtheend-symbol</s>:
<s> a b
<s> b b
<s> b a
<s> a a
Demonstratethatyourbigrammodeldoesnotassignasingleprobabilitydis-
tributionacrossallsentencelengthsbyshowingthatthesumoftheprobability
ofthefourpossible2wordsentencesoverthealphabet a,b is1.0, andthe
{ }
sumoftheprobabilityofallpossible3wordsentencesoverthealphabet a,b
{ }
isalso1.0.
3.6 Suppose we train a trigram language model with add-one smoothing on a
givencorpus. ThecorpuscontainsVwordtypes. Expressaformulaforesti-
matingP(w3w1,w2),wherew3isawordwhichfollowsthebigram(w1,w2),
|
in terms of various n-gram counts and V. Use the notation c(w1,w2,w3) to
denotethenumberoftimesthattrigram(w1,w2,w3)occursinthecorpus,and
soonforbigramsandunigrams.
3.7 Wearegiventhefollowingcorpus,modifiedfromtheoneinthechapter:
<s> I am Sam </s>
<s> Sam I am </s>
<s> I am Sam </s>
<s> I do not like green eggs and Sam </s>
If we use linear interpolation smoothing between a maximum-likelihood bi-
grammodelandamaximum-likelihoodunigrammodelwithλ = 1 andλ =
1 2 2
1, what is P(Samam)? Include <s> and </s> in your counts just like any
2 |
othertoken.
3.8 Writeaprogramtocomputeunsmoothedunigramsandbigrams.
3.9 Runyourn-gramprogramontwodifferentsmallcorporaofyourchoice(you
might use email text or newsgroups). Now compare the statistics of the two
corpora. Whatarethedifferencesinthemostcommonunigramsbetweenthe
two? Howaboutinterestingdifferencesinbigrams?
3.10 Addanoptiontoyourprogramtogeneraterandomsentences.
3.11 Addanoptiontoyourprogramtocomputetheperplexityofatestset.
3.12 You are given a training set of 100 numbers that consists of 91 zeros and 1
eachoftheotherdigits1-9. Nowweseethefollowingtestset: 00000300
00. Whatistheunigramperplexity?58 CHAPTER4 • NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENT
CHAPTER
4 Naive Bayes and Sentiment
Classification
Classification lies at the heart of both human and machine intelligence. Deciding
what letter, word, or image has been presented to our senses, recognizing faces
or voices, sorting mail, assigning grades to homeworks; these are all examples of
assigningacategorytoaninput.Thepotentialchallengesofthistaskarehighlighted
bythefabulistJorgeLuisBorges(1964),whoimaginedclassifyinganimalsinto:
(a)thosethatbelongtotheEmperor,(b)embalmedones,(c)thosethat
aretrained,(d)sucklingpigs,(e)mermaids,(f)fabulousones,(g)stray
dogs, (h) those that are included in this classification, (i) those that
trembleasiftheyweremad,(j)innumerableones,(k)thosedrawnwith
averyfinecamel’shairbrush,(l)others,(m)thosethathavejustbroken
aflowervase,(n)thosethatresemblefliesfromadistance.
Manylanguageprocessingtasksinvolveclassification,althoughluckilyourclasses
aremucheasiertodefinethanthoseofBorges.Inthischapterweintroducethenaive
text Bayesalgorithmandapplyittotextcategorization,thetaskofassigningalabelor
categorization
categorytoanentiretextordocument.
sentiment Wefocusononecommontextcategorizationtask,sentimentanalysis,theex-
analysis
traction of sentiment, the positive or negative orientation that a writer expresses
towardsomeobject.Areviewofamovie,book,orproductonthewebexpressesthe
author’ssentimenttowardtheproduct,whileaneditorialorpoliticaltextexpresses
sentimenttowardacandidateorpoliticalaction. Extractingconsumerorpublicsen-
timentisthusrelevantforfieldsfrommarketingtopolitics.
The simplest version of sentiment analysis is a binary classification task, and
thewordsofthereviewprovideexcellentcues. Consider,forexample,thefollow-
ingphrasesextractedfrompositiveandnegativereviewsofmoviesandrestaurants.
Wordslikegreat,richly,awesome,andpathetic,andawfulandridiculouslyarevery
informativecues:
+ ...zanycharactersandrichlyappliedsatire,andsomegreatplottwists
Itwaspathetic. Theworstpartaboutitwastheboxingscenes...
−
+ ...awesomecaramelsauceandsweettoastyalmonds. Ilovethisplace!
...awfulpizzaandridiculouslyoverpriced...
−
spamdetection Spam detection is another important commercial application, the binary clas-
sification task of assigning an email to one of the two classes spam or not-spam.
Manylexicalandotherfeaturescanbeusedtoperformthisclassification. Forex-
ampleyoumightquitereasonablybesuspiciousofanemailcontainingphraseslike
“onlinepharmaceutical”or“WITHOUTANYCOST”or“DearWinner”.
Another thing we might want to know about a text is the language it’s written
in. Texts on social media, for example, can be in any number of languages and
languageid we’ll need to apply different processing. The task of language id is thus the first
stepinmostlanguageprocessingpipelines. Relatedtextclassificationtaskslikeau-
authorship thorshipattribution—determiningatext’sauthor—arealsorelevanttothedigital
attribution
humanities,socialsciences,andforensiclinguistics.4.1 • NAIVEBAYESCLASSIFIERS 59
Finally, one of the oldest tasks in text classification is assigning a library sub-
ject category or topic label to a text. Deciding whether a research paper concerns
epidemiologyorinstead,perhaps,embryology,isanimportantcomponentofinfor-
mationretrieval.Varioussetsofsubjectcategoriesexist,suchastheMeSH(Medical
SubjectHeadings)thesaurus. Infact,aswewillsee,subjectcategoryclassification
isthetaskforwhichthenaiveBayesalgorithmwasinventedin1961Maron(1961).
Classification is essential for tasks below the level of the document as well.
We’vealreadyseenperioddisambiguation(decidingifaperiodistheendofasen-
tence or part of a word), and word tokenization (deciding if a character should be
a word boundary). Even language modeling can be viewed as classification: each
wordcanbethoughtofasaclass,andsopredictingthenextwordisclassifyingthe
context-so-farintoaclassforeachnextword. Apart-of-speechtagger(Chapter8)
classifieseachoccurrenceofawordinasentenceas,e.g.,anounoraverb.
The goal of classification is to take a single observation, extract some useful
features, and thereby classify the observation into one of a set of discrete classes.
Onemethodforclassifyingtextistousehandwrittenrules. Therearemanyareasof
language processing where handwritten rule-based classifiers constitute a state-of-
the-artsystem,oratleastpartofit.
Rules can be fragile, however, as situations or data change over time, and for
sometaskshumansaren’tnecessarilygoodatcomingupwiththerules. Mostcases
of classification in language processing are instead done via supervised machine
supervised
machine learning,andthiswillbethesubjectoftheremainderofthischapter. Insupervised
learning
learning,wehaveadatasetofinputobservations,eachassociatedwithsomecorrect
output (a ‘supervision signal’). The goal of the algorithm is to learn how to map
fromanewobservationtoacorrectoutput.
Formally, the task of supervised classification is to take an input x and a fixed
set of output classes Y = y ,y ,...,y and return a predicted class y Y. For
1 2 M
{ } ∈
text classification, we’ll sometimes talk about c (for “class”) instead of y as our
output variable, and d (for “document”) instead of x as our input variable. In the
supervisedsituationwehaveatrainingsetofNdocumentsthathaveeachbeenhand-
labeledwithaclass: (d ,c ),....,(d ,c ) . Ourgoalistolearnaclassifierthatis
1 1 N N
{ }
capable of mapping from a new document d to its correct class c C, whereC is
∈
somesetofusefuldocumentclasses.Aprobabilisticclassifieradditionallywilltell
us the probability of the observation being in the class. This full distribution over
the classes can be useful information for downstream decisions; avoiding making
discretedecisionsearlyoncanbeusefulwhencombiningsystems.
Many kinds of machine learning algorithms are used to build classifiers. This
chapter introduces naive Bayes; the following one introduces logistic regression.
Theseexemplifytwowaysofdoingclassification. Generativeclassifierslikenaive
Bayes build a model of how a class could generate some input data. Given an ob-
servation, theyreturntheclassmostlikelytohavegeneratedtheobservation. Dis-
criminativeclassifierslikelogisticregressioninsteadlearnwhatfeaturesfromthe
inputaremostusefultodiscriminatebetweenthedifferentpossibleclasses. While
discriminative systems are often more accurate and hence more commonly used,
generativeclassifiersstillhavearole.
4.1 Naive Bayes Classifiers
naiveBayes In this section we introduce the multinomial naive Bayes classifier, so called be-
classifier60 CHAPTER4 • NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENT
cause it is a Bayesian classifier that makes a simplifying (naive) assumption about
howthefeaturesinteract.
TheintuitionoftheclassifierisshowninFig.4.1. Werepresentatextdocument
bagofwords as if it were a bag of words, that is, an unordered set of words with their position
ignored,keepingonlytheirfrequencyinthedocument. Intheexampleinthefigure,
insteadofrepresentingthewordorderinallthephraseslike“Ilovethismovie”and
“I would recommend it”, we simply note that the word I occurred 5 times in the
entireexcerpt,thewordit6times,thewordslove,recommend,andmovieonce,and
soon.
it 6
I 5
I love this movie! It's sweet, the 4
but with satirical humor. The fairy always lovetoit to 3
dialogue is great and the it whimsical it I and 3
adventure scenes are fun... and seen are anyone seen 2
friend
It manages to be whimsical happy dialogue yet 1
and romantic while laughing adventure recommend would 1
at the conventions of the whosweet of msa ot vir ieical it whimsical 1
fairy tale genre. I would it I butto romantic I times 1
recommend it to just about several yet sweet 1
humor
anyone. I've seen it several again it the satirical 1
times, and I'm always happy th toe sces ne ee sn I would adventure 1
to see it again whenever I the themanages genre 1
fun times
have a friend who hasn't I and and fairy 1
about
seen it yet! whenever while humor 1
have
conventions have 1
with great 1
… …
Figure4.1 IntuitionofthemultinomialnaiveBayesclassifierappliedtoamoviereview. Thepositionofthe
wordsisignored(thebag-of-wordsassumption)andwemakeuseofthefrequencyofeachword.
NaiveBayesisaprobabilisticclassifier, meaningthatforadocumentd, outof
allclassesc C theclassifierreturnstheclasscˆwhichhasthemaximumposterior
∈
ˆ probabilitygiventhedocument. InEq.4.1weusethehatnotationˆ tomean“our
estimateofthecorrectclass”.
cˆ = argmaxP(cd) (4.1)
|
c C
∈
Bayesian This idea of Bayesian inference has been known since the work of Bayes (1763),
inference
and was first applied to text classification by Mosteller and Wallace (1964). The
intuition of Bayesian classification is to use Bayes’ rule to transform Eq. 4.1 into
other probabilities that have some useful properties. Bayes’ rule is presented in
Eq. 4.2; it gives us a way to break down any conditional probability P(xy) into
|
threeotherprobabilities:
P(yx)P(x)
P(xy)= | (4.2)
| P(y)
WecanthensubstituteEq.4.2intoEq.4.1togetEq.4.3:
P(d c)P(c)
cˆ=argmaxP(cd)=argmax | (4.3)
| P(d)
c C c C
∈ ∈4.1 • NAIVEBAYESCLASSIFIERS 61
WecanconvenientlysimplifyEq.4.3bydroppingthedenominatorP(d). This
ispossiblebecausewewillbecomputing P(d |c)P(c) foreachpossibleclass.ButP(d)
P(d)
doesn’tchangeforeachclass;wearealwaysaskingaboutthemostlikelyclassfor
the same document d, which must have the same probability P(d). Thus, we can
choosetheclassthatmaximizesthissimplerformula:
cˆ=argmaxP(cd)=argmaxP(d c)P(c) (4.4)
| |
c C c C
∈ ∈
WecallNaiveBayesagenerativemodelbecausewecanreadEq.4.4asstating
a kind of implicit assumption about how a document is generated: first a class is
sampledfromP(c),andthenthewordsaregeneratedbysamplingfromP(d c). (In
|
factwecouldimaginegeneratingartificialdocuments,oratleasttheirwordcounts,
byfollowingthisprocess). We’llsaymoreaboutthisintuitionofgenerativemodels
inChapter5.
To return to classification: we compute the most probable class cˆ given some
documentdbychoosingtheclasswhichhasthehighestproductoftwoprobabilities:
prior thepriorprobabilityoftheclassP(c)andthelikelihoodofthedocumentP(d c):
probability |
likelihood
likelihood prior
cˆ=argmax P(d c) P(c) (4.5)
|
c C
∈ (cid:122) (cid:125)(cid:124) (cid:123) (cid:122)(cid:125)(cid:124)(cid:123)
Withoutlossofgeneralization,wecanrepresentadocumentdasasetoffeatures
f ,f ,...,f :
1 2 n
likelihood prior
cˆ=argmaxP(f ,f ,....,f c) P(c) (4.6)
1 2 n
|
c C
∈ (cid:122) (cid:125)(cid:124) (cid:123) (cid:122)(cid:125)(cid:124)(cid:123)
Unfortunately, Eq. 4.6 is still too hard to compute directly: without some sim-
plifying assumptions, estimating the probability of every possible combination of
features(forexample,everypossiblesetofwordsandpositions)wouldrequirehuge
numbers of parameters and impossibly large training sets. Naive Bayes classifiers
thereforemaketwosimplifyingassumptions.
Thefirstisthebag-of-wordsassumptiondiscussedintuitivelyabove:weassume
positiondoesn’tmatter,andthattheword“love”hasthesameeffectonclassification
whether it occurs as the 1st, 20th, or last word in the document. Thus we assume
thatthefeatures f ,f ,...,f onlyencodewordidentityandnotposition.
1 2 n
naiveBayes ThesecondiscommonlycalledthenaiveBayesassumption: thisisthecondi-
assumption
tionalindependenceassumptionthattheprobabilitiesP(f c)areindependentgiven
i
|
theclasscandhencecanbe‘naively’multipliedasfollows:
P(f 1,f 2,....,f n c) = P(f 1 c) P(f 2 c) ... P(f n c) (4.7)
| | · | · · |
ThefinalequationfortheclasschosenbyanaiveBayesclassifieristhus:
c NB = argmaxP(c) P(f c) (4.8)
|
c C
∈ (cid:89)f ∈F
ToapplythenaiveBayesclassifiertotext, weneedtoconsiderwordpositions, by
simplywalkinganindexthrougheverywordpositioninthedocument:
positions allwordpositionsintestdocument
←
c NB = argmaxP(c) P(w i c) (4.9)
|
c C
∈ i ∈p (cid:89)ositions62 CHAPTER4 • NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENT
NaiveBayescalculations, likecalculationsforlanguagemodeling, aredoneinlog
space, to avoid underflow and increase speed. Thus Eq. 4.9 is generally instead
expressedas
c NB = argmaxlogP(c)+ logP(w i c) (4.10)
|
c C
∈ i ∈p (cid:88)ositions
Byconsideringfeaturesinlogspace,Eq.4.10computesthepredictedclassasalin-
earfunctionofinputfeatures. Classifiersthatusealinearcombinationoftheinputs
to make a classification decision —like naive Bayes and also logistic regression—
linear arecalledlinearclassifiers.
classifiers
4.2 Training the Naive Bayes Classifier
HowcanwelearntheprobabilitiesP(c)andP(f c)? Let’sfirstconsiderthemaxi-
i
|
mumlikelihoodestimate. We’llsimplyusethefrequenciesinthedata. Fortheclass
priorP(c)weaskwhatpercentageofthedocumentsinourtrainingsetareineach
class c. Let N be the number of documents in our training data with class c and
c
N bethetotalnumberofdocuments. Then:
doc
N
Pˆ(c)= c (4.11)
N
doc
TolearntheprobabilityP(f c),we’llassumeafeatureisjusttheexistenceofaword
i
|
in the document’s bag of words, and so we’ll want P(w c), which we compute as
i
|
thefractionoftimesthewordw appearsamongallwordsinalldocumentsoftopic
i
c. Wefirstconcatenatealldocumentswithcategorycintoonebig“categoryc”text.
Thenweusethefrequencyofw inthisconcatenateddocumenttogiveamaximum
i
likelihoodestimateoftheprobability:
count(w,c)
Pˆ(w i c) = i (4.12)
| count(w,c)
w V
∈
HerethevocabularyVconsistsoftheu(cid:80)nionofallthewordtypesinallclasses,not
justthewordsinoneclassc.
There is a problem, however, with maximum likelihood training. Imagine we
aretryingtoestimatethelikelihoodoftheword“fantastic”givenclasspositive,but
supposetherearenotrainingdocumentsthatbothcontaintheword“fantastic”and
are classified as positive. Perhaps the word “fantastic” happens to occur (sarcasti-
cally?) intheclass negative. Insuchacasetheprobabilityforthisfeaturewillbe
zero:
count(“fantastic”,positive)
Pˆ(“fantastic”positive) = =0 (4.13)
| count(w,positive)
w V
∈
But since naive Bayes naively multiplies(cid:80)all the feature likelihoods together, zero
probabilities in the likelihood term for any class will cause the probability of the
classtobezero,nomattertheotherevidence!
The simplest solution is the add-one (Laplace) smoothing introduced in Chap-
ter3.WhileLaplacesmoothingisusuallyreplacedbymoresophisticatedsmoothing4.2 • TRAININGTHENAIVEBAYESCLASSIFIER 63
algorithms in language modeling, it is commonlyused in naive Bayes text catego-
rization:
count(w,c)+1 count(w,c)+1
Pˆ(w i c) = i = i (4.14)
| (count(w,c)+1) count(w,c) + V
w ∈V w ∈V | |
(cid:80) (cid:0)(cid:80) (cid:1)
NoteonceagainthatitiscrucialthatthevocabularyVconsistsoftheunionofallthe
wordtypesinallclasses,notjustthewordsinoneclassc(trytoconvinceyourself
whythismustbetrue;seetheexerciseattheendofthechapter).
Whatdowedoaboutwordsthatoccurinourtestdatabutarenotinourvocab-
ularyatallbecausetheydidnotoccurinanytrainingdocumentinanyclass? The
unknownword solution for such unknown words is to ignore them—remove them from the test
documentandnotincludeanyprobabilityforthematall.
Finally,somesystemschoosetocompletelyignoreanotherclassofwords: stop
stopwords words,veryfrequentwordsliketheanda. Thiscanbedonebysortingthevocabu-
larybyfrequencyinthetrainingset,anddefiningthetop10–100vocabularyentries
asstopwords,oralternativelybyusingoneofthemanypredefinedstopwordlists
available online. Then each instance of these stop words is simply removed from
bothtrainingandtestdocumentsasifithadneveroccurred. Inmosttextclassifica-
tionapplications,however,usingastopwordlistdoesn’timproveperformance,and
soitismorecommontomakeuseoftheentirevocabularyandnotuseastopword
list.
Fig.4.2showsthefinalalgorithm.
functionTRAINNAIVEBAYES(D,C)returnslog P(c)andlog P(wc)
|
foreachclassc C #CalculateP(c)terms
∈
N =numberofdocumentsinD
doc
Nc=numberofdocumentsfromDinclassc
Nc
logprior[c] log
← N
doc
V vocabularyofD
←
bigdoc[c] append(d)ford Dwithclassc
← ∈
foreachwordwinV # CalculateP(wc)terms
|
count(w,c) #ofoccurrencesofwinbigdoc[c]
←
count(w,c) + 1
loglikelihood[w,c] log
← (count(w,c) + 1)
returnlogprior,loglikelihood,V
w(cid:48)inV (cid:48)
(cid:80)
functionTESTNAIVEBAYES(testdoc,logprior,loglikelihood,C,V)returnsbestc
foreachclassc C
∈
sum[c] logprior[c]
←
foreachpositioniintestdoc
word testdoc[i]
←
ifword V
∈
sum[c] sum[c]+loglikelihood[word,c]
←
returnargmax sum[c]
c
Figure4.2 ThenaiveBayesalgorithm,usingadd-1smoothing. Touseadd-α smoothing
instead,changethe+1to+αforloglikelihoodcountsintraining.64 CHAPTER4 • NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENT
4.3 Worked example
Let’s walk through an example of training and testing naive Bayes with add-one
smoothing. We’ll use a sentiment analysis domain with the two classes positive
(+) and negative (-), and take the following miniature training and test documents
simplifiedfromactualmoviereviews.
Cat Documents
Training - justplainboring
- entirelypredictableandlacksenergy
- nosurprisesandveryfewlaughs
+ verypowerful
+ themostfunfilmofthesummer
Test ? predictablewithnofun
ThepriorP(c)forthetwoclassesiscomputedviaEq.4.11as Nc :
Ndoc
3 2
P( )= P(+)=
− 5 5
The word with doesn’t occur in the training set, so we drop it completely (as
mentionedabove, wedon’tuseunknownwordmodelsfornaiveBayes). Thelike-
lihoodsfromthetrainingsetfortheremainingthreewords“predictable”,“no”,and
“fun”,areasfollows,fromEq.4.14(computingtheprobabilitiesfortheremainder
ofthewordsinthetrainingsetisleftasanexerciseforthereader):
1+1 0+1
P(“predictable” )= P(“predictable”+)=
|− 14+20 | 9+20
1+1 0+1
P(“no” )= P(“no”+)=
|− 14+20 | 9+20
0+1 1+1
P(“fun” )= P(“fun”+)=
|− 14+20 | 9+20
ForthetestsentenceS=“predictablewithnofun”,afterremovingtheword‘with’,
thechosenclass,viaEq.4.9,isthereforecomputedasfollows:
3 2 2 1
P( )P(S ) = × × =6.1 10−5
− |− 5× 343 ×
2 1 1 2
P(+)P(S+) = × × =3.2 10−5
| 5× 293 ×
Themodelthuspredictstheclassnegativeforthetestsentence.
4.4 Optimizing for Sentiment Analysis
WhilestandardnaiveBayestextclassificationcanworkwellforsentimentanalysis,
somesmallchangesaregenerallyemployedthatimproveperformance.
First,forsentimentclassificationandanumberofothertextclassificationtasks,
whetherawordoccursornotseemstomattermorethanitsfrequency. Thusitoften
improves performance to clip the word counts in each document at 1 (see the end4.4 • OPTIMIZINGFORSENTIMENTANALYSIS 65
ofthechapterforpointerstotheseresults). Thisvariantiscalledbinarymultino-
binarynaive mial naive Bayes or binary naive Bayes. The variant uses the same algorithm as
Bayes
inFig.4.2exceptthatforeachdocumentweremoveallduplicatewordsbeforecon-
catenating them into the single big document during training and we also remove
duplicate words from test documents. Fig. 4.3 shows an example in which a set
of four documents (shortened and text-normalized for this example) are remapped
to binary, with the modified counts shown in the table on the right. The example
is worked without add-1 smoothing to make the differences clearer. Note that the
resultscountsneednotbe1; thewordgreathasacountof2evenforbinarynaive
Bayes,becauseitappearsinmultipledocuments.
NB Binary
Counts Counts
Fouroriginaldocuments: + +
− −
it was pathetic the worst part was the and 2 0 1 0
− boxing 0 1 0 1
boxingscenes
film 1 0 1 0
noplottwistsorgreatscenes
great 3 1 2 1
−
+ andsatireandgreatplottwists it 0 1 0 1
+ greatscenesgreatfilm no 0 1 0 1
or 0 1 0 1
Afterper-documentbinarization: part 0 1 0 1
pathetic 0 1 0 1
it was pathetic the worst part boxing
− plot 1 1 1 1
scenes
satire 1 0 1 0
noplottwistsorgreatscenes scenes 1 2 1 2
−
+ andsatiregreatplottwists the 0 2 0 1
+ greatscenesfilm twists 1 1 1 1
was 0 2 0 1
worst 0 1 0 1
Figure4.3 AnexampleofbinarizationforthebinarynaiveBayesalgorithm.
Asecondimportantadditioncommonlymadewhendoingtextclassificationfor
sentimentistodealwithnegation. ConsiderthedifferencebetweenIreallylikethis
movie(positive)andIdidn’tlikethismovie(negative). Thenegationexpressedby
didn’t completely alters the inferences we draw from the predicate like. Similarly,
negationcanmodifyanegativewordtoproduceapositivereview(don’tdismissthis
film,doesn’tletusgetbored).
Averysimplebaselinethatiscommonlyusedinsentimentanalysistodealwith
negation is the following: during text normalization, prepend the prefix NOT to
everywordafteratokenoflogicalnegation(n’t,not,no,never)untilthenextpunc-
tuationmark. Thusthephrase
didn’t like this movie , but I
becomes
didn’t NOT_like NOT_this NOT_movie , but I
Newlyformed‘words’likeNOT like,NOT recommendwillthusoccurmoreof-
ten in negative document and act as cues for negative sentiment, while words like
NOT bored,NOT dismisswillacquirepositiveassociations.WewillreturninChap-
ter20totheuseofparsingtodealmoreaccuratelywiththescoperelationshipbe-
tweenthesenegationwordsandthepredicatestheymodify,butthissimplebaseline
worksquitewellinpractice.66 CHAPTER4 • NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENT
Finally, in some situations we might have insufficient labeled training data to
trainaccuratenaiveBayesclassifiersusingallwordsinthetrainingsettoestimate
positive and negative sentiment. In such cases we can instead derive the positive
sentiment and negative word features from sentiment lexicons, lists of words that are pre-
lexicons
annotatedwithpositiveornegativesentiment.FourpopularlexiconsaretheGeneral
General Inquirer(Stoneetal.,1966),LIWC(Pennebakeretal.,2007),theopinionlexicon
Inquirer
LIWC ofHuandLiu(2004a)andtheMPQASubjectivityLexicon(Wilsonetal.,2005).
For example the MPQA subjectivity lexicon has 6885 words each marked for
whetheritisstronglyorweaklybiasedpositiveornegative. Someexamples:
+ : admirable,beautiful,confident,dazzling,ecstatic,favor,glee,great
: awful,bad,bias,catastrophe,cheat,deny,envious,foul,harsh,hate
−
A common way to use lexicons in a naive Bayes classifier is to add a feature
that is counted whenever a word from that lexicon occurs. Thus we might add a
feature called ‘this word occurs in the positive lexicon’, and treat all instances of
words in the lexicon as counts for that one feature, instead of counting each word
separately. Similarly, we might add as a second feature ‘this word occurs in the
negativelexicon’ofwordsinthenegativelexicon. Ifwehavelotsoftrainingdata,
andifthetestdatamatchesthetrainingdata,usingjusttwofeatureswon’tworkas
wellasusingallthewords. Butwhentrainingdataissparseornotrepresentativeof
thetestset,usingdenselexiconfeaturesinsteadofsparseindividual-wordfeatures
maygeneralizebetter.
We’ll return to this use of lexicons in Chapter 25, showing how these lexicons
canbelearnedautomatically,andhowtheycanbeappliedtomanyothertasksbe-
yondsentimentclassification.
4.5 Naive Bayes for other text classification tasks
In the previous section we pointed out that naive Bayes doesn’t require that our
classifieruseallthewordsinthetrainingdataasfeatures. Infactfeaturesinnaive
Bayescanexpressanypropertyoftheinputtextwewant.
spamdetection Consider the task of spam detection, deciding if a particular piece of email is
anexampleofspam(unsolicitedbulkemail)—oneofthefirstapplicationsofnaive
Bayestotextclassification(Sahamietal.,1998).
Acommonsolutionhere,ratherthanusingallthewordsasindividualfeatures,
is to predefine likely sets of words or phrases as features, combined with features
that are not purely linguistic. For example the open-source SpamAssassin tool1
predefinesfeatureslikethephrase“onehundredpercentguaranteed”,orthefeature
mentionsmillionsofdollars,whichisaregularexpressionthatmatchessuspiciously
largesumsofmoney. ButitalsoincludesfeatureslikeHTMLhasalowratiooftext
to image area, that aren’t purely linguistic and might require some sophisticated
computation, or totally non-linguistic features about, say, the path that the email
tooktoarrive. MoresampleSpamAssassinfeatures:
• Emailsubjectlineisallcapitalletters
• Containsphrasesofurgencylike“urgentreply”
• Emailsubjectlinecontains“onlinepharmaceutical”
• HTMLhasunbalanced“head”tags
1 https://spamassassin.apache.org4.6 • NAIVEBAYESASALANGUAGEMODEL 67
• Claimsyoucanberemovedfromthelist
languageid For other tasks, like language id—determining what language a given piece
of text is written in—the most effective naive Bayes features are not words at all,
but charactern-grams, 2-grams (‘zw’)3-grams (‘nya’, ‘Vo’), or4-grams (‘iez’,
‘thei’),or,evensimplerbyten-grams,whereinsteadofusingthemultibyteUnicode
characterrepresentationscalledcodepoints,wejustpretendeverythingisastringof
rawbytes. Becausespacescountasabyte,byten-gramscanmodelstatisticsabout
thebeginningorendingofwords. AwidelyusednaiveBayessystem,langid.py
(LuiandBaldwin,2012)beginswithallpossiblen-gramsoflengths1-4,usingfea-
tureselectiontowinnowdowntothemostinformative7000finalfeatures.
LanguageIDsystemsaretrainedonmultilingualtext,suchasWikipedia(Wiki-
pediatextin68differentlanguageswasusedin(LuiandBaldwin,2011)),ornewswire.
Tomakesurethatthismultilingualtextcorrectlyreflectsdifferentregions,dialects,
and socioeconomic classes, systems also add Twitter text in many languages geo-
taggedtomanyregions(importantforgettingworldEnglishdialectsfromcountries
withlargeAnglophonepopulationslikeNigeriaorIndia),BibleandQurantransla-
tions,slangwebsiteslikeUrbanDictionary,corporaofAfricanAmericanVernacular
English(Blodgettetal.,2016),andsoon(Jurgensetal.,2017).
4.6 Naive Bayes as a Language Model
As we saw in the previous section, naive Bayes classifiers can use any sort of fea-
ture: dictionaries,URLs,emailaddresses,networkfeatures,phrases,andsoon. But
if, asintheprevioussection, weuseonlyindividualwordfeatures, andweuseall
of the words in the text (not a subset), then naive Bayes has an important similar-
ity to language modeling. Specifically, a naive Bayes model can be viewed as a
set of class-specific unigram language models, in which the model for each class
instantiatesaunigramlanguagemodel.
SincethelikelihoodfeaturesfromthenaiveBayesmodelassignaprobabilityto
eachwordP(word c),themodelalsoassignsaprobabilitytoeachsentence:
|
P(sc)= P(w c) (4.15)
i
| |
i positions
∈ (cid:89)
ThusconsideranaiveBayesmodelwiththeclassespositive(+)andnegative(-)
andthefollowingmodelparameters:
w P(w+) P(w-)
| |
I 0.1 0.2
love 0.1 0.001
this 0.01 0.01
fun 0.05 0.005
film 0.1 0.1
... ... ...
Eachofthetwocolumnsaboveinstantiatesalanguagemodelthatcanassigna
probabilitytothesentence“Ilovethisfunfilm”:
P(“Ilovethisfunfilm”+) = 0.1 0.1 0.01 0.05 0.1=0.0000005
| × × × ×
P(“Ilovethisfunfilm” ) = 0.2 0.001 0.01 0.005 0.1=.0000000010
|− × × × ×68 CHAPTER4 • NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENT
As it happens, the positive model assigns a higher probability to the sentence:
P(s pos)>P(sneg). Note that this is just the likelihood part of the naive Bayes
| |
model; once we multiply in the prior a full naive Bayes model might well make a
differentclassificationdecision.
4.7 Evaluation: Precision, Recall, F-measure
Tointroducethemethodsforevaluatingtextclassification,let’sfirstconsidersome
simplebinarydetectiontasks. Forexample, inspamdetection, ourgoalistolabel
every text as being in the spam category (“positive”) or not in the spam category
(“negative”). For each item (email document) we therefore need to know whether
oursystemcalleditspamornot.Wealsoneedtoknowwhethertheemailisactually
spamornot,i.e. thehuman-definedlabelsforeachdocumentthatwearetryingto
goldlabels match. Wewillrefertothesehumanlabelsasthegoldlabels.
Orimagineyou’retheCEOoftheDeliciousPieCompanyandyouneedtoknow
whatpeoplearesayingaboutyourpiesonsocialmedia,soyoubuildasystemthat
detects tweets concerning Delicious Pie. Here the positive class is tweets about
DeliciousPieandthenegativeclassisallothertweets.
In both cases, we need a metric for knowing how well our spam detector (or
pie-tweet-detector) is doing. To evaluate any system for detecting things, we start
confusion bybuildingaconfusionmatrixliketheoneshowninFig.4.4. Aconfusionmatrix
matrix
isatableforvisualizinghowanalgorithmperformswithrespecttothehumangold
labels,usingtwodimensions(systemoutputandgoldlabels),andeachcelllabeling
asetofpossibleoutcomes. Inthespamdetectioncase, forexample, truepositives
are documents that are indeed spam (indicated by human-created gold labels) that
oursystemcorrectlysaidwerespam. Falsenegativesaredocumentsthatareindeed
spambutoursystemincorrectlylabeledasnon-spam.
To the bottom right of the table is the equation for accuracy, which asks what
percentageofalltheobservations(forthespamorpieexamplesthatmeansallemails
or tweets) our system labeled correctly. Although accuracy might seem a natural
metric,wegenerallydon’tuseitfortextclassificationtasks.That’sbecauseaccuracy
doesn’tworkwellwhentheclassesareunbalanced(asindeedtheyarewithspam,
whichisalargemajorityofemail,orwithtweets,whicharemainlynotaboutpie).
gold standard labels
gold positive gold negative
system ps oy ss it te im ve true positive false positive precision = tpt +p fp
output
system
labels negative false negative true negative
tp tp+tn
recall = accuracy =
tp+fn tp+fp+tn+fn
Figure4.4 Aconfusionmatrixforvisualizinghowwellabinaryclassificationsystemper-
formsagainstgoldstandardlabels.
To make this more explicit, imagine that we looked at a million tweets, and
let’s say that only 100 of them are discussing their love (or hatred) for our pie,4.7 • EVALUATION: PRECISION,RECALL,F-MEASURE 69
whiletheother999,900aretweetsaboutsomethingcompletelyunrelated.Imaginea
simpleclassifierthatstupidlyclassifiedeverytweetas“notaboutpie”.Thisclassifier
wouldhave999,900truenegativesandonly100falsenegativesforanaccuracyof
999,900/1,000,000or99.99%! Whatanamazingaccuracylevel! Surelyweshould
be happy with this classifier? But of course this fabulous ‘no pie’ classifier would
becompletelyuseless,sinceitwouldn’tfindasingleoneofthecustomercomments
wearelookingfor. Inotherwords,accuracyisnotagoodmetricwhenthegoalis
todiscoversomethingthatisrare,oratleastnotcompletelybalancedinfrequency,
whichisaverycommonsituationintheworld.
That’swhyinsteadofaccuracywegenerallyturntotwoothermetricsshownin
precision Fig.4.4: precisionandrecall. Precisionmeasuresthepercentageoftheitemsthat
thesystemdetected(i.e.,thesystemlabeledaspositive)thatareinfactpositive(i.e.,
arepositiveaccordingtothehumangoldlabels). Precisionisdefinedas
truepositives
Precision=
truepositives+falsepositives
recall Recallmeasuresthepercentageofitemsactuallypresentintheinputthatwere
correctlyidentifiedbythesystem. Recallisdefinedas
truepositives
Recall=
truepositives+falsenegatives
Precision and recall will help solve the problem with the useless “nothing is
pie” classifier. This classifier, despite having a fabulous accuracy of 99.99%, has
a terrible recall of 0 (since there are no true positives, and 100 false negatives, the
recallis0/100). Youshouldconvinceyourselfthattheprecisionatfindingrelevant
tweetsisequallyproblematic.Thusprecisionandrecall,unlikeaccuracy,emphasize
truepositives: findingthethingsthatwearesupposedtobelookingfor.
Therearemanywaystodefineasinglemetricthatincorporatesaspectsofboth
F-measure precision and recall. The simplest of these combinations is the F-measure (van
Rijsbergen,1975),definedas:
(β2+1)PR
F =
β β2P+R
The β parameter differentially weights the importance of recall and precision,
based perhaps on the needs of an application. Values of β >1 favor recall, while
valuesofβ <1favorprecision. Whenβ =1,precisionandrecallareequallybal-
F1 anced;thisisthemostfrequentlyusedmetric,andiscalledF β=1orjustF 1:
2PR
F = (4.16)
1
P+R
F-measurecomesfromaweightedharmonicmeanofprecisionandrecall. The
harmonicmeanofasetofnumbersisthereciprocalofthearithmeticmeanofrecip-
rocals:
n
HarmonicMean(a ,a ,a ,a ,...,a )= (4.17)
1 2 3 4 n 1 + 1 + 1 +...+ 1
a1 a2 a3 an
andhenceF-measureis
1 1 α (β2+1)PR
F = or withβ2= − F = (4.18)
α1+(1 α)1 α β2P+R
P − R (cid:18) (cid:19)70 CHAPTER4 • NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENT
Harmonic mean is used because it is a conservative metric; the harmonic mean of
twovaluesisclosertotheminimumofthetwovaluesthanthearithmeticmeanis.
Thusitweighsthelowerofthetwonumbersmoreheavily.
4.7.1 Evaluatingwithmorethantwoclasses
Uptonowwehavebeendescribingtextclassificationtaskswithonlytwoclasses.
But lots of classification tasks in language processing have more than two classes.
Forsentimentanalysiswegenerallyhave3classes(positive,negative,neutral)and
even more classes are common for tasks like part-of-speech tagging, word sense
disambiguation, semantic role labeling, emotion detection, and so on. Luckily the
naiveBayesalgorithmisalreadyamulti-classclassificationalgorithm.
gold labels
urgent normal spam
8
urgent 8 10 1 precisionu=
8+10+1
system 60
output normal 5 60 50 precisionn=
5+60+50
200
spam 3 30 200 precisions=
3+30+200
recallu = recalln = recalls =
8 60 200
8+5+3 10+60+30 1+50+200
Figure4.5 Confusionmatrixforathree-classcategorizationtask,showingforeachpairof
classes(c ,c ),howmanydocumentsfromc were(in)correctlyassignedtoc .
1 2 1 2
But we’ll need to slightly modify our definitions of precision and recall. Con-
sider the sample confusion matrix for a hypothetical 3-way one-of email catego-
rization decision (urgent, normal, spam) shown in Fig. 4.5. The matrix shows, for
example,thatthesystemmistakenlylabeledonespamdocumentasurgent,andwe
haveshownhowtocomputeadistinctprecisionandrecallvalueforeachclass. In
ordertoderiveasinglemetricthattellsushowwellthesystemisdoing,wecancom-
macroaveraging bine these values in two ways. In macroaveraging, we compute the performance
microaveraging foreachclass,andthenaverageoverclasses. Inmicroaveraging,wecollectthede-
cisionsforallclassesintoasingleconfusionmatrix,andthencomputeprecisionand
recallfromthattable. Fig.4.6showstheconfusionmatrixforeachclassseparately,
andshowsthecomputationofmicroaveragedandmacroaveragedprecision.
Asthefigureshows,amicroaverageisdominatedbythemorefrequentclass(in
this case spam), since the counts are pooled. The macroaverage better reflects the
statisticsofthesmallerclasses,andsoismoreappropriatewhenperformanceonall
theclassesisequallyimportant.
4.8 Test sets and Cross-validation
Thetrainingandtestingprocedurefortextclassificationfollowswhatwesawwith
languagemodeling(Section3.2):weusethetrainingsettotrainthemodel,thenuse
development the development test set (also called a devset) to perhaps tune some parameters,
testset
devset4.9 • STATISTICALSIGNIFICANCETESTING 71
Class 1: Urgent Class 2: Normal Class 3: Spam Pooled
true true true true true true true true
urgent not normal not spam not yes no
system system system system
8 11 60 55 200 33 268 99
urgent normal spam yes
system 8 340 system 40 212 system 51 83 system 99 635
not not not no
8 60 200 microaverage 268
precision = = .42 precision = = .52 precision = = .86 = = .73
8+11 60+55 200+33 precision 268+99
macroaverage .42+.52+.86
= = .60
precision 3
Figure4.6 Separateconfusionmatricesforthe3classesfromthepreviousfigure,showingthepooledconfu-
sionmatrixandthemicroaveragedandmacroaveragedprecision.
andingeneraldecidewhatthebestmodelis. Oncewecomeupwithwhatwethink
isthebestmodel,werunitonthe(hithertounseen)testsettoreportitsperformance.
While the use of a devset avoids overfitting the test set, having a fixed train-
ing set, devset, and test set creates another problem: in order to save lots of data
fortraining, thetestset(ordevset)mightnotbelargeenoughtoberepresentative.
Wouldn’titbebetterifwecouldsomehowuseallourdatafortrainingandstilluse
cross-validation allourdatafortest? Wecandothisbycross-validation.
Incross-validation,wechooseanumberk,andpartitionourdataintokdisjoint
folds subsets called folds. Now we choose one of those k folds as a test set, train our
classifier on the remaining k 1 folds, and then compute the error rate on the test
−
set. Thenwerepeatwithanotherfoldasthetestset,againtrainingontheotherk 1
−
folds. Wedothissamplingprocessk timesandaveragethetestseterrorratefrom
these k runs to get an average error rate. If we choose k=10, we would train 10
different models (each on 90% of our data), test the model 10 times, and average
10-fold these10values. Thisiscalled10-foldcross-validation.
cross-validation
The only problem with cross-validation is that because all the data is used for
testing, we need the whole corpus to be blind; we can’t examine any of the data
to suggest possible features and in general see what’s going on, because we’d be
peekingatthetestset,andsuchcheatingwouldcauseustooverestimatetheperfor-
mance of our system. However, looking at the corpus to understand what’s going
onisimportantindesigningNLPsystems! Whattodo? Forthisreason,itiscom-
montocreateafixedtrainingsetandtestset,thendo10-foldcross-validationinside
the training set, but compute error rate the normal way in the test set, as shown in
Fig.4.7.
4.9 Statistical Significance Testing
Inbuildingsystemsweoftenneedtocomparetheperformanceoftwosystems.How
can we know if the new system we just built is better than our old one? Or better
thansomeothersystemdescribedintheliterature? Thisisthedomainofstatistical
hypothesistesting, andinthissectionweintroducetestsforstatisticalsignificance
forNLPclassifiers,drawingespeciallyontheworkofDroretal.(2020)andBerg-
Kirkpatricketal.(2012).
Supposewe’recomparingtheperformanceofclassifiersAandBonametricM72 CHAPTER4 • NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENT
Training Iterations Testing
1 Dev Training
2 Dev Training
3 Dev Training
4 Dev Training
Test
5 Training Dev Training
Set
6 Training Dev
7 Training Dev
8 Training Dev
9 Training Dev
10 Training Dev
Figure4.7 10-foldcross-validation
such as F , or accuracy. Perhaps we want to know if our logistic regression senti-
1
mentclassifierA(Chapter5)getsahigherF scorethanournaiveBayessentiment
1
classifierBonaparticulartestsetx. Let’scallM(A,x)thescorethatsystemAgets
ontestsetx,andδ(x)theperformancedifferencebetweenAandBonx:
δ(x)=M(A,x) M(B,x) (4.19)
−
We would like to know if δ(x)>0, meaning that our logistic regression classifier
effectsize hasahigherF 1 thanournaiveBayesclassifieronX. δ(x)iscalledtheeffectsize;
abiggerδ meansthatAseemstobewaybetterthanB;asmallδ meansAseemsto
beonlyalittlebetter.
Why don’t we just check if δ(x) is positive? Suppose we do, and we find that
theF scoreofAishigherthanB’sby.04. CanwebecertainthatAisbetter? We
1
cannot!That’sbecauseAmightjustbeaccidentallybetterthanBonthisparticularx.
Weneedsomethingmore:wewanttoknowifA’ssuperiorityoverBislikelytohold
againifwecheckedanothertestsetx,orundersomeothersetofcircumstances.
(cid:48)
Intheparadigmofstatisticalhypothesistesting,wetestthisbyformalizingtwo
hypotheses.
H : δ(x) 0
0
≤
H 1 : δ(x)>0 (4.20)
nullhypothesis ThehypothesisH 0,calledthenullhypothesis,supposesthatδ(x)isactuallynega-
tiveorzero,meaningthatAisnotbetterthanB. Wewouldliketoknowifwecan
confidentlyruleoutthishypothesis,andinsteadsupportH ,thatAisbetter.
1
WedothisbycreatingarandomvariableX rangingoveralltestsets. Nowwe
askhowlikelyisit,ifthenullhypothesisH wascorrect,thatamongthesetestsets
0
wewouldencounterthevalueofδ(x)thatwefound. Weformalizethislikelihood
p-value as the p-value: the probability, assuming the null hypothesis H 0 is true, of seeing
theδ(x)thatwesaworoneevengreater
P(δ(X) δ(x)H 0istrue) (4.21)
≥ |
Soinourexample,thisp-valueistheprobabilitythatwewouldseeδ(x)assuming
A is not better than B. If δ(x) is huge (let’s say A has a very respectable F 1 of .9
andBhasaterribleF ofonly.2onx),wemightbesurprised,sincethatwouldbe
1
extremelyunlikelytooccurifH wereinfacttrue,andsothep-valuewouldbelow
04.9 • STATISTICALSIGNIFICANCETESTING 73
(unlikelytohavesuchalargeδ ifAisinfactnotbetterthanB). Butifδ(x)isvery
small,itmightbelesssurprisingtousevenifH weretrueandAisnotreallybetter
0
thanB,andsothep-valuewouldbehigher.
A very small p-value means that the difference we observed is very unlikely
underthenullhypothesis,andwecanrejectthenullhypothesis.Whatcountsasvery
small? Itiscommontousevalueslike.05or.01asthethresholds. Avalueof.01
meansthatifthep-value(theprobabilityofobservingtheδ wesawassumingH is
0
true)islessthan.01,werejectthenullhypothesisandassumethatAisindeedbetter
statistically thanB. Wesaythataresult(e.g.,“AisbetterthanB”)isstatisticallysignificantif
significant
the δ we saw has a probability that is below the threshold and we therefore reject
thisnullhypothesis.
Howdowecomputethisprobabilityweneedforthep-value? InNLPwegen-
erally don’t use simple parametric tests like t-tests or ANOVAs that you might be
familiarwith. Parametrictestsmakeassumptionsaboutthedistributionsofthetest
statistic (such as normality) that don’t generally hold in our cases. So in NLP we
usuallyusenon-parametrictestsbasedonsampling: weartificiallycreatemanyver-
sionsoftheexperimentalsetup. Forexample,ifwehadlotsofdifferenttestsetsx
(cid:48)
wecouldjustmeasurealltheδ(x)forallthex. Thatgivesusadistribution. Now
(cid:48) (cid:48)
we set a threshold (like .01) and if we see in this distribution that 99% or more of
thosedeltasaresmallerthanthedeltaweobserved,i.e.,thatp-value(x)—theproba-
bilityofseeingaδ(x)asbigastheonewesaw—islessthan.01,thenwecanreject
thenullhypothesisandagreethatδ(x)wasasufficientlysurprisingdifferenceand
AisreallyabetteralgorithmthanB.
There are two common non-parametric tests used in NLP: approximate ran-
approximate domization (Noreen, 1989) and the bootstrap test. We will describe bootstrap
randomization
below,showingthepairedversionofthetest,whichagainismostcommoninNLP.
paired Pairedtestsarethoseinwhichwecomparetwosetsofobservationsthatarealigned:
eachobservationinonesetcanbepairedwithanobservationinanother. Thishap-
pensnaturallywhenwearecomparingtheperformanceoftwosystemsonthesame
test set; we can pair the performance of system A on an individual observation x
i
withtheperformanceofsystemBonthesamex.
i
4.9.1 ThePairedBootstrapTest
bootstraptest Thebootstraptest(EfronandTibshirani,1993)canapplytoanymetric;frompre-
cision, recall, or F1 to the BLEU metric used in machine translation. The word
bootstrapping bootstrappingreferstorepeatedlydrawinglargenumbersofsampleswithreplace-
ment(calledbootstrapsamples)fromanoriginalset. Theintuitionofthebootstrap
testisthatwecancreatemanyvirtualtestsetsfromanobservedtestsetbyrepeat-
edly sampling from it. The method only makes the assumption that the sample is
representativeofthepopulation.
Consideratinytextclassificationexamplewithatestsetxof10documents.The
first row of Fig. 4.8 shows the results of two classifiers (A and B) on this test set,
with each document labeled by one of the four possibilities: (A and B both right,
both wrong, A right and B wrong, A wrong and B right); a slash through a letter
((cid:19)B) means that that classifier got the answer wrong. On the first document both A
andBgetthecorrectclass(AB),whileontheseconddocumentAgotitrightbutB
gotitwrong(A(cid:19)B). Ifweassumeforsimplicitythatourmetricisaccuracy,Ahasan
accuracyof.70andBof.50,soδ(x)is.20.
Nowwecreatealargenumberb(perhaps105)ofvirtualtestsetsx(i),eachofsize
n=10. Fig.4.8showsacoupleofexamples. Tocreateeachvirtualtestsetx(i),we74 CHAPTER4 • NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENT
repeatedly(n=10times)selectacellfromrowxwithreplacement.Forexample,to
createthefirstcellofthefirstvirtualtestsetx(1),ifwehappenedtorandomlyselect
the second cell of the x row; we would copy the value A(cid:19)B into our new cell, and
moveontocreatethesecondcellofx(1),eachtimesampling(randomlychoosing)
fromtheoriginalxwithreplacement.
1 2 3 4 5 6 7 8 9 10 A% B% δ()
x AB AB(cid:19)(cid:19) AB (cid:0)(cid:0)AB AB(cid:19)(cid:19) (cid:0)(cid:0)AB AB(cid:19)(cid:19) AB (cid:0)(cid:0)AB(cid:19)(cid:19) AB(cid:19)(cid:19) .70 .50 .20
x(1) AB(cid:19)(cid:19) AB AB(cid:19)(cid:19) (cid:0)(cid:0)AB (cid:0)(cid:0)AB AB(cid:19)(cid:19) (cid:0)(cid:0)AB AB (cid:0)(cid:0)AB(cid:19)(cid:19) AB .60 .60 .00
x(2) AB(cid:19)(cid:19) AB (cid:0)(cid:0)AB(cid:19)(cid:19) (cid:0)(cid:0)AB (cid:0)(cid:0)AB AB (cid:0)(cid:0)AB AB(cid:19)(cid:19) AB AB .60 .70 -.10
...
x(b)
Figure4.8 The paired bootstrap test: Examples of b pseudo test sets x(i) being created
fromaninitialtruetestsetx. Eachpseudotestsetiscreatedbysamplingn=10timeswith
replacement; thusanindividualsampleisasinglecell, adocumentwithitsgoldlabeland
thecorrectorincorrectperformanceofclassifiersAandB.Ofcourserealtestsetsdon’thave
only10examples,andbneedstobelargeaswell.
Nowthat wehave theb testsets, providingasampling distribution, wecan do
statistics on how often A has an accidental advantage. There are various ways to
compute this advantage; here we follow the version laid out in Berg-Kirkpatrick
etal.(2012). AssumingH (Aisn’tbetterthanB),wewouldexpectthatδ(X),esti-
0
matedovermanytestsets,wouldbezero;amuchhighervaluewouldbesurprising,
sinceH specificallyassumesAisn’tbetterthanB. Tomeasureexactlyhowsurpris-
0
ingourobservedδ(x)is,wewouldinothercircumstancescomputethep-valueby
countingovermanytestsetshowoftenδ(x(i))exceedstheexpectedzerovalueby
δ(x)ormore:
b
1
p-value(x)= 1 δ(x(i)) δ(x) 0
b − ≥
(cid:88)i=1 (cid:16) (cid:17)
(We use the notation 1(x) to mean “1 if x is true, and 0 otherwise”.) However,
although it’s generally true that the expected value of δ(X) over many test sets,
(again assuming A isn’t better than B) is 0, this isn’t true for the bootstrapped test
sets we created. That’s because we didn’t draw these samples from a distribution
with0mean;wehappenedtocreatethemfromtheoriginaltestsetx,whichhappens
to be biased (by .20) in favor of A. So to measure how surprising is our observed
δ(x), we actually compute the p-value by counting over many test sets how often
δ(x(i))exceedstheexpectedvalueofδ(x)byδ(x)ormore:
b
1
p-value(x) = 1 δ(x(i)) δ(x) δ(x)
b − ≥
(cid:88)i=1 (cid:16) (cid:17)
b
1
= 1 δ(x(i)) 2δ(x) (4.22)
b ≥
(cid:88)i=1 (cid:16) (cid:17)
So if for example we have 10,000 test sets x(i) and a threshold of .01, and in only
47ofthetestsetsdowefindthatδ(x(i)) 2δ(x),theresultingp-valueof.0047is
≥
smallerthan.01,indicatingδ(x)isindeedsufficientlysurprising,andwecanreject
thenullhypothesisandconcludeAisbetterthanB.4.10 • AVOIDINGHARMSINCLASSIFICATION 75
functionBOOTSTRAP(testsetx, numofsamplesb)returnsp-value(x)
Calculateδ(x) #howmuchbetterdoesalgorithmAdothanBonx
s=0
fori= 1tobdo
forj=1tondo #Drawabootstrapsamplex(i)ofsizen
Selectamemberofxatrandomandaddittox(i)
Calculateδ(x(i)) # howmuchbetterdoesalgorithmAdothanBonx(i)
s s+1ifδ(x(i)) 2δ(x)
p-val← ue(x) s # onw≥ hat%ofthebsamplesdidalgorithmAbeatexpectations?
≈ b
returnp-value(x) #ifveryfewdid,ourobservedδ isprobablynotaccidental
Figure4.9 AversionofthepairedbootstrapalgorithmafterBerg-Kirkpatricketal.(2012).
ThefullalgorithmforthebootstrapisshowninFig.4.9. Itisgivenatestsetx,a
numberofsamplesb,andcountsthepercentageofthebbootstraptestsetsinwhich
δ(x (i))>2δ(x). Thispercentagethenactsasaone-sidedempiricalp-value
∗
4.10 Avoiding Harms in Classification
Itisimportanttoavoidharmsthatmayresultfromclassifiers,harmsthatexistboth
for naive Bayes classifiers and for the other classification algorithms we introduce
inlaterchapters.
representational Oneclassofharmsisrepresentationalharms(Crawford2017,Blodgettetal.
harms
2020),harmscausedbyasystemthatdemeansasocialgroup,forexamplebyper-
petuating negative stereotypes about them. For example Kiritchenko and Moham-
mad(2018)examinedtheperformanceof200sentimentanalysissystemsonpairsof
sentencesthatwereidenticalexceptforcontainingeitheracommonAfricanAmer-
ican first name (like Shaniqua) or a common European American first name (like
Stephanie), chosen from the Caliskan et al. (2017) study discussed in Chapter 6.
Theyfoundthatmostsystemsassignedlowersentimentandmorenegativeemotion
tosentenceswithAfricanAmericannames,reflectingandperpetuatingstereotypes
thatassociateAfricanAmericanswithnegativeemotions(Poppetal.,2003).
In other tasks classifiers may lead to both representational harms and other
harms, such as censorship. For example the important text classification task of
toxicity toxicitydetectionisthetaskofdetectinghatespeech, abuse, harassment, orother
detection
kinds of toxic language. While the goal of such classifiers is to help reduce soci-
etalharm,toxicityclassifierscanthemselvescauseharms. Forexample,researchers
haveshownthatsomewidelyusedtoxicityclassifiersincorrectlyflagasbeingtoxic
sentences that are non-toxic but simply mention minority identities like women
(Park et al., 2018), blind people (Hutchinson et al., 2020) or gay people (Dixon
etal.,2018),orsimplyuselinguisticfeaturescharacteristicofvarietieslikeAfrican-
American Vernacular English (Sap et al. 2019, Davidson et al. 2019). Such false
positiveerrors,ifemployedbytoxicitydetectionsystemswithouthumanoversight,
couldleadtothecensoringofdiscoursebyoraboutthesegroups.
Thesemodelproblemscanbecausedbybiasesorotherproblemsinthetraining
data; in general, machine learning systems replicate and even amplify the biases
in their training data. But these problems can also be caused by the labels (for76 CHAPTER4 • NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENT
exampleduetobiasesinthehumanlabelers),bytheresourcesused(likelexicons,
or model components like pretrained embeddings), or even by model architecture
(like what the model is trained to optimize). While the mitigation of these biases
(forexamplebycarefullyconsideringthetrainingdatasources)isanimportantarea
ofresearch,wecurrentlydon’thavegeneralsolutions.Forthisreasonit’simportant,
when introducing any NLP model, to study these kinds of factors and make them
modelcard clear. One way to do this is by releasing a model card (Mitchell et al., 2019) for
eachversionofamodel. Amodelcarddocumentsamachinelearningmodelwith
informationlike:
• trainingalgorithmsandparameters
• trainingdatasources,motivation,andpreprocessing
• evaluationdatasources,motivation,andpreprocessing
• intendeduseandusers
• model performance across different demographic or other groups and envi-
ronmentalsituations
4.11 Summary
This chapter introduced the naive Bayes model for classification and applied it to
thetextcategorizationtaskofsentimentanalysis.
• Manylanguageprocessingtaskscanbeviewedastasksofclassification.
• Textcategorization,inwhichanentiretextisassignedaclassfromafiniteset,
includessuchtasksassentimentanalysis,spamdetection,languageidenti-
fication,andauthorshipattribution.
• Sentimentanalysisclassifiesatextasreflectingthepositiveornegativeorien-
tation(sentiment)thatawriterexpressestowardsomeobject.
• NaiveBayesisagenerativemodelthatmakesthebag-of-wordsassumption
(positiondoesn’tmatter)andtheconditionalindependenceassumption(words
areconditionallyindependentofeachothergiventheclass)
• NaiveBayeswithbinarizedfeaturesseemstoworkbetterformanytextclas-
sificationtasks.
• Classifiersareevaluatedbasedonprecisionandrecall.
• Classifiersaretrainedusingdistincttraining,dev,andtestsets,includingthe
useofcross-validationinthetrainingset.
• Statistical significance tests should be used to determine whether we can be
confidentthatoneversionofaclassifierisbetterthananother.
• Designers of classifiers should carefully consider harms that may be caused
by the model, including its training data and other components, and report
modelcharacteristicsinamodelcard.
Bibliographical and Historical Notes
Multinomial naive Bayes text classification was proposed by Maron (1961) at the
RANDCorporationforthetaskofassigningsubjectcategoriestojournalabstracts.
His model introduced most of the features of the modern form presented here, ap-
proximating the classification task with one-of categorization, and implementing
add-δ smoothingandinformation-basedfeatureselection.EXERCISES 77
TheconditionalindependenceassumptionsofnaiveBayesandtheideaofBayes-
iananalysisoftextseemstohavearisenmultipletimes. ThesameyearasMaron’s
paper, Minsky (1961) proposed a naive Bayes classifier for vision and other arti-
ficial intelligence problems, and Bayesian techniques were also applied to the text
classificationtaskofauthorshipattributionbyMostellerandWallace(1963). Ithad
long been known that Alexander Hamilton, John Jay, and James Madison wrote
theanonymously-publishedFederalistpapersin1787–1788topersuadeNewYork
to ratify the United States Constitution. Yet although some of the 85 essays were
clearly attributable to one author or another, the authorship of 12 were in dispute
betweenHamiltonandMadison. MostellerandWallace(1963)trainedaBayesian
probabilistic model of the writing of Hamilton and another model on the writings
ofMadison,thencomputedthemaximum-likelihoodauthorforeachofthedisputed
essays. NaiveBayeswasfirstappliedtospamdetectioninHeckermanetal.(1998).
Metsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show
that using boolean attributes with multinomial naive Bayes works better than full
counts.BinarymultinomialnaiveBayesissometimesconfusedwithanothervariant
of naive Bayes that also uses a binary representation of whether a term occurs in
a document: Multivariate Bernoulli naive Bayes. The Bernoulli variant instead
estimates P(wc) as the fraction of documents that contain a term, and includes a
|
probabilityforwhetheratermisnotinadocument. McCallumandNigam(1998)
andWangandManning(2012)showthatthemultivariateBernoullivariantofnaive
Bayesdoesn’tworkaswellasthemultinomialalgorithmforsentimentorothertext
tasks.
There are a variety of sources covering the many kinds of text classification
tasks. ForsentimentanalysisseePangandLee(2008),andLiuandZhang(2012).
Stamatatos(2009)surveysauthorshipattributealgorithms. Onlanguageidentifica-
tion see Jauhiainen et al. (2019); Jaech et al. (2016) is an important early neural
system. Thetaskofnewswireindexingwasoftenusedasatestcasefortextclassi-
ficationalgorithms,basedontheReuters-21578collectionofnewswirearticles.
SeeManningetal.(2008)andAggarwalandZhai(2012)ontextclassification;
classificationingeneraliscoveredinmachinelearningtextbooks(Hastieetal.2001,
WittenandFrank2005,Bishop2006,Murphy2012).
Non-parametricmethodsforcomputingstatisticalsignificancewereusedfirstin
NLP in the MUC competition (Chinchor et al., 1993), and even earlier in speech
recognition (Gillick and Cox 1989, Bisani and Ney 2004). Our description of the
bootstrap draws on the description in Berg-Kirkpatrick et al. (2012). Recent work
hasfocusedonissuesincludingmultipletestsetsandmultiplemetrics(Søgaardetal.
2014,Droretal.2017).
Featureselectionisamethodofremovingfeaturesthatareunlikelytogeneralize
well.Featuresaregenerallyrankedbyhowinformativetheyareabouttheclassifica-
information tiondecision. Averycommonmetric,informationgain,tellsushowmanybitsof
gain
informationthepresenceofthewordgivesusforguessingtheclass. Otherfeature
selection metrics include χ2, pointwise mutual information, and GINI index; see
YangandPedersen(1997)foracomparisonandGuyonandElisseeff(2003)foran
introductiontofeatureselection.
Exercises
4.1 Assume the following likelihoods for each word being part of a positive or
negativemoviereview,andequalpriorprobabilitiesforeachclass.78 CHAPTER4 • NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENT
pos neg
I 0.09 0.16
always 0.07 0.06
like 0.29 0.06
foreign 0.04 0.15
films 0.08 0.11
What class will Naive bayes assign to the sentence “I always like foreign
films.”?
4.2 Given the following short movie reviews, each labeled with a genre, either
comedyoraction:
1. fun,couple,love,love comedy
2. fast,furious,shoot action
3. couple,fly,fast,fun,fun comedy
4. furious,shoot,shoot,fun action
5. fly,fast,shoot,love action
andanewdocumentD:
fast,couple,shoot,fly
computethemostlikelyclassforD.AssumeanaiveBayesclassifieranduse
add-1smoothingforthelikelihoods.
4.3 Traintwomodels, multinomialnaiveBayesandbinarizednaiveBayes, both
with add-1 smoothing, on the following document counts for key sentiment
words,withpositiveornegativeclassassignedasnoted.
doc “good” “poor” “great” (class)
d1. 3 0 3 pos
d2. 0 1 2 pos
d3. 1 3 0 neg
d4. 1 5 2 neg
d5. 0 2 0 neg
UsebothnaiveBayesmodelstoassignaclass(posorneg)tothissentence:
Agood,goodplotandgreatcharacters,butpooracting.
Recall from page 63 that with naive Bayes text classification, we simply ig-
nore(throwout)anywordthatneveroccurredinthetrainingdocument. (We
don’tthrowoutwordsthatappearinsomeclassesbutnotothers;that’swhat
add-onesmoothingisfor.) Dothetwomodelsagreeordisagree?CHAPTER
5 Logistic Regression
“Andhowdoyouknowthatthesefinebegoniasarenotofequalimportance?”
HerculePoirot,inAgathaChristie’sTheMysteriousAffairatStyles
Detective stories are as littered with clues as texts are with words. Yet for the
poorreaderitcanbechallengingtoknowhowtoweightheauthor’scluesinorder
tomakethecrucialclassificationtask: decidingwhodunnit.
Inthischapterweintroduceanalgorithmthatisadmirablysuitedfordiscovering
logistic thelinkbetweenfeaturesorcuesandsomeparticularoutcome: logisticregression.
regression
Indeed, logistic regression is one of the most important analytic tools in the social
andnaturalsciences. Innaturallanguageprocessing,logisticregressionisthebase-
line supervised machine learning algorithm for classification, and also has a very
closerelationshipwithneuralnetworks. AswewillseeinChapter7,aneuralnet-
work can be viewed as a series of logistic regression classifiers stacked on top of
eachother. Thustheclassificationandmachinelearningtechniquesintroducedhere
willplayanimportantrolethroughoutthebook.
Logisticregressioncanbeusedtoclassifyanobservationintooneoftwoclasses
(like ‘positive sentiment’ and ‘negative sentiment’), or into one of many classes.
Becausethemathematicsforthetwo-classcaseissimpler,we’lldescribethisspecial
caseoflogisticregressionfirstinthenextfewsections,andthenbrieflysummarize
theuseofmultinomiallogisticregressionformorethantwoclassesinSection5.3.
We’llintroducethemathematicsoflogisticregressioninthenextfewsections.
Butlet’sbeginwithsomehigh-levelissues.
Generative and Discriminative Classifiers: The most important difference be-
tweennaiveBayesandlogisticregressionisthatlogisticregressionisadiscrimina-
tiveclassifierwhilenaiveBayesisagenerativeclassifier.
Thesearetwoverydifferentframeworksforhow
tobuildamachinelearningmodel. Consideravisual
metaphor: imagine we’re trying to distinguish dog
imagesfromcatimages. Agenerativemodelwould
have the goal of understanding what dogs look like
andwhatcatslooklike. Youmightliterallyasksuch
amodelto‘generate’,i.e.,draw,adog. Givenatest
image,thesystemthenaskswhetherit’sthecatmodelorthedogmodelthatbetter
fits(islesssurprisedby)theimage,andchoosesthatasitslabel.
Adiscriminativemodel, bycontrast, isonlytry-
ingtolearntodistinguishtheclasses(perhapswith-
out learning much about them). So maybe all the
dogsinthetrainingdataarewearingcollarsandthe
cats aren’t. If that one feature neatly separates the
classes, the model is satisfied. If you ask such a
modelwhatitknowsaboutcatsallitcansayisthat
theydon’twearcollars.80 CHAPTER5 • LOGISTICREGRESSION
Moreformally,recallthatthenaiveBayesassignsaclassctoadocumentd not
bydirectlycomputingP(cd)butbycomputingalikelihoodandaprior
|
likelihood prior
cˆ=argmax P(d c) P(c) (5.1)
|
c C
∈ (cid:122) (cid:125)(cid:124) (cid:123) (cid:122)(cid:125)(cid:124)(cid:123)
generative A generative model like naive Bayes makes use of this likelihood term, which
model
expresseshowtogeneratethefeaturesofadocumentifweknewitwasofclassc.
discriminative Bycontrastadiscriminativemodelinthistextcategorizationscenarioattempts
model
todirectlycomputeP(cd).Perhapsitwilllearntoassignahighweighttodocument
|
features that directly improve its ability to discriminate between possible classes,
evenifitcouldn’tgenerateanexampleofoneoftheclasses.
Components of a probabilistic machine learning classifier: Like naive Bayes,
logisticregressionisaprobabilisticclassifierthatmakesuseofsupervisedmachine
learning. Machine learning classifiers require a training corpus of m input/output
pairs(x(i),y(i)).(We’llusesuperscriptsinparenthesestorefertoindividualinstances
inthetrainingset—forsentimentclassificationeachinstancemightbeanindividual
document to be classified.) A machine learning system for classification then has
fourcomponents:
1. A feature representation of the input. For each input observation x(i), this
will be a vector of features [x ,x ,...,x ]. We will generally refer to feature
1 2 n
i for input x(j) as x(j) , sometimes simplified as x, but we will also see the
i i
notation f, f(x),or,formulticlassclassification, f(c,x).
i i i
2. A classification function that computes yˆ, the estimated class, via p(yx). In
|
thenextsectionwewillintroducethesigmoidandsoftmaxtoolsforclassifi-
cation.
3. An objective function for learning, usually involving minimizing error on
trainingexamples. Wewillintroducethecross-entropylossfunction.
4. Analgorithmforoptimizingtheobjectivefunction.Weintroducethestochas-
ticgradientdescentalgorithm.
Logisticregressionhastwophases:
training: Wetrainthesystem(specificallytheweightswandb)usingstochastic
gradientdescentandthecross-entropyloss.
test: Givenatestexamplexwecomputep(yx)andreturnthehigherprobability
|
labely=1ory=0.
5.1 The sigmoid function
Thegoalofbinarylogisticregressionistotrainaclassifierthatcanmakeabinary
decisionabouttheclassofanewinputobservation. Hereweintroducethesigmoid
classifierthatwillhelpusmakethisdecision.
Considerasingleinputobservationx,whichwewillrepresentbyavectoroffea-
tures[x ,x ,...,x ](we’llshowsamplefeaturesinthenextsubsection).Theclassifier
1 2 n
outputycanbe1(meaningtheobservationisamemberoftheclass)or0(theob-
servationisnotamemberoftheclass). WewanttoknowtheprobabilityP(y=1x)
|
thatthisobservationisamemberoftheclass. Soperhapsthedecisionis“positive5.1 • THESIGMOIDFUNCTION 81
sentiment”versus“negativesentiment”,thefeaturesrepresentcountsofwordsina
document, P(y=1x) is the probability that the document has positive sentiment,
|
andP(y=0x)istheprobabilitythatthedocumenthasnegativesentiment.
|
Logisticregressionsolvesthistaskbylearning, fromatrainingset, avectorof
weightsandabiasterm.Eachweightw iisarealnumber,andisassociatedwithone
oftheinputfeaturesx. Theweightw representshowimportantthatinputfeature
i i
istotheclassificationdecision,andcanbepositive(providingevidencethatthein-
stancebeingclassifiedbelongsinthepositiveclass)ornegative(providingevidence
that the instance being classified belongs in the negative class). Thus we might
expect in a sentiment task the word awesome to have a high positive weight, and
biasterm abysmaltohaveaverynegativeweight. Thebiasterm,alsocalledtheintercept,is
intercept anotherrealnumberthat’saddedtotheweightedinputs.
Tomakeadecisiononatestinstance—afterwe’velearnedtheweightsintraining—
theclassifierfirstmultiplieseachx byitsweightw,sumsuptheweightedfeatures,
i i
andaddsthebiastermb. Theresultingsinglenumberzexpressestheweightedsum
oftheevidencefortheclass.
n
z = w ix i +b (5.2)
(cid:32) (cid:33)
i=1
(cid:88)
dotproduct In the rest of the book we’ll represent such sums using the dot product notation
fromlinearalgebra. Thedotproductoftwovectorsaandb,writtenasa b,isthe
·
sumoftheproductsofthecorrespondingelementsofeachvector. (Noticethatwe
representvectorsusingtheboldfacenotationb). Thusthefollowingisanequivalent
formationtoEq.5.2:
z = w x+b (5.3)
·
But note that nothing in Eq. 5.3 forces z to be a legal probability, that is, to lie
between 0 and 1. In fact, since weights are real-valued, the output might even be
negative;zrangesfrom ∞to∞.
−
Figure5.1 Thesigmoidfunctionσ(z)= 1 takesarealvalueandmapsittotherange
1+e−z
(0,1).Itisnearlylineararound0butoutliervaluesgetsquashedtoward0or1.
sigmoid To create a probability, we’ll pass z through the sigmoid function, σ(z). The
sigmoidfunction(namedbecauseitlookslikeans)isalsocalledthelogisticfunc-
logistic tion,andgiveslogisticregressionitsname.Thesigmoidhasthefollowingequation,
function
showngraphicallyinFig.5.1:
1 1
σ(z)= = (5.4)
1+e z 1+exp( z)
− −
(Fortherestofthebook,we’llusethenotationexp(x)tomeanex.) Thesigmoid
hasanumberofadvantages;ittakesareal-valuednumberandmapsitintotherange82 CHAPTER5 • LOGISTICREGRESSION
(0,1),whichisjustwhatwewantforaprobability.Becauseitisnearlylineararound
0butflattenstowardtheends, ittendstosquashoutliervaluestoward0or1. And
it’sdifferentiable,whichaswe’llseeinSection5.10willbehandyforlearning.
We’realmostthere. Ifweapplythesigmoidtothesumoftheweightedfeatures,
we get a number between 0 and 1. To make it a probability, we just need to make
surethatthetwocases, p(y=1)and p(y=0),sumto1. Wecandothisasfollows:
P(y=1) = σ(w x+b)
·
1
=
1+exp( (w x+b))
− ·
P(y=0) = 1 σ(w x+b)
− ·
1
= 1
−1+exp( (w x+b))
− ·
exp( (w x+b))
= − · (5.5)
1+exp( (w x+b))
− ·
Thesigmoidfunctionhastheproperty
1 σ(x)=σ( x) (5.6)
− −
sowecouldalsohaveexpressedP(y=0)asσ( (w x+b)).
− ·
5.2 Classification with Logistic Regression
Thesigmoidfunctionfromthepriorsectionthusgivesusawaytotakeaninstance
xandcomputetheprobabilityP(y=1x).
|
Howdowemakeadecisionaboutwhichclasstoapplytoatestinstancex? For
agivenx,wesayyesiftheprobabilityP(y=1x)ismorethan.5,andnootherwise.
|
decision Wecall.5thedecisionboundary:
boundary
1 if P(y=1x)>0.5
decision(x) = |
0 otherwise
(cid:26)
Let’shavesomeexamplesofapplyinglogisticregressionasaclassifierforlanguage
tasks.
5.2.1 SentimentClassification
Suppose we are doing binary sentiment classification on movie review text, and
we would like to know whether to assign the sentiment class + or to a review
−
documentdoc. We’llrepresenteachinputobservationbythe6featuresx ...x of
1 6
theinputshowninthefollowingtable;Fig.5.2showsthefeaturesinasamplemini5.2 • CLASSIFICATIONWITHLOGISTICREGRESSION 83
testdocument.
Var Definition ValueinFig.5.2
x count(positivelexiconwords doc) 3
1
∈
x count(negativelexiconwords doc) 2
2
∈
1 if “no” doc
x 3 0 otherwis∈ e 1
(cid:26)
x count(1stand2ndpronouns doc) 3
4
∈
1 if “!” doc
x 5 0 otherw∈ ise 0
(cid:26)
x ln(wordcountofdoc) ln(66)=4.19
6
Let’s assume for the moment that we’ve already learned a real-valued weight for
x =2
2
x =1
3
It's hokey . There are virtually no surprises , and the writing is second-rate .
So why was it so enjoyable ? For one thing , the cast is
great . Another nice touch is the music . I was overcome with the urge to get off
the couch and start dancing . It sucked me in , and it'll do the same to you .
x =3
x =3 x =0 x =4.19 4
1 5 6
Figure5.2 Asampleminitestdocumentshowingtheextractedfeaturesinthevectorx.
each of these features, and that the 6 weights corresponding to the 6 features are
[2.5, 5.0, 1.2,0.5,2.0,0.7],whileb=0.1. (We’lldiscussinthenextsectionhow
− −
the weights are learned.) The weight w , for example indicates how important a
1
feature the number of positive lexicon words (great, nice, enjoyable, etc.) is to
a positive sentiment decision, while w tells us the importance of negative lexicon
2
words.Notethatw =2.5ispositive,whilew = 5.0,meaningthatnegativewords
1 2
−
arenegativelyassociatedwithapositivesentimentdecision,andareabouttwiceas
importantaspositivewords.
Giventhese6featuresandtheinputreviewx, P(+x)andP( x)canbecom-
| −|
putedusingEq.5.5:
p(+x)=P(y=1x) = σ(w x+b)
| | ·
= σ([2.5, 5.0, 1.2,0.5,2.0,0.7] [3,2,1,3,0,4.19]+0.1)
− − ·
= σ(.833)
= 0.70 (5.7)
p( x)=P(y=0x) = 1 σ(w x+b)
−| | − ·
= 0.30
5.2.2 Otherclassificationtasksandfeatures
LogisticregressioniscommonlyappliedtoallsortsofNLPtasks,andanyproperty
period oftheinputcanbeafeature. Considerthetaskofperioddisambiguation:deciding
disambiguation
if a period is the end of a sentence or part of a word, by classifying each period
intooneoftwoclassesEOS(end-of-sentence)andnot-EOS.Wemightusefeatures84 CHAPTER5 • LOGISTICREGRESSION
likex belowexpressingthatthecurrentwordislowercase(perhapswithapositive
1
weight),orthatthecurrentwordisinourabbreviationsdictionary(“Prof.”)(perhaps
withanegativeweight). Afeaturecanalsoexpressaquitecomplexcombinationof
properties. For example a period following an upper case word is likely to be an
EOS, but if the word itself is St. and the previous word is capitalized, then the
periodislikelypartofashorteningofthewordstreet.
1 if “Case(w)=Lower”
x = i
1 0 otherwise
(cid:26)
1 if “w AcronymDict”
x 2 = 0 otherwi ∈ ise
(cid:26)
1 if “w =St.&Case(w )=Cap”
x
3
=
0
otherwi
ise
i −1
(cid:26)
Designing features: Features are generally designed by examining the training
setwithaneyetolinguisticintuitionsandthelinguisticliteratureonthedomain. A
careful error analysis on the training set or devset of an early version of a system
oftenprovidesinsightsintofeatures.
Forsometasksitisespeciallyhelpfultobuildcomplexfeaturesthatarecombi-
nationsofmoreprimitivefeatures. Wesawsuchafeatureforperioddisambiguation
above,whereaperiodonthewordSt. waslesslikelytobetheendofthesentence
ifthepreviouswordwascapitalized. ForlogisticregressionandnaiveBayesthese
feature combinationfeaturesorfeatureinteractionshavetobedesignedbyhand.
interactions
For many tasks (especially when feature values can reference specific words)
we’llneedlargenumbersoffeatures. Oftenthesearecreatedautomaticallyviafea-
feature turetemplates,abstractspecificationsoffeatures. Forexampleabigramtemplate
templates
forperioddisambiguationmightcreateafeatureforeverypairofwordsthatoccurs
before a period in the training set. Thus the feature space is sparse, since we only
havetocreateafeatureifthatn-gramexistsinthatpositioninthetrainingset. The
featureisgenerallycreatedasahashfromthestringdescriptions.Auserdescription
ofafeatureas,“bigram(Americanbreakfast)”ishashedintoauniqueintegerithat
becomesthefeaturenumber f.
i
Inordertoavoidtheextensivehumaneffortoffeaturedesign,recentresearchin
NLPhasfocusedonrepresentationlearning: waystolearnfeaturesautomatically
inanunsupervisedwayfromtheinput. We’llintroducemethodsforrepresentation
learninginChapter6andChapter7.
Scaling input features: When different input features have extremely different
rangesofvalues,it’scommontorescalethemsotheyhavecomparableranges. We
standardize standardizeinputvaluesbycenteringthemtoresultinazeromeanandastandard
z-score deviationofone(thistransformationissometimescalledthez-score). Thatis,ifµ i
isthemeanofthevaluesoffeaturex acrossthemobservationsintheinputdataset,
i
andσ isthestandarddeviationofthevaluesoffeaturesx acrosstheinputdataset,
i i
wecanreplaceeachfeaturex byanewfeaturex computedasfollows:
i (cid:48)i
m m
1 1 2
µ =
x(j)
σ =
x(j)
µ
i m i i (cid:118)m i − i
(cid:88)j=1 (cid:117)
(cid:117)
(cid:88)j=1(cid:16) (cid:17)
x µ(cid:116)
x (cid:48)i = i −
σ
i (5.8)
i
normalize Alternatively,wecannormalizetheinputfeaturesvaluestoliebetween0and1:5.2 • CLASSIFICATIONWITHLOGISTICREGRESSION 85
x min(x)
x (cid:48)i = max(i x−
)
mini
(x)
(5.9)
i i
−
Having input data with comparable range is useful when comparing values across
features. Datascalingisespeciallyimportantinlargeneuralnetworks,sinceithelps
speedupgradientdescent.
5.2.3 Processingmanyexamplesatonce
We’veshowntheequationsforlogisticregressionforasingleexample. Butinprac-
tice we’ll of course want to process an entire test set with many examples. Let’s
suppose we have a test set consisting of m test examples each of which we’d like
toclassify. We’llcontinuetousethenotationfrompage80,inwhichasuperscript
valueinparenthesesreferstotheexampleindexinsomesetofdata(eitherfortrain-
ing or for test). So in this case each test example x(i) has a feature vector x(i),
1 i m. (Asusual,we’llrepresentvectorsandmatricesinbold.)
≤ ≤
Onewaytocomputeeachoutputvalueyˆ(i)isjusttohaveafor-loop,andcompute
eachtestexampleoneatatime:
foreach x(i) ininput[x(1),x(2),...,x(m)]
y(i) =σ(w x(i)+b) (5.10)
·
For the first 3 test examples, then, we would be separately computing the pre-
dictedyˆ(i)asfollows:
P(y(1)=1x(1)) = σ(w x(1)+b)
| ·
P(y(2)=1x(2)) = σ(w x(2)+b)
| ·
P(y(3)=1x(3)) = σ(w x(3)+b)
| ·
But it turns out that we can slightly modify our original equation Eq. 5.5 to do
this much more efficiently. We’ll use matrix arithmetic to assign a class to all the
exampleswithonematrixoperation!
First,we’llpackalltheinputfeaturevectorsforeachinputxintoasingleinput
matrix X, where each row i is a row vector consisting of the feature vector for in-
put example x(i) (i.e., the vector x(i)). Assuming each example has f features and
weights,Xwillthereforebeamatrixofshape[m f],asfollows:
×
x(1) x(1)
...
x(1)
1 2 f
x(2) x(2)
...
x(2)

X = 1 2 f (5.11)
x(3) x(3)
...
x(3)

 1 2 f 
 ... 
 
 
Nowifweintroducebasavectoroflengthmwhichconsistsofthescalarbias
termbrepeatedmtimes,b=[b,b,...,b],andˆy=[yˆ(1),yˆ(2)...,yˆ(m)]asthevectorof
outputs(onescalaryˆ(i) foreachinputx(i) anditsfeaturevectorx(i)), andrepresent
theweightvectorwasacolumnvector,wecancomputealltheoutputswithasingle
matrixmultiplicationandoneaddition:
y = Xw+b (5.12)
YoushouldconvinceyourselfthatEq.5.12computesthesamethingasourfor-loop
inEq.5.10. Forexampleyˆ(1),thefirstentryoftheoutputvectory,willcorrectlybe:
yˆ(1)=[x( 11) ,x 2(1) ,...,x( f1) ] ·[w 1,w 2,...,w f]+b (5.13)86 CHAPTER5 • LOGISTICREGRESSION
NotethatwehadtoreorderXandw fromtheordertheyappearedininEq.5.5to
makethemultiplicationscomeoutproperly. HereisEq.5.12againwiththeshapes
shown:
y = X w + b
(m 1) (m f)(f 1) (m 1) (5.14)
× × × ×
Modern compilers and compute hardware can compute this matrix operation
very efficiently, making the computation much faster, which becomes important
whentrainingortestingonverylargedatasets.
5.2.4 Choosingaclassifier
LogisticregressionhasanumberofadvantagesovernaiveBayes. NaiveBayeshas
overlystrongconditionalindependenceassumptions. Considertwofeatureswhich
arestronglycorrelated; infact,imaginethatwejustaddthesamefeature f twice.
1
NaiveBayeswilltreatbothcopiesof f asiftheywereseparate,multiplyingthem
1
bothin,overestimatingtheevidence. Bycontrast,logisticregressionismuchmore
robust to correlated features; if two features f and f are perfectly correlated, re-
1 2
gression will simply assign part of the weight to w and part to w . Thus when
1 2
there are many correlated features, logistic regression will assign a more accurate
probabilitythannaiveBayes. Sologisticregressiongenerallyworksbetteronlarger
documentsordatasetsandisacommondefault.
Despitethelessaccurateprobabilities,naiveBayesstilloftenmakesthecorrect
classificationdecision. Furthermore, naiveBayes canworkextremelywell(some-
times even better than logistic regression) on very small datasets (Ng and Jordan,
2002)orshortdocuments(WangandManning,2012). Furthermore,naiveBayesis
easytoimplementandveryfasttotrain(there’snooptimizationstep). Soit’sstilla
reasonableapproachtouseinsomesituations.
5.3 Multinomial logistic regression
Sometimes we need more than two classes. Perhaps we might want to do 3-way
sentiment classification (positive, negative, or neutral). Or we could be assigning
someofthelabelswewillintroduceinChapter8,likethepartofspeechofaword
(choosing from 10, 30, or even 50 different parts of speech), or the named entity
typeofaphrase(choosingfromtagslikeperson,location,organization).
multinomial
logistic Insuchcasesweusemultinomiallogisticregression, alsocalledsoftmaxre-
regression
gression(inolderNLPliteratureyouwillsometimesseethenamemaxentclassi-
fier). In multinomial logistic regression we want to label each observation with a
classkfromasetofKclasses,underthestipulationthatonlyoneoftheseclassesis
thecorrectone(sometimescalledhardclassification;anobservationcannotbein
multipleclasses). Let’susethefollowingrepresentation: theoutputyforeachinput
x will be a vector of length K. If class c is the correct class, we’ll set y =1, and
c
setalltheotherelementsofytobe0,i.e.,y =1andy =0 j=c. Avectorlike
c j
∀ (cid:54)
this y, with one value=1 and the rest 0, is called a one-hot vector. The job of the
classifier is to produce an estimate vectorˆy. For each class k, the value yˆ will be
k
theclassifier’sestimateoftheprobability p(y =1x).
k
|5.3 • MULTINOMIALLOGISTICREGRESSION 87
5.3.1 Softmax
The multinomial logistic classifier uses a generalization of the sigmoid, called the
softmax softmax function, to compute p(y k =1x). The softmax function takes a vector
|
z=[z ,z ,...,z ]ofK arbitraryvaluesandmapsthemtoaprobabilitydistribution,
1 2 K
witheachvalueintherange[0,1],andallthevaluessummingto1.Likethesigmoid,
itisanexponentialfunction.
ForavectorzofdimensionalityK,thesoftmaxisdefinedas:
exp(z)
softmax(z i) =
K
expi
(z )
1 ≤i ≤K (5.15)
j=1 j
Thesoftmaxofaninputvectorz=[z(cid:80),z ,...,z ]isthusavectoritself:
1 2 K
exp(z ) exp(z ) exp(z )
softmax(z) = 1 , 2 ,..., K (5.16)
(cid:34) K i=1exp(z i) K i=1exp(z i) K i=1exp(z i)(cid:35)
Thedenominator K exp(cid:80) (z)isusedto(cid:80) normalizeallthev(cid:80) aluesintoprobabilities.
i=1 i
Thusforexamplegivenavector:
(cid:80)
z=[0.6,1.1, 1.5,1.2,3.2, 1.1]
− −
theresulting(rounded)softmax(z)is
[0.055,0.090,0.006,0.099,0.74,0.010]
Likethesigmoid,thesoftmaxhasthepropertyofsquashingvaluestoward0or1.
Thusifoneoftheinputsislargerthantheothers,itwilltendtopushitsprobability
toward1,andsuppresstheprobabilitiesofthesmallerinputs.
5.3.2 Applyingsoftmaxinlogisticregression
When we apply softmax for logistic regression, the input will (just as for the sig-
moid) be the dot product between a weight vector w and an input vector x (plus a
bias). Butnowwe’llneedseparateweightvectorsw andbiasb foreachoftheK
k k
classes. Theprobabilityofeachofouroutputclassesyˆ canthusbecomputedas:
k
exp(w x+b )
p(y k=1x) = k · k (5.17)
| K
exp(w x+b )
j j
·
j=1
(cid:88)
The form of Eq. 5.17 makes it seem that we would compute each output sep-
arately. Instead, it’s more common to set up the equation for more efficient com-
putation by modern vector processing hardware. We’ll do this by representing the
set of K weight vectors as a weight matrixW and a bias vector b. Each row k of
W corresponds to the vector of weights w . W thus has shape [K f], for K the
k
×
numberofoutputclassesand f thenumberofinputfeatures. Thebiasvectorbhas
onevalueforeachoftheK outputclasses. Ifwerepresenttheweightsinthisway,
wecancomputeyˆ,thevectorofoutputprobabilitiesforeachoftheK classes,bya
singleelegantequation:
yˆ =softmax(Wx+b) (5.18)88 CHAPTER5 • LOGISTICREGRESSION
If you work out the matrix arithmetic, you can see that the estimated score of
the first output class yˆ (before we take the softmax) will correctly turn out to be
1
w x+b .
1 1
·
Fig.5.3showsanintuitionoftheroleoftheweightvectorversusweightmatrix
in the computation of the output class probabilities for binary versus multinomial
logisticregression.
BinaryLogisticRegression
p(+) = 1- p(-)
Output y y^
sigmoid [scalar]
w
Weight vector
[1⨉f]
x
Input feature x x x … x
1 2 3 f
vector [f ⨉1]
wordcount positive lexicon count of
=3 words = 1 “no” = 0
Input words dessert was great
MultinomialLogisticRegression
p(+) p(-) p(neut)
Output y y^ ^y y^
softmax 1 2 3 These f red weights
[K⨉1] are a row of W
corresponding
Weight W to weight vector w ,
3
matrix [K⨉f] (= weights for class 3)
x
Input feature x x x … x
1 2 3 f
vector [f⨉1] wordcount positive lexicon count of
=3 words = 1 “no” = 0
Input words dessert was great
Figure5.3 Binaryversusmultinomiallogisticregression.Binarylogisticregressionusesa
singleweightvectorw,andhasascalaroutputyˆ.Inmultinomiallogisticregressionwehave
K separate weight vectors corresponding to the K classes, all packed into a single weight
matrixW,andavectoroutputyˆ.
5.3.3 FeaturesinMultinomialLogisticRegression
Featuresinmultinomiallogisticregressionactlikefeaturesinbinarylogisticregres-
sion, with the difference mentioned above that we’ll need separate weight vectors
andbiasesforeachoftheK classes. Recallourbinaryexclamationpointfeaturex
55.4 • LEARNINGINLOGISTICREGRESSION 89
frompage82:
1 if “!” doc
x 5 = 0 otherw∈ ise
(cid:26)
Inbinaryclassificationapositiveweightw onafeatureinfluencestheclassifier
5
towardy=1(positivesentiment)andanegativeweightinfluencesittowardy=0
(negative sentiment) with the absolute value indicating how important the feature
is. Formultinomiallogisticregression, bycontrast, withseparateweightsforeach
class,afeaturecanbeevidencefororagainsteachindividualclass.
In3-waymulticlasssentimentclassification, forexample, wemustassigneach
documentoneofthe3classes+, ,or0(neutral). Nowafeaturerelatedtoexcla-
−
mationmarksmighthaveanegativeweightfor0documents,andapositiveweight
for+or documents:
−
Feature Definition w w w
5,+ 5, 5,0
−
1 if “!” doc
f 5(x)
0
otherw∈
ise
3.5 3.1 −5.3
(cid:26)
Becausethesefeatureweightsaredependentbothontheinputtextandtheoutput
class,wesometimesmakethisdependenceexplicitandrepresentthefeaturesthem-
selvesas f(x,y): afunctionofboththeinputandtheclass. Usingsuchanotation
f (x) above could be represented as three features f (x,+), f (x, ), and f (x,0),
5 5 5 5
−
eachofwhichhasasingleweight. We’llusethiskindofnotationinourdescription
oftheCRFinChapter8.
5.4 Learning in Logistic Regression
Howaretheparametersofthemodel, theweightsw andbiasb, learned? Logistic
regressionisaninstanceofsupervisedclassificationinwhichweknowthecorrect
labely(either0or1)foreachobservationx. WhatthesystemproducesviaEq.5.5
is yˆ, the system’s estimate of the true y. We want to learn parameters (meaning w
andb)thatmakeyˆforeachtrainingobservationascloseaspossibletothetruey.
This requires two components that we foreshadowed in the introduction to the
chapter. The first is a metric for how close the current label (yˆ) is to the true gold
label y. Rather than measure similarity, we usually talk about the opposite of this:
thedistancebetweenthesystemoutputandthegoldoutput,andwecallthisdistance
loss the loss function or the cost function. In the next section we’ll introduce the loss
functionthatiscommonlyusedforlogisticregressionandalsoforneuralnetworks,
thecross-entropyloss.
Thesecondthingweneedisanoptimizationalgorithmforiterativelyupdating
theweightssoastominimizethislossfunction. Thestandardalgorithmforthisis
gradientdescent;we’llintroducethestochasticgradientdescentalgorithminthe
followingsection.
We’ll describe these algorithms for the simpler case of binary logistic regres-
sion in the next two sections, and then turn to multinomial logistic regression in
Section5.8.90 CHAPTER5 • LOGISTICREGRESSION
5.5 The cross-entropy loss function
Weneedalossfunctionthatexpresses,foranobservationx,howclosetheclassifier
output(yˆ=σ(w x+b))istothecorrectoutput(y,whichis0or1). We’llcallthis:
·
L(yˆ,y) = Howmuchyˆdiffersfromthetruey (5.19)
We do this via a loss function that prefers the correct class labels of the train-
ing examples to be more likely. This is called conditional maximum likelihood
estimation: we choose the parameters w,b that maximize the log probability of
thetrueylabelsinthetrainingdatagiventheobservationsx. Theresultingloss
cross-entropy functionisthenegativeloglikelihoodloss,generallycalledthecross-entropyloss.
loss
Let’s derive this loss function, applied to a single observation x. We’d like to
learnweightsthatmaximizetheprobabilityofthecorrectlabel p(yx). Sincethere
|
areonlytwodiscreteoutcomes(1or0),thisisaBernoullidistribution,andwecan
expresstheprobability p(yx)thatourclassifierproducesforoneobservationasthe
|
following (keeping in mind that if y=1, Eq. 5.20 simplifies to yˆ; if y=0, Eq. 5.20
simplifiesto1 yˆ):
−
p(yx) = yˆy(1 yˆ)1 −y (5.20)
| −
Nowwetakethelogofbothsides. Thiswillturnouttobehandymathematically,
anddoesn’thurtus;whatevervaluesmaximizeaprobabilitywillalsomaximizethe
logoftheprobability:
logp(yx) = log yˆy(1 yˆ)1 −y
| −
= ylog (cid:2)yˆ+(1 y)lo (cid:3)g(1 yˆ) (5.21)
− −
Eq.5.21describesaloglikelihoodthatshouldbemaximized. Inordertoturnthis
intoalossfunction(somethingthatweneedtominimize),we’lljustflipthesignon
Eq.5.21. Theresultisthecross-entropylossL :
CE
L CE(yˆ,y)= logp(yx) = [ylogyˆ+(1 y)log(1 yˆ)] (5.22)
− | − − −
Finally,wecanpluginthedefinitionofyˆ=σ(w x+b):
·
L CE(yˆ,y) = [ylogσ(w x+b)+(1 y)log(1 σ(w x+b))] (5.23)
− · − − ·
Let’sseeifthislossfunctiondoestherightthingforourexamplefromFig.5.2. We
wantthelosstobesmallerifthemodel’sestimateisclosetocorrect,andbiggerif
themodelisconfused. Sofirstlet’ssupposethecorrectgoldlabelforthesentiment
exampleinFig.5.2ispositive,i.e.,y=1. Inthiscaseourmodelisdoingwell,since
fromEq.5.7itindeedgavetheexampleahigherprobabilityofbeingpositive(.70)
thannegative(.30). Ifweplugσ(w x+b)=.70andy=1intoEq.5.23,theright
·
sideoftheequationdropsout,leadingtothefollowingloss(we’lluselogtomean
naturallogwhenthebaseisnotspecified):
L (yˆ,y)= [ylogσ(w x+b)+(1 y)log(1 σ(w x+b))]
CE
− · − − ·
= [logσ(w x+b)]
− ·
= log(.70)
−
= .365.6 • GRADIENTDESCENT 91
Bycontrast,let’spretendinsteadthattheexampleinFig.5.2wasactuallynegative,
i.e., y = 0 (perhaps the reviewer went on to say “But bottom line, the movie is
terrible! Ibegyounottoseeit!”). Inthiscaseourmodelisconfusedandwe’dwant
thelosstobehigher. Nowifweplugy=0and1 σ(w x+b)=.31fromEq.5.7
− ·
intoEq.5.23,theleftsideoftheequationdropsout:
L (yˆ,y)= [ylogσ(w x+b)+(1 y)log(1 σ(w x+b))]
CE
− · − − ·
= [log(1 σ(w x+b))]
− − ·
= log(.30)
−
= 1.2
Sureenough,thelossforthefirstclassifier(.36)islessthanthelossforthesecond
classifier(1.2).
Why does minimizing this negative log probability do what we want? A per-
fect classifier would assign probability 1 to the correct outcome (y=1 or y=0) and
probability 0 to the incorrect outcome. That means if y equals 1, the higher yˆ is
(the closer it is to 1), the better the classifier; the lower yˆ is (the closer it is to 0),
theworsetheclassifier. Ifyequals0, instead, thehigher1 yˆis(closerto1), the
−
bettertheclassifier. Thenegativelogofyˆ(ifthetrueyequals1)or1 yˆ(ifthetrue
−
y equals 0) is a convenient loss metric since it goes from 0 (negative log of 1, no
loss)toinfinity(negativelogof0,infiniteloss). Thislossfunctionalsoensuresthat
astheprobabilityofthecorrectanswerismaximized, theprobabilityoftheincor-
rectanswerisminimized;sincethetwosumtoone,anyincreaseintheprobability
of the correct answer is coming at the expense of the incorrect answer. It’s called
the cross-entropy loss, because Eq. 5.21 is also the formula for the cross-entropy
betweenthetrueprobabilitydistributionyandourestimateddistributionyˆ.
Nowweknowwhatwewanttominimize;inthenextsection,we’llseehowto
findtheminimum.
5.6 Gradient Descent
Our goal with gradient descent is to find the optimal weights: minimize the loss
functionwe’vedefinedforthemodel. InEq.5.24below,we’llexplicitlyrepresent
the fact that the loss function L is parameterized by the weights, which we’ll refer
toinmachinelearningingeneralasθ (inthecaseoflogisticregressionθ =w,b).
Sothegoalistofindthesetofweightswhichminimizesthelossfunction,averaged
overallexamples:
m
1
θˆ = argmin L CE(f(x(i);θ),y(i)) (5.24)
m
θ
i=1
(cid:88)
Howshallwefindtheminimumofthis(orany)lossfunction? Gradientdescentisa
methodthatfindsaminimumofafunctionbyfiguringoutinwhichdirection(inthe
spaceoftheparametersθ)thefunction’sslopeisrisingthemoststeeply,andmoving
intheoppositedirection.Theintuitionisthatifyouarehikinginacanyonandtrying
to descend most quickly down to the river at the bottom, you might look around
yourself 360 degrees, find the direction where the ground is sloping the steepest,
andwalkdownhillinthatdirection.
convex Forlogisticregression,thislossfunctionisconvenientlyconvex.Aconvexfunc-92 CHAPTER5 • LOGISTICREGRESSION
tionhasatmostoneminimum;therearenolocalminimatogetstuckin,sogradient
descent starting from any point is guaranteed to find the minimum. (By contrast,
the loss for multi-layer neural networks is non-convex, and gradient descent may
getstuckinlocalminimaforneuralnetworktrainingandneverfindtheglobalopti-
mum.)
Althoughthealgorithm(andtheconceptofgradient)aredesignedfordirection
vectors, let’s first consider a visualization of the case where the parameter of our
systemisjustasinglescalarw,showninFig.5.4.
Given a random initialization of w at some value w1, and assuming the loss
functionLhappenedtohavetheshapeinFig.5.4,weneedthealgorithmtotellus
whether at the next iteration we should move left (making w2 smaller than w1) or
right(makingw2biggerthanw1)toreachtheminimum.
Loss
one step
slope of loss at w1 of gradient
descent
is negative
w1 wmin
w
0 (goal)
Figure5.4 Thefirststepiniterativelyfindingtheminimumofthislossfunction,bymoving
winthereversedirectionfromtheslopeofthefunction.Sincetheslopeisnegative,weneed
tomovewinapositivedirection,totheright. Heresuperscriptsareusedforlearningsteps,
sow1meanstheinitialvalueofw(whichis0),w2thevalueatthesecondstep,andsoon.
gradient The gradient descent algorithm answers this question by finding the gradient
of the loss function at the current point and moving in the opposite direction. The
gradientofafunctionofmanyvariablesisavectorpointinginthedirectionofthe
greatestincreaseinafunction. Thegradientisamulti-variablegeneralizationofthe
slope, so for a function of one variable like the one in Fig. 5.4, we can informally
thinkofthegradientastheslope. ThedottedlineinFig.5.4showstheslopeofthis
hypotheticallossfunctionatpointw=w1. Youcanseethattheslopeofthisdotted
line is negative. Thus to find the minimum, gradient descent tells us to go in the
oppositedirection: movingwinapositivedirection.
The magnitude of the amount to move in gradient descent is the value of the
learningrate slope d L(f(x;w),y) weighted by a learning rate η. A higher (faster) learning
dw
ratemeansthatweshouldmovewmoreoneachstep. Thechangewemakeinour
parameteristhelearningratetimesthegradient(ortheslope,inoursingle-variable
example):
d
wt+1=wt η L(f(x;w),y) (5.25)
− dw
Now let’s extend the intuition from a function of one scalar variable w to many
variables,becausewedon’tjustwanttomoveleftorright,wewanttoknowwhere
intheN-dimensionalspace(oftheN parametersthatmakeupθ)weshouldmove.
The gradient is just such a vector; it expresses the directional components of the5.6 • GRADIENTDESCENT 93
sharpestslopealongeachofthoseNdimensions.Ifwe’rejustimaginingtwoweight
dimensions(sayforoneweightwandonebiasb),thegradientmightbeavectorwith
twoorthogonalcomponents,eachofwhichtellsushowmuchthegroundslopesin
thewdimensionandinthebdimension. Fig.5.5showsavisualizationofthevalue
ofa2-dimensionalgradientvectortakenattheredpoint.
Inanactuallogisticregression,theparametervectorwismuchlongerthan1or
2, since the input feature vector x can be quite long, and we need a weight w for
i
eachx. Foreachdimension/variablew inw(plusthebiasb),thegradientwillhave
i i
acomponentthattellsustheslopewithrespecttothatvariable. Ineachdimension
w,weexpresstheslopeasapartialderivative ∂ ofthelossfunction. Essentially
i ∂wi
we’re asking: “How much would a small change in that variable w influence the
i
totallossfunctionL?”
Formally, then, the gradient of a multi-variable function f is a vector in which
eachcomponentexpressesthepartialderivativeof f withrespecttooneofthevari-
ables. We’ll use the inverted Greek delta symbol ∇ to refer to the gradient, and
representyˆas f(x;θ)tomakethedependenceonθ moreobvious:
∂ L(f(x;θ),y)
∂w1
∂ L(f(x;θ),y)
∂w2 
.
∇L(f(x;θ),y) = . (5.26)
 . 


∂ L(f(x;θ),y)

∂wn 
 ∂ L(f(x;θ),y)
 ∂b 
 
Thefinalequationforupdatingθ basedonthegradientisthus
θt+1 = θt η∇L(f(x;θ),y) (5.27)
−
5.6.1 TheGradientforLogisticRegression
Inordertoupdateθ,weneedadefinitionforthegradient∇L(f(x;θ),y).Recallthat
forlogisticregression,thecross-entropylossfunctionis:
L CE(yˆ,y) = [ylogσ(w x+b)+(1 y)log(1 σ(w x+b))] (5.28)
− · − − ·
Cost(w,b)
b
w
Figure5.5 Visualizationofthegradientvectorattheredpointintwodimensionswand
b,showingaredarrowinthex-yplanepointinginthedirectionwewillgotolookforthe
minimum:theoppositedirectionofthegradient(recallthatthegradientpointsinthedirection
ofincreasenotdecrease).94 CHAPTER5 • LOGISTICREGRESSION
ItturnsoutthatthederivativeofthisfunctionforoneobservationvectorxisEq.5.29
(theinterestedreadercanseeSection5.10forthederivationofthisequation):
∂L (yˆ,y)
CE = [σ(w x+b) y]x
∂w · − j
j
= (yˆ y)x j (5.29)
−
You’llalsosometimesseethisequationintheequivalentform:
∂L (yˆ,y)
C ∂E w = −(y −yˆ)x j (5.30)
j
Noteintheseequationsthatthegradientwithrespecttoasingleweightw rep-
j
resents a very intuitive value: the difference between the true y and our estimated
yˆ=σ(w x+b) for that observation, multiplied by the corresponding input value
·
x .
j
5.6.2 TheStochasticGradientDescentAlgorithm
Stochasticgradientdescentisanonlinealgorithmthatminimizesthelossfunction
by computing its gradient after each training example, and nudging θ in the right
direction(theoppositedirectionofthegradient). (An“onlinealgorithm”isonethat
processes its input example by example, rather than waiting until it sees the entire
input.) Fig.5.6showsthealgorithm.
functionSTOCHASTICGRADIENTDESCENT(L(), f(),x,y)returnsθ
#where:Listhelossfunction
# fisafunctionparameterizedbyθ
# xisthesetoftraininginputsx(1), x(2),..., x(m)
# yisthesetoftrainingoutputs(labels)y(1), y(2),..., y(m)
θ 0
←
repeattildone #seecaption
Foreachtrainingtuple(x(i), y(i))(inrandomorder)
1.Optional(forreporting): #Howarewedoingonthistuple?
Computeyˆ(i) = f(x(i);θ) #Whatisourestimatedoutputyˆ?
ComputethelossL(yˆ(i),y(i)) #Howfaroffisyˆ(i)fromthetrueoutputy(i)?
2.g ∇ θL(f(x(i);θ),y(i)) #Howshouldwemoveθ tomaximizeloss?
←
3.θ θ η g #Gotheotherwayinstead
← −
returnθ
Figure5.6 Thestochasticgradientdescentalgorithm. Step1(computingtheloss)isused
mainlytoreporthowwellwearedoingonthecurrenttuple;wedon’tneedtocomputethe
loss in order to compute the gradient. The algorithm can terminate when it converges (or
whenthegradientnorm<(cid:15)),orwhenprogresshalts(forexamplewhenthelossstartsgoing
uponaheld-outset).
hyperparameter Thelearningrateη isahyperparameterthatmustbeadjusted. Ifit’stoohigh,
thelearnerwilltakestepsthataretoolarge,overshootingtheminimumoftheloss
function. If it’stoolow, thelearner willtakesteps thatare toosmall, and taketoo
longtogettotheminimum.Itiscommontostartwithahigherlearningrateandthen
slowlydecreaseit,sothatitisafunctionoftheiterationk oftraining; thenotation
η canbeusedtomeanthevalueofthelearningrateatiterationk.
k5.6 • GRADIENTDESCENT 95
We’lldiscusshyperparametersinmoredetailinChapter7,butinshort,theyare
aspecialkindofparameterforanymachinelearningmodel. Unlikeregularparam-
eters of a model (weights like w and b), which are learned by the algorithm from
the training set, hyperparameters are special parameters chosen by the algorithm
designerthataffecthowthealgorithmworks.
5.6.3 Workingthroughanexample
Let’s walk through a single step of the gradient descent algorithm. We’ll use a
simplifiedversionoftheexampleinFig.5.2asitseesasingleobservationx,whose
correctvalueisy=1(thisisapositivereview),andwithafeaturevectorx=[x ,x ]
1 2
consistingofthesetwofeatures:
x = 3 (countofpositivelexiconwords)
1
x = 2 (countofnegativelexiconwords)
2
Let’sassumetheinitialweightsandbiasinθ0areallsetto0,andtheinitiallearning
rateη is0.1:
w =w =b = 0
1 2
η = 0.1
The single update step requires that we compute the gradient, multiplied by the
learningrate
θt+1=θt η∇ L(f(x(i);θ),y(i))
θ
−
Inourminiexampletherearethreeparameters,sothegradientvectorhas3dimen-
sions,forw ,w ,andb. Wecancomputethefirstgradientasfollows:
1 2
∇ w,bL= 


∂ ∂∂ L LL C CC ∂ ∂ ∂E EE w w b( (( 1 2y yy ˆ ˆˆ , ,, y yy ) )) 

=  ( ( σσ σ (( ( ww w ·· · xx x ++ + bb b )) ) −− − yy y) )x x1 2  =  ( ( σσ σ (( ( 00 0 )) ) −− − 11 1) )x x1 2  =  − − −0 0 0. . .5 5 5x x1 2  =  − − −1 1 0. . .5 0 5

Nowthatwehaveagradient,wecomputethenewparametervectorθ1 bymoving
θ0intheoppositedirectionfromthegradient:
w 1.5 .15
1
−
θ1= w η 1.0 = .1
2
 − −   
b 0.5 .05
−
     
So after one step of gradient descent, the weights have shifted to be: w =.15,
1
w =.1,andb=.05.
2
Notethatthisobservationxhappenedtobeapositiveexample.Wewouldexpect
that after seeing more negative examples with high counts of negative words, that
theweightw wouldshifttohaveanegativevalue.
2
5.6.4 Mini-batchtraining
Stochasticgradientdescentiscalledstochasticbecauseitchoosesasinglerandom
exampleatatime,movingtheweightssoastoimproveperformanceonthatsingle
example.Thatcanresultinverychoppymovements,soit’scommontocomputethe
gradientoverbatchesoftraininginstancesratherthanasingleinstance.96 CHAPTER5 • LOGISTICREGRESSION
batchtraining Forexampleinbatchtrainingwecomputethegradientovertheentiredataset.
By seeing so many examples, batch training offers a superb estimate of which di-
rectiontomovetheweights, atthecostofspendingalotoftimeprocessingevery
singleexampleinthetrainingsettocomputethisperfectdirection.
mini-batch Acompromiseismini-batchtraining: wetrainonagroupofmexamples(per-
haps512,or1024)thatislessthanthewholedataset. (Ifmisthesizeofthedataset,
thenwearedoingbatchgradientdescent; ifm=1, wearebacktodoingstochas-
tic gradient descent.) Mini-batch training also has the advantage of computational
efficiency. Themini-batchescaneasilybevectorized,choosingthesizeofthemini-
batchbasedonthecomputationalresources. Thisallowsustoprocessalltheexam-
plesinonemini-batchinparallelandthenaccumulatetheloss,somethingthat’snot
possiblewithindividualorbatchtraining.
We just need to define mini-batch versions of the cross-entropy loss function
wedefinedinSection5.5andthegradientinSection5.6.1. Let’sextendthecross-
entropylossforoneexamplefromEq.5.22tomini-batchesofsizem.We’llcontinue
tousethenotationthatx(i) andy(i) meantheithtrainingfeaturesandtraininglabel,
respectively. Wemaketheassumptionthatthetrainingexamplesareindependent:
m
logp(traininglabels) = log p(y(i) x(i))
|
i=1
(cid:89)
m
= logp(y(i) x(i))
|
i=1
(cid:88)
m
= L CE(yˆ(i),y(i)) (5.31)
−
i=1
(cid:88)
Nowthecostfunctionforthemini-batchofmexamplesistheaveragelossforeach
example:
m
1
Cost(yˆ,y) = L (yˆ(i),y(i))
CE
m
i=1
(cid:88)
m
1
= y(i)logσ(w x(i)+b)+(1 y(i))log 1 σ(w x(i)+b) (5.32)
−m · − − ·
(cid:88)i=1 (cid:16) (cid:17)
Themini-batchgradientistheaverageoftheindividualgradientsfromEq.5.29:
m
∂Cost(yˆ,y) 1
= σ(w x(i)+b) y(i) x(i) (5.33)
∂w m · − j
j
(cid:88)i=1(cid:104) (cid:105)
Insteadofusingthesumnotation,wecanmoreefficientlycomputethegradient
in its matrix form, following the vectorization we saw on page 85, where we have
amatrixXofsize[m f]representingtheminputsinthebatch,andavectoryof
×
size[m 1]representingthecorrectoutputs:
×
∂Cost(yˆ,y) 1 (cid:124)
= (yˆ y) X
∂w m −
1 (cid:124)
= (σ(Xw+b) y) X (5.34)
m −5.7 • REGULARIZATION 97
5.7 Regularization
Numquamponendaestpluralitassinenecessitate
‘Pluralityshouldneverbeproposedunlessneeded’
WilliamofOccam
Thereisaproblemwithlearningweightsthatmakethemodelperfectlymatchthe
trainingdata. Ifafeatureisperfectlypredictiveoftheoutcomebecauseithappens
toonlyoccurinoneclass, itwillbeassignedaveryhighweight. Theweightsfor
featureswillattempttoperfectlyfitdetailsofthetrainingset, infacttooperfectly,
modelingnoisyfactorsthatjustaccidentallycorrelatewiththeclass.Thisproblemis
overfitting calledoverfitting.Agoodmodelshouldbeabletogeneralizewellfromthetraining
generalize datatotheunseentestset,butamodelthatoverfitswillhavepoorgeneralization.
regularization To avoid overfitting, a new regularization term R(θ) is added to the objective
function in Eq. 5.24, resulting in the following objective for a batch of m exam-
ples (slightlyrewritten fromEq. 5.24 tobe maximizinglog probabilityrather than
minimizingloss,andremovingthe 1 termwhichdoesn’taffecttheargmax):
m
m
θˆ = argmax logP(y(i) x(i)) αR(θ) (5.35)
| −
θ
i=1
(cid:88)
ThenewregularizationtermR(θ)isusedtopenalizelargeweights. Thusasetting
oftheweightsthatmatchesthetrainingdataperfectly—butusesmanyweightswith
highvaluestodoso—willbepenalizedmorethanasettingthatmatchesthedataa
littlelesswell, butdoessousingsmallerweights. Therearetwocommonwaysto
L2 computethisregularizationtermR(θ). L2regularizationisaquadraticfunctionof
regularization
theweightvalues,namedbecauseitusesthe(squareofthe)L2normoftheweight
values. TheL2norm, θ 2,isthesameastheEuclideandistanceofthevectorθ
|| ||
fromtheorigin. Ifθ consistsofnweights,then:
n
R(θ) = θ 2= θ2 (5.36)
|| ||2 j
j=1
(cid:88)
TheL2regularizedobjectivefunctionbecomes:
m n
θˆ = argmax logP(y(i) x(i)) α θ2 (5.37)
θ (cid:34) | (cid:35)− j
i=1 j=1
(cid:88) (cid:88)
L1 L1regularizationisalinearfunctionoftheweightvalues,namedaftertheL1norm
regularization
W 1, thesumoftheabsolutevaluesoftheweights, orManhattandistance(the
|| ||
Manhattandistanceisthedistanceyou’dhavetowalkbetweentwopointsinacity
withastreetgridlikeNewYork):
n
R(θ) = θ 1= θ i (5.38)
|| || | |
i=1
(cid:88)
TheL1regularizedobjectivefunctionbecomes:
m n
θˆ = argmax logP(y(i) x(i)) α θ j (5.39)
θ (cid:34) | (cid:35)− | |
1=i j=1
(cid:88) (cid:88)98 CHAPTER5 • LOGISTICREGRESSION
Thesekindsofregularizationcomefromstatistics,whereL1regularizationiscalled
lasso lassoregression(Tibshirani,1996)andL2regularizationiscalledridgeregression,
ridge andbotharecommonlyusedinlanguageprocessing. L2regularizationiseasierto
optimize because of its simple derivative (the derivative of θ2 is just 2θ), while
L1regularizationismorecomplex(thederivativeof θ isnon-continuousatzero).
| |
But while L2 prefers weight vectors with many small weights, L1 prefers sparse
solutions with some larger weights but many more weights set to zero. Thus L1
regularizationleadstomuchsparserweightvectors,thatis,farfewerfeatures.
Both L1 and L2 regularization have Bayesian interpretations as constraints on
thepriorofhowweightsshouldlook. L1regularizationcanbeviewedasaLaplace
prior on the weights. L2 regularization corresponds to assuming that weights are
distributed according to a Gaussian distribution with mean µ =0. In a Gaussian
or normal distribution, the further away a value is from the mean, the lower its
probability(scaledbythevarianceσ).ByusingaGaussianpriorontheweights,we
aresayingthatweightsprefertohavethevalue0. AGaussianforaweightθ is
j
1 (θ µ )2
j j
exp − (5.40)
2πσ2 (cid:32)− 2σ2 j (cid:33)
j
(cid:113)
IfwemultiplyeachweightbyaGaussianpriorontheweight,wearethusmaximiz-
ingthefollowingconstraint:
m n 1 (θ µ )2
θˆ = argmax P(y(i) x(i)) exp j − j (5.41)
θ (cid:89)i=1 | × (cid:89)j=1 2πσ2 j (cid:32)− 2σ2 j (cid:33)
(cid:113)
whichinlogspace,withµ =0,andassuming2σ2=1,correspondsto
m n
θˆ = argmax logP(y(i) x(i)) α θ2 (5.42)
| − j
θ
i=1 j=1
(cid:88) (cid:88)
whichisinthesameformasEq.5.37.
5.8 Learning in Multinomial Logistic Regression
The loss function for multinomial logistic regression generalizes the loss function
forbinarylogisticregressionfrom2toK classes. Recallthatthatthecross-entropy
lossforbinarylogisticregression(repeatedfromEq.5.22)is:
L CE(yˆ,y)= logp(yx) = [ylogyˆ+(1 y)log(1 yˆ)] (5.43)
− | − − −
The loss function for multinomial logistic regression generalizes the two terms in
Eq.5.43(onethatisnon-zerowheny=1andonethatisnon-zerowheny=0)to
Kterms. Aswementionedabove,formultinomialregressionwe’llrepresentbothy
andyˆ asvectors. Thetruelabely isavectorwithK elements,eachcorresponding
toaclass,withy =1ifthecorrectclassisc,withallotherelementsofybeing0.
c
AndourclassifierwillproduceanestimatevectorwithK elementsyˆ,eachelement
yˆ ofwhichrepresentstheestimatedprobability p(y =1x).
k k
|
The loss function for a single example x, generalizing from binary logistic re-
gression, is the sum of the logs of the K output classes, each weighted by their5.9 • INTERPRETINGMODELS 99
probabilityy (Eq.5.44). Thisturnsouttobejustthenegativelogprobabilityofthe
k
correctclassc(Eq.5.45):
K
L CE(yˆ,y) = y klogyˆ k (5.44)
−
k=1
(cid:88)
= logyˆ c, (wherecisthecorrectclass) (5.45)
−
= logpˆ(y =1x) (wherecisthecorrectclass)
c
− |
exp(w x+b )
c c
= log · (cisthecorrectclass) (5.46)
− K exp(w x+b )
j=1 j · j
HowdidwegetfromEq.5.4(cid:80)4toEq.5.45? Becauseonlyoneclass(let’scallitc)is
thecorrectone,thevectorytakesthevalue1onlyforthisvalueofk,i.e.,hasy =1
c
andy =0 j=c. ThatmeansthetermsinthesuminEq.5.44willallbe0except
j
∀ (cid:54)
forthetermcorrespondingtothetrueclassc.Hencethecross-entropylossissimply
thelogoftheoutputprobabilitycorrespondingtothecorrectclass,andwetherefore
negativelog alsocallEq.5.45thenegativeloglikelihoodloss.
likelihoodloss
Ofcourseforgradientdescentwedon’tneedtheloss,weneeditsgradient. The
gradientforasingleexampleturnsouttobeverysimilartothegradientforbinary
logisticregression,(yˆ y)x,thatwesawinEq.5.29. Let’sconsideronepieceofthe
−
gradient, the derivative for a single weight. For each class k, the weight of the ith
elementofinputxisw . Whatisthepartialderivativeofthelosswithrespectto
k,i
w ?Thisderivativeturnsouttobejustthedifferencebetweenthetruevalueforthe
k,i
classk(whichiseither1or0)andtheprobabilitytheclassifieroutputsforclassk,
weightedbythevalueoftheinputx correspondingtotheithelementoftheweight
i
vectorforclassk:
∂L
CE = (y yˆ )x
∂w − k − k i
k,i
= (y p(y =1x))x
k k i
− − |
exp(w x+b )
= −(cid:32)y k − K j=1expk (· w
j
·x+k b j)(cid:33)x i (5.47)
We’llreturntothiscaseofthegradie(cid:80)ntforsoftmaxregressionwhenweintroduce
neural networks in Chapter 7, and at that time we’ll also discuss the derivation of
thisgradientinequationsEq.7.35–Eq.7.43.
5.9 Interpreting models
Often we want to know more than just the correct classification of an observation.
Wewanttoknowwhytheclassifiermadethedecisionitdid. Thatis,wewantour
interpretable decisiontobeinterpretable. Interpretabilitycanbehardtodefinestrictly, butthe
coreideaisthatashumansweshouldknowwhyouralgorithmsreachtheconclu-
sionstheydo. Becausethefeaturestologisticregressionareoftenhuman-designed,
one way to understand a classifier’s decision is to understand the role each feature
playsinthedecision. Logisticregressioncanbecombinedwithstatisticaltests(the
likelihood ratio test, or the Wald test); investigating whether a particular feature is
significantbyoneofthesetests,orinspectingitsmagnitude(howlargeistheweight100 CHAPTER5 • LOGISTICREGRESSION
w associated with the feature?) can help us interpret why the classifier made the
decisionitmakes. Thisisenormouslyimportantforbuildingtransparentmodels.
Furthermore,inadditiontoitsuseasaclassifier,logisticregressioninNLPand
manyotherfieldsiswidelyusedasananalytictoolfortestinghypothesesaboutthe
effectofvariousexplanatoryvariables(features). Intextclassification,perhapswe
wanttoknowiflogicallynegativewords(no,not,never)aremorelikelytobeasso-
ciatedwith negativesentiment, or ifnegative reviewsofmovies aremore likelyto
discussthecinematography. However, indoingsoit’snecessarytocontrolforpo-
tentialconfounds:otherfactorsthatmightinfluencesentiment(themoviegenre,the
yearitwasmade,perhapsthelengthofthereviewinwords). Orwemightbestudy-
ing the relationship between NLP-extracted linguistic features and non-linguistic
outcomes(hospitalreadmissions,politicaloutcomes,orproductsales),butneedto
controlforconfounds(theageofthepatient,thecountyofvoting,thebrandofthe
product). Insuchcases,logisticregressionallowsustotestwhethersomefeatureis
associatedwithsomeoutcomeaboveandbeyondtheeffectofotherfeatures.
5.10 Advanced: Deriving the Gradient Equation
Inthissectionwegivethederivationofthegradientofthecross-entropylossfunc-
tion L for logistic regression. Let’s start with some quick calculus refreshers.
CE
First,thederivativeofln(x):
d 1
ln(x)= (5.48)
dx x
Second,the(veryelegant)derivativeofthesigmoid:
dσ(z)
=σ(z)(1 σ(z)) (5.49)
dz −
chainrule Finally,thechainruleofderivatives. Supposewearecomputingthederivative
ofacompositefunction f(x)=u(v(x)). Thederivativeof f(x)isthederivativeof
u(x)withrespecttov(x)timesthederivativeofv(x)withrespecttox:
df du dv
= (5.50)
dx dv·dx
First,wewanttoknowthederivativeofthelossfunctionwithrespecttoasingle
weightw (we’llneedtocomputeitforeachweight,andforthebias):
j
∂L ∂
CE = [ylogσ(w x+b)+(1 y)log(1 σ(w x+b))]
∂w ∂w − · − − ·
j j
∂ ∂
= ylogσ(w x+b)+ (1 y)log[1 σ(w x+b)]
− ∂w · ∂w − − ·
(cid:20) j j (cid:21)
(5.51)
Next,usingthechainrule,andrelyingonthederivativeoflog:
∂L y ∂ 1 y ∂
CE = σ(w x+b) − 1 σ(w x+b)
∂w −σ(w x+b)∂w · −1 σ(w x+b)∂w − ·
j j j
· − ·
(5.52)5.11 • SUMMARY 101
Rearrangingterms:
∂L y 1 y ∂
CE = − σ(w x+b)
∂w − σ(w x+b)−1 σ(w x+b) ∂w ·
j (cid:20) · − · (cid:21) j
(5.53)
And now plugging in the derivative of the sigmoid, and using the chain rule one
moretime,weendupwithEq.5.54:
∂L y σ(w x+b) ∂(w x+b)
CE = − · σ(w x+b)[1 σ(w x+b)] ·
∂w − σ(w x+b)[1 σ(w x+b)] · − · ∂w
j (cid:20) · − · (cid:21) j
y σ(w x+b)
= − · σ(w x+b)[1 σ(w x+b)]x
− σ(w x+b)[1 σ(w x+b)] · − · j
(cid:20) · − · (cid:21)
= [y σ(w x+b)]x
j
− − ·
= [σ(w x+b) y]x j (5.54)
· −
5.11 Summary
Thischapterintroducedthelogisticregressionmodelofclassification.
• Logistic regression is a supervised machine learning classifier that extracts
real-valuedfeaturesfromtheinput, multiplieseachbyaweight, sumsthem,
and passes the sum through a sigmoid function to generate a probability. A
thresholdisusedtomakeadecision.
• Logistic regression can be used with two classes (e.g., positive and negative
sentiment)orwithmultipleclasses(multinomiallogisticregression,forex-
ampleforn-arytextclassification,part-of-speechlabeling,etc.).
• Multinomiallogisticregressionusesthesoftmaxfunctiontocomputeproba-
bilities.
• Theweights(vectorwandbiasb)arelearnedfromalabeledtrainingsetviaa
lossfunction,suchasthecross-entropyloss,thatmustbeminimized.
• Minimizingthislossfunctionisaconvexoptimizationproblem,anditerative
algorithmslikegradientdescentareusedtofindtheoptimalweights.
• Regularizationisusedtoavoidoverfitting.
• Logisticregressionisalsooneofthemostusefulanalytictools,becauseofits
abilitytotransparentlystudytheimportanceofindividualfeatures.
Bibliographical and Historical Notes
Logistic regression was developed in the field of statistics, where it was used for
theanalysisofbinarydatabythe1960s,andwasparticularlycommoninmedicine
(Cox,1969). Startinginthelate1970sitbecamewidelyusedinlinguisticsasone
of the formal foundations of the study of linguistic variation (Sankoff and Labov,
1979).
Nonetheless,logisticregressiondidn’tbecomecommoninnaturallanguagepro-
cessing until the 1990s, when it seems to have appeared simultaneously from two
directions. Thefirstsourcewastheneighboringfieldsofinformationretrievaland102 CHAPTER5 • LOGISTICREGRESSION
speech processing, both of which had made use of regression, and both of which
lent many other statistical techniques to NLP. Indeed a very early use of logistic
regressionfordocumentroutingwasoneofthefirstNLPapplicationstouse(LSI)
embeddingsaswordrepresentations(Schu¨tzeetal.,1995).
At the same time in the early 1990s logistic regression was developed and ap-
maximum plied to NLP at IBM Research under the name maximum entropy modeling or
entropy
maxent(Bergeretal.,1996),seeminglyindependentofthestatisticalliterature.Un-
derthatnameitwasappliedtolanguagemodeling(Rosenfeld,1996),part-of-speech
tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution
(Kehler,1997b),andtextclassification(Nigametal.,1999).
Moreonclassificationcanbefoundinmachinelearningtextbooks(Hastieetal.
2001,WittenandFrank2005,Bishop2006,Murphy2012).
ExercisesCHAPTER
6 Vector Semantics and
Embeddings
荃者所以在鱼，得鱼而忘荃 Netsareforfish;
Onceyougetthefish,youcanforgetthenet.
言者所以在意，得意而忘言 Wordsareformeaning;
Onceyougetthemeaning,youcanforgetthewords
庄子(Zhuangzi),Chapter26
TheasphaltthatLosAngelesisfamousforoccursmainlyonitsfreeways. But
in the middle of the city is another patch of asphalt, the La Brea tar pits, and this
asphaltpreservesmillionsoffossilbonesfromthelastoftheIceAgesofthePleis-
toceneEpoch. OneofthesefossilsistheSmilodon,orsaber-toothedtiger,instantly
recognizablebyitslongcanines.Fivemillionyearsagoorso,acompletelydifferent
sabre-tooth tiger called Thylacosmilus lived
in Argentina and other parts of South Amer-
ica. Thylacosmilus was a marsupial whereas
Smilodonwasaplacentalmammal, butThy-
lacosmilus had the same long upper canines
and, like Smilodon, had a protective bone
flange on the lower jaw. The similarity of
thesetwomammalsisoneofmanyexamples
of parallel or convergent evolution, in which particular contexts or environments
leadtotheevolutionofverysimilarstructuresindifferentspecies(Gould,1980).
The role of context is also important in the similarity of a less biological kind
of organism: the word. Words that occur in similar contexts tend to have similar
meanings. Thislinkbetweensimilarityinhowwordsaredistributedandsimilarity
distributional in what they mean is called the distributional hypothesis. The hypothesis was
hypothesis
firstformulatedinthe1950sbylinguistslikeJoos(1950),Harris(1954),andFirth
(1957), who noticed that words which are synonyms (like oculist and eye-doctor)
tended to occur in the same environment (e.g., near words like eye or examined)
withtheamountofmeaningdifferencebetweentwowords“correspondingroughly
totheamountofdifferenceintheirenvironments”(Harris,1954,157).
vector Inthischapterweintroducevectorsemantics,whichinstantiatesthislinguistic
semantics
embeddings hypothesisbylearningrepresentationsofthemeaningofwords,calledembeddings,
directlyfromtheirdistributionsintexts.Theserepresentationsareusedineverynat-
urallanguageprocessingapplicationthatmakesuseofmeaning,andthestaticem-
beddingsweintroducehereunderliethemorepowerfuldynamicorcontextualized
embeddingslikeBERTthatwewillseeinChapter11.
These word representations are also the first example in this book of repre-
representation sentationlearning, automaticallylearningusefulrepresentationsoftheinputtext.
learning
Findingsuchself-supervisedwaystolearnrepresentationsoftheinput, insteadof
creatingrepresentationsbyhandviafeatureengineering,isanimportantfocusof
NLPresearch(Bengioetal.,2013).104 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS
6.1 Lexical Semantics
Let’s begin by introducing some basic principles of word meaning. How should
we represent the meaning of a word? In the n-gram models of Chapter 3, and in
classicalNLPapplications,ouronlyrepresentationofawordisasastringofletters,
or an index in a vocabulary list. This representation is not that different from a
traditioninphilosophy,perhapsyou’veseenitinintroductorylogicclasses,inwhich
the meaning of words is represented by just spelling the word with small capital
letters;representingthemeaningof“dog”asDOG,and“cat”asCAT,orbyusingan
apostrophe(DOG’).
Representingthemeaningofawordbycapitalizingitisaprettyunsatisfactory
model.YoumighthaveseenaversionofajokedueoriginallytosemanticistBarbara
Partee(Carlson,1977):
Q:What’sthemeaningoflife?
A:LIFE’
Surelywecandobetterthanthis!Afterall,we’llwantamodelofwordmeaning
todoallsortsofthingsforus. Itshouldtellusthatsomewordshavesimilarmean-
ings(catissimilartodog),othersareantonyms(coldistheoppositeofhot),some
have positive connotations (happy) while others have negative connotations (sad).
It should represent the fact that the meanings of buy, sell, and pay offer differing
perspectivesonthesameunderlyingpurchasingevent(IfIbuysomethingfromyou,
you’ve probably sold it to me, and I likely paid you). More generally, a model of
wordmeaningshouldallowustodrawinferencestoaddressmeaning-relatedtasks
likequestion-answeringordialogue.
Inthissectionwesummarizesomeofthesedesiderata,drawingonresultsinthe
lexical linguisticstudyofwordmeaning,whichiscalledlexicalsemantics;we’llreturnto
semantics
andexpandonthislistinChapter23andChapter24.
LemmasandSenses Let’sstartbylookingathowoneword(we’llchoosemouse)
mightbedefinedinadictionary(simplifiedfromtheonlinedictionaryWordNet):
mouse (N)
1. any of numerous small rodents...
2. a hand-operated device that controls a cursor...
lemma Here the form mouse is the lemma, also called the citation form. The form
citationform mousewouldalsobethelemmaforthewordmice;dictionariesdon’thaveseparate
definitionsforinflectedformslikemice. Similarlysingisthelemmaforsing,sang,
sung. In many languages the infinitive form is used as the lemma for the verb, so
Spanishdormir“tosleep”isthelemmaforduermes“yousleep”.Thespecificforms
wordform sungorcarpetsorsingorduermesarecalledwordforms.
As the example above shows, each lemma can have multiple meanings; the
lemma mouse can refer to the rodent or the cursor control device. We call each
oftheseaspectsofthemeaningofmouseawordsense. Thefactthatlemmascan
bepolysemous(havemultiplesenses)canmakeinterpretationdifficult(issomeone
whotypes“mouseinfo”intoasearchenginelookingforapetoratool?).Chapter23
willdiscusstheproblemofpolysemy, andintroducewordsensedisambiguation,
thetaskofdeterminingwhichsenseofawordisbeingusedinaparticularcontext.
Synonymy One important component of word meaning is the relationship be-
tween word senses. For example when one word has a sense whose meaning is6.1 • LEXICALSEMANTICS 105
identical to a sense of another word, or nearly identical, we say the two senses of
synonym thosetwowordsaresynonyms. Synonymsincludesuchpairsas
couch/sofa vomit/throwup filbert/hazelnut car/automobile
Amoreformaldefinitionofsynonymy(betweenwordsratherthansenses)isthat
twowordsaresynonymousiftheyaresubstitutableforoneanotherinanysentence
without changing the truth conditions of the sentence, the situations in which the
sentencewouldbetrue. Weoftensayinthiscasethatthetwowordshavethesame
propositional propositionalmeaning.
meaning
While substitutions between some pairs of words like car / automobile or wa-
ter/H Oaretruthpreserving,thewordsarestillnotidenticalinmeaning. Indeed,
2
probablynotwowordsareabsolutelyidenticalinmeaning. Oneofthefundamental
principleof tenetsofsemantics,calledtheprincipleofcontrast(Girard1718,Bre´al1897,Clark
contrast
1987),statesthatadifferenceinlinguisticformisalwaysassociatedwithsomedif-
ference in meaning. For example, the word H O is used in scientific contexts and
2
wouldbeinappropriateinahikingguide—waterwouldbemoreappropriate—and
thisgenredifferenceispartofthemeaningoftheword. Inpractice,thewordsyn-
onymisthereforeusedtodescribearelationshipofapproximateorroughsynonymy.
Word Similarity While words don’t have many synonyms, most words do have
lotsofsimilarwords. Catisnotasynonymofdog,butcatsanddogsarecertainly
similarwords.Inmovingfromsynonymytosimilarity,itwillbeusefultoshiftfrom
talking about relations between word senses (like synonymy) to relations between
words(likesimilarity). Dealingwithwordsavoidshavingtocommittoaparticular
representationofwordsenses,whichwillturnouttosimplifyourtask.
similarity Thenotionofwordsimilarityisveryusefulinlargersemantictasks. Knowing
howsimilartwowordsarecanhelpincomputinghowsimilarthemeaningoftwo
phrasesorsentencesare,averyimportantcomponentoftaskslikequestionanswer-
ing,paraphrasing,andsummarization.Onewayofgettingvaluesforwordsimilarity
istoaskhumanstojudgehowsimilaronewordistoanother. Anumberofdatasets
have resulted from such experiments. For example the SimLex-999 dataset (Hill
et al., 2015) gives values on a scale from 0 to 10, like the examples below, which
range from near-synonyms (vanish, disappear) to pairs that scarcely seem to have
anythingincommon(hole,agreement):
vanish disappear 9.8
belief impression 5.95
muscle bone 3.65
modest flexible 0.98
hole agreement 0.3
WordRelatedness Themeaningoftwowordscanberelatedinwaysotherthan
relatedness similarity. One such class of connections is called word relatedness (Budanitsky
association andHirst,2006),alsotraditionallycalledwordassociationinpsychology.
Considerthemeaningsofthewordscoffeeandcup. Coffeeisnotsimilartocup;
they share practically no features (coffee is a plant or a beverage, while a cup is a
manufacturedobjectwithaparticularshape). Butcoffeeandcupareclearlyrelated;
they are associated by co-participating in an everyday event (the event of drinking
coffee out of a cup). Similarly scalpel and surgeon are not similar but are related
eventively(asurgeontendstomakeuseofascalpel).
One common kind of relatedness between words is if they belong to the same
semanticfield semanticfield. Asemanticfieldisasetofwordswhichcoveraparticularsemantic106 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS
domainandbearstructuredrelationswitheachother. Forexample,wordsmightbe
related by being in the semantic field of hospitals (surgeon, scalpel, nurse, anes-
thetic,hospital),restaurants(waiter,menu,plate,food,chef),orhouses(door,roof,
topicmodels kitchen,family,bed). Semanticfieldsarealsorelatedtotopicmodels,likeLatent
DirichletAllocation,LDA,whichapplyunsupervisedlearningonlargesetsoftexts
toinducesetsofassociatedwordsfromtext. Semanticfieldsandtopicmodelsare
veryusefultoolsfordiscoveringtopicalstructureindocuments.
InChapter23we’llintroducemorerelationsbetweensenseslikehypernymyor
IS-A,antonymy(opposites)andmeronymy(part-wholerelations).
Semantic Frames and Roles Closely related to semantic fields is the idea of a
semanticframe semantic frame. A semantic frame is a set of words that denote perspectives or
participants in a particular type of event. A commercial transaction, for example,
is a kind of event in which one entity trades money to another entity in return for
somegoodorservice,afterwhichthegoodchangeshandsorperhapstheserviceis
performed. This event can be encoded lexically by using verbs like buy (the event
from the perspective of the buyer), sell (from the perspective of the seller), pay
(focusingonthemonetaryaspect),ornounslikebuyer. Frameshavesemanticroles
(likebuyer,seller,goods,money),andwordsinasentencecantakeontheseroles.
Knowing that buy and sell have this relation makes it possible for a system to
knowthatasentencelikeSamboughtthebookfromLingcouldbeparaphrasedas
LingsoldthebooktoSam,andthatSamhastheroleofthebuyerintheframeand
Lingtheseller. Beingabletorecognizesuchparaphrasesisimportantforquestion
answering,andcanhelpinshiftingperspectiveformachinetranslation.
connotations Connotation Finally,wordshaveaffectivemeaningsorconnotations. Theword
connotation has different meanings in different fields, but here we use it to mean
the aspects of a word’s meaning that are related to a writer or reader’s emotions,
sentiment,opinions,orevaluations. Forexamplesomewordshavepositiveconno-
tations (happy) while others have negative connotations (sad). Even words whose
meaningsaresimilarinotherwayscanvaryinconnotation;considerthedifference
inconnotationsbetweenfake,knockoff,forgery,ontheonehand,andcopy,replica,
reproduction on the other, or innocent (positive connotation) and naive (negative
connotation). Somewordsdescribepositiveevaluation(great,love)andothersneg-
ative evaluation (terrible, hate). Positive or negative evaluation language is called
sentiment sentiment, as we saw in Chapter 4, and word sentiment plays a role in important
taskslikesentimentanalysis, stancedetection, andapplicationsofNLPtothelan-
guageofpoliticsandconsumerreviews.
Earlyworkonaffectivemeaning(Osgoodetal.,1957)foundthatwordsvaried
alongthreeimportantdimensionsofaffectivemeaning:
valence: thepleasantnessofthestimulus
arousal: theintensityofemotionprovokedbythestimulus
dominance: thedegreeofcontrolexertedbythestimulus
Thus words like happy or satisfied are high on valence, while unhappy or an-
noyedarelowonvalence. Excitedishighonarousal,whilecalmislowonarousal.
Controllingishighondominance,whileawedorinfluencedarelowondominance.
Eachwordisthusrepresentedbythreenumbers,correspondingtoitsvalueoneach
ofthethreedimensions:6.2 • VECTORSEMANTICS 107
Valence Arousal Dominance
courageous 8.05 5.5 7.38
music 7.67 5.57 6.5
heartbreak 2.45 5.65 3.58
cub 6.71 3.95 4.24
Osgood et al. (1957) noticed that in using these 3 numbers to represent the
meaning of a word, the model was representing each word as a point in a three-
dimensional space, a vector whose three dimensions corresponded to the word’s
ratingonthethreescales. Thisrevolutionaryideathatwordmeaningcouldberep-
resented as a point in space (e.g., that part of the meaning of heartbreak can be
representedasthepoint[2.45,5.65,3.58])wasthefirstexpressionofthevectorse-
manticsmodelsthatweintroducenext.
6.2 Vector Semantics
vector Vector semantics is the standard way to represent word meaning in NLP, helping
semantics
usmodelmanyoftheaspectsofwordmeaningwesawintheprevioussection. The
roots of the model lie in the 1950s when two big ideas converged: Osgood’s 1957
idea mentioned above to use a point in three-dimensional space to represent the
connotationofaword,andtheproposalbylinguistslikeJoos(1950),Harris(1954),
and Firth (1957) to define the meaning of a word by its distribution in language
use, meaning its neighboring words or grammatical environments. Their idea was
thattwowordsthatoccurinverysimilardistributions(whoseneighboringwordsare
similar)havesimilarmeanings.
Forexample,supposeyoudidn’tknowthemeaningofthewordongchoi(are-
centborrowingfromCantonese)butyouseeitinthefollowingcontexts:
(6.1) Ongchoiisdelicioussauteedwithgarlic.
(6.2) Ongchoiissuperboverrice.
(6.3) ...ongchoileaveswithsaltysauces...
Andsupposethatyouhadseenmanyofthesecontextwordsinothercontexts:
(6.4) ...spinachsauteedwithgarlicoverrice...
(6.5) ...chardstemsandleavesaredelicious...
(6.6) ...collardgreensandothersaltyleafygreens
The fact that ongchoi occurs with words like rice and garlic and delicious and
salty,asdowordslikespinach,chard,andcollardgreensmightsuggestthatongchoi
is a leafy green similar to these other leafy greens.1 We can do the same thing
computationallybyjustcountingwordsinthecontextofongchoi.
Theideaofvectorsemanticsistorepresentawordasapointinamultidimen-
sional semantic space that is derived (in ways we’ll see) from the distributions of
embeddings word neighbors. Vectors for representing words are called embeddings (although
the term is sometimes more strictly applied only to dense vectors like word2vec
(Section 6.8), rather than sparse tf-idf or PPMI vectors (Section 6.3-Section 6.6)).
Theword“embedding”derivesfromitsmathematicalsenseasamappingfromone
space or structure to another, although the meaning has shifted; see the end of the
chapter.
1 It’sinfactIpomoeaaquatica,arelativeofmorningglorysometimescalledwaterspinachinEnglish.108 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS
not good
bad
to by ’s dislike worst
that now are incredibly bad worse
a i you
than with is
very good incredibly good
amazing fantastic
terrific nice wonderful
good
Figure6.1 A two-dimensional (t-SNE) projection of embeddings for some words and
phrases, showing that words with similar meanings are nearby in space. The original 60-
dimensionalembeddingsweretrainedforsentimentanalysis.SimplifiedfromLietal.(2015)
withcolorsaddedforexplanation.
Fig. 6.1 shows a visualization of embeddings learned for sentiment analysis,
showingthelocationofselectedwordsprojecteddownfrom60-dimensionalspace
intoatwodimensionalspace. Noticethedistinctregionscontainingpositivewords,
negativewords,andneutralfunctionwords.
Thefine-grainedmodelofwordsimilarityofvectorsemanticsoffersenormous
powertoNLPapplications. NLPapplicationslikethesentimentclassifiersofChap-
ter4orChapter5dependonthesamewordsappearinginthetrainingandtestsets.
Butbyrepresentingwordsasembeddings,classifierscanassignsentimentaslongas
itseessomewordswithsimilarmeanings.Andaswe’llsee,vectorsemanticmodels
canbelearnedautomaticallyfromtextwithoutsupervision.
Inthischapterwe’llintroducethetwomostcommonlyusedmodels.Inthetf-idf
model,animportantbaseline,themeaningofawordisdefinedbyasimplefunction
of the counts of nearby words. We will see that this method results in very long
vectors that are sparse, i.e. mostly zeros (since most words simply never occur in
the context of others). We’ll introduce the word2vec model family for construct-
ingshort, densevectorsthathaveusefulsemanticproperties. We’llalsointroduce
thecosine,thestandardwaytouseembeddingstocomputesemanticsimilarity,be-
tween two words, two sentences, or two documents, an important tool in practical
applicationslikequestionanswering,summarization,orautomaticessaygrading.
6.3 Words and Vectors
“Themostimportantattributesofavectorin3-spaceare Location,Location,Location ”
{ }
RandallMunroe,https://xkcd.com/2358/
Vectorordistributionalmodelsofmeaningaregenerallybasedonaco-occurrence
matrix,awayofrepresentinghowoftenwordsco-occur. We’lllookattwopopular
matrices: theterm-documentmatrixandtheterm-termmatrix.
6.3.1 Vectorsanddocuments
term-document Inaterm-documentmatrix,eachrowrepresentsawordinthevocabularyandeach
matrix
columnrepresentsadocumentfromsomecollectionofdocuments. Fig.6.2showsa
smallselectionfromaterm-documentmatrixshowingtheoccurrenceoffourwords
infourplaysbyShakespeare.Eachcellinthismatrixrepresentsthenumberoftimes6.3 • WORDSANDVECTORS 109
aparticularword(definedbytherow)occursinaparticulardocument(definedby
thecolumn). Thusfoolappeared58timesinTwelfthNight.
AsYouLikeIt TwelfthNight JuliusCaesar HenryV
battle 1 0 7 13
good 114 80 62 89
fool 36 58 1 4
wit 20 15 2 3
Figure6.2 Theterm-documentmatrixforfourwordsinfourShakespeareplays.Eachcell
containsthenumberoftimesthe(row)wordoccursinthe(column)document.
The term-document matrix of Fig. 6.2 was first defined as part of the vector
vectorspace space model of information retrieval (Salton, 1971). In this model, a document is
model
representedasacountvector,acolumninFig.6.3.
vector Toreviewsomebasiclinearalgebra, avectoris, atheart, justalistorarrayof
numbers. SoAsYouLikeItisrepresentedasthelist[1,114,36,20](thefirstcolumn
vector in Fig.6.3) and Julius Caesar is representedas the list [7,62,1,2](the third
vectorspace column vector). A vector space is a collection of vectors, characterized by their
dimension dimension. In the example in Fig. 6.3, the document vectors are of dimension 4,
justsotheyfitonthepage;inrealterm-documentmatrices,thevectorsrepresenting
eachdocumentwouldhavedimensionality V ,thevocabularysize.
| |
Theorderingofthenumbersinavectorspaceindicatesdifferentmeaningfuldi-
mensionsonwhichdocumentsvary. Thusthefirstdimensionforboththesevectors
corresponds to the number of times the word battle occurs, and we can compare
eachdimension, notingforexamplethatthevectorsforAsYouLikeItandTwelfth
Nighthavesimilarvalues(1and0,respectively)forthefirstdimension.
AsYouLikeIt TwelfthNight JuliusCaesar HenryV
battle 1 0 7 13
good 114 80 62 89
fool 36 58 1 4
wit 20 15 2 3
Figure6.3 Theterm-documentmatrixforfourwordsinfourShakespeareplays. Thered
boxesshowthateachdocumentisrepresentedasacolumnvectoroflengthfour.
Wecanthinkofthevectorforadocumentasapointin V -dimensionalspace;
| |
thusthedocumentsinFig.6.3arepointsin4-dimensionalspace.Since4-dimensional
spacesarehardtovisualize,Fig.6.4showsavisualizationintwodimensions;we’ve
arbitrarilychosenthedimensionscorrespondingtothewordsbattleandfool.
Term-document matrices were originally defined as a means of finding similar
documentsforthetaskofdocumentinformationretrieval. Twodocumentsthatare
similar will tend to have similar words, and if two documents have similar words
their column vectors will tend to be similar. The vectors for the comedies As You
LikeIt[1,114,36,20]andTwelfthNight[0,80,58,15]lookalotmorelikeeachother
(more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or
Henry V [13,89,4,3]. This is clear with the raw numbers; in the first dimension
(battle)thecomedieshavelownumbersandtheothershavehighnumbers,andwe
can see it visually in Fig. 6.4; we’ll see very shortly how to quantify this intuition
moreformally.
Arealterm-documentmatrix,ofcourse,wouldn’tjusthave4rowsandcolumns,
let alone 2. More generally, the term-document matrix has V rows (one for each
| |
wordtypeinthevocabulary)andDcolumns(oneforeachdocumentinthecollec-110 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS
40
Henry V [4,13]
15
10 Julius Caesar [1,7]
5 As You Like It [36,1] Twelfth Night [58,0]
5 10 15 20 25 30 35 40 45 50 55 60
fool
Figure6.4 AspatialvisualizationofthedocumentvectorsforthefourShakespeareplay
documents,showingjusttwoofthedimensions,correspondingtothewordsbattleandfool.
Thecomedieshavehighvaluesforthefooldimensionandlowvaluesforthebattledimension.
tion); aswe’llsee, vocabularysizesaregenerallyinthetensofthousands, andthe
numberofdocumentscanbeenormous(thinkaboutallthepagesontheweb).
information Information retrieval (IR) is the task of finding the document d from the D
retrieval
documentsinsomecollectionthatbestmatchesaqueryq.ForIRwe’llthereforealso
representaquerybyavector, alsooflength V , andwe’llneedawaytocompare
| |
twovectorstofindhowsimilartheyare. (DoingIRwillalsorequireefficientways
tostoreandmanipulatethesevectorsbymakinguseoftheconvenientfactthatthese
vectorsaresparse,i.e.,mostlyzeros).
Laterinthechapterwe’llintroducesomeofthecomponentsofthisvectorcom-
parisonprocess: thetf-idftermweighting,andthecosinesimilaritymetric.
6.3.2 Wordsasvectors: documentdimensions
We’ve seen that documents can be represented as vectors in a vector space. But
vector semantics can also be used to represent the meaning of words. We do this
rowvector byassociatingeachwordwithawordvector—arowvectorratherthanacolumn
vector,hencewithdifferentdimensions,asshowninFig.6.5. Thefourdimensions
ofthevectorforfool,[36,58,1,4],correspondtothefourShakespeareplays. Word
counts in the same four dimensions are used to form the vectors for the other 3
words: wit,[20,15,2,3];battle,[1,0,7,13];andgood[114,80,62,89].
AsYouLikeIt TwelfthNight JuliusCaesar HenryV
battle 1 0 7 13
good 114 80 62 89
fool 36 58 1 4
wit 20 15 2 3
Figure6.5 Theterm-documentmatrixforfourwordsinfourShakespeareplays. Thered
boxesshowthateachwordisrepresentedasarowvectoroflengthfour.
Fordocuments,wesawthatsimilardocumentshadsimilarvectors,becausesim-
ilar documents tend to have similar words. This same principle applies to words:
similarwordshavesimilarvectorsbecausetheytendtooccurinsimilardocuments.
Theterm-documentmatrixthusletsusrepresentthemeaningofawordbythedoc-
umentsittendstooccurin.
elttab6.3 • WORDSANDVECTORS 111
6.3.3 Wordsasvectors: worddimensions
An alternative to using the term-document matrix to represent words as vectors of
documentcounts,istousetheterm-termmatrix,alsocalledtheword-wordma-
word-word trixortheterm-contextmatrix,inwhichthecolumnsarelabeledbywordsrather
matrix
thandocuments.Thismatrixisthusofdimensionality V V andeachcellrecords
| |×| |
thenumberoftimestherow(target)wordandthecolumn(context)wordco-occur
in some context in some training corpus. The context could be the document, in
whichcasethecellrepresentsthenumberoftimesthetwowordsappearinthesame
document. It is most common, however, to use smaller contexts, generally a win-
dow around the word, for example of 4 words to the left and 4 words to the right,
inwhichcasethecellrepresentsthenumberoftimes(insometrainingcorpus)the
columnwordoccursinsucha 4wordwindowaroundtherowword. Herearefour
±
examplesofwordsintheirwindows:
istraditionallyfollowedby cherry pie,atraditionaldessert
oftenmixed,suchas strawberry rhubarbpie. Applepie
computerperipheralsandpersonal digital assistants. Thesedevicesusually
acomputer. Thisincludes information availableontheinternet
Ifwethentakeeveryoccurrenceofeachword(saystrawberry)andcountthe
contextwordsaroundit,wegetaword-wordco-occurrencematrix.Fig.6.6showsa
simplifiedsubsetoftheword-wordco-occurrencematrixforthesefourwordscom-
putedfromtheWikipediacorpus(Davies,2015).
aardvark ... computer data result pie sugar ...
cherry 0 ... 2 8 9 442 25 ...
strawberry 0 ... 0 0 1 60 19 ...
digital 0 ... 1670 1683 85 5 4 ...
information 0 ... 3325 3982 378 5 13 ...
Figure6.6 Co-occurrencevectorsforfourwordsintheWikipediacorpus,showingsixof
thedimensions(hand-pickedforpedagogicalpurposes). Thevectorfordigitalisoutlinedin
red.Notethatarealvectorwouldhavevastlymoredimensionsandthusbemuchsparser.
Note in Fig. 6.6 that the two words cherry and strawberry are more similar to
eachother(bothpieandsugartendtooccurintheirwindow)thantheyaretoother
wordslikedigital;conversely,digitalandinformationaremoresimilartoeachother
than,say,tostrawberry. Fig.6.7showsaspatialvisualization.
4000
information
3000 [3982,3325]
digital
2000 [1683,1670]
1000
1000 2000 3000 4000
data
Figure6.7 Aspatialvisualizationofwordvectorsfordigitalandinformation,showingjust
twoofthedimensions,correspondingtothewordsdataandcomputer.
Note that V , the dimensionality of the vector, is generally the size of the vo-
| |
cabulary, often between 10,000 and 50,000 words (using the most frequent words
retupmoc112 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS
inthetrainingcorpus; keepingwordsafteraboutthemostfrequent50,000orsois
generallynothelpful). Sincemostofthesenumbersarezerothesearesparsevector
representations;thereareefficientalgorithmsforstoringandcomputingwithsparse
matrices.
Nowthatwehavesomeintuitions,let’smoveontoexaminethedetailsofcom-
putingwordsimilarity. Afterwardswe’lldiscussmethodsforweightingcells.
6.4 Cosine for measuring similarity
To measure similarity between two target words v and w, we need a metric that
takestwovectors(ofthesamedimensionality,eitherbothwithwordsasdimensions,
henceoflength V ,orbothwithdocumentsasdimensions,oflength D)andgives
| | | |
ameasureoftheirsimilarity.Byfarthemostcommonsimilaritymetricisthecosine
oftheanglebetweenthevectors.
Thecosine—likemostmeasuresforvectorsimilarityusedinNLP—isbasedon
dotproduct thedotproductoperatorfromlinearalgebra,alsocalledtheinnerproduct:
innerproduct
N
dotproduct(v,w)=v w= v iw i=v 1w 1+v 2w 2+...+v Nw N (6.7)
·
i=1
(cid:88)
Thedotproductactsasasimilaritymetricbecauseitwilltendtobehighjustwhen
thetwovectorshavelargevaluesinthesamedimensions. Alternatively,vectorsthat
havezerosindifferentdimensions—orthogonalvectors—willhaveadotproductof
0,representingtheirstrongdissimilarity.
This raw dot product, however, has a problem as a similarity metric: it favors
vectorlength longvectors. Thevectorlengthisdefinedas
N
v = v2 (6.8)
| | (cid:118) i
(cid:117)i=1
(cid:117)(cid:88)
(cid:116)
Thedotproductishigherifavectorislonger,withhighervaluesineachdimension.
More frequent words have longer vectors, since they tend to co-occur with more
wordsandhavehigherco-occurrencevalueswitheachofthem.Therawdotproduct
thuswillbehigherforfrequentwords. Butthisisaproblem;we’dlikeasimilarity
metricthattellsushowsimilartwowordsareregardlessoftheirfrequency.
We modify the dot product to normalize for the vector length by dividing the
dotproductbythelengthsofeachofthetwovectors. Thisnormalizeddotproduct
turnsouttobethesameasthecosineoftheanglebetweenthetwovectors,following
fromthedefinitionofthedotproductbetweentwovectorsaandb:
a b = a b cosθ
· | || |
a b
· = cosθ (6.9)
a b
| || |
cosine Thecosinesimilaritymetricbetweentwovectorsvandwthuscanbecomputedas:6.5 • TF-IDF:WEIGHINGTERMSINTHEVECTOR 113
N
vw
i i
v w
cosine(v,w)= · = (cid:88)i=1 (6.10)
v w
N N
| || |
v2 w2
(cid:118) i(cid:118) i
(cid:117)i=1 (cid:117)i=1
(cid:117)(cid:88) (cid:117)(cid:88)
(cid:116) (cid:116)
For some applications we pre-normalize each vector, by dividing it by its length,
unitvector creatingaunitvectoroflength1. Thuswecouldcomputeaunitvectorfromaby
dividingitby a. Forunitvectors,thedotproductisthesameasthecosine.
| |
Thecosinevaluerangesfrom1forvectorspointinginthesamedirection,through
0fororthogonalvectors,to-1forvectorspointinginoppositedirections. Butsince
rawfrequencyvaluesarenon-negative,thecosineforthesevectorsrangesfrom0–1.
Let’sseehowthecosinecomputeswhichofthewordscherryordigitaliscloser
inmeaningtoinformation,justusingrawcountsfromthefollowingshortenedtable:
pie data computer
cherry 442 8 2
digital 5 1683 1670
information 5 3982 3325
442 5+8 3982+2 3325
cos(cherry,information) = ∗ ∗ ∗ =.018
√4422+82+22√52+39822+33252
5 5+1683 3982+1670 3325
cos(digital,information) = ∗ ∗ ∗ =.996
√52+16832+16702√52+39822+33252
Themodeldecidesthatinformationiswayclosertodigitalthanitistocherry,a
resultthatseemssensible. Fig.6.8showsavisualization.
500
cherry
digital information
500 1000 1500 2000 2500 3000
Dimension 2: ‘computer’
Figure6.8 A (rough) graphical demonstration of cosine similarity, showing vectors for
threewords(cherry,digital,andinformation)inthetwodimensionalspacedefinedbycounts
ofthewordscomputerandpienearby.Thefiguredoesn’tshowthecosine,butithighlightsthe
angles;notethattheanglebetweendigitalandinformationissmallerthantheanglebetween
cherryandinformation.Whentwovectorsaremoresimilar,thecosineislargerbuttheangle
issmaller; thecosinehasitsmaximum(1)whentheanglebetweentwovectorsissmallest
(0◦);thecosineofallotheranglesislessthan1.
6.5 TF-IDF: Weighing terms in the vector
Theco-occurrencematricesaboverepresenteachcellbyfrequencies,eitherofwords
withdocuments(Fig.6.5),orwordswithotherwords(Fig.6.6). Butrawfrequency
’eip‘
:1
noisnemiD114 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS
isnotthebestmeasureofassociationbetweenwords.Rawfrequencyisveryskewed
andnotverydiscriminative. Ifwewanttoknowwhatkindsofcontextsareshared
bycherryandstrawberrybutnotbydigitalandinformation,we’renotgoingtoget
good discrimination from words like the, it, or they, which occur frequently with
all sorts of words and aren’t informative about any particular word. We saw this
alsoinFig.6.3fortheShakespearecorpus;thedimensionforthewordgoodisnot
verydiscriminativebetweenplays;goodissimplyafrequentwordandhasroughly
equivalenthighfrequenciesineachoftheplays.
It’s a bit of a paradox. Words that occur nearby frequently (maybe pie nearby
cherry) are more important than words that only appear once or twice. Yet words
thataretoofrequent—ubiquitous,liketheorgood—areunimportant. Howcanwe
balancethesetwoconflictingconstraints?
Therearetwocommonsolutionstothisproblem: inthissectionwe’lldescribe
thetf-idfweighting,usuallyusedwhenthedimensionsaredocuments. Inthenext
weintroducethePPMIalgorithm(usuallyusedwhenthedimensionsarewords).
Thetf-idfweighting(the‘-’hereisahyphen,notaminussign)istheproduct
oftwoterms,eachtermcapturingoneofthesetwointuitions:
termfrequency Thefirstisthetermfrequency(Luhn,1957): thefrequencyofthewordt inthe
documentd. Wecanjustusetherawcountasthetermfrequency:
tf t,d = count(t,d) (6.11)
More commonly we squash the raw frequency a bit, by using the log of the fre-
10
quency instead. The intuition is that a word appearing 100 times in a document
doesn’tmakethatword100timesmorelikelytoberelevanttothemeaningofthe
document. Becausewecan’ttakethelogof0,wenormallyadd1tothecount:2
tf t,d = log 10(count(t,d)+1) (6.12)
If we use log weighting, terms which occur 0 times in a document would have
tf=log (1)=0, 10 times in a document tf=log (11)=1.04, 100 times tf=
10 10
log (101)=2.004,1000timestf=3.00044,andsoon.
10
The second factor in tf-idf is used to give a higher weight to words that occur
only in a few documents. Terms that are limited to a few documents are useful
fordiscriminatingthosedocumentsfromtherestofthecollection;termsthatoccur
document frequentlyacrosstheentirecollectionaren’tashelpful. Thedocumentfrequency
frequency
df of a term t is the number of documents it occurs in. Document frequency is
t
not the same as the collection frequency of a term, which is the total number of
times the word appears in the whole collection in any document. Consider in the
collectionofShakespeare’s37 playsthetwowordsRomeo and action. The words
haveidenticalcollectionfrequencies(theybothoccur113timesinalltheplays)but
very different document frequencies, since Romeo only occurs in a single play. If
our goal is to find documents about the romantic tribulations of Romeo, the word
Romeoshouldbehighlyweighted,butnotaction:
CollectionFrequency DocumentFrequency
Romeo 113 1
action 113 31
WeemphasizediscriminativewordslikeRomeoviatheinversedocumentfre-
idf quencyoridftermweight(SparckJones,1972). Theidfisdefinedusingthefrac-
(cid:26)
1+log count(t,d) if count(t,d)>0
2 Orwecanusethisalternative: tft,d=
0
10
otherwise6.5 • TF-IDF:WEIGHINGTERMSINTHEVECTOR 115
tion N/df, where N is the total number of documents in the collection, and df is
t t
thenumberofdocumentsinwhichtermt occurs. Thefewerdocumentsinwhicha
termoccurs,thehigherthisweight. Thelowestweightof1isassignedtotermsthat
occurinallthedocuments. It’susuallyclearwhatcountsasadocument: inShake-
spearewewoulduseaplay; whenprocessingacollectionofencyclopediaarticles
likeWikipedia,thedocumentisaWikipediapage;inprocessingnewspaperarticles,
thedocumentisasinglearticle. Occasionallyyourcorpusmightnothaveappropri-
atedocumentdivisionsandyoumightneedtobreakupthecorpusintodocuments
yourselfforthepurposesofcomputingidf.
Because of the large number of documents in many collections, this measure
too is usually squashed with a log function. The resulting definition for inverse
documentfrequency(idf)isthus
N
idf t = log 10 df (6.13)
(cid:18) t(cid:19)
HerearesomeidfvaluesforsomewordsintheShakespearecorpus, rangingfrom
extremelyinformativewordswhichoccurinonlyoneplaylikeRomeo,tothosethat
occurinafewlikesaladorFalstaff,tothosewhichareverycommonlikefoolorso
commonastobecompletelynon-discriminativesincetheyoccurinall37playslike
goodorsweet.3
Word df idf
Romeo 1 1.57
salad 2 1.27
Falstaff 4 0.967
forest 12 0.489
battle 21 0.246
wit 34 0.037
fool 36 0.012
good 37 0
sweet 37 0
tf-idf The tf-idf weighted value w t,d for word t in document d thus combines term
frequencytf (definedeitherbyEq.6.11orbyEq.6.12)withidffromEq.6.13:
t,d
w t,d =tf t,d idf t (6.14)
×
Fig.6.9appliestf-idfweightingtotheShakespeareterm-documentmatrixinFig.6.2,
using the tf equation Eq. 6.12. Note that the tf-idf values for the dimension corre-
spondingtothewordgoodhavenowallbecome0;sincethiswordappearsinevery
document,thetf-idfweightingleadsittobeignored.Similarly,thewordfool,which
appearsin36outofthe37plays,hasamuchlowerweight.
The tf-idf weighting is the way for weighting co-occurrence matrices in infor-
mation retrieval, but also plays a role in many other aspects of natural language
processing. It’salsoagreatbaseline,thesimplethingtotryfirst. We’lllookatother
weightingslikePPMI(PositivePointwiseMutualInformation)inSection6.6.
3 SweetwasoneofShakespeare’sfavoriteadjectives, afactprobablyrelatedtotheincreaseduseof
sugarinEuropeanrecipesaroundtheturnofthe16thcentury(Jurafsky,2014,p.175).116 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS
AsYouLikeIt TwelfthNight JuliusCaesar HenryV
battle 0.074 0 0.22 0.28
good 0 0 0 0
fool 0.019 0.021 0.0036 0.0083
wit 0.049 0.044 0.018 0.022
Figure6.9 A tf-idf weighted term-document matrix for four words in four Shakespeare
plays,usingthecountsinFig.6.2. Forexamplethe0.049valueforwitinAsYouLikeItis
the product of tf=log (20+1)=1.322 and idf=.037. Note that the idf weighting has
10
eliminatedtheimportanceoftheubiquitouswordgoodandvastlyreducedtheimpactofthe
almost-ubiquitouswordfool.
6.6 Pointwise Mutual Information (PMI)
Analternativeweightingfunctiontotf-idf,PPMI(positivepointwisemutualinfor-
mation),isusedforterm-term-matrices,whenthevectordimensionscorrespondto
wordsratherthandocuments.PPMIdrawsontheintuitionthatthebestwaytoweigh
theassociationbetweentwowordsistoaskhowmuchmorethetwowordsco-occur
inourcorpusthanwewouldhaveaprioriexpectedthemtoappearbychance.
po min utw tuis ae
l
Pointwisemutualinformation(Fano,1961)4isoneofthemostimportantcon-
information
ceptsinNLP.Itisameasureofhowoftentwoeventsxandyoccur,comparedwith
whatwewouldexpectiftheywereindependent:
P(x,y)
I(x,y)=log (6.16)
2P(x)P(y)
Thepointwisemutualinformationbetweenatargetwordwandacontextword
c(ChurchandHanks1989,ChurchandHanks1990)isthendefinedas:
P(w,c)
PMI(w,c)=log (6.17)
2P(w)P(c)
The numerator tells us how often we observed the two words together (assuming
we compute probability by using the MLE). The denominator tells us how often
wewouldexpectthetwowordstoco-occurassumingtheyeachoccurredindepen-
dently; recall that the probability of two independent events both occurring is just
the product of the probabilities of the two events. Thus, the ratio gives us an esti-
mateofhowmuchmorethetwowordsco-occurthanweexpectbychance. PMIis
ausefultoolwheneverweneedtofindwordsthatarestronglyassociated.
PMI values range from negative to positive infinity. But negative PMI values
(which imply things are co-occurring less often than we would expect by chance)
tend to be unreliable unless our corpora are enormous. To distinguish whether
twowordswhoseindividualprobabilityiseach10 6 occurtogetherlessoftenthan
−
chance, we would need to be certain that the probability of the two occurring to-
getherissignificantlylessthan10 12,andthiskindofgranularitywouldrequirean
−
enormous corpus. Furthermore it’s not clear whether it’s even possible to evaluate
such scores of ‘unrelatedness’ with human judgments. For this reason it is more
4 PMIisbasedonthemutualinformationbetweentworandomvariablesXandY,definedas:
(cid:88)(cid:88) P(x,y)
I(X,Y)= P(x,y)log (6.15)
2P(x)P(y)
x y
Inaconfusionofterminology,Fanousedthephrasemutualinformationtorefertowhatwenowcall
pointwisemutualinformationandthephraseexpectationofthemutualinformationforwhatwenowcall
mutualinformation6.6 • POINTWISEMUTUALINFORMATION(PMI) 117
PPMI commontousePositivePMI(calledPPMI)whichreplacesallnegativePMIvalues
withzero(ChurchandHanks1989,Daganetal.1993,NiwaandNitta1994)5:
P(w,c)
PPMI(w,c)=max(log ,0) (6.18)
2P(w)P(c)
Moreformally,let’sassumewehaveaco-occurrencematrixFwithWrows(words)
andCcolumns(contexts),where f givesthenumberoftimeswordw occurswith
ij i
context c . This can be turned into a PPMI matrix where PPMI gives the PPMI
j ij
value of word w with context c (which we can also express as PPMI(w,c ) or
i j i j
PPMI(w=i,c= j))asfollows:
p ij = W i=1 fij C j=1fij, p i ∗ = W i(cid:80)=1C j=1 C j=fi 1j fij, p ∗j = W i(cid:80)=1W i=1 C j=fi 1j fij (6.19)
(cid:80) (cid:80) (cid:80) (cid:80) (cid:80) (cid:80)
p
ij
PPMI ij=max(log
2 p p
,0) (6.20)
i j
∗ ∗
Let’sseesomePPMIcalculations. We’lluseFig.6.10,whichrepeatsFig.6.6plus
all the count marginals, and let’s pretend for ease of calculation that these are the
onlywords/contextsthatmatter.
computer data result pie sugar count(w)
cherry 2 8 9 442 25 486
strawberry 0 0 1 60 19 80
digital 1670 1683 85 5 4 3447
information 3325 3982 378 5 13 7703
count(context) 4997 5673 473 512 61 11716
Figure6.10 Co-occurrence counts for four words in 5 contexts in the Wikipedia corpus,
togetherwiththemarginals,pretendingforthepurposeofthiscalculationthatnootherword-
s/contextsmatter.
ThusforexamplewecouldcomputePPMI(information,data),assumingwepre-
tendedthatFig.6.6encompassedalltherelevantwordcontexts/dimensions,asfol-
lows:
3982
P(w=information,c=data) = =.3399
11716
7703
P(w=information) = =.6575
11716
5673
P(c=data) = =.4842
11716
PPMI(information,data) = log (.3399/(.6575 .4842))=.0944
2 ∗
Fig. 6.11 shows the joint probabilities computed from the counts in Fig. 6.10, and
Fig.6.12showsthePPMIvalues.Notsurprisingly,cherryandstrawberryarehighly
associatedwithbothpieandsugar,anddataismildlyassociatedwithinformation.
PMIhastheproblemofbeingbiasedtowardinfrequentevents;veryrarewords
tendtohaveveryhighPMIvalues.Onewaytoreducethisbiastowardlowfrequency
5 PositivePMIalsocleanlysolvestheproblemofwhattodowithzerocounts,using0toreplacethe
∞fromlog(0).
−118 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS
p(w,context) p(w)
computer data result pie sugar p(w)
cherry 0.0002 0.0007 0.0008 0.0377 0.0021 0.0415
strawberry 0.0000 0.0000 0.0001 0.0051 0.0016 0.0068
digital 0.1425 0.1436 0.0073 0.0004 0.0003 0.2942
information 0.2838 0.3399 0.0323 0.0004 0.0011 0.6575
p(context) 0.4265 0.4842 0.0404 0.0437 0.0052
Figure6.11 ReplacingthecountsinFig.6.6withjointprobabilities,showingthemarginals
intherightcolumnandthebottomrow.
computer data result pie sugar
cherry 0 0 0 4.38 3.30
strawberry 0 0 0 4.10 5.51
digital 0.18 0.01 0 0 0
information 0.02 0.09 0.28 0 0
Figure6.12 ThePPMImatrixshowingtheassociationbetweenwordsandcontextwords,
computedfromthecountsinFig.6.11.Notethatmostofthe0PPMIvaluesareonesthathad
anegativePMI;forexamplePMI(cherry,computer)=-6.7,meaningthatcherryandcomputer
co-occuronWikipedialessoftenthanwewouldexpectbychance,andwithPPMIwereplace
negativevaluesbyzero.
eventsistoslightlychangethecomputationforP(c),usingadifferentfunctionP (c)
α
thatraisestheprobabilityofthecontextwordtothepowerofα:
P(w,c)
PPMI (w,c)=max(log ,0) (6.21)
α 2P(w)P (c)
α
count(c)α
P (c)= (6.22)
α count(c)α
c
Levy et al. (2015) found that a set(cid:80)ting of α =0.75 improved performance of
embeddingsonawiderangeoftasks(drawingonasimilarweightingusedforskip-
grams described below in Eq. 6.32). This works because raising the count to α =
0.75increasestheprobabilityassignedtorarecontexts,andhencelowerstheirPMI
(P (c)>P(c)whencisrare).
α
AnotherpossiblesolutionisLaplacesmoothing:BeforecomputingPMI,asmall
constant k (values of 0.1-3 are common) is added to each of the counts, shrinking
(discounting)allthenon-zerovalues. Thelargerthek,themorethenon-zerocounts
arediscounted.
6.7 Applications of the tf-idf or PPMI vector models
Insummary, thevectorsemanticsmodelwe’vedescribedsofarrepresentsatarget
wordasavectorwithdimensionscorrespondingeithertothedocumentsinalarge
collection(theterm-documentmatrix)ortothecountsofwordsinsomeneighboring
window(theterm-termmatrix). Thevaluesineachdimensionarecounts,weighted
by tf-idf (for term-document matrices) or PPMI (for term-term matrices), and the
vectorsaresparse(sincemostvaluesarezero).
The model computes the similarity between two words x and y by taking the
cosineoftheirtf-idforPPMIvectors;highcosine,highsimilarity.Thisentiremodel6.8 • WORD2VEC 119
issometimesreferredtoasthetf-idfmodelorthePPMImodel,aftertheweighting
function.
Thetf-idfmodelofmeaningisoftenusedfordocumentfunctionslikedeciding
if two documents are similar. We represent a document by taking the vectors of
centroid all the words in the document, and computing the centroid of all those vectors.
The centroid is the multidimensional version of the mean; the centroid of a set of
vectorsisasinglevectorthathastheminimumsumofsquareddistancestoeachof
the vectors in the set. Given k word vectors w 1,w 2,...,w k, the centroid document
document vectord is:
vector
w +w +...+w
1 2 k
d= (6.23)
k
Giventwodocuments,wecanthencomputetheirdocumentvectorsd andd ,and
1 2
estimatethesimilaritybetweenthetwodocumentsbycos(d ,d ). Documentsim-
1 2
ilarity is also useful for all sorts of applications; information retrieval, plagiarism
detection, news recommender systems, and even for digital humanities tasks like
comparingdifferentversionsofatexttoseewhicharesimilartoeachother.
Either the PPMI model or the tf-idf model can be used to compute word simi-
larity,fortaskslikefindingwordparaphrases,trackingchangesinwordmeaning,or
automaticallydiscoveringmeaningsofwordsindifferentcorpora. Forexample,we
canfindthe10mostsimilarwordstoanytargetwordwbycomputingthecosines
betweenwandeachoftheV 1otherwords,sorting,andlookingatthetop10.
−
6.8 Word2vec
Intheprevioussectionswesawhowtorepresentawordasasparse,longvectorwith
dimensionscorrespondingtowordsinthevocabularyordocumentsinacollection.
Wenowintroduceamorepowerfulwordrepresentation: embeddings,shortdense
vectors. Unlikethevectorswe’veseensofar, embeddingsareshort, withnumber
ofdimensionsd rangingfrom50-1000,ratherthanthemuchlargervocabularysize
V ornumberofdocumentsDwe’veseen. Thesed dimensionsdon’thaveaclear
| |
interpretation. And the vectors are dense: instead of vector entries being sparse,
mostly-zero counts or functions of counts, the values will be real-valued numbers
thatcanbenegative.
ItturnsoutthatdensevectorsworkbetterineveryNLPtaskthansparsevectors.
Whilewedon’tcompletelyunderstandallthereasonsforthis,wehavesomeintu-
itions.Representingwordsas300-dimensionaldensevectorsrequiresourclassifiers
tolearnfarfewerweightsthanifwerepresentedwordsas50,000-dimensionalvec-
tors,andthesmallerparameterspacepossiblyhelpswithgeneralizationandavoid-
ing overfitting. Dense vectors may also do a better job of capturing synonymy.
For example, in a sparse vector representation, dimensions for synonyms like car
and automobile dimension are distinct and unrelated; sparse vectors may thus fail
to capture the similarity between a word with car as a neighbor and a word with
automobileasaneighbor.
skip-gram Inthissectionweintroduceonemethodforcomputingembeddings: skip-gram
SGNS withnegativesampling,sometimescalledSGNS.Theskip-gramalgorithmisone
word2vec of two algorithms in a software package called word2vec, and so sometimes the
algorithm is loosely referred to as word2vec (Mikolov et al. 2013a, Mikolov et al.
2013b). Theword2vecmethodsarefast, efficienttotrain, andeasilyavailableon-120 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS
line with code and pretrained embeddings. Word2vec embeddings are static em-
static beddings,meaningthatthemethodlearnsonefixedembeddingforeachwordinthe
embeddings
vocabulary.InChapter11we’llintroducemethodsforlearningdynamiccontextual
embeddingslikethepopularfamilyofBERTrepresentations,inwhichthevector
foreachwordisdifferentindifferentcontexts.
Theintuitionofword2vecisthatinsteadofcountinghowofteneachwordwoc-
cursnear,say,apricot,we’llinsteadtrainaclassifieronabinarypredictiontask: “Is
wordwlikelytoshowupnearapricot?”Wedon’tactuallycareaboutthisprediction
task;insteadwe’lltakethelearnedclassifierweightsasthewordembeddings.
Therevolutionaryintuitionhereisthatwecanjustuserunningtextasimplicitly
supervised training data for such a classifier; a word c that occurs near the target
wordapricotactsasgold‘correctanswer’tothequestion“Iswordclikelytoshow
self-supervision up near apricot?” This method, often called self-supervision, avoids the need for
anysortofhand-labeledsupervisionsignal. Thisideawasfirstproposedinthetask
ofneurallanguagemodeling,whenBengioetal.(2003)andCollobertetal.(2011)
showed that a neural language model (a neural network that learned to predict the
next word from prior words) could just use the next word in running text as its
supervisionsignal,andcouldbeusedtolearnanembeddingrepresentationforeach
wordaspartofdoingthispredictiontask.
We’ll see how to do neural networks in the next chapter, but word2vec is a
much simpler model than the neural network language model, in two ways. First,
word2vec simplifies the task (making it binary classification instead of word pre-
diction). Second,word2vecsimplifiesthearchitecture(trainingalogisticregression
classifier instead of a multi-layer neural network with hidden layers that demand
moresophisticatedtrainingalgorithms). Theintuitionofskip-gramis:
1. Treatthetargetwordandaneighboringcontextwordaspositiveexamples.
2. Randomlysampleotherwordsinthelexicontogetnegativesamples.
3. Uselogisticregressiontotrainaclassifiertodistinguishthosetwocases.
4. Usethelearnedweightsastheembeddings.
6.8.1 Theclassifier
Let’s start by thinking about the classification task, and then turn to how to train.
Imagineasentencelikethefollowing,withatargetwordapricot,andassumewe’re
usingawindowof 2contextwords:
±
... lemon, a [tablespoon of apricot jam, a] pinch ...
c1 c2 w c3 c4
Our goal is to train a classifier such that, given a tuple (w,c) of a target word
w paired with a candidate context word c (for example (apricot, jam), or perhaps
(apricot, aardvark))itwillreturntheprobabilitythatcisarealcontextword(true
forjam,falseforaardvark):
P(+w,c) (6.24)
|
The probability that word c is not a real context word for w is just 1 minus
Eq.6.24:
P( w,c)=1 P(+w,c) (6.25)
−| − |
How does the classifier compute the probability P? The intuition of the skip-
grammodelistobasethisprobabilityonembeddingsimilarity: awordislikelyto6.8 • WORD2VEC 121
occurnearthetargetifitsembeddingvectorissimilartothetargetembedding. To
compute similarity between these dense embeddings, we rely on the intuition that
two vectors are similar if they have a high dot product (after all, cosine is just a
normalizeddotproduct). Inotherwords:
Similarity(w,c) c w (6.26)
≈ ·
Thedotproductc w isnotaprobability,it’sjustanumberrangingfrom ∞to∞
· −
(sincetheelementsinword2vecembeddingscanbenegative,thedotproductcanbe
negative).Toturnthedotproductintoaprobability,we’llusethelogisticorsigmoid
functionσ(x),thefundamentalcoreoflogisticregression:
1
σ(x)= (6.27)
1+exp( x)
−
Wemodeltheprobabilitythatwordcisarealcontextwordfortargetwordwas:
1
P(+w,c) = σ(c w)= (6.28)
| · 1+exp( c w)
− ·
Thesigmoidfunctionreturnsanumberbetween0and1,buttomakeitaprobability
we’llalsoneedthetotalprobabilityofthetwopossibleevents(cisacontextword,
andcisn’tacontextword)tosumto1. Wethusestimatetheprobabilitythatwordc
isnotarealcontextwordforwas:
P( w,c) = 1 P(+w,c)
−| − |
1
= σ( c w)= (6.29)
− · 1+exp(c w)
·
Equation 6.28 gives us the probability for one word, but there are many context
wordsinthewindow. Skip-grammakesthesimplifyingassumptionthatallcontext
wordsareindependent,allowingustojustmultiplytheirprobabilities:
L
P(+w,c 1:L) = σ(c i w) (6.30)
| ·
i=1
(cid:89)
L
logP(+w,c 1:L) = logσ(c i w) (6.31)
| ·
i=1
(cid:88)
Insummary,skip-gramtrainsaprobabilisticclassifierthat,givenatesttargetword
wanditscontextwindowofLwordsc ,assignsaprobabilitybasedonhowsimilar
1:L
thiscontextwindowistothetargetword. Theprobabilityisbasedonapplyingthe
logistic(sigmoid)functiontothedotproductoftheembeddingsofthetargetword
witheachcontextword. Tocomputethisprobability,wejustneedembeddingsfor
eachtargetwordandcontextwordinthevocabulary.
Fig. 6.13 shows the intuition of the parameters we’ll need. Skip-gram actually
storestwoembeddingsforeachword,oneforthewordasatarget,andoneforthe
wordconsideredascontext. Thustheparametersweneedtolearnaretwomatrices
W and C, each containing an embedding for every one of the V words in the
| |
vocabularyV.6 Let’snowturntolearningtheseembeddings(whichistherealgoal
oftrainingthisclassifierinthefirstplace).
6 Inprinciplethetargetmatrixandthecontextmatrixcouldusedifferentvocabularies,butwe’llsimplify
byassumingonesharedvocabularyV.122 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS
1..d
aardvark 1
apricot
W target words
… …
& = zebra |V|
aardvark |V|+1
apricot
C context & noise
… … words
zebra 2V
Figure6.13 The embeddings learned by the skipgram model. The algorithm stores two
embeddings for each word, the target embedding (sometimes called the input embedding)
andthecontextembedding(sometimescalledtheoutputembedding). Theparameterθ that
thealgorithmlearnsisthusamatrixof2V vectors,eachofdimensiond,formedbyconcate-
| |
natingtwomatrices,thetargetembeddingsWandthecontext+noiseembeddingsC.
6.8.2 Learningskip-gramembeddings
The learning algorithm for skip-gram embeddings takes as input a corpus of text,
andachosenvocabularysizeN.Itbeginsbyassigningarandomembeddingvector
for each of the N vocabulary words, and then proceeds to iteratively shift the em-
beddingofeachwordwtobemoreliketheembeddingsofwordsthatoccurnearby
intexts, andlessliketheembeddingsofwordsthatdon’toccurnearby. Let’sstart
byconsideringasinglepieceoftrainingdata:
... lemon, a [tablespoon of apricot jam, a] pinch ...
c1 c2 w c3 c4
Thisexamplehasatargetwordw(apricot),and4contextwordsintheL= 2
±
window,resultingin4positivetraininginstances(ontheleftbelow):
positiveexamples+ negativeexamples-
w c pos w c neg w c neg
apricot tablespoon apricot aardvark apricot seven
apricot of apricot my apricot forever
apricot jam apricot where apricot dear
apricot a apricot coaxial apricot if
For training a binary classifier we also need negative examples. In fact skip-
gram with negative sampling (SGNS) uses more negative examples than positive
examples(withtheratiobetweenthemsetbyaparameterk). Soforeachofthese
(w,c ) training instances we’ll create k negative samples, each consisting of the
pos
targetwplusa‘noiseword’c . Anoisewordisarandomwordfromthelexicon,
neg
constrained not to be the target word w. The right above shows the setting where
k =2, so we’ll have 2 negative examples in the negative training set for each
−
positiveexamplew,c .
pos
The noise words are chosen according to their weighted unigram frequency
p (w), where α is a weight. If we were sampling according to unweighted fre-
α
quencyp(w),itwouldmeanthatwithunigramprobabilityp(“the”)wewouldchoose
the word the as a noise word, with unigram probability p(“aardvark”) we would
chooseaardvark,andsoon. Butinpracticeitiscommontosetα=.75,i.e. usethe6.8 • WORD2VEC 123
3
weighting p4(w):
count(w)α
P α(w)=
count(w)α
(6.32)
w (cid:48)
(cid:48)
(cid:80)
Settingα =.75givesbetterperformancebecauseitgivesrarenoisewordsslightly
higher probability: for rare words, P (w)>P(w). To illustrate this intuition, it
α
mighthelptoworkouttheprobabilitiesforanexamplewithtwoevents,P(a)=.99
andP(b)=.01:
.99.75
P (a) = =.97
α .99.75+.01.75
.01.75
P α(b) =
.99.75+.01.75
=.03 (6.33)
Giventhesetofpositiveandnegativetraininginstances,andaninitialsetofembed-
dings,thegoalofthelearningalgorithmistoadjustthoseembeddingsto
• Maximizethesimilarityofthetargetword,contextwordpairs(w,c )drawn
pos
fromthepositiveexamples
• Minimizethesimilarityofthe(w,c )pairsfromthenegativeexamples.
neg
Ifweconsideroneword/contextpair(w,c )withitsknoisewordsc ...c ,
pos neg1 negk
we can express these two goals as the following loss function L to be minimized
(hencethe );herethefirsttermexpressesthatwewanttheclassifiertoassignthe
−
real context word c a high probability of being a neighbor, and the second term
pos
expressesthatwewanttoassigneachofthenoisewordsc ahighprobabilityof
negi
beinganon-neighbor,allmultipliedbecauseweassumeindependence:
k
L = log P(+w,c ) P( w,c )
CE
− (cid:34) |
pos
−|
negi
(cid:35)
i=1
(cid:89)
k
= logP(+w,c )+ logP( w,c )
−(cid:34) |
pos
−|
negi
(cid:35)
i=1
(cid:88)
k
= logP(+w,c )+ log 1 P(+w,c )
−(cid:34) |
pos
− |
negi
(cid:35)
i=1
(cid:88) (cid:0) (cid:1)
k
= −(cid:34)logσ(c pos ·w)+ logσ( −c negi·w)
(cid:35)
(6.34)
i=1
(cid:88)
That is, we want to maximize the dot product of the word with the actual context
words,andminimizethedotproductsofthewordwiththeknegativesamplednon-
neighborwords.
We minimize this loss function using stochastic gradient descent. Fig. 6.14
showstheintuitionofonestepoflearning.
To get the gradient, we need to take the derivative of Eq. 6.34 with respect to
thedifferentembeddings. Itturnsoutthederivativesarethefollowing(weleavethe124 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS
aardvark
move apricot and jam closer,
apricot w increasing c pos (cid:122) w
W
“…apricot jam…”
zebra
!
aardvark
move apricot and matrix apart
decreasing c (cid:122) w
jam c neg1
pos
C
matrix c neg1
k=2
Tolstoy c neg2 move apricot and Tolstoy apart
decreasing c (cid:122) w
neg2
zebra
Figure6.14 Intuitionofonestepofgradientdescent. Theskip-grammodeltriestoshift
embeddingssothetargetembeddings(hereforapricot)arecloserto(haveahigherdotprod-
uctwith)contextembeddingsfornearbywords(herejam)andfurtherfrom(lowerdotproduct
with)contextembeddingsfornoisewordsthatdon’toccurnearby(hereTolstoyandmatrix).
proofasanexerciseattheendofthechapter):
∂L
CE = [σ(c pos w) 1]w (6.35)
∂c · −
pos
∂L
CE = [σ(c neg w)]w (6.36)
∂c ·
neg
k
∂L
∂wCE = [σ(c pos ·w) −1]c pos+ [σ(c negi·w)]c negi (6.37)
i=1
(cid:88)
Theupdateequationsgoingfromtimestept tot+1instochasticgradientdescent
arethus:
ct+1 = ct η[σ(ct wt) 1]wt (6.38)
pos pos− pos· −
ct+1 = ct η[σ(ct wt)]wt (6.39)
neg neg− neg·
k
wt+1 = wt −η (cid:34)[σ(c pos ·wt) −1]c pos+ [σ(c negi·wt)]c negi
(cid:35)
(6.40)
i=1
(cid:88)
Justasinlogisticregression,then,thelearningalgorithmstartswithrandomlyini-
tializedWandCmatrices,andthenwalksthroughthetrainingcorpususinggradient
descenttomoveWandCsoastominimizethelossinEq.6.34bymakingtheup-
datesin(Eq.6.38)-(Eq.6.40).
Recallthattheskip-grammodellearnstwoseparateembeddingsforeachwordi:
embet da dr ig ne gt thetargetembeddingw iandthecontextembeddingc i,storedintwomatrices,the
context targetmatrixWandthecontextmatrixC. It’scommontojustaddthemtogether,
embedding
representingwordiwiththevectorw +c. AlternativelywecanthrowawaytheC
i i
matrixandjustrepresenteachwordibythevectorw.
i
As with the simple count-based methods like tf-idf, the context window size L
affects the performance of skip-gram embeddings, and experiments often tune the
parameterLonadevset.6.9 • VISUALIZINGEMBEDDINGS 125
6.8.3 Otherkindsofstaticembeddings
fasttext There are many kinds of static embeddings. An extension of word2vec, fasttext
(Bojanowskietal.,2017),addressesaproblemwithword2vecaswehavepresented
it so far: it has no good way to deal with unknown words—words that appear in
a test corpus but were unseen in the training corpus. A related problem is word
sparsity,suchasinlanguageswithrichmorphology,wheresomeofthemanyforms
for each noun and verb may only occur rarely. Fasttext deals with these problems
byusingsubwordmodels,representingeachwordasitselfplusabagofconstituent
n-grams,withspecialboundarysymbols<and>addedtoeachword.Forexample,
withn=3thewordwherewouldberepresentedbythesequence<where>plusthe
charactern-grams:
<wh, whe, her, ere, re>
Then a skipgram embedding is learned for each constituent n-gram, and the word
whereisrepresentedbythesumofalloftheembeddingsofitsconstituentn-grams.
Unknownwordscanthenbepresentedonlybythesumoftheconstituentn-grams.
Afasttextopen-sourcelibrary,includingpretrainedembeddingsfor157languages,
Rohde,Gonnerman,Plaut ModelingWordMeaningUsingLexicalCo-Occurrence
isavailableathttps://fasttext.cc.
AnotherverywidelyusedstaticembeddingmodelisGloVe(Penningtonetal.,
2014), short for Global Vectors, because the model is based on capturing global
RUSSIA
corpusstatistics. GFRAlNoCEVeisbasedonratiosofprobabilitiesfromtheword-wordco-
CHINA
occurrencematrix,combiningtheintuitionsofcount-basedmodelslikePPMIwhile
WRIST
EAUSRIOAPE
ARAMNKLE alsocapturingtheAlAFiMRnEICRAeICaArstructuresusedbymethodslikeword2vec.
EFI ANS RHHG AOE FNRU AE DCL YD EEER ItturnsoutthatBR dA MeZ OIL n SCs OWeembeddingslikeword2vecactuallyhaveanelegantmath-
TOE LEG ematicalrelationshipwithsparseembeddingslikePPMI,inwhichword2veccanbe
FOOT
TOOTNHOHSEEAD seen as implicitly THo OA KWp YA OtIIimizing a shifted version of a PPMI matrix (Levy and Gold-
berg,2014c).
MONTREAL
MOUSE
CHAICTALAGNOTA
D6OG.9 Visualizing Embeddings
CAT
TURTLE
PKUIPTPTYEN COW LION NASHVILLE
OYSTER “Iseewellinmanydimensionsaslongasthedimensionsarearoundtwo.”
ThelateeconomistMartinShubik
BULL
Figure8:MultidimensionalscValiinsgufaorlithzrienegnouenmclbasesdesd.ings is an important goal in helping understand, apply, and
improvethesemodelsofwordmeaning. Buthowcanwevisualizea(forexample)
100-dimensionalvector?
The simplest way to visualize the meaning of a word
WRIST
SHA ON UK LL DE ER w embedded in a space is to list the most similar words to
ARM
LEG w by sorting the vectors for all words in the vocabulary by
HAND
HEF AO DOT theircosinewiththevectorforw. Forexamplethe7closest
NOSE
FINGER wordstofrogusingtheGloVeembeddingsare: frogs,toad,
TOE
FA EC AE R litoria, leptodactylidae, rana, lizard, andeleutherodactylus
EYE
TOOTH (Penningtonetal.,2014).
DOG
CAT PUPPY Yet another visualization method is to use a clustering
KITTEN
COW algorithm to show a hierarchical representation of which
MOUSE
TU OR YT SLE TER words are similar to others in the embedding space. The
LION
BULL uncaptioned figure on the left uses hierarchical clustering
CHICAGO
ATLA MN OT NA TREAL of some embedding vectors for nouns as a visualization
NASHVILLE
TOKYO method(Rohdeetal.,2006).
CHINA
RUSSIA
AFRICA
ASIA
EUROPE
AMERICA
BRAZIL
MOSCOW
FRANCE
HAWAII
Figure9:Hierarchicalclusteringforthreenounclassesusingdistancesbasedonvectorcorrelations.
20126 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS
Probablythemostcommonvisualizationmethod,how-
ever,istoprojectthe100dimensionsofaworddowninto2
dimensions. Fig.6.1showedonesuchvisualization,asdoes
Fig.6.16, usingaprojectionmethodcalledt-SNE(vander
MaatenandHinton,2008).
6.10 Semantic properties of embeddings
Inthissectionwebrieflysummarizesomeofthesemanticpropertiesofembeddings
thathavebeenstudied.
Different types of similarity or association: One parameter of vector semantic
models that is relevant to both sparse tf-idf vectors and dense word2vec vectors is
thesizeofthecontextwindowusedtocollectcounts. Thisisgenerallybetween1
and10wordsoneachsideofthetargetword(foratotalcontextof2-20words).
Thechoicedependsonthegoalsoftherepresentation. Shortercontextwindows
tendtoleadtorepresentationsthatareabitmoresyntactic,sincetheinformationis
comingfromimmediatelynearbywords.Whenthevectorsarecomputedfromshort
contextwindows,themostsimilarwordstoatargetwordwtendtobesemantically
similarwordswiththesamepartsofspeech. Whenvectorsarecomputedfromlong
contextwindows,thehighestcosinewordstoatargetwordwtendtobewordsthat
aretopicallyrelatedbutnotsimilar.
For example Levy and Goldberg (2014a) showed that using skip-gram with a
windowof 2,themostsimilarwordstothewordHogwarts(fromtheHarryPotter
±
series) were names of other fictional schools: Sunnydale (from Buffy the Vampire
Slayer)orEvernight(fromavampireseries).Withawindowof 5,themostsimilar
±
words to Hogwarts were other words topically related to the Harry Potter series:
Dumbledore,Malfoy,andhalf-blood.
It’salsooftenusefultodistinguishtwokindsofsimilarityorassociationbetween
first-order words (Schu¨tze and Pedersen, 1993). Two words have first-order co-occurrence
co-occurrence
(sometimescalledsyntagmaticassociation)iftheyaretypicallynearbyeachother.
Thuswroteisafirst-orderassociateofbookorpoem.Twowordshavesecond-order
second-order co-occurrence (sometimes called paradigmatic association) if they have similar
co-occurrence
neighbors. Thuswroteisasecond-orderassociateofwordslikesaidorremarked.
Analogy/RelationalSimilarity: Anothersemanticpropertyofembeddingsistheir
abilitytocapturerelationalmeanings. Inanimportantearlyvectorspacemodelof
parallelogram cognition, RumelhartandAbrahamson(1973)proposedtheparallelogrammodel
model
for solving simple analogy problems of the form a is to b as a* is to what?. In
such problems, a system is given a problem like apple:tree::grape:?, i.e., apple is
to tree as grape is to , and must fill in the word vine. In the parallelogram
m
#
o »del,# i l l u »strated in Fig. 6.15, the vector from
#
t h e »word apple to the word tree (=
tree apple)isaddedtothevectorforgrape(grape);thenearestwordtothatpoint
−
isreturned.
Inearlyworkwithsparseembeddings,scholarsshowedthatsparsevectormod-
els of meaning could solve such analogy problems (Turney and Littman, 2005),
but the parallelogram method received more modern attention because of its suc-
cess with word2vec or GloVe vectors (Mikolov et al. 2013c, Levy and Go#l d b »erg
2
#
0 1 »4b, #P e n n i »ngton et al. 2014). Fo #r e x »ample, the res#u l t »of th# e e x »press#i o n»king
−
man+woman is a vector close to queen. Similarly, Paris France+Italy results
−6.10 • SEMANTICPROPERTIESOFEMBEDDINGS 127
tree
apple
vine
grape
Figure6.15 Theparallelogrammodelforanalogyproblems(RumelhartandAbrahamson,
# » # » # » # »
1973):thelocationofvinecanbefoundbysubtractingapplefromtreeandaddinggrape.
# »
in a vector that is close to Rome. The embedding model thus seems to be extract-
ingrepresentationsofrelationslikeMALE-FEMALE,orCAPITAL-CITY-OF,oreven
COMPARATIVE/SUPERLATIVE,asshowninFig.6.16fromGloVe.
(a) (b)
Figure6.16 RelationalpropertiesoftheGloVevectorspace,shownbyprojectingvectorsontotwodimen-
# » # » # » # »
sions. (a) king man+woman is close to queen. (b) offsets seem to capture comparative and superlative
−
morphology(Penningtonetal.,2014).
Foraa:b::a :b problem, meaningthealgorithmisgivenvectorsa, b, and
∗ ∗
a andmustfindb ,theparallelogrammethodisthus:
∗ ∗
bˆ ∗=argmin distance(x,b a+a ∗) (6.41)
x −
withsomedistancefunction,suchasEuclideandistance.
There are some caveats. For example, the closest value returned by the paral-
lelogram algorithm in word2vec or GloVe embedding spaces is usually not in fact
b* but one of the 3 input words or their morphological variants (i.e., cherry:red ::
potato:x returns potato or potatoes instead of brown), so these must be explicitly
excluded. Furthermore while embedding spaces perform well if the task involves
frequent words, small distances, and certain relations (like relating countries with
their capitals or verbs/nouns with their inflected forms), the parallelogram method
with embeddings doesn’t work as well for other relations (Linzen 2016, Gladkova
etal.2016,Schluter2018,Ethayarajhetal.2019a),andindeedPetersonetal.(2020)
argue that the parallelogram method is in general too simple to model the human
cognitiveprocessofforminganalogiesofthiskind.128 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS
6.10.1 EmbeddingsandHistoricalSemantics
Embeddingscanalsobeausefultoolforstudyinghowmeaningchangesovertime,
by computing multiple embedding spaces, each from texts written in a particular
timeperiod. ForexampleFig.6.17showsavisualizationofchangesinmeaningin
English words over the last two centuries, computed by building separate embed-
CdiHngAPspTaEceRs5fo.rDeaYcNhAdeMcaICdeSfOroCmIAhListRoEriPcaRlEcoSrEpNorTaAliTkIeOGNoSogOlFenW-gOraRmDsM(LEinAeNtIaNl.G, 79
2012b)andtheCorpusofHistoricalAmericanEnglish(Davies,2012).
Figure6.17 A t-SNE visualization of the semantic change of 3 words in English using
Figure5.1: Two-dimensionalvisualizationofsemanticchangeinEnglishusingSGNS
word2vec vectors. The modern sense of each word, and the grey context words, are com-
vectors (see Section 5.8 for the visualization algorithm). A, The word gay shifted
putedfromthemostrecent(modern)time-pointembeddingspace. Earlierpointsarecom-
frommeaning“cheerful”or“frolicsome”toreferringtohomosexuality. A,Intheearly
putedfromearlierhistoricalembeddingspaces. Thevisualizationsshowthechangesinthe
20th century broadcast referred to “casting out seeds”; with the rise of television and
wordgayfrommeaningsrelatedto“cheerful”or“frolicsome”toreferringtohomosexuality,
rthaedidoeviteslompmeaenntinogftshheifmteoddetron““ttrraannssmmisitstioinng”sseignnsealosf”.brCoa,dAcawsftuflroumnditesrowreignitnaalspernosceesosfof
psoewjoirnagtisoened,sa,sanitdsthhieftpeedjofrraotimonmofeathneinwgo“rdfualwl ofuflaawseit”sthoiftmedeafrnoimngm“etaenrirnibgl“efuolrlaofppawalel”ing”
[t2o1m2]e.aning“terribleorappalling”(Hamiltonetal.,2016b).
that adverbials (e.g., actually) have a general tendency to undergo subjectification
6.11 BiawhseraenthdeyEshimft fbroemdodbjienctigvesstatements about the world (e.g., “Sorry, the car is
actuallybroken”)tosubjectivestatements(e.g., “Ican’tbelieveheactuallydidthat”,
indicating surprise/disbelief).
In addition to their ability to learn word meaning from text, embeddings, alas,
also reproduce the implicit biases and stereotypes that were latent in the text. As
5.2.2 Computational linguistic studies
the prior section just showed, embeddings can roughly model relational similar-
ity: ‘queen’ as the closest word to ‘king’ - ‘man’ + ‘woman’ implies the analogy
Therearealsoanumberofrecentworksanalyzingsemanticchangeusingcomputational
man:woman::king:queen. Butthesesameembeddinganalogiesalsoexhibitgender
msteertehootdysp.e[s2.00F]ourseexlaamtepnltesBemolaunktbiacsaineatlayls.is(2t0o1a6n)afilynzdethhoawt twheorcdlomseesatnoincgcuspbartoioanden
atond‘cnoamrprouwteropverorgtriammem. e[1r1’3-]‘umsaenr’a+w‘cwoo-omcacnu’rriennwceorvde2cvtoercsetmobpeedrdfoinrgmstaraniunemdboenr of
news text is ‘homemaker’, and that the embeddings similarly suggest the analogy
historical case-studies on semantic change, and [252] perform a similar set of small-
‘father’isto‘doctor’as‘mother’isto‘nurse’. ThiscouldresultinwhatCrawford
scale case-studies using temporal topic models. [87] construct point-wise mutual
allocational (2017) and Blodgett et al. (2020) call an allocational harm, when a system allo-
harm
icnaftoersmreastoiounr-cbeass(ejdobesmobrecdrdedinitg)suannfadirfloyutnoddtihffaetresnetmgarnotuipcsc.hFaonrgeexsaumnpcloevaerlgeodribtyhmthseir
mtheatthuosdeheamdbreedadsionngasbalesapgarreteomfeanstewaritchhhfourmhainrinjugdpgomteennttisa.l[p1r2o9g]raanmdm[1e1rs9]oursdeo“cntoeursral”
mightthusincorrectlydownweightdocumentswithwomen’snames.
word-embedding methods to detect linguistic change points. Finally, [257] analyze
Itturnsoutthatembeddingsdon’tjustreflectthestatisticsoftheirinput,butalso
historical co-occurrences to test whether synonyms tend to change in similar ways.
bias amplifybias;genderedtermsbecomemoregenderedinembeddingspacethanthey
amplification
wereintheinputtextstatistics(Zhaoetal.2017,Ethayarajhetal.2019b,Jiaetal.
2020), and biases are more exaggerated than in actual labor employment statistics
(Gargetal.,2018).
Embeddingsalsoencodetheimplicitassociationsthatareapropertyofhuman
reasoning. The Implicit Association Test (Greenwald et al., 1998) measures peo-6.12 • EVALUATINGVECTORMODELS 129
ple’sassociationsbetweenconcepts(like‘flowers’or‘insects’)andattributes(like
‘pleasantness’ and ‘unpleasantness’) by measuring differences in the latency with
which they label words in the various categories.7 Using such methods, people
in the United States have been shown to associate African-American names with
unpleasant words (more than European-American names), male names more with
mathematicsandfemalenameswiththearts,andoldpeople’snameswithunpleas-
antwords(Greenwaldetal.1998,Noseketal.2002a,Noseketal.2002b). Caliskan
etal.(2017)replicatedallthesefindingsofimplicitassociationsusingGloVevectors
and cosine similarity instead of human latencies. For example African-American
nameslike‘Leroy’and‘Shaniqua’hadahigherGloVecosinewithunpleasantwords
while European-American names (‘Brad’, ‘Greg’, ‘Courtney’) had a higher cosine
withpleasantwords. Theseproblemswithembeddingsareanexampleofarepre-
representational sentationalharm(Crawford2017,Blodgettetal.2020),whichisaharmcausedby
harm
asystemdemeaningorevenignoringsomesocialgroups.Anyembedding-awareal-
gorithmthatmadeuseofwordsentimentcouldthusexacerbatebiasagainstAfrican
Americans.
Recent research focuses on ways to try to remove these kinds of biases, for
examplebydevelopingatransformationoftheembeddingspacethatremovesgen-
derstereotypesbutpreservesdefinitionalgender(Bolukbasietal.2016,Zhaoetal.
2017) or changing the training procedure (Zhao et al., 2018b). However, although
debiasing these sorts of debiasing may reduce bias in embeddings, they do not eliminate it
(GonenandGoldberg,2019),andthisremainsanopenproblem.
Historical embeddings are also being used to measure biases in the past. Garg
et al. (2018) used embeddings from historical texts to measure the association be-
tween embeddings for occupations and embeddings for names of various ethnici-
tiesorgenders(forexampletherelativecosinesimilarityofwomen’snamesversus
men’s to occupation words like ‘librarian’ or ‘carpenter’) across the 20th century.
They found that the cosines correlate with the empirical historical percentages of
women or ethnic groups in those occupations. Historical embeddings also repli-
catedoldsurveysofethnicstereotypes;thetendencyofexperimentalparticipantsin
1933toassociateadjectiveslike‘industrious’or‘superstitious’with, e.g., Chinese
ethnicity,correlateswiththecosinebetweenChineselastnamesandthoseadjectives
usingembeddingstrainedon1930stext. Theyalsowereabletodocumenthistorical
genderbiases,suchasthefactthatembeddingsforadjectivesrelatedtocompetence
(‘smart’,‘wise’,‘thoughtful’,‘resourceful’)hadahighercosinewithmalethanfe-
malewords, andshowedthatthisbiashasbeenslowlydecreasingsince1960. We
return in later chapters to this question about the role of bias in natural language
processing.
6.12 Evaluating Vector Models
The most important evaluation metric for vector models is extrinsic evaluation on
tasks, i.e., using vectors in an NLP task and seeing whether this improves perfor-
manceoversomeothermodel.
7 Roughlyspeaking,ifhumansassociate‘flowers’with‘pleasantness’and‘insects’with‘unpleasant-
ness’,whentheyareinstructedtopushagreenbuttonfor‘flowers’(daisy,iris,lilac)and‘pleasantwords’
(love,laughter,pleasure)andaredbuttonfor‘insects’(flea,spider,mosquito)and‘unpleasantwords’
(abuse,hatred,ugly)theyarefasterthaninanincongruousconditionwheretheypusharedbuttonfor
‘flowers’and‘unpleasantwords’andagreenbuttonfor‘insects’and‘pleasantwords’.130 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS
Nonethelessitisusefultohaveintrinsicevaluations. Themostcommonmetric
is to test their performance on similarity, computing the correlation between an
algorithm’swordsimilarityscoresandwordsimilarityratingsassignedbyhumans.
WordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0
to 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77.
SimLex-999 (Hill et al., 2015) is a more difficult dataset that quantifies similarity
(cup, mug) rather than relatedness (cup, coffee), and including both concrete and
abstractadjective,nounandverbpairs.TheTOEFLdatasetisasetof80questions,
eachconsistingofatargetwordwith4additionalwordchoices;thetaskistochoose
which is the correct synonym, as in the example: Levied is closest in meaning to:
imposed,believed,requested,correlated(LandauerandDumais,1997). Allofthese
datasetspresentwordswithoutcontext.
Slightly more realistic are intrinsic similarity tasks that include context. The
StanfordContextualWordSimilarity(SCWS)dataset(Huangetal.,2012)andthe
Word-in-Context(WiC)dataset(PilehvarandCamacho-Collados,2019)offerricher
evaluationscenarios.SCWSgiveshumanjudgmentson2,003pairsofwordsintheir
sententialcontext,whileWiCgivestargetwordsintwosententialcontextsthatare
either in the same or different senses; see Section 23.5.3. The semantic textual
similaritytask(Agirreetal.2012,Agirreetal.2015)evaluatestheperformanceof
sentence-level similarity algorithms, consisting of a set of pairs of sentences, each
pairwithhuman-labeledsimilarityscores.
Another task used for evaluation is the analogy task, discussed on page 126,
wherethesystemhastosolveproblemsoftheformaistobasa*istob*,givena,b,
anda*andhavingtofindb*(TurneyandLittman,2005).Anumberofsetsoftuples
havebeencreatedforthistask,(Mikolovetal.2013a,Mikolovetal.2013c,Glad-
kova et al. 2016), covering morphology (city:cities::child:children), lexicographic
relations(leg:table::spout:teapot)andencyclopediarelations(Beijing:China::Dublin:Ireland),
somedrawingfromtheSemEval-2012Task2datasetof79differentrelations(Jur-
gensetal.,2012).
Allembeddingalgorithmssufferfrominherentvariability. Forexamplebecause
of randomness in the initialization and the random negative sampling, algorithms
like word2vec may produce different results even from the same dataset, and in-
dividual documents in a collection may strongly impact the resulting embeddings
(Tianetal.2016,HellrichandHahn2016,AntoniakandMimno2018). Whenem-
beddings are used to study word associations in particular corpora, therefore, it is
bestpracticetotrainmultipleembeddingswithbootstrapsamplingoverdocuments
andaveragetheresults(AntoniakandMimno,2018).
6.13 Summary
• Invectorsemantics,awordismodeledasavector—apointinhigh-dimensional
space,alsocalledanembedding. Inthischapterwefocusonstaticembed-
dings,whereeachwordismappedtoafixedembedding.
• Vector semantic models fall into two classes: sparse and dense. In sparse
modelseachdimensioncorrespondstoawordinthevocabularyV andcells
are functions of co-occurrence counts. The term-document matrix has a
rowforeachword(term)inthevocabularyandacolumnforeachdocument.
The word-context or term-term matrix has a row for each (target) word inBIBLIOGRAPHICALANDHISTORICALNOTES 131
the vocabulary and a column for each context term in the vocabulary. Two
sparseweightingsarecommon: thetf-idfweightingwhichweightseachcell
byitstermfrequencyandinversedocumentfrequency,andPPMI(point-
wise positive mutual information), which is most common for word-context
matrices.
• Dense vector models have dimensionality 50–1000. Word2vec algorithms
likeskip-gramareapopularwaytocomputedenseembeddings. Skip-gram
trainsalogisticregressionclassifiertocomputetheprobabilitythattwowords
are‘likelytooccurnearbyintext’. Thisprobabilityiscomputedfromthedot
productbetweentheembeddingsforthetwowords.
• Skip-gramusesstochasticgradientdescenttotraintheclassifier,bylearning
embeddingsthathaveahighdotproductwithembeddingsofwordsthatoccur
nearbyandalowdotproductwithnoisewords.
• Other important embedding algorithms include GloVe, a method based on
ratiosofwordco-occurrenceprobabilities.
• Whether using sparse or dense vectors, word and document similarities are
computedbysomefunctionofthedotproductbetweenvectors. Thecosine
oftwovectors—anormalizeddotproduct—isthemostpopularsuchmetric.
Bibliographical and Historical Notes
The idea of vector semantics arose out of research in the 1950s in three distinct
fields: linguistics, psychology, and computer science, each of which contributed a
fundamentalaspectofthemodel.
The idea that meaning is related to the distribution of words in context was
widespread in linguistic theory of the 1950s, among distributionalists like Zellig
Harris,MartinJoos,andJ.R.Firth,andsemioticianslikeThomasSebeok. AsJoos
(1950)putit,
thelinguist’s“meaning”ofamorpheme...isbydefinitionthesetofconditional
probabilitiesofitsoccurrenceincontextwithallothermorphemes.
The idea that the meaning of a word might be modeled as a point in a multi-
dimensionalsemanticspacecamefrompsychologistslikeCharlesE.Osgood,who
hadbeenstudyinghowpeoplerespondedtothemeaningofwordsbyassigningval-
uesalongscaleslikehappy/sadorhard/soft. Osgoodetal.(1957)proposedthatthe
meaning of a word in general could be modeled as a point in a multidimensional
Euclidean space, and that the similarity of meaning between two words could be
modeledasthedistancebetweenthesepointsinthespace.
Afinalintellectualsourceinthe1950sandearly1960swasthefieldthencalled
mechanical mechanicalindexing,nowknownasinformationretrieval.Inwhatbecameknown
indexing
as the vector space model for information retrieval (Salton 1971, Sparck Jones
1986),researchersdemonstratednewwaystodefinethemeaningofwordsinterms
ofvectors(Switzer,1965), andrefinedmethodsforwordsimilaritybasedonmea-
sures of statistical association between words like mutual information (Giuliano,
1965) and idf (Sparck Jones, 1972), and showed that the meaning of documents
couldberepresentedinthesamevectorspacesusedforwords.
Some of the philosophical underpinning of the distributional way of thinking
came from the late writings of the philosopher Wittgenstein, who was skeptical of132 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS
the possibility of building a completely formal theory of meaning definitions for
eachword,suggestinginsteadthat“themeaningofawordisitsuseinthelanguage”
(Wittgenstein,1953,PI43).Thatis,insteadofusingsomelogicallanguagetodefine
eachword,ordrawingondenotationsortruthvalues,Wittgenstein’sideaisthatwe
shoulddefineawordbyhowitisusedbypeopleinspeakingandunderstandingin
theirday-to-dayinteractions, thusprefiguringthemovementtowardembodiedand
experientialmodelsinlinguisticsandNLP(GlenbergandRobertson2000,Lakeand
Murphy2021,Bisketal.2020,BenderandKoller2020).
Moredistantlyrelatedistheideaofdefiningwordsbyavectorofdiscretefea-
tures,whichhasrootsatleastasfarbackasDescartesandLeibniz(Wierzbicka1992,
Wierzbicka 1996). By the middle of the 20th century, beginning with the work of
Hjelmslev (Hjelmslev, 1969) (originally 1943) and fleshed out in early models of
generative grammar (Katz and Fodor, 1963), the idea arose of representing mean-
semantic ingwithsemanticfeatures,symbolsthatrepresentsomesortofprimitivemeaning.
feature
Forexamplewordslikehen,rooster,orchick,havesomethingincommon(theyall
describechickens)andsomethingdifferent(theirageandsex),representableas:
hen +female, +chicken, +adult
rooster -female, +chicken, +adult
chick +chicken, -adult
Thedimensionsusedbyvectormodelsofmeaningtodefinewords,however,are
onlyabstractlyrelatedtothisideaofasmallfixednumberofhand-builtdimensions.
Nonetheless, there has been some attempt to show that certain dimensions of em-
bedding models do contribute some specific compositional aspect of meaning like
theseearlysemanticfeatures.
Theuseofdensevectorstomodelwordmeaning,andindeedthetermembed-
ding, grew out of the latent semantic indexing (LSI) model (Deerwester et al.,
1988)recastasLSA(latentsemanticanalysis)(Deerwesteretal.,1990). InLSA
SVD singularvaluedecomposition—SVD—isappliedtoaterm-documentmatrix(each
cell weighted by log frequency and normalized by entropy), and then the first 300
dimensionsareusedastheLSAembedding. SingularValueDecomposition(SVD)
is a method for finding the most important dimensions of a data set, those dimen-
sionsalongwhichthedatavariesthemost. LSAwasthenquicklywidelyapplied:
as a cognitive model Landauer and Dumais (1997), and for tasks like spell check-
ing (Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and
Jurafsky1998,Bellegarda2000)morphologyinduction(SchoneandJurafsky2000,
Schone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Juraf-
sky, 2001a), and essay grading (Rehder et al., 1998). Related models were simul-
taneouslydevelopedandappliedtowordsensedisambiguationbySchu¨tze(1992b).
LSA also led to the earliest use of embeddings to represent words in a probabilis-
tic classifier, in the logistic regression document router of Schu¨tze et al. (1995).
The idea of SVD on the term-term matrix (rather than the term-document matrix)
asamodelofmeaningforNLPwasproposedsoonafterLSAbySchu¨tze(1992b).
Schu¨tzeappliedthelow-rank(97-dimensional)embeddingsproducedbySVDtothe
taskofwordsensedisambiguation,analyzedtheresultingsemanticspace,andalso
suggested possible techniques like dropping high-order dimensions. See Schu¨tze
(1997a).
A number of alternative matrix models followed on from the early SVD work,
including Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Latent
DirichletAllocation(LDA)(Bleietal.,2003),andNon-negativeMatrixFactoriza-
tion(NMF)(LeeandSeung,1999).EXERCISES 133
TheLSAcommunityseemstohavefirstusedtheword“embedding”inLandauer
etal.(1997),inavariantofitsmathematicalmeaningasamappingfromonespace
or mathematical structure to another. In LSA, the word embedding seems to have
describedthemappingfromthespaceofsparsecountvectorstothelatentspaceof
SVDdensevectors. Althoughthewordthusoriginallymeantthemappingfromone
spacetoanother,ithasmetonymicallyshiftedtomeantheresultingdensevectorin
thelatentspace,anditisinthissensethatwecurrentlyusetheword.
By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that
neurallanguagemodelscouldalsobeusedtodevelopembeddingsaspartofthetask
ofwordprediction. CollobertandWeston(2007),CollobertandWeston(2008),and
Collobertetal.(2011)thendemonstratedthatembeddingscouldbeusedtorepresent
wordmeaningsforanumberofNLPtasks. Turianetal.(2010)comparedthevalue
of different kinds of embeddings for different NLP tasks. Mikolov et al. (2011)
showed that recurrent neural nets could be used as language models. The idea of
simplifyingthehiddenlayeroftheseneuralnetlanguagemodelstocreatetheskip-
gram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The
negativesamplingtrainingalgorithmwasproposedinMikolovetal.(2013b). There
are numerous surveys of static embeddings and their parameterizations (Bullinaria
andLevy2007,BullinariaandLevy2012,LapesaandEvert2014,KielaandClark
2014,Levyetal.2015).
SeeManningetal.(2008)foradeeperunderstandingoftheroleofvectorsinin-
formationretrieval,includinghowtocomparequerieswithdocuments,moredetails
ontf-idf,andissuesofscalingtoverylargedatasets. SeeKim(2019)foraclearand
comprehensivetutorialonword2vec.Cruse(2004)isausefulintroductorylinguistic
textonlexicalsemantics.
Exercises134 CHAPTER7 • NEURALNETWORKSANDNEURALLANGUAGEMODELS
CHAPTER
7 Neural Networks and Neural
Language Models
“[M]achinesofthischaractercanbehaveinaverycomplicatedmannerwhen
thenumberofunitsislarge.”
AlanTuring(1948)“IntelligentMachines”,page6
Neural networks are a fundamental computational tool for language process-
ing, and a very old one. They are called neural because their origins lie in the
McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simplified model of the
human neuron as a kind of computing element that could be described in terms of
propositionallogic. Butthemodernuseinlanguageprocessingnolongerdrawson
theseearlybiologicalinspirations.
Instead, a modern neural network is a network of small computing units, each
ofwhichtakesavectorofinputvaluesandproducesasingleoutputvalue. Inthis
chapter we introduce the neural net applied to classification. The architecture we
feedforward introduceiscalledafeedforwardnetworkbecausethecomputationproceedsiter-
atively from one layer of units to the next. The use of modern neural nets is often
deeplearning calleddeeplearning,becausemodernnetworksareoftendeep(havemanylayers).
Neuralnetworkssharemuchofthesamemathematicsaslogisticregression.But
neuralnetworksareamorepowerfulclassifierthanlogisticregression,andindeeda
minimalneuralnetwork(technicallyonewithasingle‘hiddenlayer’)canbeshown
tolearnanyfunction.
Neuralnetclassifiersaredifferentfromlogisticregressioninanotherway. With
logistic regression, we applied the regression classifier to many different tasks by
developingmanyrichkindsoffeaturetemplatesbasedondomainknowledge.When
workingwithneuralnetworks,itismorecommontoavoidmostusesofrichhand-
derived features, instead building neural networks that take raw words as inputs
and learn to induce features as part of the process of learning to classify. We saw
examplesofthiskindofrepresentationlearningforembeddingsinChapter6. Nets
that are very deep are particularly good at representation learning. For that reason
deepneuralnetsaretherighttoolfortasksthatoffersufficientdatatolearnfeatures
automatically.
Inthischapterwe’llintroducefeedforwardnetworksasclassifiers,andalsoap-
ply them to the simple task of language modeling: assigning probabilities to word
sequencesandpredictingupcomingwords. Insubsequentchapterswe’llintroduce
many other aspects of neural models, such as recurrent neural networks (Chap-
ter9),theTransformer(Chapter10),andmaskedlanguagemodeling(Chapter11).7.1 • UNITS 135
7.1 Units
Thebuildingblockofaneuralnetworkisasinglecomputationalunit. Aunittakes
a set of real valued numbers as input, performs some computation on them, and
producesanoutput.
Atitsheart,aneuralunitistakingaweightedsumofitsinputs,withoneaddi-
biasterm tional term in the sum called a bias term. Given a set of inputs x 1...x n, a unit has
asetofcorrespondingweightsw ...w andabiasb, sotheweightedsumzcanbe
1 n
representedas:
z=b+ wx (7.1)
i i
i
(cid:88)
Oftenit’smoreconvenienttoexpressthisweightedsumusingvectornotation;recall
vector from linear algebra that a vector is, at heart, just a list or array of numbers. Thus
we’lltalkaboutzintermsofaweightvectorw,ascalarbiasb,andaninputvector
x,andwe’llreplacethesumwiththeconvenientdotproduct:
z=w x+b (7.2)
·
AsdefinedinEq.7.2,zisjustarealvaluednumber.
Finally, instead of using z, a linear function of x, as the output, neural units
apply a non-linear function f to z. We will refer to the output of this function as
activation the activation value for the unit, a. Since we are just modeling a single unit, the
activationforthenodeisinfactthefinaloutputofthenetwork,whichwe’llgenerally
cally. Sothevalueyisdefinedas:
y=a= f(z)
We’ll discuss three popular non-linear functions f() below (the sigmoid, the tanh,
andtherectifiedlinearunitorReLU)butit’spedagogicallyconvenienttostartwith
sigmoid thesigmoidfunctionsincewesawitinChapter5:
1
y=σ(z)= (7.3)
1+e z
−
Thesigmoid(showninFig.7.1)hasanumberofadvantages;itmapstheoutput
into the range (0,1), which is useful in squashing outliers toward 0 or 1. And it’s
differentiable,whichaswesawinSection5.10willbehandyforlearning.
Figure7.1 The sigmoid function takes a real value and maps it to the range (0,1). It is
nearlylineararound0butoutliervaluesgetsquashedtoward0or1.
SubstitutingEq.7.2intoEq.7.3givesustheoutputofaneuralunit:
1
y=σ(w x+b)= (7.4)
· 1+exp( (w x+b))
− ·136 CHAPTER7 • NEURALNETWORKSANDNEURALLANGUAGEMODELS
Fig.7.2showsafinalschematicofabasicneuralunit. Inthisexampletheunit
takes3inputvaluesx ,x ,andx ,andcomputesaweightedsum,multiplyingeach
1 2 3
valuebyaweight(w ,w ,andw ,respectively),addsthemtoabiastermb,andthen
1 2 3
passestheresultingsumthroughasigmoidfunctiontoresultinanumberbetween0
and1.
x
1 w 1
x w 2 ∑ z σ a y
2
w
3
x b
3
+1
Figure7.2 Aneuralunit,taking3inputsx 1,x 2,andx 3(andabiasbthatwerepresentasa
weightforaninputclampedat+1)andproducinganoutputy. Weincludesomeconvenient
intermediatevariables: theoutputofthesummation,z,andtheoutputofthesigmoid,a. In
thiscasetheoutputoftheunityisthesameasa,butindeepernetworkswe’llreserveyto
meanthefinaloutputoftheentirenetwork,leavingaastheactivationofanindividualnode.
Let’swalkthroughanexamplejusttogetanintuition. Let’ssupposewehavea
unitwiththefollowingweightvectorandbias:
w = [0.2,0.3,0.9]
b = 0.5
Whatwouldthisunitdowiththefollowinginputvector:
x = [0.5,0.6,0.1]
Theresultingoutputywouldbe:
1 1 1
y=σ(w x+b)= = = =.70
· 1+e (wx+b) 1+e (.5 .2+.6 .3+.1 .9+.5) 1+e 0.87
− · − ∗ ∗ ∗ −
Inpractice,thesigmoidisnotcommonlyusedasanactivationfunction. Afunction
tanh thatisverysimilarbutalmostalwaysbetteristhetanhfunctionshowninFig.7.3a;
tanhisavariantofthesigmoidthatrangesfrom-1to+1:
ez e z
−
y=tanh(z)= − (7.5)
ez+e z
−
Thesimplestactivationfunction, andperhapsthemostcommonlyused, istherec-
ReLU tified linear unit, also called the ReLU, shown in Fig. 7.3b. It’s just the same as z
whenzispositive,and0otherwise:
y=ReLU(z)=max(z,0) (7.6)
Theseactivationfunctionshavedifferentpropertiesthatmakethemusefulfordiffer-
entlanguageapplicationsornetworkarchitectures. Forexample,thetanhfunction
hasthenicepropertiesofbeingsmoothlydifferentiableandmappingoutliervalues
towardthemean. Therectifierfunction,ontheotherhand,hasnicepropertiesthat7.2 • THEXORPROBLEM 137
(a) (b)
Figure7.3 ThetanhandReLUactivationfunctions.
resultfromitbeingveryclosetolinear. Inthesigmoidortanhfunctions,veryhigh
saturated valuesofzresultinvaluesofythataresaturated,i.e.,extremelycloseto1,andhave
derivativesverycloseto0. Zeroderivativescauseproblemsforlearning,becauseas
we’ll see in Section 7.6, we’ll train networks by propagating an error signal back-
wards, multiplying gradients (partial derivatives) from each layer of the network;
gradientsthatarealmost0causetheerrorsignaltogetsmallerandsmalleruntilitis
vanishing toosmalltobeusedfortraining,aproblemcalledthevanishinggradientproblem.
gradient
Rectifiersdon’thavethisproblem,sincethederivativeofReLUforhighvaluesofz
is1ratherthanverycloseto0.
7.2 The XOR problem
Earlyinthehistoryofneuralnetworksitwasrealizedthatthepowerofneuralnet-
works, as with the real neurons that inspired them, comes from combining these
unitsintolargernetworks.
Oneofthemostcleverdemonstrationsoftheneedformulti-layernetworkswas
the proof by Minsky and Papert (1969) that a single neural unit cannot compute
someverysimplefunctionsofitsinput. Considerthetaskofcomputingelementary
logical functions of two inputs, like AND, OR, and XOR. As a reminder, here are
thetruthtablesforthosefunctions:
AND OR XOR
x1 x2 y x1 x2 y x1 x2 y
0 0 0 0 0 0 0 0 0
0 1 0 0 1 1 0 1 1
1 0 0 1 0 1 1 0 1
1 1 1 1 1 1 1 1 0
perceptron Thisexamplewasfirstshownfortheperceptron,whichisaverysimpleneural
unitthathasabinaryoutputanddoesnothaveanon-linearactivationfunction. The
outputyofaperceptronis0or1,andiscomputedasfollows(usingthesameweight
w,inputx,andbiasbasinEq.7.2):
0, ifw x+b 0
y= · ≤ (7.7)
1, ifw x+b>0
(cid:26) ·138 CHAPTER7 • NEURALNETWORKSANDNEURALLANGUAGEMODELS
It’s very easy to build a perceptron that can compute the logical AND and OR
functionsofitsbinaryinputs;Fig.7.4showsthenecessaryweights.
x x
1 1
1 1
x 1 x 1
2 2
-1 0
+1 +1
(a) (b)
Figure7.4 Theweightswandbiasbforperceptronsforcomputinglogicalfunctions. The
inputsareshownasx andx andthebiasasaspecialnodewithvalue+1whichismultiplied
1 2
withthebiasweightb. (a)logicalAND,withweightsw =1andw =1andbiasweight
1 2
b= 1. (b) logical OR, with weights w =1 and w =1 and bias weight b=0. These
1 2
−
weights/biasesarejustonefromaninfinitenumberofpossiblesetsofweightsandbiasesthat
wouldimplementthefunctions.
It turns out, however, that it’s not possible to build a perceptron to compute
logicalXOR!(It’sworthspendingamomenttogiveitatry!)
Theintuitionbehindthisimportantresultreliesonunderstandingthatapercep-
tron is a linear classifier. For a two-dimensional input x and x , the perceptron
1 2
equation,w x +w x +b=0istheequationofaline. (Wecanseethisbyputting
1 1 2 2
it in the standard linear format: x =( w /w )x +( b/w ).) This line acts as a
2 1 2 1 2
− −
decision decisionboundaryintwo-dimensionalspaceinwhichtheoutput0isassignedtoall
boundary
inputslyingononesideoftheline,andtheoutput1toallinputpointslyingonthe
othersideoftheline. Ifwehadmorethan2inputs,thedecisionboundarybecomes
ahyperplaneinsteadofaline,buttheideaisthesame,separatingthespaceintotwo
categories.
Fig.7.5showsthepossiblelogicalinputs(00,01,10,and11)andthelinedrawn
byonepossiblesetofparametersforanANDandanORclassifier.Noticethatthere
issimplynowaytodrawalinethatseparatesthepositivecasesofXOR(01and10)
linearly fromthenegativecases(00and11). WesaythatXORisnotalinearlyseparable
separable
function. Ofcoursewecoulddrawaboundarywithacurve,orsomeotherfunction,
butnotasingleline.
7.2.1 Thesolution: neuralnetworks
WhiletheXORfunctioncannotbecalculatedbyasingleperceptron,itcanbecal-
culatedbyalayerednetworkofperceptronunits. Ratherthanseethiswithnetworks
ofsimpleperceptrons,however,let’sseehowtocomputeXORusingtwolayersof
ReLU-basedunitsfollowingGoodfellowetal.(2016). Fig.7.6showsafigurewith
the input being processed by two layers of neural units. The middle layer (called
h)hastwounits,andtheoutputlayer(calledy)hasoneunit. Asetofweightsand
biasesareshownforeachReLUthatcorrectlycomputestheXORfunction.
Let’swalkthroughwhathappenswiththeinputx=[0,0]. Ifwemultiplyeach
inputvaluebytheappropriateweight,sum,andthenaddthebiasb,wegetthevector
[0,-1],andwethenapplytherectifiedlineartransformationtogivetheoutputofthe
h layer as [0, 0]. Now we once again multiply by the weights, sum, and add the
bias (0 in this case) resulting in the value 0. The reader should work through the
computationoftheremaining3possibleinputpairstoseethattheresultingyvalues
are1fortheinputs[0,1]and[1,0]and0for[0,0]and[1,1].7.2 • THEXORPROBLEM 139
x x x
2 2 2
1 1 1
?
0 0 0
x x x
1 1 1
0 1 0 1 0 1
a) x AND x b) x OR x c) x XOR x
1 2 1 2 1 2
Figure7.5 ThefunctionsAND,OR,andXOR,representedwithinputx 1 onthex-axisandinputx 2 onthe
y-axis. Filledcirclesrepresentperceptronoutputsof1,andwhitecirclesperceptronoutputsof0. Thereisno
waytodrawalinethatcorrectlyseparatesthetwocategoriesforXOR.FigurestyledafterRussellandNorvig
(2002).
x 1 h
1 1
1
1
y
1
1
-2
x
2 1 h 2 0
0
-1
+1 +1
Figure7.6 XORsolutionafterGoodfellowetal.(2016). TherearethreeReLUunits, in
twolayers;we’vecalledthemh ,h (hfor“hiddenlayer”)andy . Asbefore,thenumbers
1 2 1
onthearrowsrepresenttheweightswforeachunit,andwerepresentthebiasbasaweight
onaunitclampedto+1,withthebiasweights/unitsingray.
It’s also instructive to look at the intermediate results, the outputs of the two
hiddennodesh andh . Weshowedinthepreviousparagraphthatthehvectorfor
1 2
the inputs x = [0, 0] was [0, 0]. Fig. 7.7b shows the values of the h layer for all
4 inputs. Notice that hidden representations of the two input points x = [0, 1] and
x=[1, 0](thetwocaseswithXORoutput=1)aremergedtothesinglepointh=
[1,0]. Themergermakesiteasytolinearlyseparatethepositiveandnegativecases
ofXOR.Inotherwords,wecanviewthehiddenlayerofthenetworkasforminga
representationoftheinput.
InthisexamplewejuststipulatedtheweightsinFig.7.6. Butforrealexamples
theweightsforneuralnetworksarelearnedautomaticallyusingtheerrorbackprop-
agationalgorithmtobeintroducedinSection7.6. Thatmeansthehiddenlayerswill
learntoformusefulrepresentations. Thisintuition, thatneuralnetworkscanauto-
matically learn useful representations of the input, is one of their key advantages,
andonethatwewillreturntoagainandagaininlaterchapters.140 CHAPTER7 • NEURALNETWORKSANDNEURALLANGUAGEMODELS
x 2 h 2
1 1
0 0
x
1 h
0 1 0 1 2 1
a) The original x space b) The new (linearly separable) h space
Figure7.7 The hidden layer forming a new representation of the input. (b) shows the
representationofthehiddenlayer,h,comparedtotheoriginalinputrepresentationxin(a).
Notice that the input point [0, 1] has been collapsed with the input point [1, 0], making it
possibletolinearlyseparatethepositiveandnegativecasesofXOR.AfterGoodfellowetal.
(2016).
7.3 Feedforward Neural Networks
Let’snowwalkthroughaslightlymoreformalpresentationofthesimplestkindof
feedforward neuralnetwork,thefeedforwardnetwork. Afeedforwardnetworkisamultilayer
network
networkinwhichtheunitsareconnectedwithnocycles; theoutputsfromunitsin
each layer are passed to units in the next higher layer, and no outputs are passed
back to lower layers. (In Chapter 9 we’ll introduce networks with cycles, called
recurrentneuralnetworks.)
Forhistoricalreasonsmultilayernetworks,especiallyfeedforwardnetworks,are
multi-layer sometimescalledmulti-layerperceptrons(orMLPs);thisisatechnicalmisnomer,
perceptrons
MLP since the units in modern multilayer networks aren’t perceptrons (perceptrons are
purely linear, but modern networks are made up of units with non-linearities like
sigmoids),butatsomepointthenamestuck.
Simple feedforward networks have three kinds of nodes: input units, hidden
units,andoutputunits.
Fig.7.8showsapicture.Theinputlayerxisavectorofsimplescalarvaluesjust
aswesawinFig.7.2.
hiddenlayer Thecoreoftheneuralnetworkisthehiddenlayerhformedofhiddenunitsh i,
eachofwhichisaneuralunitasdescribedinSection7.1,takingaweightedsumof
itsinputsandthenapplyinganon-linearity. Inthestandardarchitecture,eachlayer
fully-connected isfully-connected, meaningthateachunitineachlayertakesasinputtheoutputs
fromalltheunitsinthepreviouslayer,andthereisalinkbetweeneverypairofunits
fromtwoadjacentlayers. Thuseachhiddenunitsumsoveralltheinputunits.
Recallthatasinglehiddenunithasasparametersaweightvectorandabias. We
representtheparametersfortheentirehiddenlayerbycombiningtheweightvector
andbiasforeachunitiintoasingleweightmatrixWandasinglebiasvectorbfor
thewholelayer(seeFig.7.8). EachelementW oftheweightmatrixWrepresents
ji
theweightoftheconnectionfromtheithinputunitx tothe jthhiddenunith .
i j
TheadvantageofusingasinglematrixW fortheweightsoftheentirelayeris
thatnowthehiddenlayercomputationforafeedforwardnetworkcanbedonevery
efficiently with simple matrix operations. In fact, the computation only has three7.3 • FEEDFORWARDNEURALNETWORKS 141
W U
x
1 y
1
h
1
h
x 2 y
2 2
h
3
x
n h
0 n
1
y
b n
+1 2
input layer hidden layer output layer
Figure7.8 Asimple2-layerfeedforwardnetwork,withonehiddenlayer,oneoutputlayer,
andoneinputlayer(theinputlayerisusuallynotcountedwhenenumeratinglayers).
steps: multiplyingtheweightmatrixbytheinputvectorx,addingthebiasvectorb,
andapplyingtheactivationfunctiong(suchasthesigmoid,tanh,orReLUactivation
functiondefinedabove).
Theoutputofthehiddenlayer,thevectorh,isthusthefollowing(forthisexam-
plewe’llusethesigmoidfunctionσ asouractivationfunction):
h=σ(Wx+b) (7.8)
Notice that we’re applying the σ function here to a vector, while in Eq. 7.3 it was
applied to a scalar. We’re thus allowing σ(), and indeed any activation function
·
g(),toapplytoavectorelement-wise,sog[z ,z ,z ]=[g(z ),g(z ),g(z )].
1 2 3 1 2 3
·
Let’sintroducesomeconstantstorepresentthedimensionalitiesofthesevectors
and matrices. We’ll refer to the input layer as layer 0 of the network, and have n
0
represent the number of inputs, so x is a vector of real numbers of dimension n ,
0
ormoreformallyx Rn0, acolumnvectorofdimensionality[n 0,1]. Let’scallthe
∈
hiddenlayerlayer1andtheoutputlayerlayer2. Thehiddenlayerhasdimensional-
ityn 1,soh Rn1 andalsob Rn1 (sinceeachhiddenunitcantakeadifferentbias
∈ ∈
value). AndtheweightmatrixWhasdimensionalityW Rn1×n0,i.e. [n 1,n 0].
∈
TakeamomenttoconvinceyourselfthatthematrixmultiplicationinEq.7.8will
computethevalueofeachh asσ n0 W x +b .
j i=1 ji i j
AswesawinSection7.2,theresultingvalueh(forhiddenbutalsoforhypoth-
(cid:0)(cid:80) (cid:1)
esis) forms a representation of the input. The role of the output layer is to take
this new representation h and compute a final output. This output could be a real-
valued number, but in many cases the goal of the network is to make some sort of
classificationdecision,andsowewillfocusonthecaseofclassification.
Ifwearedoingabinarytasklikesentimentclassification,wemighthaveasin-
gleoutputnode,anditsscalarvalueyistheprobabilityofpositiveversusnegative
sentiment. If we are doing multinomial classification, such as assigning a part-of-
speechtag,wemighthaveoneoutputnodeforeachpotentialpart-of-speech,whose
outputvalueistheprobabilityofthatpart-of-speech,andthevaluesofalltheoutput
nodesmustsumtoone. Theoutputlayeristhusavectory thatgivesaprobability
distributionacrosstheoutputnodes.
Let’sseehowthishappens. Likethehiddenlayer,theoutputlayerhasaweight
matrix(let’scallitU),butsomemodelsdon’tincludeabiasvectorbintheoutput
…
…
…142 CHAPTER7 • NEURALNETWORKSANDNEURALLANGUAGEMODELS
layer, sowe’llsimplifybyeliminatingthebiasvectorinthisexample. Theweight
matrixismultipliedbyitsinputvector(h)toproducetheintermediateoutputz:
z=Uh
There are n
2
output nodes, so z Rn2, weight matrix U has dimensionality U
∈ ∈
Rn2×n1,andelementU
ij
istheweightfromunit jinthehiddenlayertounitiinthe
outputlayer.
However,zcan’tbetheoutputoftheclassifier,sinceit’savectorofreal-valued
numbers,whilewhatweneedforclassificationisavectorofprobabilities. Thereis
normalizing a convenient function for normalizing a vector of real values, by which we mean
convertingittoavectorthatencodesaprobabilitydistribution(allthenumberslie
softmax between 0 and 1 and sum to 1): the softmax function that we saw on page 87 of
Chapter 5. More generally for any vector z of dimensionality d, the softmax is
definedas:
exp(z)
softmax(z i) =
d
expi
(z )
1 ≤i ≤d (7.9)
j=1 j
Thusforexamplegivenavector (cid:80)
z=[0.6,1.1, 1.5,1.2,3.2, 1.1], (7.10)
− −
thesoftmaxfunctionwillnormalizeittoaprobabilitydistribution(shownrounded):
softmax(z)=[0.055,0.090,0.0067,0.10,0.74,0.010] (7.11)
You may recall that we used softmax to create a probability distribution from a
vectorofreal-valuednumbers(computedfromsummingweightstimesfeatures)in
themultinomialversionoflogisticregressioninChapter5.
That means we can think of a neural network classifier with one hidden layer
asbuildingavectorhwhichisahiddenlayerrepresentationoftheinput,andthen
running standard multinomial logistic regression on the features that the network
developsinh. Bycontrast,inChapter5thefeaturesweremainlydesignedbyhand
via feature templates. So a neural network is like multinomial logistic regression,
but(a)withmanylayers,sinceadeepneuralnetworkislikelayerafterlayeroflo-
gisticregressionclassifiers;(b)withthoseintermediatelayershavingmanypossible
activation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we’ll
continuetouseσ forconveniencetomeananyactivationfunction);(c)ratherthan
formingthefeaturesbyfeaturetemplates,thepriorlayersofthenetworkinducethe
featurerepresentationsthemselves.
Herearethefinalequationsforafeedforwardnetworkwithasinglehiddenlayer,
whichtakesaninputvectorx,outputsaprobabilitydistributiony,andisparameter-
izedbyweightmatricesWandUandabiasvectorb:
h = σ(Wx+b)
z = Uh
y = softmax(z) (7.12)
And just to remember the shapes of all our variables, x Rn0, h Rn1, b Rn1,
∈ ∈ ∈
W Rn1×n0,U Rn2×n1,andtheoutputvectory Rn2.We’llcallthisnetworka2-
∈ ∈ ∈
layernetwork(wetraditionallydon’tcounttheinputlayerwhennumberinglayers,
butdocounttheoutputlayer).Sobythisterminologylogisticregressionisa1-layer
network.7.3 • FEEDFORWARDNEURALNETWORKS 143
7.3.1 Moredetailsonfeedforwardnetworks
Let’s now set up some notation to make it easier to talk about deeper networks of
depth more than 2. We’ll use superscripts in square brackets to mean layer num-
bers, starting at 0 for the input layer. So W[1] will mean the weight matrix for the
(first)hiddenlayer,andb[1] willmeanthebiasvectorforthe(first)hiddenlayer. n
j
willmeanthenumberof unitsatlayer j. We’ll useg()tostandfortheactivation
·
function, which will tend to be ReLU or tanh for intermediate layers and softmax
foroutputlayers. We’llusea[i] tomeantheoutputfromlayeri,andz[i] tomeanthe
combination of weights and biases W[i]a[i 1]+b[i]. The 0th layer is for inputs, so
−
we’llrefertotheinputsxmoregenerallyasa[0].
Thuswecanre-representour2-layernetfromEq.7.12asfollows:
z[1] = W[1]a[0]+b[1]
a[1] = g[1](z[1])
z[2] = W[2]a[1]+b[2]
a[2] = g[2](z[2])
yˆ = a[2] (7.13)
Notethatwiththisnotation,theequationsforthecomputationdoneateachlayerare
thesame. Thealgorithmforcomputingtheforwardstepinann-layerfeedforward
network,giventheinputvectora[0]isthussimply:
foriin1,...,n
z[i] = W[i]a[i 1] + b[i]
−
a[i] = g[i](z[i])
yˆ = a[n]
Theactivationfunctionsg()aregenerallydifferentatthefinallayer. Thusg[2]
·
mightbesoftmaxformultinomialclassificationorsigmoidforbinaryclassification,
whileReLUortanhmightbetheactivationfunctiong()attheinternallayers.
·
The need for non-linear activation functions One of the reasons we use non-
linearactivationfunctionsforeachlayerinaneuralnetworkisthatifwedidnot,the
resultingnetworkisexactlyequivalenttoasingle-layernetwork. Let’sseewhythis
istrue. Imaginethefirsttwolayersofsuchanetworkofpurelylinearlayers:
z[1] = W[1]x+b[1]
z[2] = W[2]z[1]+b[2]
Wecanrewritethefunctionthatthenetworkiscomputingas:
z[2] = W[2]z[1]+b[2]
= W[2](W[1]x+b[1])+b[2]
= W[2]W[1]x+W[2]b[1]+b[2]
= W (cid:48)x+b (cid:48) (7.14)
Thisgeneralizestoanynumberoflayers.Sowithoutnon-linearactivationfunctions,
a multilayer network is just a notational variant of a single layer network with a
different set of weights, and we lose all the representational power of multilayer
networks.144 CHAPTER7 • NEURALNETWORKSANDNEURALLANGUAGEMODELS
Replacingthebiasunit Indescribingnetworks,wewilloftenuseaslightlysim-
plifiednotationthatrepresentsexactlythesamefunctionwithoutreferringtoanex-
plicitbiasnodeb. Instead,weaddadummynodea toeachlayerwhosevaluewill
0
alwaysbe1.
Thuslayer0,theinputlayer,willhaveadummynodea[0]
=1,layer1
0
willhavea[1]
=1,andsoon. Thisdummynodestillhasanassociatedweight,and
0
thatweightrepresentsthebiasvalueb. Forexampleinsteadofanequationlike
h=σ(Wx+b) (7.15)
we’lluse:
h=σ(Wx) (7.16)
Butnowinsteadofourvectorxhavingn values: x=x ,...,x ,itwillhaven +
0 1 n0 0
1 values, with a new 0th dummy value x =1: x=x ,...,x . And instead of
0 0 n0
computingeachh asfollows:
j
n0
h j=σ W jix i+b j , (7.17)
(cid:32) (cid:33)
i=1
(cid:88)
we’llinsteaduse:
n0
h j=σ W jix i , (7.18)
(cid:32) (cid:33)
i=0
(cid:88)
wherethevalueW replaceswhathadbeenb . Fig.7.9showsavisualization.
j0 j
W U
W U
h
x 1 y x =1
1 1 0 y
h 1
1
h
x 2 2 y 2 x 1 h 2 y
h 2
3
h
x 3 2
x
n h 0 n
1 y h
+1 b n 2 x n n 1 y n
0 2
(a) (b)
Figure7.9 Replacingthebiasnode(shownina)withx 0(b).
We’ll continue showing the bias as b when we go over the learning algorithm
inSection7.6,butthenwe’llswitchtothissimplifiednotationwithoutexplicitbias
termsfortherestofthebook.
7.4 Feedforward networks for NLP: Classification
Let’s see how to apply feedforward networks to NLP tasks! In this section we’ll
lookatclassificationtaskslikesentimentanalysis;inthenextsectionwe’llintroduce
neurallanguagemodeling.
…
… …
…
…
…7.4 • FEEDFORWARDNETWORKSFORNLP:CLASSIFICATION 145
Let’sbeginwithasimple2-layersentimentclassifier. Youmightimaginetaking
ourlogisticregressionclassifierfromChapter5,whichcorrespondstoa1-layernet-
work,andjustaddingahiddenlayer. Theinputelementx couldbescalarfeatures
i
like those in Fig. 5.2, e.g., x = count(words doc), x = count(positive lexicon
1 2
∈
words doc), x =1if“no” doc, andsoon. Andtheoutputlayeryˆ couldhave
3
∈ ∈
twonodes(oneeachforpositiveandnegative),or3nodes(positive,negative,neu-
tral),inwhichcaseyˆ wouldbetheestimatedprobabilityofpositivesentiment,yˆ
1 2
theprobabilityofnegativeandyˆ theprobabilityofneutral. Theresultingequations
3
wouldbejustwhatwesawabovefora2-layernetwork(asalways, we’llcontinue
tousetheσ tostandforanynon-linearity,whethersigmoid,ReLUorother).
x = [x ,x ,...x ] (eachx isahand-designedfeature)
1 2 N i
h = σ(Wx+b)
z = Uh
yˆ = softmax(z) (7.19)
Fig.7.10showsasketchofthisarchitecture. Aswementionedearlier, addingthis
hiddenlayertoourlogisticregressionclassifierallowsthenetworktorepresentthe
non-linearinteractionsbetweenfeatures.Thisalonemightgiveusabettersentiment
classifier.
h
1
dessert wordcount x
=3 1 y^ p(+)
h 1
2
positive lexicon x
was words = 1 2 h 3 y^ 2 p(-)
great count of “no” x y^ 3 p(neut)
3
= 0
h
dh
Input words x W U y
h
[n⨉1] [d ⨉n] [3⨉d ] [3⨉1]
h [d ⨉1] h
h
Input layer Hidden layer Output layer
n=3 features softmax
Figure7.10 Feedforwardnetworksentimentanalysisusingtraditionalhand-builtfeatures
oftheinputtext.
MostapplicationsofneuralnetworksforNLPdosomethingdifferent,however.
Insteadofusinghand-builthuman-engineeredfeaturesastheinputtoourclassifier,
we draw on deep learning’s ability to learn features from the data by representing
wordsasembeddings,liketheword2vecorGloVeembeddingswesawinChapter6.
Therearevariouswaystorepresentaninputforclassification. Onesimplebaseline
pooling istoapplysomesortofpoolingfunctiontotheembeddingsofallthewordsinthe
input. Forexample,foratextwithninputwords/tokensw ,...,w ,wecanturnthe
1 n
n embeddings e(w ),...,e(w ) (each of dimensionality d) into a single embedding
1 n
alsoofdimensionalityd byjustsummingtheembeddings,orbytakingtheirmean
(summingandthendividingbyn):
n
1
x mean= e(w i) (7.20)
n
i=1
(cid:88)
…146 CHAPTER7 • NEURALNETWORKSANDNEURALLANGUAGEMODELS
Therearemanyotheroptions,liketakingtheelement-wisemax. Theelement-wise
max of a set of n vectors is a new vector whose kth element is the max of the kth
elements of all the n vectors. Here are the equations for this classifier assuming
meanpooling;thearchitectureissketchedinFig.7.11:
x = mean(e(w ),e(w ),...,e(w ))
1 2 n
h = σ(Wx+b)
z = Uh
yˆ = softmax(z) (7.21)
h1
dessert
em “b de ed sd si en rg
t”
for y^
p(+)
pooling 1
h2
embedding for ^y p(-)
was “was” 2
h3
embedding for ^y p(neut) great “great” 3
h
dh
Input words x W U y
h
[d⨉1] [d h⨉d]
[d ⨉1]
[3⨉d h] [3⨉1]
h
Input layer Hidden layer Output layer
pooled softmax
embedding
Figure7.11 Feedforwardnetworksentimentanalysisusingapooledembeddingofthein-
putwords.
WhileEq.7.21showshowtoaclassifyasingleexamplex,inpracticewewant
toefficientlyclassifyanentiretestsetofmexamples. Wedothisbyvectoringthe
process, just as we saw with logistic regression; instead of using for-loops to go
througheachexample,we’llusematrixmultiplicationtodotheentirecomputation
ofanentiretestsetatonce.First,wepackalltheinputfeaturevectorsforeachinput
xintoasingleinputmatrixX,witheachrowiarowvectorconsistingofthepooled
embeddingforinputexamplex(i) (i.e.,thevectorx(i)). Ifthedimensionalityofour
pooledinputembeddingisd,Xwillbeamatrixofshape[m d].
×
WewillthenneedtoslightlymodifyEq.7.21. Xisofshape[m d]andWisof
×
shape[d d],sowe’llhavetoreorderhowwemultiplyXandWandtransposeW
h
×
sotheycorrectlymultiplytoyieldamatrixHofshape[m d ]. Thebiasvectorb
h
×
fromEq.7.21ofshape[1 d ]willnowhavetobereplicatedintoamatrixofshape
h
×
[m d ]. We’llneedtosimilarlyreorderthenextstepandtransposeU. Finally,our
h
out× put matrix Yˆ will be of shape [m 3] (or more generally [m d ], where d is
o o
thenumberofoutputclasses),withea× chrowiofouroutputmatr× ixYˆ consistingof
the output vector yˆ(i).‘ Here are the final equations for computing the output class
distributionforanentiretestset:
(cid:124)
H = σ(XW +b)
(cid:124)
Z = HU
Yˆ = softmax(Z) (7.22)
+
…7.5 • FEEDFORWARDNEURALLANGUAGEMODELING 147
The idea of using word2vec or GloVe embeddings as our input representation—
andmoregenerallytheideaofrelyingonanotheralgorithmtohavealreadylearned
pretraining an embedding representation for our input words—is called pretraining. Using
pretrainedembeddingrepresentations,whethersimplestaticwordembeddingslike
word2vec or the much more powerful contextual embeddings we’ll introduce in
Chapter 11, is one of the central ideas of deep learning. (It’s also possible, how-
ever,totrainthewordembeddingsaspartofanNLPtask; we’lltalkabouthowto
dothisinSection7.7inthecontextoftheneurallanguagemodelingtask.)
7.5 Feedforward Neural Language Modeling
Asoursecondapplicationoffeedforwardnetworks,let’sconsiderlanguagemodel-
ing:predictingupcomingwordsfrompriorwordcontext.Neurallanguagemodeling
isanimportantNLPtaskinitself,anditplaysaroleinmanyimportantalgorithms
fortaskslikemachinetranslation,summarization,speechrecognition,grammarcor-
rection, and dialogue. We’ll describe simple feedforward neural language models,
firstintroducedbyBengioetal.(2003). Whilemodernneurallanguagemodelsuse
more powerful architectures like the recurrent nets or transformer networks to be
introduced in Chapter 9, the feedforward language model introduces many of the
importantconceptsofneurallanguagemodeling.
Neurallanguagemodelshavemanyadvantagesoverthen-gramlanguagemod-
elsofChapter3. Comparedton-grammodels,neurallanguagemodelscanhandle
muchlongerhistories,cangeneralizebetterovercontextsofsimilarwords,andare
more accurate at word-prediction. On the other hand, neural net language models
aremuchmorecomplex,areslowerandneedmoreenergytotrain,andarelessin-
terpretable than n-gram models, so for many (especially smaller) tasks an n-gram
languagemodelisstilltherighttool.
Afeedforwardneurallanguagemodel(LM)isafeedforwardnetworkthattakes
as input at timet a representation of some number of previous words (w ,w ,
t 1 t 2
− −
etc.) andoutputsaprobabilitydistributionoverpossiblenextwords. Thus—likethe
n-gram LM—the feedforward neural LM approximates the probability of a word
given the entire prior context P(w w ) by approximating based on the N 1
t 1:t 1
| − −
previouswords:
P(w t w 1,...,w t 1) P(w t w t N+1,...,w t 1) (7.23)
| − ≈ | − −
Inthefollowingexampleswe’llusea4-gramexample,sowe’llshowaneuralnetto
estimatetheprobabilityP(w =iw ,w ,w ).
t t 3 t 2 t 1
| − − −
Neural language models represent words in this prior context by their embed-
dings, rather than just by their word identity as used in n-gram language models.
Using embeddings allows neural language models to generalize better to unseen
data. Forexample,supposewe’veseenthissentenceintraining:
Ihavetomakesurethatthecatgetsfed.
buthaveneverseenthewords“getsfed”aftertheword“dog”. Ourtestsethasthe
prefix“Iforgottomakesurethatthedoggets”. What’sthenextword? Ann-gram
languagemodelwillpredict“fed”after“thatthecatgets”,butnotafter“thatthedog
gets”.ButaneuralLM,knowingthat“cat”and“dog”havesimilarembeddings,will
beabletogeneralizefromthe“cat”contexttoassignahighenoughprobabilityto
“fed”evenafterseeing“dog”.148 CHAPTER7 • NEURALNETWORKSANDNEURALLANGUAGEMODELS
7.5.1 Forwardinferenceintheneurallanguagemodel
forward Let’s walk through forward inference or decoding for neural language models.
inference
Forward inference is the task, given an input, of running a forward pass on the
networktoproduceaprobabilitydistributionoverpossibleoutputs,inthiscasenext
words.
We first represent each of the N previous words as a one-hot vector of length
one-hotvector V ,i.e.,withonedimensionforeachwordinthevocabulary. Aone-hotvectoris
| |
a vector that has one element equal to 1—in the dimension corresponding to that
word’sindexinthevocabulary—whilealltheotherelementsaresettozero. Thus
inaone-hotrepresentationfortheword“toothpaste”,supposingitisV ,i.e.,index
5
5inthevocabulary,x =1,andx =0 i=5,asshownhere:
5 i
∀ (cid:54)
[0 0 0 0 1 0 0 ... 0 0 0 0]
1 2 3 4 5 6 7 ... ... |V|
The feedforward neural language model (sketched in Fig. 7.13) has a moving
window that can see N words into the past. We’ll let N equal 3, so the 3 words
w , w , and w are each represented as a one-hot vector. We then multiply
t 1 t 2 t 3
the− seone− -hotvecto− rsbytheembeddingmatrixE. TheembeddingweightmatrixE
hasacolumnforeachword,eachacolumnvectorofd dimensions,andhencehas
dimensionalityd V . Multiplyingbyaone-hotvectorthathasonlyonenon-zero
×| |
elementx =1simplyselectsouttherelevantcolumnvectorforwordi,resultingin
i
theembeddingforwordi,asshowninFig.7.12.
|V| 1 1
d E ✕ 5 = d
5 |V|
e
5
Figure7.12 Selecting the embedding vector for wordV 5 by multiplying the embedding
matrixEwithaone-hotvectorwitha1inindex5.
The3resultingembeddingvectorsareconcatenatedtoproducee,theembedding
layer.Thisisfollowedbyahiddenlayerandanoutputlayerwhosesoftmaxproduces
aprobabilitydistributionoverwords. Forexampley ,thevalueofoutputnode42,
42
istheprobabilityofthenextwordw beingV ,thevocabularywordwithindex42
t 42
(whichistheword‘fish’inourexample).
Here’sthealgorithmindetailforourminiexample:
1. Select three embeddings from E: Given the three previous words, we look
uptheirindices,create3one-hotvectors,andthenmultiplyeachbytheem-
beddingmatrixE. Considerw . Theone-hotvectorfor‘for’(index35)is
t 3
multipliedbytheembeddingm− atrixE,togivethefirstpartofthefirsthidden
embedding layer, the embedding layer. Since each column of the input matrix E is an
layer
embedding for a word, and the input is a one-hot column vector x for word
i
V,theembeddinglayerforinputwwillbeEx =e,theembeddingforword
i i i
i. We now concatenate the three embeddings for the three context words to
producetheembeddinglayere.
2. Multiply by W: We multiply byW (and add b) and pass through the ReLU
(orother)activationfunctiontogetthehiddenlayerh.7.5 • FEEDFORWARDNEURALLANGUAGEMODELING 149
…
^
and 0 1 y 1 p(aardvark|…)
0
1 35
thanks
h1
0
w t-3 0 |V| E ^ y 34 p(do|…)
for
0 1 h2
0
all w t-2 10 992 y^ 42 p(fish|…)
00
|V| E
h3
the ^y
w t-1 0 0 1 35102
h 0 dh
? 1 451
w
t 00 |V| E e W h U
^y
|V| p(zebra|…)
…
x d⨉|V| 3d⨉1 d h⨉3d d h⨉1 |V|⨉d h y
|V|⨉3 |V|⨉1
input layer embedding hidden output layer
one-hot layer layer softmax
vectors
Figure7.13 Forwardinferenceinafeedforwardneurallanguagemodel. Ateachtimestep
tthenetworkcomputesad-dimensionalembeddingforeachcontextword(bymultiplyinga
one-hotvectorbytheembeddingmatrixE),andconcatenatesthe3resultingembeddingsto
gettheembeddinglayere. TheembeddingvectoreismultipliedbyaweightmatrixWand
thenanactivationfunctionisappliedelement-wisetoproducethehiddenlayerh,whichis
thenmultipliedbyanotherweightmatrixU. Finally,asoftmaxoutputlayerpredictsateach
nodeitheprobabilitythatthenextwordwt willbevocabularywordVi.
3. MultiplybyU:hisnowmultipliedbyU
4. Applysoftmax: Afterthesoftmax,eachnodeiintheoutputlayerestimates
theprobabilityP(w =iw ,w ,w )
t t 1 t 2 t 3
| − − −
Insummary,theequationsforaneurallanguagemodelwithawindowsizeof3,
givenone-hotinputvectorsforeachinputcontextword,are:
e = [Ex ;Ex ;Ex ]
t 3 t 2 t 1
− − −
h = σ(We+b)
z = Uh
yˆ = softmax(z) (7.24)
Note that we formed the embedding layer e by concatenating the 3 embeddings
for the three context vectors; we’ll often use semicolons to mean concatenation of
vectors.
In the next section we’ll introduce a general algorithm for training neural net-
works, and then return to how to specifically train the neural language model in
Section7.7.
…
…
…
…
…150 CHAPTER7 • NEURALNETWORKSANDNEURALLANGUAGEMODELS
7.6 Training Neural Nets
Afeedforwardneuralnetisaninstanceofsupervisedmachinelearninginwhichwe
know the correct output y for each observation x. What the system produces, via
Eq.7.13,isyˆ,thesystem’sestimateofthetruey. Thegoalofthetrainingprocedure
is to learn parameters W[i] and b[i] for each layer i that make yˆ for each training
observationascloseaspossibletothetruey.
Ingeneral,wedoallthisbydrawingonthemethodsweintroducedinChapter5
forlogisticregression,sothereadershouldbecomfortablewiththatchapterbefore
proceeding.
First, we’ll need a loss function that models the distance between the system
outputandthegoldoutput,andit’scommontousethelossfunctionusedforlogistic
regression,thecross-entropyloss.
Second, to find the parameters that minimize this loss function, we’ll use the
gradientdescentoptimizationalgorithmintroducedinChapter5.
Third, gradientdescentrequiresknowingthegradientofthelossfunction, the
vector that contains the partial derivative of the loss function with respect to each
of the parameters. In logistic regression, for each observation we could directly
computethederivativeofthelossfunctionwithrespecttoanindividualworb. But
forneuralnetworks,withmillionsofparametersinmanylayers,it’smuchharderto
see how to compute the partial derivative of some weight in layer 1 when the loss
isattachedtosomemuchlaterlayer. Howdowepartialoutthelossoverallthose
intermediatelayers? Theansweristhealgorithmcallederrorbackpropagationor
backwarddifferentiation.
7.6.1 Lossfunction
cross-entropy Thecross-entropylossthatisusedinneuralnetworksisthesameonewesawfor
loss
logistic regression. If the neural network is being used as a binary classifier, with
the sigmoid at the final layer, the loss function is the same logistic regression loss
wesawinEq.5.22:
L CE(yˆ,y)= logp(yx) = [ylogyˆ+(1 y)log(1 yˆ)] (7.25)
− | − − −
Ifweareusingthenetworktoclassifyinto3ormoreclasses,thelossfunctionis
exactlythesameasthelossformultinomialregressionthatwesawinChapter5on
page99. Let’sbrieflysummarizetheexplanationhereforconvenience. First,when
wehavemorethan2classeswe’llneedtorepresentbothy andyˆ asvectors. Let’s
assume we’re doing hard classification, where only one class is the correct one.
The true label y is then a vector with K elements, each corresponding to a class,
withy =1ifthecorrectclassisc,withallotherelementsofybeing0. Recallthat
c
avectorlikethis,withonevalueequalto1andtherest0,iscalledaone-hotvector.
AndourclassifierwillproduceanestimatevectorwithK elementsyˆ,eachelement
yˆ ofwhichrepresentstheestimatedprobability p(y =1x).
k k
|
ThelossfunctionforasingleexamplexisthenegativesumofthelogsoftheK
outputclasses,eachweightedbytheirprobabilityy :
k
K
L CE(yˆ,y)= y klogyˆ k (7.26)
−
k=1
(cid:88)
Wecansimplifythisequationfurther;let’sfirstrewritetheequationusingthefunc-
tion 1 which evaluates to 1 if the condition in the brackets is true and to 0 oth-
{}7.6 • TRAININGNEURALNETS 151
erwise. ThismakesitmoreobviousthatthetermsinthesuminEq.7.26willbe0
exceptforthetermcorrespondingtothetrueclassforwhichy =1:
k
K
L (yˆ,y) = 1 y =1 logyˆ
CE k k
− { }
k=1
(cid:88)
Inotherwords,thecross-entropylossissimplythenegativelogoftheoutputproba-
bilitycorrespondingtothecorrectclass,andwethereforealsocallthisthenegative
negativelog loglikelihoodloss:
likelihoodloss
L CE(yˆ,y) = logyˆ c (wherecisthecorrectclass) (7.27)
−
PlugginginthesoftmaxformulafromEq.7.9,andwithK thenumberofclasses:
exp(z )
L CE(yˆ,y) = −log
K
expc
(z )
(wherecisthecorrectclass) (7.28)
j=1 j
(cid:80)
7.6.2 ComputingtheGradient
How do we compute the gradient of this loss function? Computing the gradient
requires the partial derivative of the loss function with respect to each parameter.
For a network with one weight layer and sigmoid output (which is what logistic
regressionis),wecouldsimplyusethederivativeofthelossthatweusedforlogistic
regressioninEq.7.29(andderivedinSection5.10):
∂L (yˆ,y)
CE = (yˆ y)x
j
∂w −
j
= (σ(w x+b) y)x j (7.29)
· −
Orforanetworkwithoneweightlayerandsoftmaxoutput(=multinomiallogistic
regression), we could use the derivative of the softmax loss from Eq. 5.47, shown
foraparticularweightw andinputx
k i
∂L (yˆ,y)
CE = (y yˆ )x
∂w − k − k i
k,i
= (y p(y =1x))x
k k i
− − |
exp(w x+b )
= −(cid:32)y k − K j=1expk (· w
j
·x+k b j)(cid:33)x i (7.30)
(cid:80)
Butthesederivativesonlygivecorrectupdatesforoneweightlayer:thelastone!
Fordeepnetworks,computingthegradientsforeachweightismuchmorecomplex,
sincewearecomputingthederivativewithrespecttoweightparametersthatappear
all the way back in the very early layers of the network, even though the loss is
computedonlyattheveryendofthenetwork.
Thesolutiontocomputingthisgradientisanalgorithmcallederrorbackprop-
errorback- agation or backprop (Rumelhart et al., 1986). While backprop was invented spe-
propagation
ciallyforneuralnetworks, itturnsouttobethesameasamoregeneralprocedure
called backward differentiation, which depends on the notion of computation
graphs. Let’sseehowthatworksinthenextsubsection.152 CHAPTER7 • NEURALNETWORKSANDNEURALLANGUAGEMODELS
7.6.3 ComputationGraphs
Acomputationgraphisarepresentationoftheprocessofcomputingamathematical
expression,inwhichthecomputationisbrokendownintoseparateoperations,each
ofwhichismodeledasanodeinagraph.
ConsidercomputingthefunctionL(a,b,c)=c(a+2b). Ifwemakeeachofthe
componentadditionandmultiplicationoperationsexplicit,andaddnames(dande)
fortheintermediateoutputs,theresultingseriesofcomputationsis:
d = 2 b
∗
e = a+d
L = c e
∗
We can now represent this as a graph, with nodes for each operation, and di-
rected edges showing the outputs from each operation as the inputs to the next, as
in Fig. 7.14. The simplest use of computation graphs is to compute the value of
thefunctionwithsomegiveninputs. Inthefigure,we’veassumedtheinputsa=3,
b=1,c= 2,andwe’veshowntheresultoftheforwardpasstocomputethere-
−
sultL(3,1, 2)= 10. Intheforwardpassofacomputationgraph,weapplyeach
− −
operation left to right, passing the outputs of each computation as the input to the
nextnode.
forward pass
a=3
a
e=5
e=a+d
d=2
b=1
b d = 2b L=ce L=-10
c=-2
c
Figure7.14 ComputationgraphforthefunctionL(a,b,c)=c(a+2b),withvaluesforinput
nodesa=3,b=1,c= 2,showingtheforwardpasscomputationofL.
−
7.6.4 Backwarddifferentiationoncomputationgraphs
The importance of the computation graph comes from the backward pass, which
is used to compute the derivatives that we’ll need for the weight update. In this
exampleourgoalistocomputethederivativeoftheoutputfunctionLwithrespect
to eachof the input variables, i.e., ∂L, ∂L, and ∂L. The derivative ∂L tells ushow
∂a ∂b ∂c ∂a
muchasmallchangeinaaffectsL.
chainrule Backwards differentiation makes use of the chain rule in calculus, so let’s re-
mind ourselves of that. Suppose we are computing the derivative of a composite
function f(x)=u(v(x)). Thederivativeof f(x)isthederivativeofu(x)withrespect
tov(x)timesthederivativeofv(x)withrespecttox:
df du dv
= (7.31)
dx dv·dx
Thechainruleextendstomorethantwofunctions. Ifcomputingthederivativeofa
compositefunction f(x)=u(v(w(x))),thederivativeof f(x)is:
df du dv dw
= (7.32)
dx dv·dw· dx7.6 • TRAININGNEURALNETS 153
Theintuitionofbackwarddifferentiationistopassgradientsbackfromthefinal
nodetoallthenodesinthegraph.Fig.7.15showspartofthebackwardcomputation
atonenodee. Eachnodetakesanupstreamgradientthatispassedinfromitsparent
nodetotheright,andforeachofitsinputscomputesalocalgradient(thegradient
ofitsoutputwithrespecttoitsinput),andusesthechainruletomultiplythesetwo
tocomputeadownstreamgradienttobepassedontothenextearliernode.
d e
d e L
∂L ∂L ∂e ∂e ∂L
=
∂d ∂e ∂d ∂d ∂e
downstream local upstream
gradient gradient gradient
Figure7.15 Eachnode(likeehere)takesanupstreamgradient,multipliesitbythelocal
gradient(thegradientofitsoutputwithrespecttoitsinput),andusesthechainruletocompute
a downstream gradient to be passed on to a prior node. A node may have multiple local
gradientsifithasmultipleinputs.
Let’s now compute the 3 derivatives we need. Since in the computation graph
L=ce,wecandirectlycomputethederivative ∂L:
∂c
∂L
=e (7.33)
∂c
Fortheothertwo,we’llneedtousethechainrule:
∂L ∂L∂e
=
∂a ∂e∂a
∂L ∂L∂e∂d
= (7.34)
∂b ∂e∂d ∂b
Eq.7.34andEq.7.33thusrequirefiveintermediatederivatives: ∂L, ∂L, ∂e, ∂e,and
∂e ∂c ∂a ∂d
∂d,whichareasfollows(makinguseofthefactthatthederivativeofasumisthe
∂b
sumofthederivatives):
∂L ∂L
L=ce : =c, =e
∂e ∂c
∂e ∂e
e=a+d : =1, =1
∂a ∂d
∂d
d=2b : =2
∂b
In the backward pass, we compute each of these partials along each edge of the
graphfromrighttoleft,usingthechainrulejustaswedidabove. Thuswebeginby
computingthedownstreamgradientsfromnodeL,whichare ∂L and ∂L.Fornodee,
∂e ∂c
wethenmultiplythisupstreamgradient ∂L bythelocalgradient(thegradientofthe
∂e
outputwithrespecttotheinput), ∂e togettheoutputwesendbacktonoded: ∂L.
∂d ∂d
Andsoon,untilwehaveannotatedthegraphallthewaytoalltheinputvariables.
Theforwardpassconvenientlyalreadywillhavecomputedthevaluesoftheforward
intermediatevariablesweneed(likedande)tocomputethesederivatives. Fig.7.16
showsthebackwardpass.154 CHAPTER7 • NEURALNETWORKSANDNEURALLANGUAGEMODELS
a=3
a
∂L ∂L∂e
= =-2
∂a ∂e ∂a e=d+a e=5
d=2
b=1
∂e ∂e
b d = 2b ∂L =∂L∂e
=-2
∂a =1 ∂d =1 ∂ ∂L e=-2 L=ce L=-10
∂L ∂L∂d ∂d ∂d ∂e ∂d
= =-4 =2 ∂L
∂b ∂d ∂b ∂b =-2
∂e
c=-2 ∂L
=5
∂c
∂L
c =5 backward pass
∂c
Figure7.16 ComputationgraphforthefunctionL(a,b,c)=c(a+2b),showingthebackwardpasscomputa-
tionof ∂L, ∂L,and ∂L.
∂a ∂b ∂c
Backwarddifferentiationforaneuralnetwork
Of course computation graphs for real neural networks are much more complex.
Fig.7.17showsasamplecomputationgraphfora2-layerneuralnetworkwithn =
0
2, n =2, and n =1, assuming binary classification and hence using a sigmoid
1 2
outputunitforsimplicity. Thefunctionthatthecomputationgraphiscomputingis:
z[1] = W[1]x+b[1]
a[1] = ReLU(z[1])
z[2] = W[2]a[1]+b[2]
a[2] = σ(z[2])
yˆ = a[2] (7.35)
Forthebackwardpasswe’llalsoneedtocomputethelossL. Thelossfunction
forbinarysigmoidoutputfromEq.7.25is
L CE(yˆ,y) = [ylogyˆ+(1 y)log(1 yˆ)] (7.36)
− − −
Ouroutputyˆ=a[2],sowecanrephrasethisas
L CE(a[2],y) = yloga[2]+(1 y)log(1 a[2]) (7.37)
− − −
(cid:104) (cid:105)
The weights that need updating (those for which we need to know the partial
derivativeofthelossfunction)areshowninteal. Inordertodothebackwardpass,
we’llneedtoknowthederivativesofallthefunctionsinthegraph. Wealreadysaw
inSection5.10thederivativeofthesigmoidσ:
dσ(z)
=σ(z)(1 σ(z)) (7.38)
dz −
We’ll also need the derivatives of each of the other activation functions. The
derivativeoftanhis:
dtanh(z)
=1 tanh2(z) (7.39)
dz −7.6 • TRAININGNEURALNETS 155
[1]
w
11 *
[1]
w
12 z[1] = a[1] =
* 1 1
+ ReLU
x
1
*
[1]
b [2]
x
2
1 w 1[ 12] z + = a[2] = σ L (a[2] ,y)
*
*
[1] [2]
w [1] [1] w
21 * z 2 = a 2 = 12
+ ReLU
[1]
w [2]
22 b
1
[1]
b
2
Figure7.17 Samplecomputationgraphforasimple2-layerneuralnet(=1hiddenlayer)withtwoinputunits
and2hiddenunits. We’veadjustedthenotationabittoavoidlongequationsinthenodesbyjustmentioning
[1]
thefunctionthatisbeingcomputed,andtheresultingvariablename.Thusthe*totherightofnodew means
11
thatw[1] istobemultipliedbyx ,andthenodez[1]=+meansthatthevalueofz[1] iscomputedbysumming
11 1
[1]
thethreenodesthatfeedintoit(thetwoproducts,andthebiastermb ).
i
ThederivativeoftheReLUis
dReLU(z) 0 for z<0
= (7.40)
dz 1 for z 0
(cid:26) ≥
We’ll give the start of the computation, computing the derivative of the loss
function L with respect to z, or ∂L (and leaving the rest of the computation as an
∂z
exerciseforthereader). Bythechainrule:
∂L ∂L ∂a[2]
= (7.41)
∂z ∂a[2] ∂z
Solet’sfirstcompute ∂L ,takingthederivativeofEq.7.37,repeatedhere:
∂a[2]
L (a[2],y) = yloga[2]+(1 y)log(1 a[2])
CE
− − −
(cid:104) (cid:105)
∂L ∂log(a[2]) ∂log(1 a[2])
= y +(1 y) −
∂a[2] −(cid:32)(cid:32) ∂a[2] (cid:33) − ∂a[2] (cid:33)
1 1
= y +(1 y) ( 1)
− a[2] − 1 a[2] −
(cid:18)(cid:18) (cid:19) − (cid:19)
y y 1
= + − (7.42)
− a[2] 1 a[2]
(cid:18) − (cid:19)
Next,bythederivativeofthesigmoid:
∂a[2]
=a[2](1 a[2])
∂z −156 CHAPTER7 • NEURALNETWORKSANDNEURALLANGUAGEMODELS
Finally,wecanusethechainrule:
∂L ∂L ∂a[2]
=
∂z ∂a[2] ∂z
y y 1
= + − a[2](1 a[2])
− a[2] 1 a[2] −
(cid:18) − (cid:19)
= a[2] y (7.43)
−
Continuingthebackwardcomputationofthegradients(nextbypassingthegra-
[2]
dientsoverb andthetwoproductnodes,andsoon,backtoallthetealnodes),is
1
leftasanexerciseforthereader.
7.6.5 Moredetailsonlearning
Optimizationinneuralnetworksisanon-convexoptimizationproblem,morecom-
plexthanforlogisticregression,andforthatandotherreasonstherearemanybest
practicesforsuccessfullearning.
Forlogisticregressionwecaninitializegradientdescentwithalltheweightsand
biaseshavingthevalue0. Inneuralnetworks,bycontrast,weneedtoinitializethe
weightswithsmallrandomnumbers. It’salsohelpfultonormalizetheinputvalues
tohave0meanandunitvariance.
Variousformsofregularizationareusedtopreventoverfitting. Oneofthemost
dropout important is dropout: randomly dropping some units and their connections from
the network during training (Hinton et al. 2012, Srivastava et al. 2014). Tuning
hyperparameter ofhyperparametersisalsoimportant. Theparametersofaneuralnetworkarethe
weightsW andbiasesb;thosearelearnedbygradientdescent.Thehyperparameters
arethingsthatarechosenbythealgorithmdesigner; optimalvaluesaretunedona
devsetratherthanbygradientdescentlearningonthetrainingset. Hyperparameters
includethelearningrateη,themini-batchsize,themodelarchitecture(thenumber
oflayers,thenumberofhiddennodesperlayer,thechoiceofactivationfunctions),
how to regularize, and so on. Gradient descent itself also has many architectural
variantssuchasAdam(KingmaandBa,2015).
Finally, most modern neural networks are built using computation graph for-
malismsthatmakeiteasyandnaturaltodogradientcomputationandparallelization
on vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017)
and TensorFlow (Abadi et al., 2015) are two of the most popular. The interested
reader should consult a neural network textbook for further details; some sugges-
tionsareattheendofthechapter.
7.7 Training the neural language model
Nowthatwe’veseenhowtotrainagenericneuralnet,let’stalkaboutthearchitec-
turefortraininganeurallanguagemodel,settingtheparametersθ =E,W,U,b.
freeze For some tasks, it’s ok to freeze the embedding layer E with initial word2vec
values. Freezing means we use word2vec or some other pretraining algorithm to
compute the initial embedding matrix E, and then hold it constant while we only
modifyW,U,andb,i.e.,wedon’tupdateEduringlanguagemodeltraining. How-
ever,oftenwe’dliketolearntheembeddingssimultaneouslywithtrainingthenet-
work. Thisisusefulwhenthetaskthenetworkisdesignedfor(likesentimentclas-7.7 • TRAININGTHENEURALLANGUAGEMODEL 157
sification, translation, or parsing) places strong constraints on what makes a good
representationforwords.
Let’sseehowtotraintheentiremodelincludingE,i.e. tosetalltheparameters
θ =E,W,U,b. We’lldothisviagradientdescent(Fig.5.6),usingerrorbackprop-
agation on the computation graph to compute the gradient. Training thus not only
sets the weights W and U of the network, but also as we’re predicting upcoming
words, we’relearningtheembeddingsEforeachwordthatbestpredictupcoming
words.
w t=fish L = −log P(fish | for, all, the)
…
^
and 0 1 y 1 p(aardvark|…)
0
1 35
thanks
h1
w t-3 00 |V| E ^ y 34 p(do|…)
for
0 1 h2
0
all w t-2 10 992 y^ 42 p(fish|…)
00
|V| E
h3
the ^y
w t-1 0 0 1 35102
h 0 dh
fish w t 1 00 4 |V5 |1
E
e W h U ^y |V| p(zebra|…)
…
x d⨉|V| 3d⨉1 d h⨉3d d h⨉1 |V|⨉d h y
|V|⨉3 |V|⨉1
input layer embedding hidden output layer
one-hot layer layer softmax
vectors
Figure7.18 Learningallthewaybacktoembeddings. Again,theembeddingmatrixEis
sharedamongthe3contextwords.
Fig.7.18showsthesetupforawindowsizeofN=3contextwords. Theinputx
consistsof3one-hotvectors,fullyconnectedtotheembeddinglayervia3instanti-
ationsoftheembeddingmatrixE. Wedon’twanttolearnseparateweightmatrices
formappingeachofthe3previouswordstotheprojectionlayer.Wewantonesingle
embeddingdictionaryE that’ssharedamongthesethree. That’sbecauseovertime,
many different words will appear as w or w , and we’d like to just represent
t 2 t 1
− −
eachwordwithonevector,whichevercontextpositionitappearsin. Recallthatthe
embeddingweightmatrixE hasacolumnforeachword,eachacolumnvectorofd
dimensions,andhencehasdimensionalityd V .
×| |
Generallytrainingproceedsbytakingasinputaverylongtext,concatenatingall
thesentences,startingwithrandomweights,andtheniterativelymovingthroughthe
textpredictingeachwordw. Ateachwordw,weusethecross-entropy(negative
t t
loglikelihood)loss. Recallthatthegeneralformforthis(repeatedfromEq.7.27is:
L CE(yˆ,y) = logyˆ i, (whereiisthecorrectclass) (7.44)
−
Forlanguagemodeling,theclassesarethewordsinthevocabulary,soyˆ heremeans
i
theprobabilitythatthemodelassignstothecorrectnextwordw:
t
L CE= logp(w t w t 1,...,w t n+1) (7.45)
− | − −
…
…
…
…
…158 CHAPTER7 • NEURALNETWORKSANDNEURALLANGUAGEMODELS
Theparameterupdateforstochasticgradientdescentforthislossfromstepstos+1
isthen:
∂[ logp(w w ,...,w )]
θs+1=θs η − t | t −1 t −n+1 (7.46)
− ∂θ
This gradient can be computed in any standard neural network framework which
willthenbackpropagatethroughθ =E,W,U,b.
Training the parameters to minimize loss will result both in an algorithm for
languagemodeling(awordpredictor)butalsoanewsetofembeddingsEthatcan
beusedaswordrepresentationsforothertasks.
7.8 Summary
• Neural networks are built out of neural units, originally inspired by human
neuronsbutnowsimplyanabstractcomputationaldevice.
• Eachneuralunitmultipliesinputvaluesbyaweightvector, addsabias, and
then applies a non-linear activation function like sigmoid, tanh, or rectified
linearunit.
• Inafully-connected,feedforwardnetwork,eachunitinlayeriisconnected
toeachunitinlayeri+1,andtherearenocycles.
• Thepowerofneuralnetworkscomesfromtheabilityofearlylayerstolearn
representationsthatcanbeutilizedbylaterlayersinthenetwork.
• Neural networks are trained by optimization algorithms like gradient de-
scent.
• Errorbackpropagation,backwarddifferentiationonacomputationgraph,
isusedtocomputethegradientsofthelossfunctionforanetwork.
• Neurallanguagemodelsuseaneuralnetworkasaprobabilisticclassifier,to
computetheprobabilityofthenextwordgiventhepreviousnwords.
• Neurallanguagemodelscanusepretrainedembeddings,orcanlearnembed-
dingsfromscratchintheprocessoflanguagemodeling.
Bibliographical and Historical Notes
Theoriginsofneuralnetworkslieinthe1940sMcCulloch-Pittsneuron(McCul-
loch and Pitts, 1943), a simplified model of the human neuron as a kind of com-
putingelementthatcouldbedescribedintermsofpropositionallogic. Bythelate
1950sandearly1960s,anumberoflabs(includingFrankRosenblattatCornelland
Bernard Widrow at Stanford) developed research into neural networks; this phase
saw the development of the perceptron (Rosenblatt, 1958), and the transformation
ofthethresholdintoabias,anotationwestilluse(WidrowandHoff,1960).
Thefieldofneuralnetworksdeclinedafteritwasshownthatasingleperceptron
unit was unable to model functions as simple as XOR (Minsky and Papert, 1969).
Whilesomesmallamountofworkcontinuedduringthenexttwodecades,amajor
revival for the field didn’t come until the 1980s, when practical tools for building
deeper networks like error backpropagation became widespread (Rumelhart et al.,BIBLIOGRAPHICALANDHISTORICALNOTES 159
1986). During the 1980s a wide variety of neural network and related architec-
turesweredeveloped,particularlyforapplicationsinpsychologyandcognitivesci-
ence (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart
connectionist and McClelland 1986a, Elman 1990), for which the term connectionist or paral-
leldistributedprocessingwasoftenused(FeldmanandBallard1982,Smolensky
1988). Many of the principles and techniques developed in this period are foun-
dationaltomodernwork,includingtheideasofdistributedrepresentations(Hinton,
1986),recurrentnetworks(Elman,1990),andtheuseoftensorsforcompositionality
(Smolensky,1990).
Bythe1990slargerneuralnetworksbegantobeappliedtomanypracticallan-
guageprocessingtasksaswell,likehandwritingrecognition(LeCunetal.1989)and
speechrecognition(MorganandBourlard1990). Bytheearly2000s,improvements
incomputerhardwareandadvancesinoptimizationandtrainingtechniquesmadeit
possibletotrainevenlargeranddeepernetworks,leadingtothemoderntermdeep
learning(Hintonetal.2006,Bengioetal.2007). Wecovermorerelatedhistoryin
Chapter9andChapter16.
There are a number of excellent books on the subject. Goldberg (2017) has
superb coverage of neural networks for natural language processing. For neural
networksingeneralseeGoodfellowetal.(2016)andNielsen(2015).160 CHAPTER8 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES
CHAPTER
8 Sequence Labeling for Parts of
Speech and Named Entities
Toeachwordawarblingnote
AMidsummerNight’sDream,V.I
DionysiusThraxofAlexandria(c.100B.C.),orperhapssomeoneelse(itwasalong
time ago), wrote a grammatical sketch of Greek (a “techne¯”) that summarized the
linguisticknowledgeofhisday.Thisworkisthesourceofanastonishingproportion
ofmodernlinguisticvocabulary, includingthewordssyntax, diphthong, clitic, and
partsofspeech analogy. Also included are a description of eight parts of speech: noun, verb,
pronoun, preposition, adverb, conjunction, participle, andarticle. Althoughearlier
scholars (including Aristotle as well as the Stoics) had their own lists of parts of
speech,itwasThrax’ssetofeightthatbecamethebasisfordescriptionsofEuropean
languagesforthenext2000years.(AllthewaytotheSchoolhouseRockeducational
televisionshowsofourchildhood,whichhadsongsabout8partsofspeech,likethe
lategreatBobDorough’sConjunctionJunction.) Thedurabilityofpartsofspeech
throughtwomillenniaspeakstotheircentralityinmodelsofhumanlanguage.
Proper names are another important and anciently studied linguistic category.
While parts of speech are generally assigned to individual words or morphemes, a
propernameisoftenanentiremultiwordphrase, likethename“MarieCurie”, the
location“NewYorkCity”,ortheorganization“StanfordUniversity”. We’llusethe
namedentity term named entity for, roughly speaking, anything that can be referred to with a
propername: aperson,alocation,anorganization,althoughaswe’llseethetermis
commonlyextendedtoincludethingsthataren’tentitiesperse.
POS Parts of speech (also known as POS) and named entities are useful clues to
sentencestructureandmeaning.Knowingwhetherawordisanounoraverbtellsus
aboutlikelyneighboringwords(nounsinEnglishareprecededbydeterminersand
adjectives,verbsbynouns)andsyntacticstructure(verbshavedependencylinksto
nouns),makingpart-of-speechtaggingakeyaspectofparsing. Knowingifanamed
entitylikeWashingtonisanameofaperson,aplace,orauniversityisimportantto
many natural language processing tasks like question answering, stance detection,
orinformationextraction.
Inthischapterwe’llintroducethetaskofpart-of-speechtagging, takingase-
quenceofwordsandassigningeachwordapartofspeechlikeNOUNorVERB,and
thetaskofnamedentityrecognition(NER),assigningwordsorphrasestagslike
PERSON,LOCATION,orORGANIZATION.
Such tasks in which we assign, to each word x in an input word sequence, a
i
labely,sothattheoutputsequenceY hasthesamelengthastheinputsequenceX
i
sequence arecalledsequencelabelingtasks. We’llintroduceclassicsequencelabelingalgo-
labeling
rithms,onegenerative—theHiddenMarkovModel(HMM)—andonediscriminative—
theConditionalRandomField(CRF).Infollowingchapterswe’llintroducemodern
sequencelabelersbasedonRNNsandTransformers.8.1 • (MOSTLY)ENGLISHWORDCLASSES 161
8.1 (Mostly) English Word Classes
Untilnowwehavebeenusingpart-of-speechtermslikenounandverbratherfreely.
In this section we give more complete definitions. While word classes do have
semantictendencies—adjectives, forexample, oftendescribepropertiesandnouns
people—partsofspeecharedefinedinsteadbasedontheirgrammaticalrelationship
withneighboringwordsorthemorphologicalpropertiesabouttheiraffixes.
Tag Description Example
ADJ Adjective:nounmodifiersdescribingproperties red,young,awesome
ADV Adverb:verbmodifiersoftime,place,manner very,slowly,home,yesterday
NOUN wordsforpersons,places,things,etc. algorithm,cat,mango,beauty
VERB wordsforactionsandprocesses draw,provide,go
PROPN Propernoun:nameofaperson,organization,place,etc.. Regina,IBM,Colorado
INTJ Interjection:exclamation,greeting,yes/noresponse,etc. oh,um,yes,hello
ADP Adposition (Preposition/Postposition): marks a noun’s in,on,by,under
spacial,temporal,orotherrelation
AUX Auxiliary:helpingverbmarkingtense,aspect,mood,etc., can,may,should,are
CCONJ CoordinatingConjunction:joinstwophrases/clauses and,or,but
DET Determiner:marksnounphraseproperties a,an,the,this
NUM Numeral one,two,first,second
PART Particle: afunctionwordthatmustbeassociatedwithan- ’s,not,(infinitive)to
otherword
PRON Pronoun:ashorthandforreferringtoanentityorevent she,who,I,others
SCONJ Subordinating Conjunction: joins a main clause with a that,which
subordinateclausesuchasasententialcomplement
PUNCT Punctuation ˙, ,()
SYM Symbolslike$oremoji $,%
X Other asdf,qwfg
Figure8.1 The17partsofspeechintheUniversalDependenciestagset(deMarneffeetal.,2021). Features
canbeaddedtomakefiner-graineddistinctions(withpropertieslikenumber,case,definiteness,andsoon).
closedclass Parts of speech fall into two broad categories: closed class and open class.
openclass Closed classes are those with relatively fixed membership, such as prepositions—
newprepositionsarerarelycoined. Bycontrast,nounsandverbsareopenclasses—
newnounsandverbslikeiPhoneortofaxarecontinuallybeingcreatedorborrowed.
functionword Closedclasswordsaregenerallyfunctionwordslikeof,it,and,oryou,whichtend
tobeveryshort,occurfrequently,andoftenhavestructuringusesingrammar.
Fourmajoropenclassesoccurinthelanguagesoftheworld: nouns(including
propernouns),verbs,adjectives,andadverbs,aswellasthesmalleropenclassof
interjections. Englishhasallfive,althoughnoteverylanguagedoes.
noun Nounsarewordsforpeople,places,orthings,butincludeothersaswell. Com-
commonnoun monnounsincludeconcretetermslikecatandmango,abstractionslikealgorithm
andbeauty,andverb-liketermslikepacingasinHispacingtoandfrobecamequite
annoying. Nouns in English can occur with determiners (a goat, this bandwidth)
takepossessives(IBM’sannualrevenue),andmayoccurintheplural(goats,abaci).
countnoun Many languages, including English, divide common nouns into count nouns and
massnoun mass nouns. Count nouns can occur in the singular and plural (goat/goats, rela-
tionship/relationships) and can be counted (one goat, two goats). Mass nouns are
usedwhensomethingisconceptualizedasahomogeneousgroup.Sosnow,salt,and
propernoun communismarenotcounted(i.e.,*twosnowsor*twocommunisms).Propernouns,
likeRegina,Colorado,andIBM,arenamesofspecificpersonsorentities.
ssalCnepO
sdroWssalCdesolC
rehtO162 CHAPTER8 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES
verb Verbs refer to actions and processes, including main verbs like draw, provide,
andgo.Englishverbshaveinflections(non-third-person-singular(eat),third-person-
singular (eats), progressive (eating), past participle (eaten)). While many scholars
believethatallhumanlanguageshavethecategoriesofnounandverb,othershave
arguedthatsomelanguages,suchasRiauIndonesianandTongan,don’tevenmake
thisdistinction(Broschart1997;Evans2000;Gil2000).
adjective Adjectives often describe properties or qualities of nouns, like color (white,
black), age (old, young), and value (good, bad), but there are languages without
adjectives. In Korean, for example, the words corresponding to English adjectives
act as a subclass of verbs, so what is in English an adjective “beautiful” acts in
Koreanlikeaverbmeaning“tobebeautiful”.
adverb Adverbsareahodge-podge.Alltheitalicizedwordsinthisexampleareadverbs:
Actually,Iranhomeextremelyquicklyyesterday
Adverbs generally modify something (often verbs, hence the name “adverb”, but
locative also other adverbs and entire verb phrases). Directional adverbs or locative ad-
degree verbs(home,here,downhill)specifythedirectionorlocationofsomeaction;degree
adverbs(extremely,very,somewhat)specifytheextentofsomeaction,process,or
manner property;manneradverbs(slowly,slinkily,delicately)describethemannerofsome
temporal actionorprocess;andtemporaladverbsdescribethetimethatsomeactionorevent
tookplace(yesterday,Monday).
interjection Interjections(oh,hey,alas,uh,um)areasmalleropenclassthatalsoincludes
greetings(hello,goodbye)andquestionresponses(yes,no,uh-huh).
preposition Englishadpositionsoccurbeforenouns,hencearecalledprepositions.Theycan
indicatespatialortemporalrelations,whetherliteral(onit,beforethen,bythehouse)
ormetaphorical(ontime,withgusto,besideherself),andrelationslikemarkingthe
agentinHamletwaswrittenbyShakespeare.
particle Aparticleresemblesaprepositionoranadverbandisusedincombinationwith
a verb. Particles often have extended meanings that aren’t quite the same as the
prepositionstheyresemble, asintheparticleoverinsheturnedthepaperover. A
phrasalverb verb and a particle acting as a single unit is called a phrasal verb. The meaning
of phrasal verbs is often non-compositional—not predictable from the individual
meanings of the verb and the particle. Thus, turn down means ‘reject’, rule out
‘eliminate’,andgoon‘continue’.
determiner Determinerslikethisandthat(thischapter,thatpage)canmarkthestartofan
article Englishnounphrase. Articleslikea,an,andthe,areatypeofdeterminerthatmark
discourse properties of the noun and are quite frequent; the is the most common
wordinwrittenEnglish,withaandanrightbehind.
conjunction Conjunctions join two phrases, clauses, or sentences. Coordinating conjunc-
tionslikeand,or,andbutjointwoelementsofequalstatus. Subordinatingconjunc-
tions are used when one of the elements has some embedded status. For example,
thesubordinatingconjunctionthatin“Ithoughtthatyoumightlikesomemilk”links
themainclauseIthoughtwiththesubordinateclauseyoumightlikesomemilk.This
clause is called subordinate because this entire clause is the “content” of the main
verbthought. Subordinatingconjunctionslikethatwhichlinkaverbtoitsargument
complementizer inthiswayarealsocalledcomplementizers.
pronoun Pronouns act as a shorthand for referring to an entity or event. Personal pro-
nounsrefertopersonsorentities(you,she,I,it,me,etc.). Possessivepronounsare
formsofpersonalpronounsthatindicateeitheractualpossessionormoreoftenjust
anabstractrelationbetweenthepersonandsomeobject(my,your,his,her,its,one’s,
wh our,their). Wh-pronouns(what,who,whom,whoever)areusedincertainquestion8.2 • PART-OF-SPEECHTAGGING 163
forms,oractascomplementizers(Frida,whomarriedDiego...).
auxiliary Auxiliaryverbsmarksemanticfeaturesofamainverbsuchasitstense,whether
it is completed (aspect), whether it is negated (polarity), and whether an action is
necessary, possible, suggested, or desired (mood). English auxiliaries include the
copula copulaverbbe,thetwoverbsdoandhave,forms,aswellasmodalverbsusedto
modal markthemoodassociatedwiththeeventdepictedbythemainverb: canindicates
abilityorpossibility,maypermissionorpossibility,mustnecessity.
AnEnglish-specifictagset,the45-tagPennTreebanktagset(Marcusetal.,1993),
showninFig.8.2,hasbeenusedtolabelmanysyntacticallyannotatedcorporalike
thePennTreebankcorpora,soisworthknowingabout.
Tag Description Example Tag Description Example Tag Description Example
CC coord.conj. and,but,or NNP propernoun,sing. IBM TO “to” to
CD cardinalnumber one,two NNPS propernoun,plu. Carolinas UH interjection ah,oops
DT determiner a,the NNS noun,plural llamas VB verbbase eat
EX existential‘there’ there PDT predeterminer all,both VBD verbpasttense ate
FW foreignword meaculpa POS possessiveending ’s VBG verbgerund eating
IN preposition/ of,in,by PRP personalpronoun I,you,he VBN verb past partici- eaten
subordin-conj ple
JJ adjective yellow PRP$ possess.pronoun your,one’s VBP verbnon-3sg-pr eat
JJR comparativeadj bigger RB adverb quickly VBZ verb3sgpres eats
JJS superlativeadj wildest RBR comparativeadv faster WDT wh-determ. which,that
LS listitemmarker 1,2,One RBS superlatv.adv fastest WP wh-pronoun what,who
MD modal can,should RP particle up,off WP$ wh-possess. whose
NN singormassnoun llama SYM symbol +,%,& WRB wh-adverb how,where
Figure8.2 PennTreebankpart-of-speechtags.
Below we show some examples with each word tagged according to both the
UDandPenntagsets. NoticethatthePenntagsetdistinguishestenseandparticiples
onverbs,andhasaspecialtagfortheexistentialthereconstructioninEnglish. Note
thatsinceNewEnglandJournalofMedicineisapropernoun,bothtagsetsmarkits
componentnounsasNNP,includingjournalandmedicine, whichmightotherwise
belabeledascommonnouns(NOUN/NN).
(8.1) There/PRO/EXare/VERB/VBP70/NUM/CDchildren/NOUN/NNS
there/ADV/RB./PUNC/.
(8.2) Preliminary/ADJ/JJfindings/NOUN/NNSwere/AUX/VBDreported/VERB/VBN
in/ADP/INtoday/NOUN/NN’s/PART/POSNew/PROPN/NNP
England/PROPN/NNPJournal/PROPN/NNPof/ADP/INMedicine/PROPN/NNP
8.2 Part-of-Speech Tagging
part-of-speech Part-of-speechtaggingistheprocessofassigningapart-of-speechtoeachwordin
tagging
a text. The input is a sequence x ,x ,...,x of (tokenized) words and a tagset, and
1 2 n
theoutputisasequencey ,y ,...,y oftags,eachoutputy correspondingexactlyto
1 2 n i
oneinputx,asshownintheintuitioninFig.8.3.
i
ambiguous Taggingisadisambiguationtask;wordsareambiguous—havemorethanone
possible part-of-speech—and the goal is to find the correct tag for the situation.
For example, book can be a verb (book that flight) or a noun (hand me that book).
That can be a determiner (Does that flight serve dinner) or a complementizer (I164 CHAPTER8 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES
y y y y y
1 2 3 4 5
NOUN AUX VERB DET NOUN
Part of Speech Tagger
Janet will back the bill
x x x x x
1 2 3 4 5
Figure8.3 Thetaskofpart-of-speechtagging: mappingfrominputwordsx 1,x 2,...,xn to
outputPOStagsy 1,y 2,...,yn.
ambiguity thought that your flight was earlier). The goal of POS-tagging is to resolve these
resolution
ambiguities,choosingthepropertagforthecontext.
accuracy The accuracy of part-of-speech tagging algorithms (the percentage of test set
tagsthatmatchhumangoldlabels)isextremelyhigh. Onestudyfoundaccuracies
over97%across15languagesfromtheUniversalDependency(UD)treebank(Wu
andDredze,2019).AccuraciesonvariousEnglishtreebanksarealso97%(nomatter
the algorithm; HMMs, CRFs, BERT perform similarly). This 97% number is also
aboutthehumanperformanceonthistask,atleastforEnglish(Manning,2011).
Types: WSJ Brown
Unambiguous (1tag) 44,432 (86%) 45,799 (85%)
Ambiguous (2+tags) 7,025 (14%) 8,050 (15%)
Tokens:
Unambiguous (1tag) 577,421 (45%) 384,349 (33%)
Ambiguous (2+tags) 711,780 (55%) 786,646 (67%)
Figure8.4 TagambiguityintheBrownandWSJcorpora(Treebank-345-tagtagset).
We’ll introduce algorithms for the task in the next few sections, but first let’s
explore the task. Exactly how hard is it? Fig. 8.4 shows that most word types
(85-86%)areunambiguous(JanetisalwaysNNP,hesitantlyisalwaysRB).Butthe
ambiguous words, though accounting for only 14-15% of the vocabulary, are very
common, and 55-67% of word tokens in running text are ambiguous. Particularly
ambiguous common words include that, back, down, put and set; here are some
examplesofthe6differentpartsofspeechforthewordback:
earningsgrowthtookaback/JJseat
asmallbuildingintheback/NN
aclearmajorityofsenatorsback/VBPthebill
Davebegantoback/VBtowardthedoor
enablethecountrytobuyback/RPdebt
Iwastwenty-oneback/RBthen
Nonetheless, many words are easy to disambiguate, because their different tags
aren’t equally likely. For example, a can be a determiner or the letter a, but the
determinersenseismuchmorelikely.
Thisideasuggestsausefulbaseline: givenanambiguousword,choosethetag
whichismostfrequentinthetrainingcorpus. Thisisakeyconcept:
MostFrequentClassBaseline:Alwayscompareaclassifieragainstabaselineat
leastasgoodasthemostfrequentclassbaseline(assigningeachtokentotheclass
itoccurredinmostofteninthetrainingset).8.3 • NAMEDENTITIESANDNAMEDENTITYTAGGING 165
The most-frequent-tag baseline has an accuracy of about 92%1. The baseline
thusdiffersfromthestate-of-the-artandhumanceiling(97%)byonly5%.
8.3 Named Entities and Named Entity Tagging
Part of speech tagging can tell us that words like Janet, Stanford University, and
Colorado are all proper nouns; being a proper noun is a grammatical property of
these words. But viewed from a semantic perspective, these proper nouns refer to
differentkindsofentities: Janetisaperson,StanfordUniversityisanorganization,
andColoradoisalocation.
namedentity A named entity is, roughly speaking, anything that can be referred to with a
propername:aperson,alocation,anorganization.Thetaskofnamedentityrecog-
namedentity nition(NER)istofindspansoftextthatconstitutepropernamesandtagthetypeof
recognition
NER theentity. Fourentitytagsaremostcommon:PER(person),LOC(location),ORG
(organization), or GPE (geo-political entity). However, the term named entity is
commonly extended to include things that aren’t entities per se, including dates,
times,andotherkindsoftemporalexpressions,andevennumericalexpressionslike
prices. Here’sanexampleoftheoutputofanNERtagger:
Citing high fuel prices, [ ORG United Airlines] said [ TIME Friday] it
has increased fares by [
MONEY
$6] per round trip on flights to some
citiesalsoservedbylower-costcarriers. [
ORG
AmericanAirlines],a
unitof[ ORGAMRCorp.],immediatelymatchedthemove,spokesman
[
PER
TimWagner]said. [
ORG
United],aunitof[
ORG
UALCorp.],
said the increase took effect [ TIME Thursday] and applies to most
routeswhereitcompetesagainstdiscountcarriers,suchas[ LOCChicago]
to[ LOCDallas]and[ LOCDenver]to[ LOCSanFrancisco].
Thetextcontains13mentionsofnamedentitiesincluding5organizations, 4loca-
tions,2times,1person,and1mentionofmoney. Figure8.5showstypicalgeneric
namedentitytypes.Manyapplicationswillalsoneedtousespecificentitytypeslike
proteins,genes,commercialproducts,orworksofart.
Type Tag SampleCategories Examplesentences
People PER people,characters Turingisagiantofcomputerscience.
Organization ORG companies,sportsteams TheIPCCwarnedaboutthecyclone.
Location LOC regions,mountains,seas Mt. SanitasisinSunshineCanyon.
Geo-PoliticalEntity GPE countries,states PaloAltoisraisingthefeesforparking.
Figure8.5 Alistofgenericnamedentitytypeswiththekindsofentitiestheyreferto.
Namedentitytaggingisausefulfirststepinlotsofnaturallanguageprocessing
tasks.Insentimentanalysiswemightwanttoknowaconsumer’ssentimenttowarda
particularentity.Entitiesareausefulfirststageinquestionanswering,orforlinking
text to information in structured knowledge sources like Wikipedia. And named
entity tagging is also central to tasks involving building semantic representations,
likeextractingeventsandtherelationshipbetweenparticipants.
Unlike part-of-speech tagging, where there is no segmentation problem since
each word gets one tag, the task of named entity recognition is to find and label
spans of text, and is difficult partly because of the ambiguity of segmentation; we
1 InEnglish,ontheWSJcorpus,testedonsections22-24.166 CHAPTER8 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES
needtodecidewhat’sanentityandwhatisn’t,andwheretheboundariesare.Indeed,
mostwordsinatextwillnotbenamedentities. Anotherdifficultyiscausedbytype
ambiguity. ThementionJFKcanrefertoaperson,theairportinNewYork,orany
numberofschools,bridges,andstreetsaroundtheUnitedStates. Someexamplesof
thiskindofcross-typeconfusionaregiveninFigure8.6.
[ Washington]wasbornintoslaveryonthefarmofJamesBurroughs.
PER
[ Washington]wentup2gamesto1inthefour-gameseries.
ORG
Blairarrivedin[ Washington]forwhatmaywellbehislaststatevisit.
LOC
InJune,[ Washington]passedaprimaryseatbeltlaw.
GPE
Figure8.6 ExamplesoftypeambiguitiesintheuseofthenameWashington.
Thestandardapproachtosequencelabelingforaspan-recognitionproblemlike
NERisBIOtagging(RamshawandMarcus,1995). Thisisamethodthatallowsus
totreatNERlikeaword-by-wordsequencelabelingtask,viatagsthatcaptureboth
theboundaryandthenamedentitytype. Considerthefollowingsentence:
[ PER Jane Villanueva ] of [ ORG United] , a unit of [ ORG United Airlines
Holding],saidthefareappliestothe[ LOCChicago]route.
BIO Figure 8.7 shows the same excerpt represented with BIO tagging, as well as
variantscalledIOtaggingandBIOEStagging. InBIOtaggingwelabelanytoken
that begins a span of interest with the label B, tokens that occur inside a span are
taggedwithanI,andanytokensoutsideofanyspanofinterestarelabeledO. While
thereisonlyoneOtag,we’llhavedistinctBandItagsforeachnamedentityclass.
Thenumberoftagsisthus2n+1tags,wherenisthenumberofentitytypes. BIO
taggingcanrepresentexactlythesameinformationasthebracketednotation,buthas
theadvantagethatwecanrepresentthetaskinthesamesimplesequencemodeling
wayaspart-of-speechtagging: assigningasinglelabely toeachinputwordx:
i i
Words IOLabel BIOLabel BIOESLabel
Jane I-PER B-PER B-PER
Villanueva I-PER I-PER E-PER
of O O O
United I-ORG B-ORG B-ORG
Airlines I-ORG I-ORG I-ORG
Holding I-ORG I-ORG E-ORG
discussed O O O
the O O O
Chicago I-LOC B-LOC S-LOC
route O O O
. O O O
Figure8.7 NERasasequencemodel,showingIO,BIO,andBIOEStaggings.
We’vealso showntwo varianttagging schemes: IOtagging, which losessome
information by eliminating the B tag, and BIOES tagging, which adds an end tag
E for the end of a span, and a span tag S for a span consisting of only one word.
A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label each
token in a text with tags that indicate the presence (or absence) of particular kinds
ofnamedentities.8.4 • HMMPART-OF-SPEECHTAGGING 167
8.4 HMM Part-of-Speech Tagging
Inthissectionweintroduceourfirstsequencelabelingalgorithm,theHiddenMarkov
Model,andshowhowtoapplyittopart-of-speechtagging. Recallthatasequence
labeler is a model whose job is to assign a label to each unit in a sequence, thus
mapping a sequence of observations to a sequence of labels of the same length.
TheHMMisaclassicmodelthatintroducesmanyofthekeyconceptsofsequence
modelingthatwewillseeagaininmoremodernmodels.
AnHMMisaprobabilisticsequencemodel: givenasequenceofunits(words,
letters,morphemes,sentences,whatever),itcomputesaprobabilitydistributionover
possiblesequencesoflabelsandchoosesthebestlabelsequence.
8.4.1 MarkovChains
Markovchain TheHMMisbasedonaugmentingtheMarkovchain. AMarkovchainisamodel
that tells us something about the probabilities of sequences of random variables,
states,eachofwhichcantakeonvaluesfromsomeset. Thesesetscanbewords,or
tags, or symbols representing anything, for example the weather. A Markov chain
makesaverystrongassumptionthatifwewanttopredictthefutureinthesequence,
allthatmattersisthecurrentstate. Allthestatesbeforethecurrentstatehavenoim-
pactonthefutureexceptviathecurrentstate.It’sasiftopredicttomorrow’sweather
youcouldexaminetoday’sweatherbutyouweren’tallowedtolookatyesterday’s
weather.
.8
are .2
.1
.1
COLD2 .1 .4 .5
.5
.1
.3
HOT1 WARM3 uniformly .5 charming
.6 .3 .6 .1 .6 .2
(a) (b)
Figure8.8 A Markov chain for weather (a) and one for words (b), showing states and
transitions.Astartdistributionπ isrequired;settingπ=[0.1,0.7,0.2]for(a)wouldmeana
probability0.7ofstartinginstate2(cold),probability0.1ofstartinginstate1(hot),etc.
More formally, consider a sequence of state variables q ,q ,...,q. A Markov
1 2 i
Markov modelembodiestheMarkovassumptionontheprobabilitiesofthissequence:that
assumption
whenpredictingthefuture,thepastdoesn’tmatter,onlythepresent.
MarkovAssumption: P(q i=aq 1...q i 1)=P(q i=aq i 1) (8.3)
| − | −
Figure 8.8a shows a Markov chain for assigning a probability to a sequence of
weatherevents, forwhichthevocabularyconsistsof HOT, COLD, and WARM. The
statesarerepresentedasnodesinthegraph,andthetransitions,withtheirprobabil-
ities, asedges. Thetransitionsareprobabilities: thevaluesofarcsleavingagiven
statemustsumto1.Figure8.8bshowsaMarkovchainforassigningaprobabilityto
asequenceofwordsw ...w. ThisMarkovchainshouldbefamiliar;infact,itrepre-
1 t
sentsabigramlanguagemodel,witheachedgeexpressingtheprobabilityp(w w )!
i j
|
GiventhetwomodelsinFig.8.8,wecanassignaprobabilitytoanysequencefrom
ourvocabulary.168 CHAPTER8 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES
Formally,aMarkovchainisspecifiedby thefollowingcomponents:
Q=q 1q 2...q N asetofN states
A=a 11a 12...a N1...a NN a transition probability matrix A, each a ij represent-
ingtheprobabilityofmovingfromstateitostate j,s.t.
n a =1 i
j=1 ij ∀
π =π 1,π 2,...,π
N
aninitialprobabilitydistributionoverstates. π
i
isthe
(cid:80)
probability that the Markov chain will start in state i.
Somestates jmayhaveπ =0,meaningthattheycannot
j
beinitialstates. Also, n π =1
i=1 i
Beforeyougoon,usethesampleprobabilitiesin(cid:80)Fig.8.8a(withπ=[0.1,0.7,0.2])
tocomputetheprobabilityofeachofthefollowingsequences:
(8.4) hothothothot
(8.5) coldhotcoldhot
Whatdoesthedifferenceintheseprobabilitiestellyouaboutareal-worldweather
factencodedinFig.8.8a?
8.4.2 TheHiddenMarkovModel
A Markov chain is useful when we need to compute a probability for a sequence
of observable events. In many cases, however, the events we are interested in are
hidden hidden: we don’t observe them directly. For example we don’t normally observe
part-of-speechtagsinatext. Rather,weseewords,andmustinferthetagsfromthe
wordsequence. Wecallthetagshiddenbecausetheyarenotobserved.
hiddenMarkov AhiddenMarkovmodel(HMM)allowsustotalkaboutbothobservedevents
model
(likewordsthatweseeintheinput)andhiddenevents(likepart-of-speechtags)that
we think of as causal factors in our probabilistic model. An HMM is specified by
thefollowingcomponents:
Q=q 1q 2...q N asetofN states
A=a 11...a ij...a
NN
atransitionprobabilitymatrixA,eacha
ij
representingtheprobability
ofmovingfromstateitostate j,s.t. N a =1 i
j=1 ij ∀
O=o 1o 2...o
T
a sequence of T observations, each one drawn from a vocabularyV =
(cid:80)
v ,v ,...,v
1 2 V
B=b i(o t) asequenceofobservationlikelihoods,alsocalledemissionprobabili-
ties,eachexpressingtheprobabilityofanobservationo
t
beinggenerated
fromastateq
i
π =π 1,π 2,...,π
N
aninitialprobabilitydistributionoverstates. π
i
istheprobabilitythat
the Markov chain will start in state i. Some states j may have π =0,
j
meaningthattheycannotbeinitialstates. Also, n π =1
i=1 i
(cid:80)
A first-order hidden Markov model instantiates two simplifying assumptions.
First,aswithafirst-orderMarkovchain,theprobabilityofaparticularstatedepends
onlyonthepreviousstate:
MarkovAssumption: P(q i q 1,...,q i 1)=P(q i q i 1) (8.6)
| − | −
Second,theprobabilityofanoutputobservationo dependsonlyonthestatethat
i
producedtheobservationq andnotonanyotherstatesoranyotherobservations:
i
OutputIndependence: P(o i q 1,...q i,...,q T,o 1,...,o i,...,o T)=P(o i q i) (8.7)
| |8.4 • HMMPART-OF-SPEECHTAGGING 169
8.4.3 ThecomponentsofanHMMtagger
Let’sstartbylookingatthepiecesofanHMMtagger,andthenwe’llseehowtouse
ittotag. AnHMMhastwocomponents,theAandBprobabilities.
TheAmatrixcontainsthetagtransitionprobabilitiesP(t t )whichrepresent
i i 1
| −
theprobabilityofatagoccurringgiventheprevioustag. Forexample,modalverbs
likewillareverylikelytobefollowedbyaverbinthebaseform,aVB,likerace,so
weexpectthisprobabilitytobehigh.Wecomputethemaximumlikelihoodestimate
of this transition probability by counting, out of the times we see the first tag in a
labeledcorpus,howoftenthefirsttagisfollowedbythesecond:
C(t ,t)
i 1 i
P(t i t i 1)= − (8.8)
| − C(t i 1)
−
IntheWSJcorpus,forexample,MDoccurs13124timesofwhichitisfollowed
byVB10471,foranMLEestimateof
C(MD,VB) 10471
P(VBMD)= = =.80 (8.9)
| C(MD) 13124
Let’swalkthroughanexample,seeinghowtheseprobabilitiesareestimatedand
usedinasampletaggingtask,beforewereturntothealgorithmfordecoding.
InHMMtagging,theprobabilitiesareestimatedbycountingonataggedtraining
corpus. Forthisexamplewe’llusethetaggedWSJcorpus.
TheBemissionprobabilities,P(w t),representtheprobability,givenatag(say
i i
|
MD),thatitwillbeassociatedwithagivenword(saywill). TheMLEoftheemis-
sionprobabilityis
C(t,w)
i i
P(w t)= (8.10)
i i
| C(t)
i
Ofthe13124occurrencesofMDintheWSJcorpus,itisassociatedwithwill4046
times:
C(MD,will) 4046
P(will MD)= = =.31 (8.11)
| C(MD) 13124
WesawthiskindofBayesianmodelinginChapter4;recallthatthislikelihood
termisnotasking“whichisthemostlikelytagforthewordwill?” Thatwouldbe
theposteriorP(MDwill).Instead,P(willMD)answerstheslightlycounterintuitive
| |
question“IfweweregoingtogenerateaMD,howlikelyisitthatthismodalwould
bewill?”
The A transition probabilities, and B observation likelihoods of the HMM are
illustrated in Fig. 8.9 for three states in an HMM part-of-speech tagger; the full
taggerwouldhaveonestateforeachtag.
8.4.4 HMMtaggingasdecoding
Foranymodel, suchasanHMM,thatcontainshiddenvariables, thetaskofdeter-
miningthehiddenvariablessequencecorrespondingtothesequenceofobservations
decoding iscalleddecoding. Moreformally,
Decoding: GivenasinputanHMMλ =(A,B)andasequenceofob-
servationsO=o ,o ,...,o ,findthemostprobablesequenceofstates
1 2 T
Q=q q q ...q .
1 2 3 T170 CHAPTER8 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES
B2 a22
P("aardvark" | MD)
...
P(“will” | MD)
...
P("the" | MD) MD
... 2 B3
P(“back” | MD)
. P..
("zebra" | MD)
a12 a32 P ...("aardvark" | NN)
a11 a21
a23
a33 P
.
P..( (“ "w thi ell”
"
|
|
N NN N)
)
B1 a13 ...
P(“back” | NN)
P . P..( (" “waa ilr l”d |v a Vr Bk )" | VB) VB 1 a31 NN 3 . P.. ("zebra" | NN)
...
P("the" | VB)
...
P(“back” | VB)
...
P("zebra" | VB)
Figure8.9 An illustration of the two parts of an HMM representation: the A transition
probabilitiesusedtocomputethepriorprobability,andtheBobservationlikelihoodsthatare
associatedwitheachstate,onelikelihoodforeachpossibleobservationword.
For part-of-speech tagging, the goal of HMM decoding is to choose the tag
sequence t ...t that is most probable given the observation sequence of n words
1 n
w ...w :
1 n
tˆ =argmaxP(t ...t w ...w ) (8.12)
1:n 1 n 1 n
|
t1...tn
Thewaywe’lldothisintheHMMistouseBayes’ruletoinsteadcompute:
P(w ...w t ...t )P(t ...t )
tˆ =argmax 1 n |1 n 1 n (8.13)
1:n
P(w ...w )
t1...tn 1 n
Furthermore,wesimplifyEq.8.13bydroppingthedenominatorP(wn):
1
tˆ =argmaxP(w ...w t ...t )P(t ...t ) (8.14)
1:n 1 n 1 n 1 n
|
t1...tn
HMM taggers make two further simplifying assumptions. The first is that the
probabilityofawordappearingdependsonlyonitsowntagandisindependentof
neighboringwordsandtags:
n
P(w 1...w n t 1...t n) P(w i t i) (8.15)
| ≈ |
i=1
(cid:89)
The second assumption, the bigram assumption, is that the probability of a tag is
dependentonlyontheprevioustag,ratherthantheentiretagsequence;
n
P(t 1...t n) P(t i t i 1) (8.16)
≈ | −
i=1
(cid:89)
Plugging the simplifying assumptions from Eq. 8.15 and Eq. 8.16 into Eq. 8.14
resultsinthefollowingequationforthemostprobabletagsequencefromabigram
tagger:
emissiontransition
n
tˆ 1:n=argmaxP(t 1...t n w 1...w n) argmax P(w i t i) P(t i t i 1) (8.17)
t1...tn | ≈ t1...tn
(cid:89)i=1 (cid:122)
(cid:125)(cid:124)|
(cid:123) (cid:122)
(cid:125)| (cid:124)−
(cid:123)
The two parts of Eq. 8.17 correspond neatly to the B emission probability and A
transitionprobabilitythatwejustdefinedabove!8.4 • HMMPART-OF-SPEECHTAGGING 171
8.4.5 TheViterbiAlgorithm
Viterbi The decoding algorithm for HMMs is the Viterbi algorithm shown in Fig. 8.10.
algorithm
Asaninstanceofdynamicprogramming,Viterbiresemblesthedynamicprogram-
mingminimumeditdistancealgorithmofChapter2.
functionVITERBI(observationsoflenT,state-graphoflenN)returnsbest-path,path-prob
createapathprobabilitymatrixviterbi[N,T]
foreachstatesfrom1toNdo ;initializationstep
viterbi[s,1] π s bs(o 1)
← ∗
backpointer[s,1] 0
←
foreachtimesteptfrom2toTdo ;recursionstep
foreachstatesfrom1toNdo
N
viterbi[s,t] ←m s(cid:48)a =x
1
viterbi[s(cid:48),t −1]
∗
a s(cid:48),s
∗
bs(ot)
N
backpointer[s,t] ←argmax viterbi[s(cid:48),t −1]
∗
a s(cid:48),s
∗
bs(ot)
s(cid:48)=1
N
bestpathprob max viterbi[s,T] ;terminationstep
← s=1
N
bestpathpointer argmax viterbi[s,T] ;terminationstep
←
s=1
bestpath thepathstartingatstatebestpathpointer,thatfollowsbackpointer[]tostatesbackintime
←
returnbestpath,bestpathprob
Figure8.10 Viterbialgorithmforfindingtheoptimalsequenceoftags. Givenanobservationsequenceand
anHMMλ =(A,B),thealgorithmreturnsthestatepaththroughtheHMMthatassignsmaximumlikelihood
totheobservationsequence.
TheViterbialgorithmfirstsetsupaprobabilitymatrixorlattice,withonecol-
umnforeachobservationo andonerowforeachstateinthestategraph. Eachcol-
t
umnthushasacellforeachstateq inthesinglecombinedautomaton. Figure8.11
i
showsanintuitionofthislatticeforthesentenceJanetwillbackthebill.
Eachcellofthelattice,v(j),representstheprobabilitythattheHMMisinstate
t
j after seeing the first t observations and passing through the most probable state
sequenceq ,...,q , giventheHMMλ. Thevalueofeachcellv(j)iscomputed
1 t 1 t
−
byrecursivelytakingthemostprobablepaththatcouldleadustothiscell.Formally,
eachcellexpressestheprobability
v(j)= max P(q ...q ,o ,o ...o,q = j λ) (8.18)
t 1 t 1 1 2 t t
q1,...,qt 1 − |
−
We represent the most probable path by taking the maximum over all possible
previous state sequences max . Like other dynamic programming algorithms,
q1,...,qt 1
Viterbifillseachcellrecursively−. Giventhatwehadalreadycomputedtheprobabil-
ityofbeingineverystateattimet 1,wecomputetheViterbiprobabilitybytaking
−
themostprobableoftheextensionsofthepathsthatleadtothecurrentcell. Fora
givenstateq attimet,thevaluev(j)iscomputedas
j t
N
v t(j) = maxv t 1(i)a ijb j(o t) (8.19)
i=1 −
ThethreefactorsthataremultipliedinEq.8.19forextendingthepreviouspathsto
computetheViterbiprobabilityattimet are172 CHAPTER8 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES
DT DT DT DT DT
RB RB RB RB RB
NN NN NN NN NN
JJ JJ JJ JJ JJ
VB VB VB VB VB
MD MD MD MD MD
NNP NNP NNP NNP NNP
Janet will back the bill
Figure8.11 AsketchofthelatticeforJanetwillbackthebill,showingthepossibletags(qi)
foreachwordandhighlightingthepathcorrespondingtothecorrecttagsequencethroughthe
hiddenstates.States(partsofspeech)whichhaveazeroprobabilityofgeneratingaparticular
wordaccordingtotheBmatrix(suchastheprobabilitythatadeterminerDTwillberealized
asJanet)aregreyedout.
v
t
1(i) thepreviousViterbipathprobabilityfromtheprevioustimestep
−
a
ij
thetransitionprobabilityfrompreviousstateq itocurrentstateq
j
b j(o t) thestateobservationlikelihoodoftheobservationsymbolo
t
given
thecurrentstate j
8.4.6 Workingthroughanexample
Let’s tag the sentence Janet will back the bill; the goal is the correct series of tags
(seealsoFig.8.11):
(8.20) Janet/NNPwill/MDback/VBthe/DTbill/NN
NNP MD VB JJ NN RB DT
<s> 0.2767 0.0006 0.0031 0.0453 0.0449 0.0510 0.2026
NNP 0.3777 0.0110 0.0009 0.0084 0.0584 0.0090 0.0025
MD 0.0008 0.0002 0.7968 0.0005 0.0008 0.1698 0.0041
VB 0.0322 0.0005 0.0050 0.0837 0.0615 0.0514 0.2231
JJ 0.0366 0.0004 0.0001 0.0733 0.4509 0.0036 0.0036
NN 0.0096 0.0176 0.0014 0.0086 0.1216 0.0177 0.0068
RB 0.0068 0.0102 0.1011 0.1012 0.0120 0.0728 0.0479
DT 0.1147 0.0021 0.0002 0.2157 0.4744 0.0102 0.0017
Figure8.12 TheAtransitionprobabilitiesP(tit i 1)computedfromtheWSJcorpuswith-
outsmoothing.Rowsarelabeledwiththeconditi| on− ingevent;thusP(VBMD)is0.7968.
|
LettheHMMbedefinedbythetwotablesinFig.8.12andFig.8.13.Figure8.12
liststhea probabilitiesfortransitioningbetweenthehiddenstates(part-of-speech
ij
tags). Figure8.13expressestheb(o)probabilities,theobservationlikelihoodsof
i t
wordsgiventags. Thistableis(slightlysimplified)fromcountsintheWSJcorpus.
SothewordJanetonlyappearsasanNNP,backhas4possiblepartsofspeech,and8.4 • HMMPART-OF-SPEECHTAGGING 173
Janet will back the bill
NNP 0.000032 0 0 0.000048 0
MD 0 0.308431 0 0 0
VB 0 0.000028 0.000672 0 0.000028
JJ 0 0 0.000340 0 0
NN 0 0.000200 0.000223 0 0.002337
RB 0 0 0.010446 0 0
DT 0 0 0 0.506099 0
Figure8.13 ObservationlikelihoodsBcomputedfromtheWSJcorpuswithoutsmoothing,
simplifiedslightly.
the word the can appear as a determiner or as an NNP (in titles like “Somewhere
OvertheRainbow”allwordsaretaggedasNNP).
v1(7) v2(7)
q 7 DT
q 6 RB
v1(6) v2(6) mav x3 *( 6 .0)=
104
v1(5) v2(5)= *
P(RB|NN)
v3(5)=
q 5 NN max * .0002 * P(NN|NN) max * .
= .0000000001 000223
q 4 JJ P(JJ | .s 0t 4a 5rt) = 0v 41 5(4 *0)= = 0. v2(4) maxv 3 *( .4 0) 0= 034
qq 23 MVB
D
PP (M( =V =
D
.B 0|. s| 0 0s t0 0ata 3 6rtr 1 )t ) .. 00v 00 v1 = 03 1(
6
13 (0
2
) xx=
) =
00
= *
*
P
*
= PP
(
=
M0( (M
M
0
DD D| |J MJ |V)
DB ))
m m=a
a
2x v
v
x .
7* 22 2
*
7(( . . 203 2.5 3)0 e) e 0= 0
=
--
8
80 1 2 3 =8 maxv 3 *( .3 0) 0= 067
0 = 0
q 1 NNP P(NN =P .2|s 8tart) .2 =8 v .* 01 . 0( 01 00) 0 0= 00 932 * .P( 00M 0D| 00N 9N .*. 9P 0) e-1 8 = v2(1)
backtrace
π start backtrace start start start start
Janet will back the bill
t
o1 o2 o3 o4 o5
Figure8.14 ThefirstfewentriesintheindividualstatecolumnsfortheViterbialgorithm. Eachcellkeeps
theprobabilityofthebestpathsofarandapointertothepreviouscellalongthatpath. Wehaveonlyfilledout
columns1and2;toavoidcluttermostcellswithvalue0areleftempty. Therestisleftasanexerciseforthe
reader. Afterthecellsarefilledin,backtracingfromtheendstate,weshouldbeabletoreconstructthecorrect
statesequenceNNPMDVBDTNN.
Figure 8.14 shows a fleshed-out version of the sketch we saw in Fig. 8.11, the
Viterbilatticeforcomputingthebesthiddenstatesequencefortheobservationse-
quenceJanetwillbackthebill.
ThereareN =5statecolumns. Webeginincolumn1(forthewordJanet)by
setting the Viterbi value in each cell to the product of the π transition probability
(thestartprobabilityforthatstatei,whichwegetfromthe<s>entryofFig.8.12),174 CHAPTER8 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES
andtheobservationlikelihoodofthewordJanetgiventhetagforthatcell. Mostof
the cells in the column are zero since the word Janet cannot be any of those tags.
ThereadershouldfindthisinFig.8.14.
Next,eachcellinthewillcolumngetsupdated. Foreachstate,wecomputethe
value viterbi[s,t] by taking the maximum over the extensions of all the paths from
the previous column that lead to the current cell according to Eq. 8.19. We have
shown the values for the MD, VB, and NN cells. Each cell gets the max of the 7
valuesfromthepreviouscolumn,multipliedbytheappropriatetransitionprobabil-
ity;asithappensinthiscase,mostofthemarezerofromthepreviouscolumn. The
remainingvalueismultipliedbytherelevantobservationprobability,andthe(triv-
ial)maxistaken. Inthiscasethefinalvalue,2.772e-8,comesfromtheNNPstateat
thepreviouscolumn. ThereadershouldfillintherestofthelatticeinFig.8.14and
backtracetoseewhetherornottheViterbialgorithmreturnsthegoldstatesequence
NNPMDVBDTNN.
8.5 Conditional Random Fields (CRFs)
While the HMM is a useful and powerful model, it turns out that HMMs need a
number of augmentations to achieve high accuracy. For example, in POS tagging
unknown as inother tasks, weoften run into unknown words: proper names andacronyms
words
are created very often, and even new common nouns and verbs enter the language
at a surprising rate. It would be great to have ways to add arbitrary features to
helpwiththis,perhapsbasedoncapitalizationormorphology(wordsstartingwith
capital letters are likely to be proper nouns, words ending with -ed tend to be past
tense(VBDorVBN),etc.) Orknowingthepreviousorfollowingwordsmightbea
usefulfeature(ifthepreviouswordisthe,thecurrenttagisunlikelytobeaverb).
Although we could try to hack the HMM to find ways to incorporate some of
these,ingeneralit’shardforgenerativemodelslikeHMMstoaddarbitraryfeatures
directlyintothemodelinacleanway. We’vealreadyseenamodelforcombining
arbitraryfeaturesinaprincipledway: log-linearmodelslikethelogisticregression
modelofChapter5!Butlogisticregressionisn’tasequencemodel;itassignsaclass
toasingleobservation.
Luckily, there is a discriminative sequence model based on log-linear models:
CRF the conditional random field (CRF). We’ll describe here the linear chain CRF,
theversionoftheCRFmostcommonlyusedforlanguageprocessing, andtheone
whoseconditioningcloselymatchestheHMM.
AssumingwehaveasequenceofinputwordsX =x ...x andwanttocompute
1 n
asequenceofoutputtagsY =y ...y . InanHMMtocomputethebesttagsequence
1 n
thatmaximizesP(Y X)werelyonBayes’ruleandthelikelihoodP(X Y):
| |
Yˆ = argmaxp(Y X)
|
Y
= argmaxp(X Y)p(Y)
|
Y
= argmax p(x i y i) p(y i y i 1) (8.21)
Y | | −
i i
(cid:89) (cid:89)
InaCRF,bycontrast, wecomputetheposterior p(Y X)directly, trainingtheCRF
|8.5 • CONDITIONALRANDOMFIELDS(CRFS) 175
todiscriminateamongthepossibletagsequences:
Yˆ = argmaxP(Y X) (8.22)
Y Y |
∈
However,theCRFdoesnotcomputeaprobabilityforeachtagateachtimestep. In-
stead,ateachtimesteptheCRFcomputeslog-linearfunctionsoverasetofrelevant
features,andtheselocalfeaturesareaggregatedandnormalizedtoproduceaglobal
probabilityforthewholesequence.
Let’s introduce the CRF more formally, again using X andY as the input and
outputsequences.ACRFisalog-linearmodelthatassignsaprobabilitytoanentire
output(tag)sequenceY,outofallpossiblesequencesY,giventheentireinput(word)
sequence X. We can think of a CRF as like a giant version of what multinomial
logistic regression does for a single token. Recall that the feature function f in
regular multinomial logistic regression can be viewed as a function of a tuple: a
token x and a label y (page 89). In a CRF, the function F maps an entire input
sequence X and an entire output sequenceY to a feature vector. Let’s assume we
haveK features,withaweightw foreachfeatureF:
k k
K
exp w F(X,Y)
k k
(cid:32) (cid:33)
p(Y X) =
(cid:88)k=1
(8.23)
| K
exp w kF k(X,Y(cid:48))
Y(cid:88)(cid:48)∈Y (cid:32) (cid:88)k=1 (cid:33)
It’scommontoalsodescribethesameequationbypullingoutthedenominatorinto
afunctionZ(X):
K
1
p(Y X) = exp w kF k(X,Y) (8.24)
| Z(X) (cid:32) (cid:33)
k=1
(cid:88)
K
Z(X) = exp w kF k(X,Y(cid:48)) (8.25)
Y(cid:88)(cid:48)∈Y (cid:32) (cid:88)k=1 (cid:33)
We’llcalltheseK functionsF k(X,Y)globalfeatures,sinceeachoneisaproperty
oftheentireinputsequenceX andoutputsequenceY. Wecomputethembydecom-
posingintoasumoflocalfeaturesforeachpositioniinY:
n
F k(X,Y)= f k(y i 1,y i,X,i) (8.26)
−
i=1
(cid:88)
Eachoftheselocalfeatures f inalinear-chainCRFisallowedtomakeuseofthe
k
currentoutputtokeny,thepreviousoutputtokeny ,theentireinputstringX (or
i i 1
−
any subpart of it), and the current position i. This constraint to only depend on
the current and previous output tokens y i and y i 1 are what characterizes a linear
linearchain chainCRF.Aswewillsee,thislimitationmake− sitpossibletouseversionsofthe
CRF
efficientViterbiandForward-BackwardsalgorithmsfromtheHMM.AgeneralCRF,
bycontrast,allowsafeaturetomakeuseofanyoutputtoken,andarethusnecessary
fortasksinwhichthedecisiondependondistantoutputtokens, likey . General
i 4
−
CRFs require more complex inference, and are less commonly used for language
processing.176 CHAPTER8 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES
8.5.1 FeaturesinaCRFPOSTagger
Let’slookatsomeofthesefeaturesindetail,sincethereasontouseadiscriminative
sequencemodelisthatit’seasiertoincorporatealotoffeatures.2
Again, in a linear-chain CRF, each local feature f at position i can depend on
k
anyinformationfrom: (y ,y,X,i). Sosomelegalfeaturesrepresentingcommon
i 1 i
−
situationsmightbethefollowing:
1 x =the, y =DET
i i
1{
y =PROPN,x
=}
Street,y =NUM
i i+1 i 1
1{
y =VERB,y =AUX
− }
i i 1
{ − }
Forsimplicity, we’llassumeallCRFfeaturestakeonthevalue1or0. Above, we
explicitlyusethenotation1 x tomean“1ifxistrue,and0otherwise”. Fromnow
on,we’llleaveoffthe1wh{ en}
wedefinefeatures,butyoucanassumeeachfeature
hasitthereimplicitly.
Althoughtheideaofwhatfeaturestouseisdonebythesystemdesignerbyhand,
feature thespecificfeaturesareautomaticallypopulatedbyusingfeaturetemplatesaswe
templates
brieflymentionedinChapter5. Herearesometemplatesthatonlyuseinformation
from(y ,y,X,i):
i 1 i
−
y,x , y,y , y,x ,x
i i i i 1 i i 1 i+2
(cid:104) (cid:105) (cid:104) − (cid:105) (cid:104) − (cid:105)
These templates automatically populate the set of features from every instance in
thetrainingandtestset. ThusforourexampleJanet/NNPwill/MDback/VBthe/DT
bill/NN, when x is the word back, the following features would be generated and
i
havethevalue1(we’veassignedthemarbitraryfeaturenumbers):
f : y =VBandx =back
3743 i i
f : y =VBandy =MD
156 i i 1
−
f : y =VBandx =willandx =bill
99732 i i 1 i+2
−
It’s also important to have features that help with unknown words. One of the
wordshape most important is word shape features, which represent the abstract letter pattern
of the word by mapping lower-case letters to ‘x’, upper-case to ‘X’, numbers to
’d’, andretainingpunctuation. ThusforexampleI.M.F.wouldmaptoX.X.X.and
DC10-30wouldmaptoXXdd-dd. Asecondclassofshorterwordshapefeaturesis
alsoused. Inthesefeaturesconsecutivecharactertypesareremoved,sowordsinall
caps map to X, words with initial-caps map to Xx, DC10-30 would be mapped to
Xd-dbutI.M.FwouldstillmaptoX.X.X.Prefixandsuffixfeaturesarealsouseful.
Insummary,herearesomesamplefeaturetemplatesthathelpwithunknownwords:
x containsaparticularprefix(perhapsfromallprefixesoflength 2)
i
≤
x containsaparticularsuffix(perhapsfromallsuffixesoflength 2)
i
≤
x’swordshape
i
x’sshortwordshape
i
Forexamplethewordwell-dressedmightgeneratethefollowingnon-zeroval-
uedfeaturevalues:
2 BecauseinHMMsallcomputationisbasedonthetwoprobabilitiesP(tagtag)andP(wordtag),if
| |
wewanttoincludesomesourceofknowledgeintothetaggingprocess,wemustfindawaytoencode
theknowledgeintooneofthesetwoprobabilities. Eachtimeweaddafeaturewehavetodoalotof
complicatedconditioningwhichgetsharderandharderaswehavemoreandmoresuchfeatures.8.5 • CONDITIONALRANDOMFIELDS(CRFS) 177
prefix(x)=w
i
prefix(x)=we
i
suffix(x)=ed
i
suffix(x)=d
i
word-shape(x)=xxxx-xxxxxxx
i
short-word-shape(x)=x-x
i
The known-word templates are computed for every word seen in the training
set; the unknown word features can also be computed for all words in training, or
onlyontrainingwordswhosefrequencyisbelowsomethreshold. Theresultofthe
known-word templates and word-signature features is a very large set of features.
Generallyafeaturecutoffisusedinwhichfeaturesarethrownoutiftheyhavecount
<5inthetrainingset.
RememberthatinaCRFwedon’tlearnweightsforeachoftheselocalfeatures
f . Instead,wefirstsumthevaluesofeachlocalfeature(forexamplefeature f )
k 3743
overtheentiresentence,tocreateeachglobalfeature(forexampleF ). Itisthose
3743
globalfeaturesthatwillthenbemultipliedbyweightw . Thusfortrainingand
3743
inferencethereisalwaysafixedsetofK featureswithK weights,eventhoughthe
lengthofeachsentenceisdifferent.
8.5.2 FeaturesforCRFNamedEntityRecognizers
A CRF for NER makes use of very similar features to a POS tagger, as shown in
Figure8.15.
identityofw,identityofneighboringwords
i
embeddingsforw,embeddingsforneighboringwords
i
partofspeechofw,partofspeechofneighboringwords
i
presenceofw iinagazetteer
w containsaparticularprefix(fromallprefixesoflength 4)
i
≤
w containsaparticularsuffix(fromallsuffixesoflength 4)
i
≤
wordshapeofw,wordshapeofneighboringwords
i
shortwordshapeofw,shortwordshapeofneighboringwords
i
gazetteerfeatures
Figure8.15 Typicalfeaturesforafeature-basedNERsystem.
gazetteer One feature that isespecially useful for locations is a gazetteer, a list ofplace
names, often providingmillions ofentriesfor locationswith detailedgeographical
andpoliticalinformation.3 Thiscanbeimplementedasabinaryfeatureindicatinga
phraseappearsinthelist. Otherrelatedresourceslikename-lists,forexamplefrom
theUnitedStatesCensusBureau4,canbeused,ascanotherentitydictionarieslike
listsofcorporationsorproducts,althoughtheymaynotbeashelpfulasagazetteer
(Mikheevetal.,1999).
The sample named entity token L’Occitane would generate the following non-
zerovaluedfeaturevalues(assumingthatL’Occitaneisneitherinthegazetteernor
thecensus).
3 www.geonames.org
4 www.census.gov178 CHAPTER8 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES
prefix(x)=L suffix(x)=tane
i i
prefix(x)=L’ suffix(x)=ane
i i
prefix(x)=L’O suffix(x)=ne
i i
prefix(x)=L’Oc suffix(x)=e
i i
word-shape(x)=X’Xxxxxxxx short-word-shape(x)=X’Xx
i i
Figure 8.16 illustrates the result of adding part-of-speech tags and some shape
informationtoourearlierexample.
Words POS Shortshape Gazetteer BIOLabel
Jane NNP Xx 0 B-PER
Villanueva NNP Xx 1 I-PER
of IN x 0 O
United NNP Xx 0 B-ORG
Airlines NNP Xx 0 I-ORG
Holding NNP Xx 0 I-ORG
discussed VBD x 0 O
the DT x 0 O
Chicago NNP Xx 1 B-LOC
route NN x 0 O
. . . 0 O
Figure8.16 Some NER features for a sample sentence, assuming that Chicago and Vil-
lanuevaarelistedaslocationsinagazetteer.Weassumefeaturesonlytakeonthevalues0or
1,sothefirstPOSfeature,forexample,wouldberepresentedas1 POS=NNP .
{ }
8.5.3 InferenceandTrainingforCRFs
HowdowefindthebesttagsequenceYˆ foragiveninputX? WestartwithEq.8.22:
Yˆ = argmaxP(Y X)
Y Y |
∈
K
1
= argmax exp w kF k(X,Y) (8.27)
Y Y Z(X) (cid:32) (cid:33)
∈ (cid:88)k=1
K n
= argmaxexp w k f k(y i 1,y i,X,i) (8.28)
Y ∈Y (cid:32) (cid:88)k=1 (cid:88)i=1 − (cid:33)
K n
= argmax w k f k(y i 1,y i,X,i) (8.29)
Y ∈Y (cid:88)k=1 (cid:88)i=1 −
n K
= argmax w kf k(y i 1,y i,X,i) (8.30)
Y ∈Y (cid:88)i=1 (cid:88)k=1 −
WecanignoretheexpfunctionandthedenominatorZ(X),aswedoabove,because
exp doesn’t change the argmax, and the denominator Z(X) is constant for a given
observationsequenceX.
Howshouldwedecodetofindthisoptimaltagsequenceyˆ? JustaswithHMMs,
we’llturntotheViterbialgorithm,whichworksbecause,liketheHMM,thelinear-
chainCRFdependsateachtimestepononlyonepreviousoutputtokeny .
i 1
−
Concretely,thisinvolvesfillinganN T arraywiththeappropriatevalues,main-
×
tainingbackpointersasweproceed. AswithHMMViterbi,whenthetableisfilled,
we simply follow pointers back from the maximum value in the final column to
retrievethedesiredsetoflabels.8.6 • EVALUATIONOFNAMEDENTITYRECOGNITION 179
TherequisitechangesfromHMMViterbihavetodoonlywithhowwefilleach
cell. RecallfromEq.8.19thattherecursivestepoftheViterbiequationcomputes
theViterbivalueoftimet forstate jas
N
v t(j) = max v t 1(i)a ijb j(o t); 1 j N,1<t T (8.31)
i=1 − ≤ ≤ ≤
whichistheHMMimplementationof
N
v t(j) = max v t 1(i)P(s j s i)P(o t s j) 1 j N,1<t T (8.32)
i=1 − | | ≤ ≤ ≤
TheCRFrequiresonlyaslightchangetothislatterformula, replacingtheaandb
priorandlikelihoodprobabilitieswiththeCRFfeatures:
K
N
v t(j) = max v t 1(i)+ w kf k(y t 1,y t,X,t) 1 j N,1<t T (8.33)
i=1 − − ≤ ≤ ≤
k=1
(cid:88)
Learning in CRFs relies on the same supervised learning algorithms we presented
forlogisticregression. Givenasequenceofobservations,featurefunctions,andcor-
respondingoutputs,weusestochasticgradientdescenttotraintheweightstomaxi-
mizethelog-likelihoodofthetrainingcorpus.Thelocalnatureoflinear-chainCRFs
means that the forward-backward algorithm introduced for HMMs in Appendix A
canbeextendedtoaCRFversionthatwillefficientlycomputethenecessaryderiva-
tives. Aswithlogisticregression,L1orL2regularizationisimportant.
8.6 Evaluation of Named Entity Recognition
Part-of-speech taggers are evaluated by the standard metric of accuracy. Named
entityrecognizersareevaluatedbyrecall, precision, andF1 measure. Recallthat
recallistheratioofthenumberofcorrectlylabeledresponsestothetotalthatshould
havebeenlabeled;precisionistheratioofthenumberofcorrectlylabeledresponses
tothetotallabeled;andF-measureistheharmonicmeanofthetwo.
ToknowifthedifferencebetweentheF scoresoftwoNERsystemsisasignif-
1
icantdifference, weusethepairedbootstraptest, orthesimilarrandomizationtest
(Section4.9).
Fornamedentitytagging,theentityratherthanthewordistheunitofresponse.
Thus inthe examplein Fig. 8.16, the twoentities Jane Villanueva and United Air-
linesHoldingandthenon-entitydiscussedwouldeachcountasasingleresponse.
Thefactthatnamedentitytagginghasasegmentationcomponentwhichisnot
presentintasksliketextcategorizationorpart-of-speechtaggingcausessomeprob-
lems with evaluation. For example, a system that labeled Jane but not Jane Vil-
lanuevaasapersonwouldcausetwoerrors,afalsepositiveforOandafalsenega-
tiveforI-PER.Inaddition,usingentitiesastheunitofresponsebutwordsastheunit
oftrainingmeansthatthereisamismatchbetweenthetrainingandtestconditions.
8.7 Further Details
In this section we summarize a few remaining details of the data and models for
part-of-speechtaggingandNER,beginningwithdata.Sincethealgorithmswehave180 CHAPTER8 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES
presentedaresupervised,havinglabeleddataisessentialfortrainingandtesting. A
widevarietyofdatasetsexistforpart-of-speechtaggingand/orNER.TheUniversal
Dependencies (UD) dataset (de Marneffe et al., 2021) has POS tagged corpora in
overahundredlanguages,asdothePennTreebanksinEnglish,Chinese,andArabic.
OntoNotes has corpora labeled for named entities in English, Chinese, and Arabic
(Hovy et al., 2006). Named entity tagged corpora are also available in particular
domains,suchasforbiomedical(Badaetal.,2012)andliterarytext(Bammanetal.,
2019).
8.7.1 Rule-basedMethods
Whilemachinelearned(neuralorCRF)sequencemodelsarethenorminacademic
research, commercial approaches to NER are often based on pragmatic combina-
tions of lists and rules, with some smaller amount of supervised machine learning
(Chiticariu et al., 2013). For example in the IBM System T architecture, a user
specifies declarative constraints for tagging tasks in a formal query language that
includesregularexpressions,dictionaries,semanticconstraints,andotheroperators,
whichthesystemcompilesintoanefficientextractor(Chiticariuetal.,2018).
Onecommonapproachistomakerepeatedrule-basedpassesoveratext,starting
withruleswithveryhighprecisionbutlowrecall,and,insubsequentstages,using
machine learning methods that take the output of the first pass into account (an
approachfirstworkedoutforcoreference(Leeetal.,2017a)):
1. First,usehigh-precisionrulestotagunambiguousentitymentions.
2. Then,searchforsubstringmatchesofthepreviouslydetectednames.
3. Useapplication-specificnameliststofindlikelydomain-specificmentions.
4. Finally,applysupervisedsequencelabelingtechniquesthatusetagsfrompre-
viousstagesasadditionalfeatures.
Rule-based methods were also the earliest methods for part-of-speech tagging.
Rule-based taggers like the English Constraint Grammar system (Karlsson et al.
1995,Voutilainen1999)useatwo-stageformalisminventedinthe1950sand1960s:
(1)amorphologicalanalyzerwithtensofthousandsofwordstementriesreturnsall
partsofspeechforaword,then(2)alargesetofthousandsofconstraintsareapplied
totheinputsentencetoruleoutpartsofspeechinconsistentwiththecontext.
8.7.2 POSTaggingforMorphologicallyRichLanguages
Augmentations to tagging algorithms become necessary when dealing with lan-
guageswithrichmorphologylikeCzech,HungarianandTurkish.
Theseproductiveword-formationprocessesresultinalargevocabularyforthese
languages: a250,000wordtokencorpusofHungarianhasmorethantwiceasmany
wordtypesasasimilarlysizedcorpusofEnglish(OraveczandDienes,2002),while
a10millionwordtokencorpusofTurkishcontainsfourtimesasmanywordtypes
as a similarly sized English corpus (Hakkani-Tu¨r et al., 2002). Large vocabular-
ies mean many unknown words, and these unknown words cause significant per-
formance degradations in a wide variety of languages (including Czech, Slovene,
Estonian,andRomanian)(Hajicˇ,2000).
Highly inflectional languages also have much more information than English
coded in word morphology, like case (nominative, accusative, genitive) or gender
(masculine, feminine). Because this information is important for tasks like pars-
ingandcoreferenceresolution,part-of-speechtaggersformorphologicallyrichlan-8.8 • SUMMARY 181
guagesneedtolabelwordswithcaseandgenderinformation. Tagsetsformorpho-
logicallyrichlanguagesarethereforesequencesofmorphologicaltagsratherthana
singleprimitivetag.Here’saTurkishexample,inwhichthewordizinhasthreepos-
siblemorphological/part-of-speechtagsandmeanings(Hakkani-Tu¨retal.,2002):
1. Yerdekiizintemizlenmesigerek. iz+Noun+A3sg+Pnon+Gen
Thetraceonthefloorshouldbecleaned.
2. U¨zerindeparmakizinkalmis¸. iz+Noun+A3sg+P2sg+Nom
Yourfingerprintislefton(it).
3. Ic¸erigirmekic¸inizinalmangerekiyor. izin+Noun+A3sg+Pnon+Nom
Youneedpermissiontoenter.
UsingamorphologicalparsesequencelikeNoun+A3sg+Pnon+Genasthepart-
of-speech tag greatly increases the number of parts of speech, and so tagsets can
be 4 to 10 times larger than the 50–100 tags we have seen for English. With such
large tagsets, each word needs to be morphologically analyzed to generate the list
of possible morphological tag sequences (part-of-speech tags) for the word. The
roleofthetaggeristhentodisambiguateamongthesetags. Thismethodalsohelps
with unknown words since morphological parsers can accept unknown stems and
stillsegmenttheaffixesproperly.
8.8 Summary
Thischapterintroducedpartsofspeechandnamedentities,andthetasksofpart-
of-speechtaggingandnamedentityrecognition:
• Languages generally have a small set of closed class words that are highly
frequent, ambiguous, and act as function words, and open-class words like
nouns,verbs,adjectives.Variouspart-of-speechtagsetsexist,ofbetween40
and200tags.
• Part-of-speech tagging is the process of assigning a part-of-speech label to
eachofasequenceofwords.
• Namedentitiesarewordsforpropernounsreferringmainlytopeople,places,
andorganizations,butextendedtomanyothertypesthataren’tstrictlyentities
orevenpropernouns.
• Twocommonapproachestosequencemodelingareagenerativeapproach,
HMMtagging,andadiscriminativeapproach,CRFtagging. Wewillseea
neuralapproachinfollowingchapters.
• TheprobabilitiesinHMMtaggersareestimatedbymaximumlikelihoodes-
timation on tag-labeled training corpora. The Viterbi algorithm is used for
decoding,findingthemostlikelytagsequence
• ConditionalRandomFieldsorCRFtaggerstrainalog-linearmodelthatcan
choosethebesttagsequencegivenanobservationsequence,basedonfeatures
thatconditionontheoutputtag,theprioroutputtag,theentireinputsequence,
and the current timestep. They use the Viterbi algorithm for inference, to
choose the best sequence of tags, and a version of the Forward-Backward
algorithm(seeAppendixA)fortraining,182 CHAPTER8 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES
Bibliographical and Historical Notes
Whatisprobablytheearliestpart-of-speechtaggerwaspartoftheparserinZellig
Harris’sTransformationsandDiscourseAnalysisProject(TDAP),implementedbe-
tween June 1958 and July 1959 at the University of Pennsylvania (Harris, 1962),
althoughearliersystemshadusedpart-of-speechdictionaries. TDAPused14hand-
written rules for part-of-speech disambiguation; the use of part-of-speech tag se-
quencesandtherelativefrequencyoftagsforawordprefiguresmodernalgorithms.
Theparserwasimplementedessentiallyasacascadeoffinite-statetransducers;see
JoshiandHopely(1999)andKarttunen(1999)forareimplementation.
The Computational Grammar Coder (CGC) of Klein and Simmons (1963) had
threecomponents: alexicon, amorphologicalanalyzer, andacontextdisambigua-
tor. The small 1500-word lexicon listed only function words and other irregular
words. Themorphologicalanalyzerusedinflectionalandderivationalsuffixestoas-
signpart-of-speechclasses. Thesewererunoverwordstoproducecandidateparts
ofspeechwhichwerethendisambiguatedbyasetof500contextrulesbyrelyingon
surroundingislandsofunambiguouswords.Forexample,onerulesaidthatbetween
anARTICLEandaVERB,theonlyallowablesequenceswereADJ-NOUN,NOUN-
ADVERB,orNOUN-NOUN.TheTAGGITtagger(GreeneandRubin,1971)used
the same architecture as Klein and Simmons (1963), with a bigger dictionary and
moretags(87). TAGGITwasappliedtotheBrowncorpusand,accordingtoFrancis
andKucˇera(1982,p. 9),accuratelytagged77%ofthecorpus;theremainderofthe
Brown corpus was then tagged by hand. All these early algorithms were based on
atwo-stagearchitectureinwhichadictionarywasfirstusedtoassigneachworda
set of potential parts of speech, and then lists of handwritten disambiguation rules
winnowedthesetdowntoasinglepartofspeechperword.
ProbabilitieswereusedintaggingbyStolzetal.(1965)andacompleteproba-
bilistictaggerwithViterbidecodingwassketchedbyBahlandMercer(1976). The
Lancaster-Oslo/Bergen(LOB)corpus,aBritishEnglishequivalentoftheBrowncor-
pus, wastaggedintheearly1980’swiththeCLAWStagger(Marshall1983; Mar-
shall1987;Garside1987),aprobabilisticalgorithmthatapproximatedasimplified
HMMtagger.Thealgorithmusedtagbigramprobabilities,butinsteadofstoringthe
wordlikelihoodofeachtag,thealgorithmmarkedtagseitherasrare(P(tagword)<
|
.01)infrequent(P(tagword)<.10)ornormallyfrequent(P(tagword)>.10).
| |
DeRose (1988) developed a quasi-HMM algorithm, including the use of dy-
namic programming, although computing P(t w)P(w) instead of P(wt)P(w). The
| |
same year, the probabilistic PARTS tagger of Church 1988, 1989 was probably the
first implemented HMM tagger, described correctly in Church (1989), although
Church (1988) also described the computation incorrectly as P(t w)P(w) instead
|
ofP(wt)P(w). Church(p.c.) explainedthathehadsimplifiedforpedagogicalpur-
|
posesbecauseusingtheprobabilityP(t w)madetheideaseemmoreunderstandable
|
as“storingalexiconinanalmoststandardform”.
LatertaggersexplicitlyintroducedtheuseofthehiddenMarkovmodel(Kupiec
1992; Weischedeletal.1993; Schu¨tzeandSinger1994). Merialdo(1994)showed
that fully unsupervised EM didn’t work well for the tagging task and that reliance
onhand-labeleddatawasimportant. Charniaketal.(1993)showedtheimportance
ofthemostfrequenttagbaseline;the92.3%numberwegiveabovewasfromAbney
et al. (1999). See Brants (2000) for HMM tagger implementation details, includ-
ing the extension to trigram contexts, and the use of sophisticated unknown word
features;itsperformanceisstillclosetostateofthearttaggers.EXERCISES 183
Log-linear models for POS tagging were introduced by Ratnaparkhi (1996),
whointroducedasystemcalledMXPOSTwhichimplementedamaximumentropy
Markov model (MEMM), a slightly simpler version of a CRF. Around the same
time,sequencelabelerswereappliedtothetaskofnamedentitytagging,firstwith
HMMs (Bikel et al., 1997) and MEMMs (McCallum et al., 2000), and then once
CRFs were developed (Lafferty et al. 2001), they were also applied to NER (Mc-
CallumandLi,2003). Awideexplorationoffeaturesfollowed(Zhouetal.,2005).
Neural approaches to NER mainly follow from the pioneering results of Collobert
etal.(2011),whoappliedaCRFontopofaconvolutionalnet. BiLSTMswithword
and character-based embeddings as input followed shortly and became a standard
neural algorithm for NER (Huang et al. 2015, Ma and Hovy 2016, Lample et al.
2016)followedbythemorerecentuseofTransformersandBERT.
Theideaofusinglettersuffixesforunknownwordsisquiteold;theearlyKlein
andSimmons(1963)systemcheckedallfinallettersuffixesoflengths1-5. Theun-
knownwordfeaturesdescribedonpage176comemainlyfromRatnaparkhi(1996),
withaugmentationsfromToutanovaetal.(2003)andManning(2011).
StateoftheartPOStaggersuseneuralalgorithms,eitherbidirectionalRNNsor
TransformerslikeBERT;seeChapter9andChapter11. HMM(Brants2000;Thede
andHarper1999)andCRFtaggeraccuraciesarelikelyjustatadlower.
Manning(2011)investigatestheremaining2.7%oferrorsinahigh-performing
tagger(Toutanovaetal.,2003). Hesuggeststhatathirdorhalfoftheseremaining
errorsareduetoerrorsorinconsistenciesinthetrainingdata,athirdmightbesolv-
ablewithricherlinguisticmodels, andfortheremainderthetaskisunderspecified
orunclear.
Supervised tagging relies heavily on in-domain training data hand-labeled by
experts. Waystorelaxthisassumptionincludeunsupervisedalgorithmsforcluster-
ingwordsintopart-of-speech-likeclasses,summarizedinChristodoulopoulosetal.
(2010),andwaystocombinelabeledandunlabeleddata,forexamplebyco-training
(Clarketal.2003;Søgaard2010).
See Householder (1995) for historical notes on parts of speech, and Sampson
(1987)andGarsideetal.(1997)ontheprovenanceoftheBrownandothertagsets.
Exercises
8.1 Findonetaggingerrorineachofthefollowingsentencesthataretaggedwith
thePennTreebanktagset:
1. I/PRPneed/VBPa/DTflight/NNfrom/INAtlanta/NN
2. Does/VBZthis/DTflight/NNserve/VBdinner/NNS
3. I/PRPhave/VBa/DTfriend/NNliving/VBGin/INDenver/NNP
4. Can/VBPyou/PRPlist/VBthe/DTnonstop/JJafternoon/NNflights/NNS
8.2 Use the Penn Treebank tagset to tag each word in the following sentences
from Damon Runyon’s short stories. You may ignore punctuation. Some of
thesearequitedifficult;doyourbest.
1. Itisanicenight.
2. ThiscrapgameisoveragarageinFifty-secondStreet...
3. ...Nobodyevertakesthenewspapersshesells...
4. He is a tall, skinny guy with a long, sad, mean-looking kisser, and a
mournfulvoice.184 CHAPTER8 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES
5. ...IamsittinginMindy’srestaurantputtingonthegefilltefish,whichis
adishIamveryfondof,...
6. Whenaguyandadollgettotakingpeeksbackandforthateachother,
whythereyouareindeed.
8.3 Now compare your tags from the previous exercise with one or two friend’s
answers. Onwhichwordsdidyoudisagreethemost? Why?
8.4 Implement the “most likely tag” baseline. Find a POS-tagged training set,
anduseittocomputeforeachwordthetagthatmaximizes p(t w). Youwill
|
needtoimplementasimpletokenizertodealwithsentenceboundaries. Start
byassumingthatallunknownwordsareNNandcomputeyourerrorrateon
knownandunknownwords. Nowwriteatleastfiverulestodoabetterjobof
taggingunknownwords,andshowthedifferenceinerrorrates.
8.5 BuildabigramHMMtagger. Youwillneedapart-of-speech-taggedcorpus.
Firstsplitthecorpusintoatrainingsetandtestset. Fromthelabeledtraining
set, train the transition and observation probabilities of the HMM tagger di-
rectlyonthehand-taggeddata. ThenimplementtheViterbialgorithmsoyou
candecodeatestsentence. Nowrunyouralgorithmonthetestset. Reportits
errorrateandcompareitsperformancetothemostfrequenttagbaseline.
8.6 Doanerroranalysisofyourtagger. Buildaconfusionmatrixandinvestigate
the most frequent errors. Propose some features for improving the perfor-
manceofyourtaggerontheseerrors.
8.7 Developasetofregularexpressionstorecognizethecharactershapefeatures
describedonpage176.
8.8 The BIO and other labeling schemes given in this chapter aren’t the only
possibleone. Forexample,theBtagcanbereservedonlyforthosesituations
where an ambiguity exists between adjacent entities. Propose a new set of
BIO tagsforusewithyourNERsystem. Experimentwithitandcompareits
performancewiththeschemespresentedinthischapter.
8.9 Namesofworksofart(books,movies,videogames,etc.) arequitedifferent
from the kinds of named entities we’ve discussed in this chapter. Collect a
list of names of works of art from a particular category from a Web-based
source(e.g., gutenberg.org, amazon.com, imdb.com, etc.). Analyzeyourlist
andgiveexamplesofwaysthatthenamesinitarelikelytobeproblematicfor
thetechniquesdescribedinthischapter.
8.10 DevelopanNERsystemspecifictothecategoryofnamesthatyoucollectedin
thelastexercise. Evaluateyoursystemonacollectionoftextlikelytocontain
instancesofthesenamedentities.CHAPTER
9 RNNs and LSTMs
Timewillexplain.
JaneAusten,Persuasion
Languageisaninherentlytemporalphenomenon. Spokenlanguageisasequenceof
acousticeventsovertime,andwecomprehendandproducebothspokenandwritten
languageasacontinuousinputstream. Thetemporalnatureoflanguageisreflected
inthemetaphorsweuse;wetalkoftheflowofconversations,newsfeeds,andtwitter
streams,allofwhichemphasizethatlanguageisasequencethatunfoldsintime.
This temporal nature is reflected in some language processing algorithms. For
example,theViterbialgorithmweintroducedforHMMpart-of-speechtaggingpro-
ceedsthroughtheinputawordatatime,carryingforwardinformationgleanedalong
theway. Yetothermachinelearningapproaches,likethosewe’vestudiedforsenti-
mentanalysisorothertextclassificationtasksdon’thavethistemporalnature–they
assumesimultaneousaccesstoallaspectsoftheirinput.
The feedforward networks of Chapter 7 also assumed simultaneous access, al-
thoughtheyalsohadasimplemodelfortime. Recallthatweappliedfeedforward
networks to language modeling by having them look only at a fixed-size window
ofwords,andthenslidingthiswindowovertheinput,makingindependentpredic-
tionsalongtheway. Thissliding-windowapproachisalsousedinthetransformer
architecturewewillintroduceinChapter10.
This chapter introduces a deep learning architecture that offers an alternative
wayofrepresentingtime: recurrentneuralnetworks(RNNs),andtheirvariantslike
LSTMs. RNNshaveamechanismthatdealsdirectlywiththesequentialnatureof
language,allowingthemtohandlethetemporalnatureoflanguagewithouttheuseof
arbitraryfixed-sizedwindows. Therecurrentnetworkoffersanewwaytorepresent
the prior context, in its recurrent connections, allowing the model’s decision to
dependoninformationfromhundredsofwordsinthepast. We’llseehowtoapply
the model to the task of language modeling, to sequence modeling tasks like part-
of-speechtagging,andtotextclassificationtaskslikesentimentanalysis.
9.1 Recurrent Neural Networks
A recurrent neural network (RNN) is any network that contains a cycle within its
networkconnections, meaningthatthevalueofsomeunitisdirectly, orindirectly,
dependent on its own earlier outputs as an input. While powerful, such networks
aredifficulttoreasonaboutandtotrain. However,withinthegeneralclassofrecur-
rent networks there are constrained architectures that have proven to be extremely
effectivewhenappliedtolanguage. Inthissection,weconsideraclassofrecurrent
Elman networksreferredtoasElmanNetworks(Elman,1990)orsimplerecurrentnet-
Networks186 CHAPTER9 • RNNSANDLSTMS
works. Thesenetworksareusefulintheirownrightandserveasthebasisformore
complexapproachesliketheLongShort-TermMemory(LSTM)networksdiscussed
laterinthischapter. InthischapterwhenweusethetermRNNwe’llbereferringto
thesesimplermoreconstrainednetworks(althoughyouwilloftenseethetermRNN
tomeananynetwithrecurrentpropertiesincludingLSTMs).
x t h t y t
Figure9.1 SimplerecurrentneuralnetworkafterElman(1990).Thehiddenlayerincludes
arecurrentconnectionaspartofitsinput. Thatis, theactivationvalueofthehiddenlayer
depends on the current input as well as the activation value of the hidden layer from the
previoustimestep.
Fig. 9.1 illustrates the structure of an RNN. As with ordinary feedforward net-
works, an input vector representing the current input, x, is multiplied by a weight
t
matrixandthenpassedthroughanon-linearactivationfunctiontocomputetheval-
ues for a layer of hidden units. This hidden layer is then used to calculate a cor-
respondingoutput, y. Inadeparturefromourearlierwindow-basedapproach, se-
t
quences are processed by presenting one item at a time to the network. We’ll use
subscriptstorepresenttime,thusx willmeantheinputvectorxattimet. Thekey
t
differencefromafeedforwardnetworkliesintherecurrentlinkshowninthefigure
withthedashedline. Thislinkaugmentstheinputtothecomputationatthehidden
layerwiththevalueofthehiddenlayerfromtheprecedingpointintime.
The hidden layer from the previous time step provides a form of memory, or
context, that encodes earlier processing and informs the decisions to be made at
later points in time. Critically, this approach does not impose a fixed-length limit
onthispriorcontext;thecontextembodiedintheprevioushiddenlayercaninclude
informationextendingbacktothebeginningofthesequence.
AddingthistemporaldimensionmakesRNNsappeartobemorecomplexthan
non-recurrent architectures. But in reality, they’re not all that different. Given an
inputvectorandthevaluesforthehiddenlayerfromtheprevioustimestep, we’re
still performing the standard feedforward calculation introduced in Chapter 7. To
see this, consider Fig. 9.2 which clarifies the nature of the recurrence and how it
factorsintothecomputationatthehiddenlayer. Themostsignificantchangeliesin
thenewsetofweights,U,thatconnectthehiddenlayerfromtheprevioustimestep
tothecurrenthiddenlayer. Theseweightsdeterminehowthenetworkmakesuseof
pastcontextincalculatingtheoutputforthecurrentinput.Aswiththeotherweights
inthenetwork,theseconnectionsaretrainedviabackpropagation.
9.1.1 InferenceinRNNs
Forward inference (mapping a sequence of inputs to a sequence of outputs) in an
RNNisnearlyidenticaltowhatwe’vealreadyseenwithfeedforwardnetworks. To
compute an output y for an input x, we need the activation value for the hidden
t t
layerh. Tocalculatethis,wemultiplytheinputx withtheweightmatrixW,and
t t
the hidden layer from the previous time step h with the weight matrix U. We
t 1
−
add these values together and pass them through a suitable activation function, g,
to arrive at the activation value for the current hidden layer, h. Once we have the
t9.1 • RECURRENTNEURALNETWORKS 187
y
t
V
h
t
U W
h x
t-1 t
Figure9.2 Simplerecurrentneuralnetworkillustratedasafeedforwardnetwork.
valuesforthehiddenlayer,weproceedwiththeusualcomputationtogeneratethe
outputvector.
h t = g(Uh t 1+Wx t) (9.1)
−
y t = f(Vh t) (9.2)
It’sworthwhileheretobecarefulaboutspecifyingthedimensionsoftheinput,hid-
denandoutputlayers,aswellastheweightmatricestomakesurethesecalculations
arecorrect. Let’srefertotheinput, hiddenandoutputlayerdimensionsasd , d ,
in h
and d
out
respectively. Given this, our three parameter matrices are: W Rdh×din,
∈
U Rdh×dh,andV Rdout×dh.
∈ ∈
Inthecommonlyencounteredcaseofsoftclassification, computingy consists
t
of a softmax computation that provides a probability distribution over the possible
outputclasses.
y t = softmax(Vh t) (9.3)
The fact that the computation at timet requires the value of the hidden layer from
timet 1mandatesanincrementalinferencealgorithmthatproceedsfromthestart
−
ofthesequencetotheendasillustratedinFig.9.3. Thesequentialnatureofsimple
recurrentnetworkscanalsobeseenbyunrollingthenetworkintimeasisshownin
Fig. 9.4. In this figure, the various layers of units are copied for each time step to
illustratethattheywillhavedifferingvaluesovertime. However,thevariousweight
matricesaresharedacrosstime.
functionFORWARDRNN(x,network)returnsoutputsequencey
h 0
0
←
fori 1toLENGTH(x) do
←
h
i
g(Uh
i 1
+ Wx i)
y i← f(Vh i−)
←
returny
Figure9.3 Forwardinferenceinasimplerecurrentnetwork.ThematricesU,VandWare
sharedacrosstime,whilenewvaluesforhandyarecalculatedwitheachtimestep.188 CHAPTER9 • RNNSANDLSTMS
y
3
V
y h
2 3
V U W
h
y 2 x
1 3
U W
V
h x
1 2
U W
h x
0 1
Figure9.4 Asimplerecurrentneuralnetworkshownunrolledintime. Networklayersarerecalculatedfor
eachtimestep,whiletheweightsU,VandWaresharedacrossalltimesteps.
9.1.2 Training
As with feedforward networks, we’ll use a training set, a loss function, and back-
propagation to obtain the gradients needed to adjust the weights in these recurrent
networks. AsshowninFig.9.2, wenowhave3setsofweightstoupdate: W, the
weightsfromtheinputlayertothehiddenlayer, U, theweightsfromtheprevious
hiddenlayertothecurrenthiddenlayer,andfinallyV,theweightsfromthehidden
layertotheoutputlayer.
Fig. 9.4 highlights two considerations that we didn’t have to worry about with
backpropagation in feedforward networks. First, to compute the loss function for
the output at timet we need the hidden layer from timet 1. Second, the hidden
−
layerattimet influencesboththeoutputattimet andthehiddenlayerattimet+1
(andhencetheoutputandlossatt+1). Itfollowsfromthisthattoassesstheerror
accruingtoh,we’llneedtoknowitsinfluenceonboththecurrentoutputaswellas
t
theonesthatfollow.
Tailoringthebackpropagationalgorithmtothissituationleadstoatwo-passal-
gorithm for training the weights in RNNs. In the first pass, we perform forward
inference, computing h, y, accumulating the loss at each step in time, saving the
t t
value of the hidden layer at each step for use at the next time step. In the second
phase,weprocessthesequenceinreverse,computingtherequiredgradientsaswe
go, computing and saving the error term for use in the hidden layer for each step
backwardintime. Thisgeneralapproachiscommonlyreferredtoasbackpropaga-
backpropaga-
tionthrough tionthroughtime(Werbos1974,Rumelhartetal.1986,Werbos1990).
time
Fortunately, with modern computational frameworks and adequate computing
resources, there is no need for a specialized approach to training RNNs. As illus-
tratedinFig.9.4, explicitlyunrollingarecurrentnetworkintoafeedforwardcom-
putationalgrapheliminatesanyexplicitrecurrences, allowingthenetworkweights
tobetraineddirectly. Insuchanapproach,weprovideatemplatethatspecifiesthe
basicstructureofthenetwork,includingallthenecessaryparametersfortheinput,9.2 • RNNSASLANGUAGEMODELS 189
output,andhiddenlayers,theweightmatrices,aswellastheactivationandoutput
functionstobeused. Then,whenpresentedwithaspecificinputsequence,wecan
generateanunrolledfeedforwardnetworkspecifictothatinput,andusethatgraph
toperformforwardinferenceortrainingviaordinarybackpropagation.
Forapplicationsthatinvolvemuchlongerinputsequences,suchasspeechrecog-
nition, character-levelprocessing, orstreamingcontinuousinputs, unrollinganen-
tireinputsequencemaynotbefeasible. Inthesecases,wecanunrolltheinputinto
manageablefixed-lengthsegmentsandtreateachsegmentasadistincttrainingitem.
9.2 RNNs as Language Models
Let’sseehowtoapplyRNNstothelanguagemodelingtask. RecallfromChapter3
that language models predict the next word in a sequence given some preceding
context. Forexample,iftheprecedingcontextis“Thanksforallthe”andwewant
toknowhowlikelythenextwordis“fish”wewouldcompute:
P(fishThanksforallthe)
|
Languagemodelsgiveustheabilitytoassignsuchaconditionalprobabilitytoevery
possiblenextword,givingusadistributionovertheentirevocabulary. Wecanalso
assignprobabilitiestoentiresequencesbycombiningtheseconditionalprobabilities
withthechainrule:
n
P(w ) = P(w w )
1:n i <i
|
i=1
(cid:89)
Then-gramlanguagemodelsofChapter3computetheprobabilityofawordgiven
countsofitsoccurrencewiththen 1priorwords. Thecontextisthusofsizen 1.
− −
ForthefeedforwardlanguagemodelsofChapter7,thecontextisthewindowsize.
RNN language models (Mikolov et al., 2010) process the input sequence one
word at a time, attempting to predict the next word from the current word and the
previoushiddenstate.RNNsthusdon’thavethelimitedcontextproblemthatn-gram
modelshave,orthefixedcontextthatfeedforwardlanguagemodelshave,sincethe
hiddenstatecaninprinciplerepresentinformationaboutalloftheprecedingwords
allthewaybacktothebeginningofthesequence. Fig.9.5sketchesthisdifference
between a FFN language model and an RNN language model, showing that the
RNNlanguagemodelusesh , thehiddenstatefromtheprevioustimestep, asa
t 1
−
representationofthepastcontext.
9.2.1 ForwardInferenceinanRNNlanguagemodel
Forward inference in a recurrent language model proceeds exactly as described in
Section9.1.1. TheinputsequenceX=[x ;...;x;...;x ]consistsofaseriesofword
1 t N
embeddings each represented as a one-hot vector of size V 1, and the output
| |×
prediction,y,isavectorrepresentingaprobabilitydistributionoverthevocabulary.
Ateachstep,themodelusesthewordembeddingmatrixEtoretrievetheembedding
forthecurrentword,andthencombinesitwiththehiddenlayerfromtheprevious
step to compute a new hidden layer. This hidden layer is then used to generate
an output layer which is passed through a softmax layer to generate a probability190 CHAPTER9 • RNNSANDLSTMS
a) y t b) y t
U V
h t h t-1 U h t
W W
x x x x
t-2 t-1 t t
Figure9.5 Simplified sketch of (a) a feedforward neural language model versus (b) an
RNNlanguagemodelmovingthroughatext.
distributionovertheentirevocabulary. Thatis,attimet:
e t = Ex t (9.4)
h t = g(Uh t 1+We t) (9.5)
−
y t = softmax(Vh t) (9.6)
ThevectorresultingfromVhcanbethoughtofasasetofscoresoverthevocabulary
giventheevidenceprovidedinh. Passingthesescoresthroughthesoftmaxnormal-
izesthescoresintoaprobabilitydistribution. Theprobabilitythataparticularword
iinthevocabularyisthenextwordisrepresentedbyy[i],theithcomponentofy:
t t
P(w t+1=iw 1,...,w t) = y t[i] (9.7)
|
Theprobabilityofanentiresequenceisjusttheproductoftheprobabilitiesofeach
iteminthesequence,wherewe’llusey[w]tomeantheprobabilityofthetrueword
i i
w attimestepi.
i
n
P(w 1:n) = P(w i w 1:i 1) (9.8)
| −
i=1
(cid:89)
n
= y i[w i] (9.9)
i=1
(cid:89)
9.2.2 TraininganRNNlanguagemodel
self-supervision TotrainanRNNasalanguagemodel,weusethesameself-supervisionalgorithm
wesawearlier:wetakeacorpusoftextastrainingmaterialandateachtimesteptwe
havethemodelpredictthenextword. Wecallsuchamodelself-supervisedbecause
we don’t have to add any special gold labels to the data; the natural sequence of
words is its own supervision! We simply train the model to minimize the error in
predicting the true next word in the training sequence, using cross-entropy as the
lossfunction. Recallthatthecross-entropylossmeasuresthedifferencebetweena
predictedprobabilitydistributionandthecorrectdistribution.
L CE = y t[w]logyˆ t[w] (9.10)
−
w V
(cid:88)∈
Inthecaseoflanguagemodeling,thecorrectdistributiony comesfromknowingthe
t
nextword. Thisisrepresentedasaone-hotvectorcorrespondingtothevocabulary9.2 • RNNSASLANGUAGEMODELS 191
Next word long and thanks for all
Loss …
y
Softmax over
Vocabulary
Vh
h
RNN …
Input …
e
Embeddings
So long and thanks for
Figure9.6 TrainingRNNsaslanguagemodels.
wheretheentryfortheactualnextwordis1, andalltheotherentriesare0. Thus,
the cross-entropy loss for language modeling is determined by the probability the
modelassignstothecorrectnextword. Soattimet theCElossisthenegativelog
probabilitythemodelassignstothenextwordinthetrainingsequence.
L CE(yˆ t,y t) = logyˆ t[w t+1] (9.11)
−
Thus at each word positiont of the input, the model takes as input the correct se-
quence of tokens w , and uses them to compute a probability distribution over
1:t
possiblenextwordssoastocomputethemodel’slossforthenexttokenw . Then
t+1
we move to the next word, we ignore what the model predicted for the next word
andinsteadusethecorrectsequenceoftokensw toestimatetheprobabilityof
1:t+1
tokenw . Thisideathatwealwaysgivethemodelthecorrecthistorysequenceto
t+2
predictthenextword(ratherthanfeedingthemodelitsbestcasefromtheprevious
teacherforcing timestep)iscalledteacherforcing.
The weights in the network are adjusted to minimize the average CE loss over
thetrainingsequenceviagradientdescent. Fig.9.6illustratesthistrainingregimen.
9.2.3 WeightTying
Careful readers may have noticed that the input embedding matrix E and the final
layermatrixV,whichfeedstheoutputsoftmax,arequitesimilar. ThecolumnsofE
representthewordembeddingsforeachwordinthevocabularylearnedduringthe
trainingprocesswiththegoalthatwordsthathavesimilarmeaningandfunctionwill
havesimilarembeddings. And,sincethelengthoftheseembeddingscorrespondsto
thesizeofthehiddenlayerd ,theshapeoftheembeddingmatrixEisd V .
h h
×| |
ThefinallayermatrixVprovidesawaytoscorethelikelihoodofeachwordin
the vocabulary given the evidence present in the final hidden layer of the network
throughthecalculationofVh. Thisresultsindimensionality V d . Thatis,the
h
| |×
rows ofV provide a second set of learned word embeddings that capture relevant
aspects of word meaning. This leads to an obvious question – is it even necessary
Weighttying to have both? Weight tying is a method that dispenses with this redundancy and
simplyusesasinglesetofembeddingsattheinputandsoftmaxlayers. Thatis,we192 CHAPTER9 • RNNSANDLSTMS
dispensewithV anduseE inboththestartandendofthecomputation.
e t = Ex t (9.12)
h t = g(Uh t 1+We t) (9.13)
− (cid:124)
y t = softmax(E h t) (9.14)
Inadditiontoprovidingimprovedmodelperplexity, thisapproachsignificantlyre-
ducesthenumberofparametersrequiredforthemodel.
9.3 RNNs for other NLP tasks
Now that we’ve seen the basic RNN architecture, let’s consider how to apply it to
threetypesofNLPtasks: sequenceclassificationtaskslikesentimentanalysisand
topic classification, sequence labeling tasks like part-of-speech tagging, and text
generationtasks,includingwithanewarchitecturecalledtheencoder-decoder.
9.3.1 SequenceLabeling
In sequence labeling, the network’s task is to assign a label chosen from a small
fixedsetoflabelstoeachelementofasequence,likethepart-of-speechtaggingand
named entity recognition tasks from Chapter 8. In an RNN approach to sequence
labeling,inputsarewordembeddingsandtheoutputsaretagprobabilitiesgenerated
byasoftmaxlayeroverthegiventagset,asillustratedinFig.9.7.
Argmax NNP MD VB DT NN
y
Softmax over
tags
Vh
h
RNN
Layer(s)
Embeddings e
Words Janet will back the bill
Figure9.7 Part-of-speech tagging as sequence labeling with a simple RNN. Pre-trained
wordembeddingsserveasinputsandasoftmaxlayerprovidesaprobabilitydistributionover
thepart-of-speechtagsasoutputateachtimestep.
Inthisfigure,theinputsateachtimesteparepre-trainedwordembeddingscor-
responding to the input tokens. The RNN block is an abstraction that represents
anunrolledsimplerecurrentnetworkconsistingofaninputlayer,hiddenlayer,and
output layer at each time step, as well as the shared U, V and W weight matrices
that comprise the network. The outputs of the network at each time step represent
thedistributionoverthePOStagsetgeneratedbyasoftmaxlayer.9.3 • RNNSFOROTHERNLPTASKS 193
Togenerateasequenceoftagsforagiveninput,werunforwardinferenceover
theinputsequenceandselectthemostlikelytagfromthesoftmaxateachstep.Since
we’reusingasoftmaxlayertogeneratetheprobabilitydistributionovertheoutput
tagsetateachtimestep,wewillagainemploythecross-entropylossduringtraining.
9.3.2 RNNsforSequenceClassification
Another use of RNNs is to classify entire sequences rather than the tokens within
them. This is the set of tasks commonly called text classification, like sentiment
analysis or spam detection, in which we classify a text into two or three classes
(like positive or negative), as well as classification tasks with a large number of
categories,likedocument-leveltopicclassification,ormessageroutingforcustomer
serviceapplications.
ToapplyRNNsinthissetting,wepassthetexttobeclassifiedthroughtheRNN
awordatatimegeneratinganewhiddenlayerateachtimestep. Wecanthentake
thehiddenlayerforthelasttokenofthetext,h ,toconstituteacompressedrepre-
n
sentationoftheentiresequence.Wecanpassthisrepresentationh toafeedforward
n
networkthatchoosesaclassviaasoftmaxoverthepossibleclasses. Fig.9.8illus-
tratesthisapproach.
Softmax
FFN
h
n
RNN
x x x x
1 2 3 n
Figure9.8 SequenceclassificationusingasimpleRNNcombinedwithafeedforwardnet-
work.ThefinalhiddenstatefromtheRNNisusedastheinputtoafeedforwardnetworkthat
performstheclassification.
Notethatinthisapproachwedon’tneedintermediateoutputsforthewordsin
thesequenceprecedingthelastelement. Therefore,therearenolosstermsassoci-
atedwiththoseelements. Instead,thelossfunctionusedtotraintheweightsinthe
network is based entirely on the final text classification task. The output from the
softmax output from the feedforward classifier together with a cross-entropy loss
drivesthetraining. Theerrorsignalfromtheclassificationisbackpropagatedallthe
waythroughtheweightsinthefeedforwardclassifierthrough,toitsinput,andthen
throughtothethreesetsofweightsintheRNNasdescribedearlierinSection9.1.2.
Thetrainingregimenthatusesthelossfromadownstreamapplicationtoadjustthe
end-to-end weightsallthewaythroughthenetworkisreferredtoasend-to-endtraining.
training
Another option, instead of using just the last token h to represent the whole
n
pooling sequence,istousesomesortofpoolingfunctionofallthehiddenstatesh i foreach
word i in the sequence. For example, we can create a representation that pools all194 CHAPTER9 • RNNSANDLSTMS
thenhiddenstatesbytakingtheirelement-wisemean:
n
1
h mean= h i (9.15)
n
i=1
(cid:88)
Orwecantaketheelement-wisemax;theelement-wisemaxofasetofnvectorsis
anewvectorwhosekthelementisthemaxofthekthelementsofallthenvectors.
9.3.3 GenerationwithRNN-BasedLanguageModels
RNN-based language models can also be used to generate text. Text generation is
of enormous practical importance, part of tasks like question answering, machine
translation, text summarization, grammar correction, story generation, and conver-
sational dialogue; any task where a system needs to produce text, conditioned on
some other text. This use of a language model to generate text is one of the areas
in which the impact of neural language models on NLP has been the largest. Text
generation,alongwithimagegenerationandcodegeneration,constituteanewarea
ofAIthatisoftencalledgenerativeAI.
RecallbackinChapter3wesawhowtogeneratetextfromann-gramlanguage
modelbyadaptingasamplingtechniquesuggestedataboutthesametimebyClaude
Shannon (Shannon, 1951) and the psychologists George Miller and Jennifer Self-
ridge (Miller and Selfridge, 1950). We first randomly sample a word to begin a
sequence based on its suitability as the start of a sequence. We then continue to
samplewordsconditionedonourpreviouschoicesuntilwereachapre-determined
length,oranendofsequencetokenisgenerated.
Today,thisapproachofusingalanguagemodeltoincrementallygeneratewords
byrepeatedlysamplingthenextwordconditionedonourpreviouschoicesiscalled
autoregressive autoregressive generation or causal LM generation. The procedure is basically
generation
thesameasthatdescribedonpage40,butadaptedtoaneuralcontext:
• Sample a word in the output from the softmax distribution that results from
usingthebeginningofsentencemarker,<s>,asthefirstinput.
• Usethewordembeddingforthatfirstwordastheinputtothenetworkatthe
nexttimestep,andthensamplethenextwordinthesamefashion.
• Continuegeneratinguntiltheendofsentencemarker, </s>, issampledora
fixedlengthlimitisreached.
Technicallyanautoregressivemodelisamodelthatpredictsavalueattimetbased
onalinearfunctionofthepreviousvaluesattimest 1,t 2,andsoon. Although
− −
languagemodelsarenotlinear(sincetheyhavemanylayersofnon-linearities),we
loosely refer to this generation technique as autoregressive generation since the
wordgeneratedateachtimestepisconditionedonthewordselectedbythenetwork
fromthepreviousstep. Fig.9.9illustratesthisapproach. Inthisfigure,thedetailsof
theRNN’shiddenlayersandrecurrentconnectionsarehiddenwithintheblueblock.
This simple architecture underlies state-of-the-art approaches to applications
such as machine translation, summarization, and question answering. The key to
theseapproachesistoprimethegenerationcomponentwithanappropriatecontext.
That is, instead of simply using <s> to get things started we can provide a richer
task-appropriate context; for translation the context is the sentence in the source
language;forsummarizationit’sthelongtextwewanttosummarize.9.4 • STACKEDANDBIDIRECTIONALRNNARCHITECTURES 195
Sampled Word So long and ?
Softmax
RNN
Embedding
<s> So long and
Input Word
Figure9.9 AutoregressivegenerationwithanRNN-basedneurallanguagemodel.
9.4 Stacked and Bidirectional RNN architectures
Recurrentnetworksarequiteflexible. Bycombiningthefeedforwardnatureofun-
rolled computational graphs with vectors as common inputs and outputs, complex
networks can be treated as modules that can be combined in creative ways. This
sectionintroducestwoofthemorecommonnetworkarchitecturesusedinlanguage
processingwithRNNs.
9.4.1 StackedRNNs
In our examples thus far, the inputs to our RNNs have consisted of sequences of
wordorcharacterembeddings(vectors)andtheoutputshavebeenvectorsusefulfor
predictingwords,tagsorsequencelabels. However,nothingpreventsusfromusing
theentiresequenceofoutputsfromoneRNNasaninputsequencetoanotherone.
StackedRNNs StackedRNNsconsistofmultiplenetworkswheretheoutputofonelayerservesas
theinputtoasubsequentlayer,asshowninFig.9.10.
y y y y
1 2 3 n
RNN 3
RNN 2
RNN 1
x x x x
1 2 3 n
Figure9.10 Stackedrecurrentnetworks. Theoutputofalowerlevelservesastheinputto
higherlevelswiththeoutputofthelastnetworkservingasthefinaloutput.196 CHAPTER9 • RNNSANDLSTMS
StackedRNNsgenerallyoutperformsingle-layernetworks. Onereasonforthis
success seems to be that the network induces representations at differing levels of
abstractionacrosslayers. Justastheearlystagesofthehumanvisualsystemdetect
edges that are then used for finding larger regions and shapes, the initial layers of
stacked networks can induce representations that serve as useful abstractions for
furtherlayers—representationsthatmightprovedifficulttoinduceinasingleRNN.
The optimal number of stacked RNNs is specific to each application and to each
training set. However, as the number of stacks is increased the training costs rise
quickly.
9.4.2 BidirectionalRNNs
The RNN uses information from the left (prior) context to make its predictions at
time t. But in many applications we have access to the entire input sequence; in
thosecaseswewouldliketousewordsfromthecontexttotherightoft. Oneway
to do this is to run two separate RNNs, one left-to-right, and one right-to-left, and
concatenatetheirrepresentations.
Intheleft-to-rightRNNswe’vediscussedsofar,thehiddenstateatagiventime
t representseverythingthenetworkknowsaboutthesequenceuptothatpoint. The
stateisafunctionoftheinputsx ,...,x andrepresentsthecontextofthenetworkto
1 t
theleftofthecurrenttime.
hf
t
= RNN forward(x 1,...,x t) (9.16)
Thisnewnotationhf
simplycorrespondstothenormalhiddenstateattimet,repre-
t
sentingeverythingthenetworkhasgleanedfromthesequencesofar.
To take advantage of context to the right of the current input, we can train an
RNN on a reversed input sequence. With this approach, the hidden state at timet
representsinformationaboutthesequencetotherightofthecurrentinput:
hb
t
= RNN backward(x t,...x n) (9.17)
Here,thehiddenstatehb representsalltheinformationwehavediscernedaboutthe
t
sequencefromt totheendofthesequence.
bidirectional AbidirectionalRNN(SchusterandPaliwal,1997)combinestwoindependent
RNN
RNNs,onewheretheinputisprocessedfromthestarttotheend,andtheotherfrom
the end to the start. We then concatenate the two representations computed by the
networksintoasinglevectorthatcapturesboththeleftandrightcontextsofaninput
ateachpointintime. Hereweuseeitherthesemicolon”;”ortheequivalentsymbol
tomeanvectorconcatenation:
⊕
h = [hf ; hb]
t t t
= hf hb (9.18)
t⊕ t
Fig. 9.11 illustrates such a bidirectional network that concatenates the outputs of
the forward and backward pass. Other simple ways to combine the forward and
backward contexts include element-wise addition or multiplication. The output at
eachstepintimethuscapturesinformationtotheleftandtotherightofthecurrent
input. Insequencelabelingapplications,theseconcatenatedoutputscanserveasthe
basisforalocallabelingdecision.
BidirectionalRNNshavealsoproventobequiteeffectiveforsequenceclassifi-
cation. RecallfromFig.9.8thatforsequenceclassificationweusedthefinalhidden9.4 • STACKEDANDBIDIRECTIONALRNNARCHITECTURES 197
y y y y
1 2 3 n
concatenated
outputs
RNN 2
RNN 1
x 1 x 2 x 3 x n
Figure9.11 AbidirectionalRNN.Separatemodelsaretrainedintheforwardandbackward
directions, with the output of each model at each time point concatenated to represent the
bidirectionalstateatthattimepoint.
state of the RNN as the input to a subsequent feedforward classifier. A difficulty
with this approach is that the final state naturally reflects more information about
the end of the sentence than its beginning. Bidirectional RNNs provide a simple
solutiontothisproblem;asshowninFig.9.12,wesimplycombinethefinalhidden
states from the forward and backward passes (for example by concatenation) and
usethatasinputforfollow-onprocessing.
Softmax
FFN
← →
h h
1 n
←
RNN 2
h
1
→
RNN 1
h
n
x 1 x 2 x 3 x n
Figure9.12 AbidirectionalRNNforsequenceclassification. Thefinalhiddenunitsfrom
theforwardandbackwardpassesarecombinedtorepresenttheentiresequence. Thiscom-
binedrepresentationservesasinputtothesubsequentclassifier.198 CHAPTER9 • RNNSANDLSTMS
9.5 The LSTM
Inpractice,itisquitedifficulttotrainRNNsfortasksthatrequireanetworktomake
useofinformationdistantfromthecurrentpointofprocessing. Despitehavingac-
cesstotheentireprecedingsequence,theinformationencodedinhiddenstatestends
to be fairly local, more relevant to the most recent parts of the input sequence and
recentdecisions. Yetdistantinformationiscriticaltomanylanguageapplications.
Considerthefollowingexampleinthecontextoflanguagemodeling.
(9.19) Theflightstheairlinewascancellingwerefull.
Assigningahighprobabilitytowasfollowingairlineisstraightforwardsinceairline
provides a strong local context for the singular agreement. However, assigning an
appropriateprobabilitytowereisquitedifficult,notonlybecausethepluralflightsis
quitedistant,butalsobecausetheinterveningcontextinvolvessingularconstituents.
Ideally,anetworkshouldbeabletoretainthedistantinformationaboutpluralflights
untilitisneeded, whilestillprocessingtheintermediatepartsofthesequencecor-
rectly.
OnereasonfortheinabilityofRNNstocarryforwardcriticalinformationisthat
thehiddenlayers,and,byextension,theweightsthatdeterminethevaluesinthehid-
denlayer,arebeingaskedtoperformtwotaskssimultaneously:provideinformation
useful for the current decision, and updating and carrying forward information re-
quiredforfuturedecisions.
A second difficulty with training RNNs arises from the need to backpropagate
theerrorsignalbackthroughtime.RecallfromSection9.1.2thatthehiddenlayerat
timet contributestothelossatthenexttimestepsinceittakespartinthatcalcula-
tion. Asaresult,duringthebackwardpassoftraining,thehiddenlayersaresubject
torepeatedmultiplications,asdeterminedbythelengthofthesequence. Afrequent
result of this process is that the gradients are eventually driven to zero, a situation
vanishing calledthevanishinggradientsproblem.
gradients
Toaddresstheseissues,morecomplexnetworkarchitectureshavebeendesigned
toexplicitlymanagethetaskofmaintainingrelevantcontextovertime,byenabling
thenetworktolearntoforgetinformationthatisnolongerneededandtoremember
informationrequiredfordecisionsstilltocome.
ThemostcommonlyusedsuchextensiontoRNNsisthelongshort-termmem-
longshort-term ory(LSTM)network(HochreiterandSchmidhuber,1997). LSTMsdividethecon-
memory
text management problem into two subproblems: removing information no longer
needed from the context, and adding information likely to be needed for later de-
cision making. The key to solving both problems is to learn how to manage this
contextratherthanhard-codingastrategyintothearchitecture. LSTMsaccomplish
this by first adding an explicit context layer to the architecture (in addition to the
usual recurrent hidden layer), and through the use of specialized neural units that
make use of gates to control the flow of information into and out of the units that
comprisethenetworklayers. Thesegatesareimplementedthroughtheuseofaddi-
tionalweightsthatoperatesequentiallyontheinput,andprevioushiddenlayer,and
previouscontextlayers.
ThegatesinanLSTMshareacommondesignpattern;eachconsistsofafeed-
forward layer, followed by a sigmoid activation function, followed by a pointwise
multiplicationwiththelayerbeinggated.Thechoiceofthesigmoidastheactivation
functionarisesfromitstendencytopushitsoutputstoeither0or1. Combiningthis
withapointwisemultiplicationhasaneffectsimilartothatofabinarymask. Values9.5 • THELSTM 199
inthelayerbeinggatedthatalignwithvaluesnear1inthemaskarepassedthrough
nearlyunchanged;valuescorrespondingtolowervaluesareessentiallyerased.
forgetgate The first gate we’ll consider is the forget gate. The purpose of this gate is
to delete information from the context that is no longer needed. The forget gate
computes a weighted sum of the previous state’s hidden layer and the current in-
put and passes that through a sigmoid. This mask is then multiplied element-wise
by the context vector to remove the information from context that is no longer re-
quired. Element-wisemultiplicationoftwovectors(representedbytheoperator ,
(cid:12)
andsometimescalledtheHadamardproduct)isthevectorofthesamedimension
asthetwoinputvectors,whereeachelementiistheproductofelementiinthetwo
inputvectors:
f t = σ(U fh t 1+W fx t) (9.20)
−
k t = c t 1 f t (9.21)
− (cid:12)
Thenexttaskistocomputetheactualinformationweneedtoextractfromtheprevi-
oushiddenstateandcurrentinputs—thesamebasiccomputationwe’vebeenusing
forallourrecurrentnetworks.
g t = tanh(U gh t 1+W gx t) (9.22)
−
addgate Next,wegeneratethemaskfortheaddgatetoselecttheinformationtoaddtothe
currentcontext.
i t = σ(U ih t 1+W ix t) (9.23)
−
j t = g t i t (9.24)
(cid:12)
Next,weaddthistothemodifiedcontextvectortogetournewcontextvector.
c t =j t+k t (9.25)
outputgate Thefinalgatewe’lluseistheoutputgatewhichisusedtodecidewhatinforma-
tionisrequiredforthecurrenthiddenstate(asopposedtowhatinformationneeds
tobepreservedforfuturedecisions).
o t = σ(U oh t 1+W ox t) (9.26)
−
h t = o t tanh(c t) (9.27)
(cid:12)
Fig. 9.13 illustrates the complete computation for a single LSTM unit. Given the
appropriate weights for the various gates, an LSTM accepts as input the context
layer, and hidden layer from the previous time step, along with the current input
vector. Itthengeneratesupdatedcontextandhiddenvectorsasoutput. Thehidden
layer,h,canbeusedasinputtosubsequentlayersinastackedRNN,ortogenerate
t
anoutputforthefinallayerofanetwork.
9.5.1 GatedUnits,LayersandNetworks
TheneuralunitsusedinLSTMsareobviouslymuchmorecomplexthanthoseused
inbasicfeedforwardnetworks. Fortunately,thiscomplexityisencapsulatedwithin
thebasicprocessingunits, allowingustomaintainmodularityandtoeasilyexper-
iment with different architectures. To see this, consider Fig. 9.14 which illustrates
theinputsandoutputsassociatedwitheachkindofunit.200 CHAPTER9 • RNNSANDLSTMS
c t-1 ct-1
f
σ ct c t
h t-1 ht-1
tanh
tanh
g
ht h t
i
σ
x t xt
o LSTM
σ
Figure9.13 AsingleLSTMunitdisplayedasacomputationgraph. Theinputstoeachunitconsistsofthe
currentinput,x,theprevioushiddenstate,h t 1,andthepreviouscontext,c t 1. Theoutputsareanewhidden
− −
state,ht andanupdatedcontext,ct.
h ht ct ht
a a
g g
LSTM
z z
Unit
⌃ ⌃
x ht-1 xt ct-1 ht-1 xt
(a) (b) (c)
Figure9.14 Basicneuralunitsusedinfeedforward,simplerecurrentnetworks(SRN),and
longshort-termmemory(LSTM).
Atthefarleft,(a)isthebasicfeedforwardunitwhereasinglesetofweightsand
asingleactivationfunctiondetermineitsoutput,andwhenarrangedinalayerthere
are no connections among the units in the layer. Next, (b) represents the unit in a
simplerecurrentnetwork. Nowtherearetwoinputsandanadditionalsetofweights
togowithit. However,thereisstillasingleactivationfunctionandoutput.
The increased complexity of the LSTM units is encapsulated within the unit
itself.TheonlyadditionalexternalcomplexityfortheLSTMoverthebasicrecurrent
unit(b)isthepresenceoftheadditionalcontextvectorasaninputandoutput.
ThismodularityiskeytothepowerandwidespreadapplicabilityofLSTMunits.
LSTMunits(orothervarieties,likeGRUs)canbesubstitutedintoanyofthenetwork
architectures described in Section 9.4. And, as with simple RNNs, multi-layered
networksmakinguseofgatedunitscanbeunrolledintodeepfeedforwardnetworks
andtrainedintheusualfashionwithbackpropagation.Inpractice,therefore,LSTMs
ratherthanRNNshavebecomethestandardunitforanymodernsystemthatmakes
useofrecurrentnetworks.
+
+
+
+
⦿
⦿
+
⦿9.6 • SUMMARY: COMMONRNNNLPARCHITECTURES 201
9.6 Summary: Common RNN NLP Architectures
We’venowintroducedtheRNN,seenadvancedcomponentslikestackingmultiple
layersandusingtheLSTMversion,andseenhowtheRNNcanbeappliedtovarious
tasks. Let’stakeamomenttosummarizethearchitecturesfortheseapplications.
Fig. 9.15 shows the three architectures we’ve discussed so far: sequence la-
beling, sequence classification, and language modeling. In sequence labeling (for
example for part of speech tagging), we train a model to produce a label for each
inputwordortoken. Insequenceclassification,forexampleforsentimentanalysis,
we ignore the output for each token, and only take the value from the end of the
sequence (and similarly the model’s training signal comes from backpropagation
fromthatlasttoken). Inlanguagemodeling,wetrainthemodeltopredictthenext
wordateachtokenstep. Inthenextsectionwe’llintroduceafourtharchitecture,the
encoder-decoder.
y
y y y
1 2 … n
RNN RNN
x 1 x 2 … x n x 1 x 2 … x n
a) sequence labeling b) sequence classification
y y … y
1 2 m
x 2 x 3 … x t Decoder RNN
Context
RNN
Encoder RNN
x 1 x 2 … x t-1 x 1 x 2 … x n
c) language modeling d) encoder-decoder
Figure9.15 FourarchitecturesforNLPtasks. Insequencelabeling(POSornamedentitytagging)wemap
eachinputtokenxitoanoutputtokenyi.Insequenceclassificationwemaptheentireinputsequencetoasingle
class.Inlanguagemodelingweoutputthenexttokenconditionedonprevioustokens.Intheencodermodelwe
havetwoseparateRNNmodels,oneofwhichmapsfromaninputsequencextoanintermediaterepresentation
wecallthecontext,andasecondofwhichmapsfromthecontexttoanoutputsequencey.
9.7 The Encoder-Decoder Model with RNNs
Inthissectionweintroduceanewmodel,theencoder-decodermodel,whichisused
whenwearetakinganinputsequenceandtranslatingittoanoutputsequencexthat
is of a different length than the input, and doesn’t align with it in a word-to-word
way. Recallthatinthesequencelabelingtask,wehavetwosequences,buttheyare202 CHAPTER9 • RNNSANDLSTMS
thesamelength(forexampleinpart-of-speechtaggingeachtokengetsanassociated
tag),eachinputisassociatedwithaspecificoutput,andthelabelingforthatoutput
takesmostlylocalinformation. Thusdecidingwhetherawordisaverboranoun,
welookmostlyatthewordandtheneighboringwords.
Bycontrast,encoder-decodermodelsareusedespeciallyfortaskslikemachine
translation,wheretheinputsequenceandoutputsequencecanhavedifferentlengths
andthemappingbetweenatokenintheinputandatokenintheoutputcanbevery
indirect (in some languages the verb appears at the beginning of the sentence; in
otherlanguagesattheend). We’llintroducemachinetranslationindetailinChap-
ter13,butfornowwe’lljustpointoutthatthemappingforasentenceinEnglishto
asentenceinTagalogorYorubacanhaveverydifferentnumbersofwords,andthe
wordscanbeinaverydifferentorder.
encoder- Encoder-decodernetworks,sometimescalledsequence-to-sequencenetworks,
decoder
aremodelscapableofgeneratingcontextuallyappropriate, arbitrarylength, output
sequences given an input sequence. Encoder-decoder networks have been applied
toaverywiderangeofapplicationsincludingsummarization,questionanswering,
anddialogue,buttheyareparticularlypopularformachinetranslation.
The key idea underlying these networks is the use of an encoder network that
takesaninputsequenceandcreatesacontextualizedrepresentationofit,oftencalled
thecontext. Thisrepresentationisthenpassedtoadecoderwhichgeneratesatask-
specificoutputsequence. Fig.9.16illustratesthearchitecture
y 1 y 2 … y m
Decoder
Context
Encoder
x 1 x 2 … x n
Figure9.16 The encoder-decoder architecture. The context is a function of the hidden
representationsoftheinput,andmaybeusedbythedecoderinavarietyofways.
Encoder-decodernetworksconsistofthreecomponents:
1. Anencoderthatacceptsaninputsequence,xn,andgeneratesacorresponding
1
sequence of contextualized representations, hn. LSTMs, convolutional net-
1
works,andTransformerscanallbeemployedasencoders.
2. Acontextvector,c,whichisafunctionofhn,andconveystheessenceofthe
1
inputtothedecoder.
3. A decoder, which accepts c as input and generates an arbitrary length se-
quence of hidden states hm, from which a corresponding sequence of output
1
statesym,canbeobtained. Justaswithencoders,decoderscanberealizedby
1
anykindofsequencearchitecture.
In this section we’ll describe an encoder-decoder network based on a pair of
RNNs,butwe’llseeinChapter13howtoapplythemtotransformersaswell. We’ll
build up the equations for encoder-decode models by starting with the conditional
RNNlanguagemodel p(y),theprobabilityofasequencey.
Recallthatinanylanguagemodel,wecanbreakdowntheprobabilityasfollows:
p(y) = p(y 1)p(y 2 y 1)p(y 3 y 1,y 2)...P(y m y 1,...,y m 1) (9.28)
| | | −9.7 • THEENCODER-DECODERMODELWITHRNNS 203
In RNN language modeling, at a particular time t, we pass the prefix of t 1
−
tokensthroughthelanguagemodel,usingforwardinferencetoproduceasequence
of hidden states, ending with the hidden state corresponding to the last word of
the prefix. We then use the final hidden state of the prefix as our starting point to
generatethenexttoken.
More formally, if g is an activation function like tanh or ReLU, a function of
the input at time t and the hidden state at time t 1, and f is a softmax over the
−
setofpossiblevocabularyitems,thenattimet theoutputy andhiddenstateh are
t t
computedas:
h t = g(h t 1,x t) (9.29)
−
y t = f(h t) (9.30)
We only have to make one slight change to turn this language model with au-
toregressive generation into an encoder-decoder model that is a translation model
that can translate from a source text in one language to a target text in a second:
sentence add a sentence separation marker at the end of the source text, and then simply
separation
concatenatethetargettext.
Let’s use <s> for our sentence separate token, and let’s think about translating
an English source text (“the green witch arrived”), to a Spanish sentence (“llego´
labrujaverde”(whichcanbeglossedword-by-wordas‘arrivedthewitchgreen’).
We could also illustrate encoder-decoder models with a question-answer pair, or a
text-summarizationpair,butm
Let’susextorefertothesourcetext(inthiscaseinEnglish)plustheseparator
token <s>, and y to refer to the target text y (in this case in Spanish). Then an
encoder-decodermodelcomputestheprobability p(yx)asfollows:
|
p(yx) = p(y 1 x)p(y 2 y 1,x)p(y 3 y 1,y 2,x)...P(y m y 1,...,y m 1,x) (9.31)
| | | | | −
Fig.9.17showsthesetupforasimplifiedversionoftheencoder-decodermodel
(we’llseethefullmodel, whichrequiresthenewconceptofattention, inthenext
section).
Target Text
llegó la bruja verde </s>
softmax (output of source is ignored)
hidden h
layer(s) n
embedding
layer
the green witch arrived <s> llegó la bruja verde
Separator
Source Text
Figure9.17 Translatingasinglesentence(inferencetime)inthebasicRNNversionofencoder-decoderap-
proachtomachinetranslation.Sourceandtargetsentencesareconcatenatedwithaseparatortokeninbetween,
andthedecoderusescontextinformationfromtheencoder’slasthiddenstate.
Fig. 9.17 shows an English source text (“the green witch arrived”), a sentence
separator token (<s>, and a Spanish target text (“llego´ la bruja verde”). To trans-204 CHAPTER9 • RNNSANDLSTMS
late a source text, we run it through the network performing forward inference to
generatehiddenstatesuntilwegettotheendofthesource. Thenwebeginautore-
gressive generation, asking for a word in the context of the hidden layer from the
end of the source input as well as the end-of-sentence marker. Subsequent words
are conditioned on the previous hidden state and the embedding for the last word
generated.
Let’sformalizeandgeneralizethismodelabitinFig.9.18. (Tohelpkeepthings
straight, we’ll use the superscripts e and d where needed to distinguish the hidden
states of the encoder and the decoder.) The elements of the network on the left
process the input sequence x and comprise the encoder. While our simplified fig-
ureshowsonlyasinglenetworklayerfortheencoder,stackedarchitecturesarethe
norm, where the output states from the top layer of the stack are taken as the fi-
nal representation. A widely used encoder design makes use of stacked biLSTMs
wherethehiddenstatesfromtoplayersfromtheforwardandbackwardpassesare
concatenatedasdescribedinChapter9toprovidethecontextualizedrepresentations
foreachtimestep.
Decoder
y y y y </s>
1 2 3 4
(output is ignored during encoding)
softmax
lh ai yd ed re (sn
)
he 1 he 2 he 3 hh ne n = c = hd 0 h 1d h 2d h 3d h 4d h nd
embedding
layer
x x x x <s> y y y y
1 2 3 n 1 2 3 n
Encoder
Figure9.18 A more formal version of translating a sentence at inference time in the basic RNN-based
encoder-decoder architecture. The final hidden state of the encoder RNN, he, serves as the context for the
n
decoderinitsroleashd inthedecoderRNN.
0
Theentirepurposeoftheencoderistogenerateacontextualizedrepresentation
oftheinput. Thisrepresentationisembodiedinthefinalhiddenstateoftheencoder,
he. Thisrepresentation,alsocalledcforcontext,isthenpassedtothedecoder.
n
Thedecodernetworkontherighttakesthisstateandusesittoinitializethefirst
hidden state of the decoder. That is, the first decoder RNN cell uses c as its prior
hidden state hd. The decoder autoregressively generates a sequence of outputs, an
0
elementatatime,untilanend-of-sequencemarkerisgenerated. Eachhiddenstate
isconditionedontheprevioushiddenstateandtheoutputgeneratedintheprevious
state.
One weakness of this approach as described so far is that the influence of the
context vector, c, will wane as the output sequence is generated. A solution is to
makethecontextvectorcavailableateachstepinthedecodingprocessbyadding
itasaparametertothecomputationofthecurrenthiddenstate,usingthefollowing
equation(illustratedinFig.9.19):
h td =g(yˆ t 1,h td 1,c) (9.32)
− −
Nowwe’rereadytoseethefullequationsforthisversionofthedecoderinthebasic9.8 • ATTENTION 205
y y y
1 2 i
hd 1 hd 2 … hd i …
c …
Figure9.19 Allowingeveryhiddenstateofthedecoder(notjustthefirstdecoderstate)to
beinfluencedbythecontextcproducedbytheencoder.
encoder-decoder model, with context available at each decoding timestep. Recall
thatgisastand-inforsomeflavorofRNNandyˆ istheembeddingfortheoutput
t 1
−
sampledfromthesoftmaxatthepreviousstep:
c = he
n
hd = c
0
hd = g(yˆ ,hd ,c)
t t 1 t 1
− −
z = f(hd)
t t
y t = softmax(z t) (9.33)
Finally,asshownearlier,theoutputyateachtimestepconsistsofasoftmaxcom-
putation over the set of possible outputs (the vocabulary, in the case of language
modelingorMT).Wecomputethemostlikelyoutputateachtimestepbytakingthe
argmaxoverthesoftmaxoutput:
yˆ t = argmax w VP(wx,y 1...y t 1) (9.34)
∈ | −
9.7.1 TrainingtheEncoder-DecoderModel
Encoder-decoderarchitecturesaretrainedend-to-end,justaswiththeRNNlanguage
modelsofChapter9.Eachtrainingexampleisatupleofpairedstrings,asourceand
a target. Concatenated with a separator token, these source-target pairs can now
serveastrainingdata.
ForMT,thetrainingdatatypicallyconsistsofsetsofsentencesandtheirtransla-
tions. Thesecanbedrawnfromstandarddatasetsofalignedsentencepairs,aswe’ll
discussinSection13.2.2. Oncewehaveatrainingset, thetrainingitselfproceeds
aswithanyRNN-basedlanguagemodel. Thenetworkisgiventhesourcetextand
thenstartingwiththeseparatortokenistrainedautoregressivelytopredictthenext
word,asshowninFig.9.20.
Notethedifferencesbetweentraining(Fig.9.20)andinference(Fig.9.17)with
respecttotheoutputsateachtimestep. Thedecoderduringinferenceusesitsown
estimated output yˆ as the input for the next time step x . Thus the decoder will
t t+1
tendtodeviatemoreandmorefromthegoldtargetsentenceasitkeepsgenerating
teacherforcing moretokens. Intraining,therefore,itismorecommontouseteacherforcinginthe
decoder.Teacherforcingmeansthatweforcethesystemtousethegoldtargettoken
fromtrainingasthenextinputx ,ratherthanallowingittorelyonthe(possibly
t+1
erroneous)decoderoutputyˆ. Thisspeedsuptraining.
t
9.8 Attention
Thesimplicityoftheencoder-decodermodelisitscleanseparationoftheencoder—
whichbuildsarepresentationofthesourcetext—fromthedecoder,whichusesthis206 CHAPTER9 • RNNSANDLSTMS
Decoder
llegó la bruja verde </s> gold
answers
y y y y y
1 2 3 4 5
Total loss is the average L = L = L = L = L =
cross-entropy loss per -log 1 P(y) -log 2 P(y) -log 3 P(y) -log 4 P(y) -log 5 P(y) per-word
target word: 1 2 3 4 5 loss
softmax
hidden
layer(s)
embedding
layer
x 1 x 2 x 3 x 4
the green witch arrived <s> llegó la bruja verde
Encoder
Figure9.20 Training the basic RNN encoder-decoder approach to machine translation. Note that in the
decoderweusuallydon’tpropagatethemodel’ssoftmaxoutputsyˆt,butuseteacherforcingtoforceeachinput
tothecorrectgoldvaluefortraining.Wecomputethesoftmaxoutputdistributionoveryˆinthedecoderinorder
tocomputethelossateachtoken,whichcanthenbeaveragedtocomputealossforthesentence.
context to generate a target text. In the model as we’ve described it so far, this
context vector is h , the hidden state of the last (nth) time step of the source text.
n
This final hidden state is thus acting as a bottleneck: it must represent absolutely
everything about the meaning of the source text, since the only thing the decoder
knowsaboutthesourcetextiswhat’sinthiscontextvector(Fig.9.21). Information
atthebeginningofthesentence, especiallyforlongsentences, maynotbeequally
wellrepresentedinthecontextvector.
bottleneck
Encoder Decoder
bottleneck
Figure9.21 Requiringthecontextctobeonlytheencoder’sfinalhiddenstateforcesallthe
informationfromtheentiresourcesentencetopassthroughthisrepresentationalbottleneck.
attention The attention mechanism is a solution to the bottleneck problem, a way of
mechanism
allowing the decoder to get information from all the hidden states of the encoder,
notjustthelasthiddenstate.
Intheattentionmechanism,asinthevanillaencoder-decodermodel,thecontext
vectorcisasinglevectorthatisafunctionofthehiddenstatesoftheencoder,that
is,c= f(he...he). Becausethenumberofhiddenstatesvarieswiththesizeofthe
1 n
input, wecan’tusetheentiretensorofencoderhiddenstatevectorsdirectlyasthe
contextforthedecoder.
Theideaofattentionisinsteadtocreatethesinglefixed-lengthvectorcbytaking
a weighted sum of all the encoder hidden states. The weights focus on (‘attend
to’) a particular part of the source text that is relevant for the token the decoder is
currentlyproducing. Attentionthusreplacesthestaticcontextvectorwithonethat
is dynamically derived from the encoder hidden states, different for each token in9.8 • ATTENTION 207
decoding.
This context vector, c, is generated anew with each decoding step i and takes
i
all of the encoder hidden states into account in its derivation. We then make this
context available during decoding by conditioning the computation of the current
decoderhiddenstateonit(alongwiththepriorhiddenstateandthepreviousoutput
generatedbythedecoder),asweseeinthisequation(andFig.9.22):
hd i = g(yˆ i 1,hd i 1,c i) (9.35)
− −
y y y
1 2 i
hd 1 hd 2 … hd i …
c c c
1 2 i
Figure9.22 The attention mechanism allows each hidden state of the decoder to see a
different,dynamic,context,whichisafunctionofalltheencoderhiddenstates.
Thefirststepincomputingc istocomputehowmuchtofocusoneachencoder
i
state, how relevanteachencoderstateistothedecoderstatecapturedinhd . We
i 1
capturerelevancebycomputing—ateachstateiduringdecoding—ascore(h−d ,he)
i 1 j
foreachencoderstate j. −
dot-product Thesimplestsuchscore,calleddot-productattention,implementsrelevanceas
attention
similarity: measuringhowsimilarthedecoderhiddenstateistoanencoderhidden
state,bycomputingthedotproductbetweenthem:
score(hd ,he) = hd he (9.36)
i −1 j i −1· j
The score that results from this dot product is a scalar that reflects the degree of
similaritybetweenthetwovectors. Thevectorofthesescoresacrossalltheencoder
hiddenstatesgivesustherelevanceofeachencoderstatetothecurrentstepofthe
decoder.
To make use of these scores, we’ll normalize them with a softmax to create a
vectorofweights,α ,thattellsustheproportionalrelevanceofeachencoderhidden
ij
state jtothepriorhiddendecoderstate,hd .
i 1
−
α = softmax(score(hd ,he) j e)
ij i −1 j ∀ ∈
exp(score(hd ,he)
i 1 j
= − (9.37)
exp(score(hd ,he))
k i 1 k
−
Finally,giventhedistributioni(cid:80)nα,wecancomputeafixed-lengthcontextvectorfor
thecurrentdecoderstatebytakingaweightedaverageoveralltheencoderhidden
states.
c i = α ijhe j (9.38)
j
(cid:88)
With this, we finally have a fixed-length context vector that takes into account
informationfromtheentireencoderstatethatisdynamicallyupdatedtoreflectthe
needs of the decoder at each step of decoding. Fig. 9.23 illustrates an encoder-
decodernetworkwithattention,focusingonthecomputationofonecontextvector
c.
i208 CHAPTER9 • RNNSANDLSTMS
Decoder
<latexit sha1_base64="TNdNmv/RIlrhPa6LgQyjjQLqyBA=">AAACAnicdVDLSsNAFJ3UV62vqCtxM1gEVyHpI9Vd0Y3LCvYBTQyT6bSddvJgZiKUUNz4K25cKOLWr3Dn3zhpK6jogQuHc+7l3nv8mFEhTfNDyy0tr6yu5dcLG5tb2zv67l5LRAnHpIkjFvGOjwRhNCRNSSUjnZgTFPiMtP3xRea3bwkXNAqv5SQmboAGIe1TjKSSPP3AEUngjVIHsXiIvJSOpnB4Q7zR1NOLpmGaVbtqQdOwLbtk24qY5Yp9VoOWsjIUwQINT393ehFOAhJKzJAQXcuMpZsiLilmZFpwEkFihMdoQLqKhiggwk1nL0zhsVJ6sB9xVaGEM/X7RIoCISaBrzoDJIfit5eJf3ndRPZP3ZSGcSJJiOeL+gmDMoJZHrBHOcGSTRRBmFN1K8RDxBGWKrWCCuHrU/g/aZUMyzbKV5Vi/XwRRx4cgiNwAixQA3VwCRqgCTC4Aw/gCTxr99qj9qK9zltz2mJmH/yA9vYJSymYCA==</latexit> ↵ ijhe j c i
Xj y i y i+1
attention
.4 .3 .1 .2
weights
↵
ij h<latexit sha1_base64="y8s4mGdpwrGrBnuSR+p1gJJXYdo=">AAAB/nicdVDJSgNBEO2JW4zbqHjy0hgEL4YeJyQBL0EvHiOYBbIMPT09mTY9C909QhgC/ooXD4p49Tu8+Td2FkFFHxQ83quiqp6bcCYVQh9Gbml5ZXUtv17Y2Nza3jF391oyTgWhTRLzWHRcLClnEW0qpjjtJILi0OW07Y4up377jgrJ4uhGjRPaD/EwYj4jWGnJMQ+Cgedk7NSa9IgXq955MKDOrWMWUQnNAFGpYtfsakUTZNtWGUFrYRXBAg3HfO95MUlDGinCsZRdCyWqn2GhGOF0UuilkiaYjPCQdjWNcEhlP5udP4HHWvGgHwtdkYIz9ftEhkMpx6GrO0OsAvnbm4p/ed1U+bV+xqIkVTQi80V+yqGK4TQL6DFBieJjTTARTN8KSYAFJkonVtAhfH0K/yets5JVKdnX5WL9YhFHHhyCI3ACLFAFdXAFGqAJCMjAA3gCz8a98Wi8GK/z1pyxmNkHP2C8fQICDpWK</latexit> d he
i  1· j
lh ai yd ed re (sn ) he 1 he 2 he 3 he n … c i-1 hd i-1 hd i …
c
x x x x i
1 2 3 n
y y
i-1 i
Encoder
Figure9.23 Asketchoftheencoder-decodernetworkwithattention,focusingonthecomputationofc i.The
contextvaluec iisoneoftheinputstothecomputationofhd i.Itiscomputedbytakingtheweightedsumofall
theencoderhiddenstates,eachweightedbytheirdotproductwiththepriordecoderhiddenstatehd .
i 1
−
It’s also possible to create more sophisticated scoring functions for attention
models.Insteadofsimpledotproductattention,wecangetamorepowerfulfunction
thatcomputestherelevanceofeachencoderhiddenstatetothedecoderhiddenstate
byparameterizingthescorewithitsownsetofweights,W .
s
score(hd ,he) = hd W he
i 1 j t 1 s j
− −
TheweightsW,whicharethentrainedduringnormalend-to-endtraining,givethe
s
network the ability to learn which aspects of similarity between the decoder and
encoder states are important to the current application. This bilinear model also
allows the encoder and decoder to use different dimensional vectors, whereas the
simple dot-product attention requires that the encoder and decoder hidden states
havethesamedimensionality.
We’llreturntotheconceptofattentionwhenwedefinedthetransformerarchi-
tecture in Chapter 10, which is based on a slight modification of attention called
self-attention.
9.9 Summary
Thischapterhasintroducedtheconceptsofrecurrentneuralnetworksandhowthey
canbeappliedtolanguageproblems. Here’sasummaryofthemainpointsthatwe
covered:
• InsimpleRecurrentNeuralNetworkssequencesareprocessedoneelementat
atime,withtheoutputofeachneuralunitattimet basedbothonthecurrent
inputatt andthehiddenlayerfromtimet 1.
−
• RNNscanbetrainedwithastraightforwardextensionofthebackpropagation
algorithm,knownasbackpropagationthroughtime(BPTT).
• Simplerecurrentnetworksfailonlonginputsbecauseofproblemslikevan-
ishinggradients;insteadmodernsystemsusemorecomplexgatedarchitec-BIBLIOGRAPHICALANDHISTORICALNOTES 209
tures such as LSTMs that explicitly decide what to remember and forget in
theirhiddenandcontextlayers.
• Commonlanguage-basedapplicationsforRNNsinclude:
– Probabilisticlanguagemodeling: assigningaprobabilitytoasequence,
ortothenextelementofasequencegiventheprecedingwords.
– Auto-regressivegenerationusingatrainedlanguagemodel.
– Sequencelabelinglikepart-of-speechtagging,whereeachelementofa
sequenceisassignedalabel.
– Sequenceclassification,whereanentiretextisassignedtoacategory,as
inspamdetection,sentimentanalysisortopicclassification.
– Encoder-decoder architectures, where an input is mapped to an output
ofdifferentlengthandalignment.
Bibliographical and Historical Notes
InfluentialinvestigationsofRNNswereconductedinthecontextoftheParallelDis-
tributedProcessing(PDP)groupatUCSanDiegointhe1980’s. Muchofthiswork
was directed at human cognitive modeling rather than practical NLP applications
(RumelhartandMcClelland1986c,McClellandandRumelhart1986).Modelsusing
recurrenceatthehiddenlayerinafeedforwardnetwork(Elmannetworks)werein-
troducedbyElman(1990).SimilararchitectureswereinvestigatedbyJordan(1986)
with a recurrence from the output layer, and Mathis and Mozer (1995) with the
addition of a recurrent context layer prior to the hidden layer. The possibility of
unrolling a recurrent network into an equivalent feedforward network is discussed
in(RumelhartandMcClelland,1986c).
Inparallelwithworkincognitivemodeling,RNNswereinvestigatedextensively
in the continuous domain in the signal processing and speech communities (Giles
etal.1994,Robinsonetal.1996). SchusterandPaliwal(1997)introducedbidirec-
tionalRNNsanddescribedresultsontheTIMITphonemetranscriptiontask.
While theoretically interesting, the difficulty with training RNNs and manag-
ing context over long sequences impeded progress on practical applications. This
situation changed with the introduction of LSTMs in Hochreiter and Schmidhuber
(1997) and Gers et al. (2000). Impressive performance gains were demonstrated
on tasks at the boundary of signal processing and language processing including
phoneme recognition (Graves and Schmidhuber, 2005), handwriting recognition
(Gravesetal.,2007)andmostsignificantlyspeechrecognition(Gravesetal.,2013).
InterestinapplyingneuralnetworkstopracticalNLPproblemssurgedwiththe
workofCollobertandWeston(2008)andCollobertetal.(2011).Theseeffortsmade
useoflearnedwordembeddings, convolutionalnetworks, andend-to-endtraining.
Theydemonstratednearstate-of-the-artperformanceonanumberofstandardshared
tasksincludingpart-of-speechtagging,chunking,namedentityrecognitionandse-
manticrolelabelingwithouttheuseofhand-engineeredfeatures.
ApproachesthatmarriedLSTMswithpre-trainedcollectionsofword-embeddings
based on word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014)
quicklycametodominatemanycommontasks: part-of-speechtagging(Lingetal.,
2015),syntacticchunking(SøgaardandGoldberg,2016),namedentityrecognition
(ChiuandNichols,2016;MaandHovy,2016), opinionmining(IrsoyandCardie,210 CHAPTER9 • RNNSANDLSTMS
2014),semanticrolelabeling(ZhouandXu,2015a)andAMRparsing(Folandand
Martin, 2016). As with the earlier surge of progress involving statistical machine
learning,theseadvancesweremadepossiblebytheavailabilityoftrainingdatapro-
videdbyCONLL,SemEval,andothersharedtasks,aswellassharedresourcessuch
asOntonotes(Pradhanetal.,2007b),andPropBank(Palmeretal.,2005).
The modern neural encoder-decoder approach was pioneered by Kalchbrenner
and Blunsom (2013), who used a CNN encoder and an RNN decoder. Cho et al.
(2014) (who coined the name “encoder-decoder”) and Sutskever et al. (2014) then
showed how to use extended RNNs for both encoder and decoder. The idea that a
generative decoder should take as input a soft weighting of the inputs, the central
ideaofattention,wasfirstdevelopedbyGraves(2013)inthecontextofhandwriting
recognition. Bahdanau et al. (2015) extended the idea, named it “attention” and
appliedittoMT.CHAPTER
10 Transformers and Pretrained
Language Models
“Howmuchdoweknowatanytime? Muchmore,orsoIbelieve,thanwe
knowweknow.”
AgathaChristie,TheMovingFinger
Fluent speakers bring an enormous amount of knowledge to bear during compre-
hension and production of language. This knowledge is embodied in many forms,
perhaps most obviously in the vocabulary. That is, in the rich representations as-
sociated with the words we know, including their grammatical function, meaning,
real-world reference, and pragmatic function. This makes the vocabulary a useful
lenstoexploretheacquisitionofknowledgefromtext,bybothpeopleandmachines.
Estimates of the size of adult vocabularies vary widely both within and across
languages. Forexample,estimatesofthevocabularysizeofyoungadultspeakersof
American English range from 30,000 to 100,000 depending on the resources used
to make the estimate and the definition of what it means to know a word. What is
agreeduponisthatthevastmajorityofwordsthatmaturespeakersuseintheirday-
to-dayinteractionsareacquiredearlyinlifethroughspokeninteractionsincontext
with care givers and peers, usually well before the start of formal schooling. This
activevocabularyisextremelylimitedcomparedtothesizeoftheadultvocabulary
(usually on the order of 2000 words for young speakers) and is quite stable, with
very few additional words learned via casual conversation beyond this early stage.
Obviously, this leaves a very large number of words to be acquired by some other
means.
Asimpleconsequenceofthesefactsisthatchildrenhavetolearnabout7to10
words a day, every single day, to arrive at observed vocabulary levels by the time
they are 20 years of age. And indeed empirical estimates of vocabulary growth in
lateelementarythroughhighschoolareconsistentwiththisrate. Howdochildren
achievethisrateofvocabularygrowthgiventheirdailyexperiencesduringthispe-
riod? Weknowthatmostofthisgrowthisnothappeningthroughdirectvocabulary
instruction in school since these methods are largely ineffective, and are not de-
ployedataratethatwouldresultinthereliableacquisitionofwordsattherequired
rate.
The most likely remaining explanation is that the bulk of this knowledge ac-
quisitionhappensasaby-productofreading. Researchintotheaverageamountof
timechildrenspendreading,andthelexicaldiversityofthetextstheyread,indicate
thatitispossibletoachievethedesiredrate. Butthemechanismbehindthisrateof
learningmustberemarkableindeed,sinceatsomepointsduringlearningtherateof
vocabularygrowthexceedstherateatwhichnewwordsareappearingtothelearner!
Many of these facts have motivated approaches to word learning based on the
distributional hypothesis, introduced in Chapter 6. This is the idea that something
about what we’re loosely calling word meanings can be learned even without any
groundingintherealworld,solelybasedonthecontentofthetextswe’veencoun-
teredoverourlives. Thisknowledgeisbasedonthecomplexassociationofwords212 CHAPTER10 • TRANSFORMERSANDPRETRAINEDLANGUAGEMODELS
withthewordstheyco-occurwith(andwiththewordsthatthosewordsoccurwith).
The crucial insight of the distributional hypothesis is that the knowledge that
weacquirethroughthisprocesscanbebroughttobearduringlanguageprocessing
longafteritsinitialacquisitioninnovelcontexts. Ofcourse,addinggroundingfrom
vision or from real-world interaction into such models can help build even more
powerful models, but even text alone is remarkably useful, and we will limit our
attentionheretopurelytextualmodels.
pretraining Inthischapterweformalizethisideaunderthenamepretraining. Wecallpre-
training the process of learning some sort of representation of meaning for words
orsentencesbyprocessingverylargeamountsoftext.Wesaythatwepretrainalan-
guagemodel,andthenwecalltheresultingmodelspretrainedlanguagemodels.
While we have seen that the RNNs or even the FFNs of previous chapters can
be used to learn language models, in this chapter we introduce the most common
transformer architectureforlanguagemodeling: thetransformer.
The transformer offers new mechanisms (self-attention and positional encod-
ings)thathelprepresenttimeandhelpfocusonhowwordsrelatetoeachotherover
longdistances. We’llseehowtoapplythismodeltothetaskoflanguagemodeling,
andthenwe’llseehowatransformerpretrainedonlanguagemodelingcanbeused
inazeroshotmannertoperformotherNLPtasks.
10.1 Self-Attention Networks: Transformers
transformers In this section we introduce the architecture of transformers. Like the LSTMs of
Chapter 9, transformers can handle distant information. But unlike LSTMs, trans-
formers are not based on recurrent connections (which can be hard to parallelize),
whichmeansthattransformerscanbemoreefficienttoimplementatscale.
Transformersmapsequencesofinputvectors(x ,...,x )tosequencesofoutput
1 n
vectors(y ,...,y )ofthesamelength. Transformersaremadeupofstacksoftrans-
1 n
former blocks, each of which is a multilayer network made by combining simple
self-attention linearlayers,feedforwardnetworks,andself-attentionlayers,thekeyinnovationof
transformers. Self-attentionallowsanetworktodirectlyextractanduseinformation
from arbitrarily large contexts without the need to pass it through intermediate re-
currentconnectionsasinRNNs. We’llstartbydescribinghowself-attentionworks
andthenreturntohowitfitsintolargertransformerblocks.
Fig.10.1illustratestheflowofinformationinasinglecausal,orbackwardlook-
ing,self-attentionlayer. Aswiththeoveralltransformer,aself-attentionlayermaps
inputsequences(x ,...,x )tooutputsequencesofthesamelength(y ,...,y ).When
1 n 1 n
processingeachitemintheinput,themodelhasaccesstoalloftheinputsuptoand
including the one under consideration, but no access to information about inputs
beyond the current one. In addition, the computation performed for each item is
independent of all the other computations. The first point ensures that we can use
thisapproachtocreatelanguagemodelsandusethemforautoregressivegeneration,
and the second point means that we can easily parallelize both forward inference
andtrainingofsuchmodels.
Atthecoreofanattention-basedapproachistheabilitytocompareanitemof
interest to a collection of other items in a way that reveals their relevance in the
current context. In the case of self-attention, the set of comparisons are to other
elementswithinagivensequence. Theresultofthesecomparisonsisthenusedto
compute an output for the current input. For example, returning to Fig. 10.1, the10.1 • SELF-ATTENTIONNETWORKS: TRANSFORMERS 213
y y y y y
1 2 3 4 5
Self-Attention
Layer
x x x x x
1 2 3 4 5
Figure10.1 Informationflowinacausal(ormasked)self-attentionmodel. Inprocessing
each element of the sequence, the model attends to all the inputs up to, and including, the
current one. Unlike RNNs, the computations at each time step are independent of all the
otherstepsandthereforecanbeperformedinparallel.
computation of y is based on a set of comparisons between the input x and its
3 3
preceding elements x and x , and to x itself. The simplest form of comparison
1 2 3
betweenelementsinaself-attentionlayerisadotproduct. Let’srefertotheresult
ofthiscomparisonasascore(we’llbeupdatingthisequationtoaddattentiontothe
computationofthisscore):
score(x i,x j) = x i x j (10.1)
·
The result of a dot product is a scalar value ranging from ∞ to ∞, the larger
−
thevaluethemoresimilarthevectorsthatarebeingcompared. Continuingwithour
example, the first step in computing y would be to compute three scores: x x ,
3 3 1
·
x x andx x . Thentomakeeffectiveuseofthesescores,we’llnormalizethem
3 2 3 3
· ·
with a softmax to create a vector of weights, α , that indicates the proportional
ij
relevanceofeachinputtotheinputelementithatisthecurrentfocusofattention.
α ij = softmax(score(x i,x j)) j i (10.2)
∀ ≤
exp(score(x,x ))
i j
= j i (10.3)
i exp(score(x,x )) ∀ ≤
k=1 i k
Giventheproportionalsco(cid:80)resinα,wethengenerateanoutputvaluey ibytaking
thesumoftheinputsseensofar,weightedbytheirrespectiveα value.
y i = α ijx j (10.4)
j i
(cid:88)≤
The steps embodied in Equations 10.1 through 10.4 represent the core of an
attention-based approach: a set of comparisons to relevant items in some context,
anormalizationofthosescorestoprovideaprobabilitydistribution, followedbya
weighted sum using this distribution. The output y is the result of this straightfor-
wardcomputationovertheinputs.
This kind of simple attention can be useful, and indeed we saw in Chapter 9
how to use this simple idea of attention for LSTM-based encoder-decoder models
formachinetranslation.
But transformers allow us to create a more sophisticated way of representing
howwordscancontributetotherepresentationoflongerinputs. Considerthethree
different roles that each input embedding plays during the course of the attention
process.214 CHAPTER10 • TRANSFORMERSANDPRETRAINEDLANGUAGEMODELS
• As the current focus of attention when being compared to all of the other
query precedinginputs. We’llrefertothisroleasaquery.
• Initsroleasaprecedinginputbeingcomparedtothecurrentfocusofatten-
key tion. We’llrefertothisroleasakey.
value • And finally, as a value used to compute the output for the current focus of
attention.
To capture these three different roles, transformers introduce weight matrices
WQ,WK,andWV. Theseweightswillbeusedtoprojecteachinputvectorx into
i
arepresentationofitsroleasakey,query,orvalue.
q i=WQx i; k i=WKx i; v i=WVx i (10.5)
Theinputsxandoutputsyoftransformers,aswellastheintermediatevectorsafter
thevariouslayers,allhavethesamedimensionality1 d. Fornowlet’sassumethe
×
dimensionalitiesofthetransformmatricesareWQ Rd ×d,WK Rd ×d,andWV
∈ ∈ ∈
Rd ×d. Laterwe’llneedseparatedimensionsforthesematriceswhenweintroduce
multi-headedattention,solet’sjustmakeanotethatwe’llhaveadimensiond for
k
thekeyandqueryvectors,andadimensiond forthevaluevectors,bothofwhich
v
fornowwe’llsettod.Intheoriginaltransformerwork(Vaswanietal.,2017),dwas
1024.
Giventheseprojections, thescorebetweenacurrentfocusofattention, x, and
i
anelementintheprecedingcontext,x ,consistsofadotproductbetweenitsquery
j
vectorq andtheprecedingelement’skeyvectorsk . Thisdotproducthastheright
i j
shapesinceboththequeryandthekeyareofdimensionality1 d. Let’supdateour
×
previouscomparisoncalculationtoreflectthis,replacingEq.10.1withEq.10.6:
score(x i,x j) = q i k j (10.6)
·
The ensuing softmax calculation resulting in α remains the same, but the output
i,j
calculationfory isnowbasedonaweightedsumoverthevaluevectorsv.
i
y i = α ijv j (10.7)
j i
(cid:88)≤
Fig.10.2illustratesthiscalculationinthecaseofcomputingthethirdoutputy ina
3
sequence.
Theresultofadotproductcanbeanarbitrarilylarge(positiveornegative)value.
Exponentiatingsuchlargevaluescanleadtonumericalissuesandtoaneffectiveloss
of gradients during training. To avoid this, the dot product needs to be scaled in a
suitablefashion. Ascaleddot-productapproachdividestheresultofthedotproduct
by a factor related to the size of the embeddings before passing them through the
softmax. A typical approach is to divide the dot product by the square root of the
dimensionality of the query and key vectors (d ), leading us to update our scoring
k
functiononemoretime,replacingEq.10.1andEq.10.6withEq.10.8:
q k
score(x i,x j) = i · j (10.8)
√d
k
This description of the self-attention process has been from the perspective of
computingasingleoutputatasingletimestepi. However,sinceeachoutput,y,is
i
computedindependentlythisentireprocesscanbeparallelizedbytakingadvantage
ofefficientmatrixmultiplicationroutinesbypackingtheinputembeddingsoftheN10.1 • SELF-ATTENTIONNETWORKS: TRANSFORMERS 215
y
3
Output Vector
Weight and Sum
value vectors
Softmax
Key/Query
Comparisons
Wk k Wk k Wk k
Generate
Wq q Wq q Wq q
key, query, value
vectors x Wv v x Wv v x Wv v
1 2 3
Figure10.2 Calculatingthevalueofy 3,thethirdelementofasequenceusingcausal(left-
to-right)self-attention.
tokensoftheinputsequenceintoasinglematrixX RN ×d. Thatis,eachrowofX
∈
istheembeddingofonetokenoftheinput. WethenmultiplyXbythekey,query,
and value matrices (all of dimensionality d d) to produce matrices Q RN ×d,
× ∈
K RN ×d,andV RN ×d,containingallthekey,query,andvaluevectors:
∈ ∈
Q=XWQ; K=XWK; V=XWV (10.9)
Giventhesematriceswecancomputealltherequisitequery-keycomparisonssimul-
taneouslybymultiplyingQandK(cid:124)
inasinglematrixmultiplication(theproductis
of shape N N; Fig. 10.3 shows a visualization). Taking this one step further, we
×
canscalethesescores,takethesoftmax,andthenmultiplytheresultbyVresulting
inamatrixofshapeN d: avectorembeddingrepresentationforeachtokeninthe
×
input.We’vereducedtheentireself-attentionstepforanentiresequenceofNtokens
tothefollowingcomputation:
(cid:124)
QK
SelfAttention(Q,K,V) = softmax V (10.10)
√d
(cid:18) k (cid:19)
Unfortunately,thisprocessgoesabittoofarsincethecalculationofthecomparisons
(cid:124)
in QK results in a score for each query value to every key value, including those
that follow the query. This is inappropriate in the setting of language modeling
sinceguessingthenextwordisprettysimpleifyoualreadyknowit. Tofixthis,the
elements in the upper-triangular portion of the matrix are zeroed out (set to ∞),
−
thus eliminating any knowledge of words that follow in the sequence. Fig. 10.3
× ×216 CHAPTER10 • TRANSFORMERSANDPRETRAINEDLANGUAGEMODELS
(cid:124)
depictstheQK matrix. (we’llseeinChapter11howtomakeuseofwordsinthe
futurefortasksthatneedit).
q1•k1 −∞ −∞ −∞ −∞
q2•k1 q2•k2 −∞ −∞ −∞
N q3•k1 q3•k2 q3•k3 −∞ −∞
q4•k1 q4•k2 q4•k3 q4•k4 −∞
q5•k1 q5•k2 q5•k3 q5•k4 q5•k5
N
(cid:124)
Figure10.3 The N N QK matrix showing the qi kj values, with the upper-triangle
× ·
portion of the comparisons matrix zeroed out (set to ∞, which the softmax will turn to
−
zero).
Fig.10.3alsomakesitclearthatattentionisquadraticinthelengthoftheinput,
sinceateachlayerweneedtocomputedotproductsbetweeneachpairoftokensin
theinput.Thismakesitextremelyexpensivefortheinputtoatransformertoconsist
oflongdocuments(likeentireWikipediapages,ornovels),andsomostapplications
havetolimittheinputlength,forexampletoatmostapageoraparagraphoftextata
time. Findingmoreefficientattentionmechanismsisanongoingresearchdirection.
10.1.1 TransformerBlocks
The self-attention calculation lies at the core of what’s called a transformer block,
which,inadditiontotheself-attentionlayer,includesadditionalfeedforwardlayers,
residual connections, and normalizing layers. The input and output dimensions of
these blocks are matched so they can be stacked just as was the case for stacked
RNNs.
yn
Transformer Layer Normalize
Block +
Residual
connection Feedforward Layer
Layer Normalize
+
Residual
connection Self-Attention Layer
x1 x2 x3 … xn
Figure10.4 Atransformerblockshowingallthelayers.
Fig.10.4illustratesastandardtransformerblockconsistingofasingleattention10.1 • SELF-ATTENTIONNETWORKS: TRANSFORMERS 217
layer followed by a fully-connected feedforward layer with residual connections
andlayernormalizationsfollowingeach. We’vealreadyseenfeedforwardlayersin
Chapter 7, but what are residual connections and layer norm? In deep networks,
residual connections are connections that pass information from a lower layer to a
higher layer without going through the intermediate layer. Allowing information
fromtheactivationgoingforwardandthegradientgoingbackwardstoskipalayer
improves learning and gives higher level layers direct access to information from
lowerlayers(Heetal.,2016).Residualconnectionsintransformersareimplemented
byaddingalayer’sinputvectortoitsoutputvectorbeforepassingitforward. Inthe
transformer block shown in Fig. 10.4, residual connections are used with both the
attention and feedforward sublayers. These summed vectors are then normalized
usinglayernormalization(Baetal.,2016). Ifwethinkofalayerasonelongvector
ofunits,theresultingfunctioncomputedinatransformerblockcanbeexpressedas:
z = LayerNorm(x+SelfAttention(x)) (10.11)
y = LayerNorm(z+FFN(z)) (10.12)
layernorm Layer normalization (or layer norm) is one of many forms of normalization that
can be used to improve training performance in deep neural networks by keeping
thevaluesofahiddenlayerinarangethatfacilitatesgradient-basedtraining. Layer
norm is a variation of the standard score, or z-score, from statistics applied to a
singlehiddenlayer. Thefirststepinlayernormalizationistocalculatethemean,µ,
andstandarddeviation,σ,overtheelementsofthevectortobenormalized. Given
ahiddenlayerwithdimensionalityd ,thesevaluesarecalculatedasfollows.
h
1
dh
µ = x i (10.13)
d
h
i=1
(cid:88)
1
dh
σ = (cid:118)d (x i −µ)2 (10.14)
(cid:117)
(cid:117)
h (cid:88)i=1
(cid:116)
Giventhesevalues,thevectorcomponentsarenormalizedbysubtractingthemean
fromeachanddividingbythestandarddeviation. Theresultofthiscomputationis
anewvectorwithzeromeanandastandarddeviationofone.
(x µ)
ˆx= − (10.15)
σ
Finally, in the standard implementation of layer normalization, two learnable
parameters,γ andβ,representinggainandoffsetvalues,areintroduced.
LayerNorm=γˆx+β (10.16)
10.1.2 MultiheadAttention
Thedifferentwordsinasentencecanrelatetoeachotherinmanydifferentwayssi-
multaneously. Forexample,distinctsyntactic,semantic,anddiscourserelationships
canholdbetweenverbsandtheirargumentsinasentence. Itwouldbedifficultfor
a single transformer block to learn to capture all of the different kinds of parallel
relations among its inputs. Transformers address this issue with multihead self-
multihead
self-attention attentionlayers. Thesearesetsofself-attentionlayers,calledheads,thatresidein
layers
parallel layers at the same depth in a model, each with its own set of parameters.218 CHAPTER10 • TRANSFORMERSANDPRETRAINEDLANGUAGEMODELS
Giventhesedistinctsetsofparameters,eachheadcanlearndifferentaspectsofthe
relationshipsthatexistamonginputsatthesamelevelofabstraction.
Toimplementthisnotion,eachhead,i,inaself-attentionlayerisprovidedwith
its own set of key, query and value matrices: WK, WQ and WV. These are used
i i i
toprojecttheinputsintoseparatekey, value, andqueryembeddingsseparatelyfor
eachhead,withtherestoftheself-attentioncomputationremainingunchanged. In
multi-headattention,insteadofusingthemodeldimensiondthat’susedfortheinput
andoutputfromthemodel,thekeyandqueryembeddingshavedimensionalityd ,
k
andthevalueembeddingsareofdimensionalityd (intheoriginaltransformerpaper
v
d
k
=d
v
=64). Thus for each head i, we have weight layers W iQ ∈Rd ×dk, W iK
∈
Rd ×dk, and WV
i
∈Rd ×dv, and these get multiplied by the inputs packed into X to
produceQ RN ×dk,K RN ×dk,andV RN ×dv. Theoutputofeachofthehheads
∈ ∈ ∈
isofshapeN d , andsotheoutputofthemulti-headlayerwithhheadsconsists
v
×
of h vectors of shape N d . To make use of these vectors in further processing,
v
×
they are combined and then reduced down to the original input dimension d. This
is accomplished by concatenating the outputs from each head and then using yet
anotherlinearprojection,WO Rhdv×d,toreduceittotheoriginaloutputdimension
∈
foreachtoken,oratotalN d output.
×
MultiHeadAttention(X)=(head 1 head 2... head h)WO (10.17)
⊕ ⊕
Q=XWQ; K=XWK ; V=XWV (10.18)
i i i
head i=SelfAttention(Q,K,V) (10.19)
Fig. 10.5 illustrates this approach with 4 self-attention heads. This multihead
layerreplacesthesingleself-attentionlayerinthetransformerblockshownearlier
in Fig. 10.4. The rest of the transformer block with its feedforward layer, residual
connections,andlayernormsremainsthesame.
yn
Project down to d WO
Concatenate
head1 head2 head3 head4
Outputs
WQ 4, WK 4, WV 4 Head 4
Multihead
Attention WQ 3, WK 3, WV 3 Head 3
Layer WQ 2, WK 2, WV 2 Head 2
WQ 1, WK 1, WV 1 Head 1
X
x1 x2 x3 … xn
Figure10.5 Multihead self-attention: Each of the multihead self-attention layers is provided with its own
setof key, query andvalue weightmatrices. The outputsfrom eachof thelayersare concatenatedand then
projecteddowntod,thusproducinganoutputofthesamesizeastheinputsolayerscanbestacked.10.1 • SELF-ATTENTIONNETWORKS: TRANSFORMERS 219
10.1.3 Modelingwordorder: positionalembeddings
How does a transformer model the position of each token in the input sequence?
WithRNNs,informationabouttheorderoftheinputswasbuiltintothestructureof
themodel. Unfortunately,thesameisn’ttruefortransformers;themodelsaswe’ve
described them so far don’t have any notion of the relative, or absolute, positions
of the tokens in the input. This can be seen from the fact that if you scramble the
orderoftheinputsintheattentioncomputationinFig.10.2yougetexactlythesame
answer.
Onesimplesolutionistomodifytheinputembeddingsbycombiningthemwith
positional positionalembeddingsspecifictoeachpositioninaninputsequence.
embeddings
Wheredowegetthesepositionalembeddings? Thesimplestmethodistostart
withrandomlyinitializedembeddingscorrespondingtoeachpossibleinputposition
up to some maximum length. For example, just as we have an embedding for the
wordfish, we’llhaveanembeddingfortheposition3. Aswithwordembeddings,
thesepositionalembeddingsarelearnedalongwithotherparametersduringtraining.
Toproduceaninputembeddingthatcapturespositionalinformation,wejustaddthe
word embedding for each input to its corresponding positional embedding. (We
don’t concatenate the two embeddings, we just add them to produce a new vector
of the same dimensionality.). This new embedding serves as the input for further
processing. Fig.10.6showstheidea.
Transformer
Blocks
Composite
Embeddings
(input + position)
Word
Embeddings
Position
Embeddings
Janet will back the bill
Figure10.6 Asimplewaytomodelposition: simplyaddinganembeddingrepresentation
oftheabsolutepositiontotheinputwordembeddingtoproduceanewembeddingofthesame
dimenionality.
A potential problem with the simple absolute position embedding approach is
thattherewillbeplentyoftrainingexamplesfortheinitialpositionsinourinputsand
correspondingly fewer at the outer length limits. These latter embeddings may be
poorlytrainedandmaynotgeneralizewellduringtesting.Analternativeapproachto
positionalembeddingsistochooseastaticfunctionthatmapsintegerinputstoreal-
valuedvectorsinawaythatcapturestheinherentrelationshipsamongthepositions.
That is, it captures the fact that position 4 in an input is more closely related to
position5thanitistoposition17. Acombinationofsineandcosinefunctionswith
differingfrequencieswasusedintheoriginaltransformerwork. Developingbetter
positionrepresentationsisanongoingresearchtopic.
Janet
+
1
will
+
2
back
+
3
the
+
4
bill
+
5220 CHAPTER10 • TRANSFORMERSANDPRETRAINEDLANGUAGEMODELS
10.2 Transformers as Language Models
Nowthatwe’veseenallthemajorcomponentsoftransformers, let’sexaminehow
to deploy them as language models via self-supervised learning. To do this, we’ll
usethesameself-supervisionmodelweusedfortrainingRNNlanguagemodelsin
Chapter 9. Given a training corpus of plain text we’ll train the model autoregres-
sively to predict the next token in a sequence y, using cross-entropy loss. Recall
t
from Eq. 9.11 that the cross-entropy loss for language modeling is determined by
theprobabilitythemodelassignstothecorrectnextword. Soattimet theCEloss
is the negative log probability the model assigns to the next word in the training
sequence:
L CE(yˆ t,y t) = logyˆ t[w t+1] (10.20)
−
teacherforcing Asinthatcase,weuseteacherforcing. Recallthatinteacherforcing,ateachtime
stepindecodingweforcethesystemtousethegoldtargettokenfromtrainingasthe
next input x , rather than allowing it to rely on the (possibly erroneous) decoder
t+1
outputyˆ.
t
Fig. 10.7 illustrates the general training approach. At each step, given all the
preceding words, the final transformer layer produces an output distribution over
theentirevocabulary. Duringtraining,theprobabilityassignedtothecorrectword
is used to calculate the cross-entropy loss for each item in the sequence. As with
RNNs, the loss for a training sequence is the average cross-entropy loss over the
entiresequence.
Next word long and thanks for all
Loss … =
Softmax over
Vocabulary
Linear Layer
Transformer
…
Block
Input …
Embeddings
So long and thanks for
Figure10.7 Trainingatransformerasalanguagemodel.
Note the key difference between this figure and the earlier RNN-based version
shown in Fig. 9.6. There the calculation of the outputs and the losses at each step
was inherently serial given the recurrence in the calculation of the hidden states.
Withtransformers, eachtrainingitemcanbeprocessedinparallelsincetheoutput
foreachelementinthesequenceiscomputedseparately.
Once trained, we can autoregressively generate novel text just as with RNN-
basedmodels. RecallfromSection9.3.3thatusingalanguagemodeltoincremen-
tallygeneratewordsbyrepeatedlysamplingthenextwordconditionedonourpre-
autoregressive viouschoicesiscalledautoregressivegenerationorcausalLMgeneration.
generation10.3 • SAMPLING 221
RecallbackinChapter3wesawhowtogeneratetextfromann-gramlanguage
modelbyadaptingasamplingtechniquesuggestedataboutthesametimebyClaude
Shannon (Shannon, 1951) and the psychologists George Miller and Jennifer Self-
ridge (Miller and Selfridge, 1950). We first randomly sample a word to begin a
sequence based on its suitability as the start of a sequence. We then continue to
samplewordsconditionedonourpreviouschoicesuntilwereachapre-determined
length,oranendofsequencetokenisgenerated.
TheprocedureforgenerationfromtransformerLMsisbasicallythesameasthat
describedonpage40,butadaptedtoaneuralcontext:
• Sample a word in the output from the softmax distribution that results from
usingthebeginningofsentencemarker,<s>,asthefirstinput.
• Usethewordembeddingforthatfirstwordastheinputtothenetworkatthe
nexttimestep,andthensamplethenextwordinthesamefashion.
• Continuegeneratinguntiltheendofsentencemarker, </s>, issampledora
fixedlengthlimitisreached.
Technicallyanautoregressivemodelisamodelthatpredictsavalueattimetbased
onalinearfunctionofthepreviousvaluesattimest 1,t 2,andsoon. Although
− −
languagemodelsarenotlinear(sincetheyhavemanylayersofnon-linearities),we
loosely refer to this generation technique as autoregressive generation since the
wordgeneratedateachtimestepisconditionedonthewordselectedbythenetwork
fromthepreviousstep.
The use of a language model to generate text is one of the areas in which the
impact of neural language models on NLP has been the largest. Text generation,
alongwithimagegenerationandcodegeneration,constituteanewareaofAIthatis
oftencalledgenerativeAI.
Moreformally,forgeneratingfromatrainedlanguagemodel,ateachtimestep
in decoding, the output y is chosen by computing a softmax over the set of pos-
t
sibleoutputs(thevocabulary)andthenchoosingthehighestprobabilitytoken(the
argmax):
yˆ t = argmax w VP(wy 1...y t 1) (10.21)
∈ | −
greedy Choosingthesinglemostprobabletokentogenerateateachstepiscalledgreedy
decoding; a greedy algorithm is one that make a choice that is locally optimal,
whether or not it will turn out to have been the best choice with hindsight. We’ll
seeinfollowingsectionsthatthereareotheroptionstogreedydecoding.
10.3 Sampling
TBD:nucleus,topk,temperaturesampling.
10.4 Beam Search
Greedy search is not optimal, and may not find the highest probability translation.
Theproblemisthatthetokenthatlooksgoodtothedecodernowmightturnoutlater
tohavebeenthewrongchoice!222 CHAPTER10 • TRANSFORMERSANDPRETRAINEDLANGUAGEMODELS
searchtree Let’s see this by looking at the search tree, a graphical representation of the
choices the decoder makes in generating the next token. in which we view the
decoding problem as a heuristic state-space search and systematically explore the
spaceofpossibleoutputs. Insuchasearchtree,thebranchesaretheactions,inthis
case the action of generating a token, and the nodes are the states, in this case the
state of having generated a particular prefix. We are searching for the best action
sequence,i.e. thetargetstringwiththehighestprobability. Fig.10.8demonstrates
theproblem, usingamade-upexample. Noticethatthemostprobablesequenceis
okok</s>(withaprobabilityof.4*.7*1.0),butagreedysearchalgorithmwillfail
tofindit,becauseitincorrectlychoosesyesasthefirstwordsinceithasthehighest
localprobability.
p(t | t ,t )
3 1 2
p(t | t )
2 1 ok 1.0 </s>
.7
yes 1.0 </s>
p(t |start) .2
1
ok .1 </s>
.4
start .5 yes .3 ok 1.0 </s>
.1 .4
</s> .2 yes 1.0 </s>
</s>
t t t
1 2 3
Figure10.8 AsearchtreeforgeneratingthetargetstringT =t 1,t 2,...fromthevocabulary
V = yes,ok,<s> ,showingtheprobabilityofgeneratingeachtokenfromthatstate.Greedy
{ }
searchwouldchooseyesatthefirsttimestepfollowedbyyes,insteadofthegloballymost
probablesequenceokok.
Recall from Chapter 8 that for part-of-speech tagging we used dynamic pro-
gramming search (the Viterbi algorithm) to address this problem. Unfortunately,
dynamicprogrammingisnotapplicabletogenerationproblemswithlong-distance
dependenciesbetweentheoutputdecisions. Theonlymethodguaranteedtofindthe
bestsolutionisexhaustivesearch: computingtheprobabilityofeveryoneoftheVT
possiblesentences(forsomelengthvalueT)whichisobviouslytooslow.
Instead, decoding in sequence generation problems generally uses a method
beamsearch calledbeamsearch. Inbeamsearch,insteadofchoosingthebesttokentogenerate
at each timestep, we keep k possible tokens at each step. This fixed-size memory
beamwidth footprintkiscalledthebeamwidth,onthemetaphorofaflashlightbeamthatcan
beparameterizedtobewiderornarrower.
Thusatthefirststepofdecoding,wecomputeasoftmaxovertheentirevocab-
ulary,assigningaprobabilitytoeachword. Wethenselectthek-bestoptionsfrom
thissoftmaxoutput. Theseinitialkoutputsarethesearchfrontierandthesekinitial
wordsarecalledhypotheses. Ahypothesisisanoutputsequence,atranslation-so-
far,togetherwithitsprobability.
At subsequent steps, each of the k best hypotheses is extended incrementally
bybeingpassedtodistinctdecoders,whicheachgenerateasoftmaxovertheentire
vocabularytoextendthehypothesistoeverypossiblenexttoken.Eachofthesek V
∗
hypothesesisscoredbyP(y x,y ): theproductoftheprobabilityofcurrentword
i <i
|
choicemultipliedbytheprobabilityofthepaththatledtoit.Wethenprunethek V
∗
hypothesesdowntothekbesthypotheses,sotherearenevermorethankhypotheses10.4 • BEAMSEARCH 223
arrived y
2
the green y 3
hd1 hd2 y 2 y 3
y
1 a a
y
1 EOS arrived …
hd1 hd2 hd2
…
aardvark mage
a .. EOS the green ..
… the the
hd1 aardvark .. ..
witch witch
..
EOS … …
start arrived zebra zebra
..
the
… y 2 y 3
zebra a arrived
… …
aardvark aardvark
the y 2 .. ..
green green
.. ..
witch who
hd1 hd2 ze… bra the witch y 3 ze… bra
EOS the
hd1 hd2 hd2
t1 t2 EOS the witch t3
Figure10.9 Beam search decoding with a beam width of k=2. At each time step, we choose the k best
hypotheses,computetheV possibleextensionsofeachhypothesis,scoretheresultingk V possiblehypotheses
∗
andchoosethebestktocontinue. Attime1,thefrontierisfilledwiththebest2optionsfromtheinitialstate
ofthedecoder:arrivedandthe.Wethenextendeachofthose,computetheprobabilityofallthehypothesesso
far(arrivedthe,arrivedaardvark,thegreen,thewitch)andcomputethebest2(inthiscasethegreenandthe
witch)tobethesearchfrontiertoextendonthenextstep.Onthearcsweshowthedecodersthatweruntoscore
theextensionwords(althoughforsimplicitywehaven’tshownthecontextvaluecithatisinputateachstep).
atthefrontierofthesearch,andnevermorethankdecoders.
Fig. 10.9 illustrates this process with a beam width of 2 for the start of our
sentenceweusedinChapter9andwillcontinuetouseinChapter13tointroduce
machinetranslation,(Thegreenwitcharrived).
Thisprocesscontinuesuntila</s>isgeneratedindicatingthatacompletecan-
didate output has been found. At this point, the completed hypothesis is removed
fromthefrontierandthesizeofthebeamisreducedbyone. Thesearchcontinues
untilthebeamhasbeenreducedto0. Theresultwillbekhypotheses.
Let’sseehowthescoringworksindetail,scoringeachnodebyitslogprobabil-
ity. RecallfromEq.9.31thatwecanusethechainruleofprobabilitytobreakdown
p(yx)intotheproductoftheprobabilityofeachwordgivenitspriorcontext,which
|
wecanturnintoasumoflogs(foranoutputstringoflengtht):
score(y) = logP(yx)
|
= log(P(y x)P(y y ,x)P(y y ,y ,x)...P(y y ,...,y ,x))
1 2 1 3 1 2 t 1 t 1
| | | | −
t
= logP(y i y 1,...,y i 1,x) (10.22)
| −
i=1
(cid:88)224 CHAPTER10 • TRANSFORMERSANDPRETRAINEDLANGUAGEMODELS
Thus at each step, to compute the probability of a partial sentence, we simply add
thelogprobabilityoftheprefixsentencesofartothelogprobabilityofgenerating
the next token. Fig. 10.10 shows the scoring for the example sentence shown in
Fig.10.9,usingsomesimplemade-upprobabilities. Logprobabilitiesarenegative
or0,andthemaxoftwologprobabilitiesistheonethatisgreater(closerto0).
log P (arrived the|x) log P (“the green witch arrived”|x)
= -2.3 = log P (the|x) + log P(green|the,x)
+ log P(witch | the, green,x)
the +logP(arrived|the,green,witch,x)
+log P(END|the,green,witch,arrived,x)
-2.7
log P(arrived|x) -.69 log P(arrived witch|x) -3.2
=-1.6 = -3.9 mage -2.5 END
arrived -2.3 witch -2.1 -.22
arrived
-4.8
-1.6 -1.6 -2.3
log P(the green|x) -.36 -3.7 at
start log P(the|x) = -1.6 witch -1.6 came
green -.51
-.92 =-.92 -.69
the -2.7
log P(the witch|x)
-2.2 END
-1.2 = -2.1 -.51
witch -.11 arrived
-1.61 -3.8
-2.3
-4.4 by
who
log P(y |x) log P(y |y ,x) log P(y |y ,y ,x) log P(y|y ,y ,y ,x) log P(y|y,y ,y ,y ,x)
1 2 1 3 2 1 4 3 2 1 5 4 3 2 1
y y y y y
1 2 3 4 5
Figure10.10 Scoringforbeamsearchdecodingwithabeamwidthofk=2.Wemaintainthelogprobability
ofeachhypothesisinthebeambyincrementallyaddingthelogprobofgeneratingeachnexttoken.Onlythetop
kpathsareextendedtothenextstep.
Fig.10.11givesthealgorithm.
Oneproblemarisesfromthefactthatthecompletedhypothesesmayhavediffer-
entlengths. Becausemodelsgenerallyassignlowerprobabilitiestolongerstrings,
a naive algorithm would also choose shorter strings for y. This was not an issue
duringtheearlierstepsofdecoding; duetothebreadth-firstnatureofbeamsearch
allthehypothesesbeingcomparedhadthesamelength. Theusualsolutiontothisis
toapplysomeformoflengthnormalizationtoeachofthehypotheses,forexample
simplydividingthenegativelogprobabilitybythenumberofwords:
t
1
score(y)= logP(yx) = logP(y i y 1,...,y i 1,x) (10.23)
− | T − | −
i=1
(cid:88)
Beam search is common in language model generation tasks. For tasks like MT,
whichwe’llreturntoinChapter13,wegenerallyusebeamwidthskbetween5and
10. What do we do with the resulting k hypotheses? In some cases, all we need
fromourlanguagemodelgenerationalgorithmisthesinglebesthypothesis,sowe
canreturnthat. Inothercasesourdownstreamapplicationmightwanttolookatall
k hypotheses, so we can pass them all (or a subset) to the downstream application
withtheirrespectivescores.10.5 • PRETRAININGLARGELANGUAGEMODELS 225
functionBEAMDECODE(c,beam width)returnsbestpaths
y ,h 0
0 0
←
path ()
←
complete paths ()
←
state (c,y ,h ,path) ;initialstate
0 0
←
frontier state ;initialfrontier
←(cid:104) (cid:105)
whilefrontiercontainsincompletepathsandbeamwidth>0
extended frontier
←(cid:104)(cid:105)
foreachstate frontierdo
∈
y DECODE(state)
←
foreachwordi Vocabularydo
∈
successor NEWSTATE(state,i,yi)
←
extended frontier ADDTOBEAM(successor,extended frontier,
←
beam width)
foreachstateinextended frontierdo
ifstateiscompletedo
complete paths APPEND(complete paths,state)
←
extended frontier REMOVE(extended frontier,state)
←
beam width beam width-1
←
frontier extended frontier
←
returncompleted paths
functionNEWSTATE(state,word,word prob)returnsnewstate
functionADDTOBEAM(state,frontier,width)returnsupdatedfrontier
ifLENGTH(frontier)<widththen
frontier INSERT(state,frontier)
←
elseifSCORE(state)>SCORE(WORSTOF(frontier))
frontier REMOVE(WORSTOF(frontier))
←
frontier INSERT(state,frontier)
←
returnfrontier
Figure10.11 Beamsearchdecoding.
10.5 Pretraining Large Language Models
TBD:corpora,etc.
10.6 Language Models for Zero-shot Learning
TBD:HowtorecastNLPtasksaswordprediction,simplepromptingexamples(and
thentobecontinuedinChapter12)226 CHAPTER10 • TRANSFORMERSANDPRETRAINEDLANGUAGEMODELS
10.7 Potential Harms from Language Models
Large pretrained neural language models exhibit many of the potential harms dis-
cussed in Chapter 4 and Chapter 6. Many of these harms become realized when
pretrained language models are fine-tuned to downstream tasks, particularly those
involving text generation, such as in assistive technologies like web search query
completion,orpredictivetypingforemail(Olteanuetal.,2020).
For example, language models can generate toxic language. Gehman et al.
(2020)showthatmanykindsofcompletelynon-toxicpromptscannonethelesslead
large language models to output hate speech and abuse. Brown et al. (2020) and
Shengetal.(2019)showedthatlargelanguagemodelsgeneratesentencesdisplay-
ingnegativeattitudestowardminorityidentitiessuchasbeingBlackorgay.
Indeed,languagemodelsarebiasedinanumberofwaysbythedistributionsof
theirtrainingdata. Gehmanetal.(2020)showsthatlargelanguagemodeltraining
datasets include toxic text scraped from banned sites, such as Reddit communities
that have been shut down by Reddit but whose data may still exist in dumps. In
additiontoproblemsoftoxicity,internetdataisdisproportionatelygeneratedbyau-
thors from developed countries, and many large language models trained on data
fromReddit,whoseauthorsskewmaleandyoung. Suchbiasedpopulationsamples
likelyskewtheresultinggenerationawayfromtheperspectivesortopicsofunder-
represented populations. Furthermore, language models can amplify demographic
andotherbiasesintrainingdata,justaswesawforembeddingmodelsinChapter6.
Language models can also be a tool for generating text for misinformation,
phishing, radicalization, and other socially harmful activities (Brown et al., 2020).
McGuffieandNewhouse(2020)showhowlargelanguagemodelsgeneratetextthat
emulates online extremists, with the risk of amplifying extremist movements and
theirattempttoradicalizeandrecruit.
Finally,thereareimportantprivacyissues.Languagemodels,likeothermachine
learning models, can leak information about their training data. It is thus possible
for an adversary to extract individual training-data phrases from a language model
suchasanindividualperson’sname, phonenumber, andaddress(Hendersonetal.
2017,Carlinietal.2020). Thisisaproblemiflargelanguagemodelsaretrainedon
privatedatasetssuchaselectronichealthrecords(EHRs).
Mitigating all these harms is an important but unsolved research question in
NLP.Extrapretraining(Gururanganetal.,2020)onnon-toxicsubcorporaseemsto
reducealanguagemodel’stendencytogeneratetoxiclanguagesomewhat(Gehman
et al., 2020). And analyzing the data used to pretrain large language models is
importanttounderstandtoxicityandbiasingeneration,aswellasprivacy,making
itextremelyimportantthatlanguagemodelsincludedatasheets(page16)ormodel
cards(page76)givingfullreplicableinformationonthecorporausedtotrainthem.
10.8 Summary
Thischapterhasintroducedthetransformerandhowitcanbeappliedtolanguage
problems. Here’sasummaryofthemainpointsthatwecovered:
• Transformers are non-recurrent networks based on self-attention. A self-
attentionlayermapsinputsequencestooutputsequencesofthesamelength,BIBLIOGRAPHICALANDHISTORICALNOTES 227
usingattentionheadsthatmodelhowthesurroundingwordsarerelevantfor
theprocessingofthecurrentword.
• A transformer block consists of a single attention layer followed by a feed-
forward layer with residual connections and layer normalizations following
each. Transformerblockscanbestackedtomakedeeperandmorepowerful
networks.
• Commonlanguage-basedapplicationsforRNNsandtransformersinclude:
– Probabilisticlanguagemodeling: assigningaprobabilitytoasequence,
ortothenextelementofasequencegiventheprecedingwords.
– Auto-regressivegenerationusingatrainedlanguagemodel.
– Sequencelabelinglikepart-of-speechtagging,whereeachelementofa
sequenceisassignedalabel.
– Sequenceclassification,whereanentiretextisassignedtoacategory,as
inspamdetection,sentimentanalysisortopicclassification.
Bibliographical and Historical Notes
Thetransformer(Vaswanietal.,2017)wasdevelopeddrawingontwolinesofprior
research: self-attention and memory networks. Encoder-decoder attention, the
idea of using a soft weighting over the encodings of input words to inform a gen-
erativedecoder(seeChapter13)wasdevelopedbyGraves(2013)inthecontextof
handwritinggeneration,andBahdanauetal.(2015)forMT.Thisideawasextended
toself-attentionbydroppingtheneedforseparateencodinganddecodingsequences
andinsteadseeingattentionasawayofweightingthetokensincollectinginforma-
tionpassedfromlowerlayerstohigherlayers(Lingetal.,2015;Chengetal.,2016;
Liu et al., 2016b). Other aspects of the transformer, including the terminology of
key, query, and value, came from memory networks, a mechanism for adding an
externalread-writememorytonetworks,byusinganembeddingofaquerytomatch
keysrepresentingcontentinanassociativememory(Sukhbaataretal.,2015;Weston
etal.,2015;Gravesetal.,2014).228 CHAPTER11 • FINE-TUNINGANDMASKEDLANGUAGEMODELS
CHAPTER
11 Fine-Tuning and Masked Lan-
guage Models
Larvatusprodeo[Masked,Igoforward]
Descartes
In the previous chapter we saw how to pretrain transformer language models,
andhowthesepretrainedmodelscanbeusedasatoolformanykindsofNLPtasks,
bycastingthetasksaswordprediction. ThemodelsweintroducedinChapter10to
dothistaskarecausalorleft-to-righttransformermodels.
Inthischapterwe’llintroduceasecondparadigmforpretrainedlanguagemod-
els, called the bidirectional transformer encoder, trained via masked language
masked
language modeling, a method that allows the model to see entire texts at a time, including
modeling
boththerightandleftcontext. We’llintroducethemostwidely-usedversionofthe
BERT maskedlanguagemodelingarchitecture,theBERTmodel(Devlinetal.,2019).
We’llalsointroducetwoimportantideasthatareoftenusedwiththesemasked
fine-tuning language models. The first is the idea of fine-tuning. Fine-tuning is the process
of taking the network learned by these pretrained models, and further training the
model,oftenviaanaddedneuralnetclassifierthattakesthetoplayerofthenetwork
asinput,toperformsomedownstreamtasklikenamedentitytaggingorquestionan-
sweringorcoreference. Theintuitionisthatthepretrainingphaselearnsalanguage
modelthatinstantiatesarichrepresentationsofwordmeaning,thatthusenablesthe
model to more easily learn (‘be fine-tuned to’) the requirements of a downstream
languageunderstandingtask. Thepretrain-finetuneparadigmisaninstanceofwhat
transfer iscalledtransferlearninginmachinelearning:themethodofacquiringknowledge
learning
fromonetaskordomain,andthenapplyingit(transferringit)tosolveanewtask.
Thesecondideathatweintroduceinthischapteristheideaofcontextualem-
beddings: representations for words in context. The methods of Chapter 6 like
word2vec or GloVe learned a single vector embedding for each unique word w in
thevocabulary. Bycontrast,withcontextualembeddings,suchasthoselearnedby
maskedlanguagemodelslikeBERT,eachwordwwillberepresentedbyadifferent
vectoreachtimeitappearsinadifferentcontext. Whilethecausallanguagemod-
elsofChapter10alsomadeuseofcontextualembeddings,embeddingscreatedby
maskedlanguagemodelsturnouttobeparticularlyuseful.
11.1 Bidirectional Transformer Encoders
Let’sbeginbyintroducingthebidirectionaltransformerencoderthatunderliesmod-
elslikeBERTanditsdescendantslikeRoBERTa(Liuetal.,2019)orSpanBERT
(Joshi et al., 2020). In Chapter 10 we explored causal (left-to-right) transformers
that can serve as the basis for powerful language models—models that can eas-
ilybeappliedtoautoregressivegenerationproblemssuchascontextualgeneration,
summarizationandmachinetranslation. However,whenappliedtosequenceclassi-
ficationandlabelingproblemscausalmodelshaveobviousshortcomingssincethey11.1 • BIDIRECTIONALTRANSFORMERENCODERS 229
are based on an incremental, left-to-right processing of their inputs. If we want to
assignthecorrectnamed-entitytagtoeachwordinasentence,orothersophisticated
linguistic labels like the parse tags we’ll introduce in later chapters, we’ll want to
beabletotakeintoaccountinformationfromtherightcontextasweprocesseach
element. Fig. 11.1, reproduced here from Chapter 10, illustrates the information
flowinthepurelyleft-to-rightapproachofChapter10. Ascanbeseen,thehidden
state computation at each point in time is based solely on the current and earlier
elementsoftheinput,ignoringpotentiallyusefulinformationlocatedtotherightof
eachtaggingdecision.
y y y y y
1 2 3 4 5
Self-Attention
Layer
x x x x x
1 2 3 4 5
Figure11.1 Acausal,backwardlooking,transformermodellikeChapter10. Eachoutput
iscomputedindependentlyoftheothersusingonlyinformationseenearlierinthecontext.
y y y y y
1 2 3 4 5
Self-Attention
Layer
x x x x x
1 2 3 4 5
Figure11.2 Information flow in a bidirectional self-attention model. In processing each
elementofthesequence,themodelattendstoallinputs,bothbeforeandafterthecurrentone.
Bidirectional encoders overcome this limitation by allowing the self-attention
mechanismtorangeovertheentireinput,asshowninFig.11.2. Thefocusofbidi-
rectionalencodersisoncomputingcontextualizedrepresentationsofthetokensinan
inputsequencethataregenerallyusefulacrossarangeofdownstreamapplications.
Therefore, bidirectionalencodersuseself-attentiontomapsequencesofinputem-
beddings(x ,...,x )tosequencesofoutputembeddingsthesamelength(y ,...,y ),
1 n 1 n
wheretheoutputvectorshavebeencontextualizedusinginformationfromtheentire
inputsequence.
Thiscontextualizationisaccomplishedthroughtheuseofthesameself-attention
mechanismusedincausalmodels. Aswiththesemodels,thefirststepistogener-
ate a set of key, query and value embeddings for each element of the input vector
x through the use of learned weight matrices WQ, WK, and WV. These weights230 CHAPTER11 • FINE-TUNINGANDMASKEDLANGUAGEMODELS
projecteachinputvectorx intoitsspecificroleasakey,query,orvalue.
i
q i=WQx i; k i=WKx i; v i=WVx i (11.1)
Theoutputvectory correspondingtoeachinputelementx isaweightedsumofall
i i
theinputvaluevectorsv,asfollows:
n
y i = α ijv j (11.2)
j=1
(cid:88)
Theα weightsarecomputedviaasoftmaxoverthecomparisonscoresbetween
everyelementofaninputsequenceconsideredasaqueryandeveryotherelement
asakey,wherethecomparisonscoresarecomputedusingdotproducts.
exp(score )
ij
α ij = n exp(score ) (11.3)
k=1 ik
score ij = q(cid:80)i k j (11.4)
·
Since each output vector, y, is computed independently, the processing of an
i
entire sequence can be parallelized via matrix operations. The first step is to pack
the input embeddings x
i
into a matrix X RN ×dh. That is, each row of X is the
∈
embedding of one token of the input. We then multiply X by the key, query, and
valueweightmatrices(allofdimensionalityd d)toproducematricesQ RN ×d,
× ∈
K RN ×d,andV RN ×d,containingallthekey,query,andvaluevectorsinasingle
∈ ∈
step.
Q=XWQ; K=XWK; V=XWV (11.5)
Given these matrices we can compute all the requisite query-key comparisons si-
multaneously by multiplying Q and K(cid:124) in a single operation. Fig. 11.3 illustrates
theresultofthisoperationforaninputwithlength5.
q1•k1 q1•k2 q1•k3 q1•k4 q1•k5
q2•k1 q2•k2 q2•k3 q2•k4 q2•k5
N
q3•k1 q3•k2 q3•k3 q3•k4 q3•k5
q4•k1 q4•k2 q4•k3 q4•k4 q4•k5
q5•k1 q5•k2 q5•k3 q5•k4 q5•k5
N
(cid:124)
Figure11.3 TheN NQK matrixshowingthecompletesetofqi kjcomparisons.
× ·
Finally,wecanscalethesescores,takethesoftmax,andthenmultiplytheresult
byVresultinginamatrixofshapeN d whereeachrowcontainsacontextualized
×
outputembeddingcorrespondingtoeachtokenintheinput.
(cid:124)
QK
SelfAttention(Q,K,V) = softmax V (11.6)
√d
(cid:18) k (cid:19)11.1 • BIDIRECTIONALTRANSFORMERENCODERS 231
AsshowninFig.11.3, thefullsetofself-attentionscoresrepresentedbyQKT
constitute an all-pairs comparison between the keys and queries for each element
of the input. In the case of causal language models in Chapter 10, we masked the
uppertriangularportionofthismatrix(inFig.10.3)toeliminateinformationabout
futurewordssincethiswouldmakethelanguagemodelingtrainingtasktrivial.With
bidirectionalencoderswesimplyskipthemask,allowingthemodeltocontextualize
eachtokenusinginformationfromtheentireinput.
Beyond this simple change, all of the other elements of the transformer archi-
tecture remain the same for bidirectional encoder models. Inputs to the model are
segmented using subword tokenization and are combined with positional embed-
dingsbeforebeingpassedthroughaseriesofstandardtransformerblocksconsisting
of self-attention and feedforward layers augmented with residual connections and
layernormalization,asshowninFig.11.4.
yn
Transformer Layer Normalize
Block +
Residual
connection Feedforward Layer
Layer Normalize
+
Residual
connection Self-Attention Layer
x1 x2 x3 … xn
Figure11.4 Atransformerblockshowingallthelayers.
Tomakethismoreconcrete,theoriginalbidirectionaltransformerencodermodel,
BERT(Devlinetal.,2019),consistedofthefollowing:
• Asubwordvocabularyconsistingof30,000tokensgeneratedusingtheWord-
Piecealgorithm(SchusterandNakajima,2012),
• Hiddenlayersofsizeof768,
• 12layersoftransformerblocks,with12multiheadattentionlayerseach.
Theresultisamodelwithover100Mparameters. TheuseofWordPiece(oneofthe
large family of subword tokenization algorithms that includes the BPE algorithm
we saw in Chapter 2) means that BERT and its descendants are based on subword
tokens rather than words. Every input sentence first has to be tokenized, and then
all further processing takes place on subword tokens rather than words. This will
require, as we’ll see, that for some NLP tasks that require notions of words (like
namedentitytagging,orparsing)wewilloccasionallyneedtomapsubwordsback
towords.
Finally,afundamentalissuewithtransformersisthatthesizeoftheinputlayer
dictates the complexity of model. Both the time and memory requirements in a
transformergrowquadraticallywiththelengthoftheinput.It’snecessary,therefore,
to set a fixed input length that is long enough to provide sufficient context for the232 CHAPTER11 • FINE-TUNINGANDMASKEDLANGUAGEMODELS
modeltofunctionandyetstillbecomputationallytractable.ForBERT,afixedinput
sizeof512subwordtokenswasused.
11.2 Training Bidirectional Encoders
WetrainedcausaltransformerlanguagemodelsinChapter10bymakingthemiter-
atively predict the next word in a text. But eliminating the causal mask makes the
guess-the-next-wordlanguagemodelingtasktrivialsincetheanswerisnowdirectly
availablefromthecontext,sowe’reinneedofanewtrainingscheme. Fortunately,
thetraditionallearningobjectivesuggestsanapproachthatcanbeusedtotrainbidi-
rectional encoders. Instead of trying to predict the next word, the model learns to
clozetask performafill-in-the-blanktask,technicallycalledtheclozetask(Taylor,1953). To
seethis,let’sreturntothemotivatingexamplefromChapter3. Insteadofpredicting
whichwordsarelikelytocomenextinthisexample:
Please turn your homework .
we’reaskedtopredictamissingitemgiventherestofthesentence.
Please turn homework in.
Thatis,givenaninputsequencewithoneormoreelementsmissing,thelearning
taskistopredictthemissingelements. Moreprecisely,duringtrainingthemodelis
deprivedofoneormoreelementsofaninputsequenceandmustgenerateaproba-
bilitydistributionoverthevocabularyforeachofthemissingitems.Wethenusethe
cross-entropylossfromeachofthemodel’spredictionstodrivethelearningprocess.
Thisapproachcanbegeneralizedtoanyofavarietyofmethodsthatcorruptthe
traininginputandthenasksthemodeltorecovertheoriginalinput. Examplesofthe
kinds of manipulations that have been used include masks, substitutions, reorder-
ings,deletions,andextraneousinsertionsintothetrainingtext.
11.2.1 MaskingWords
TheoriginalapproachtotrainingbidirectionalencodersiscalledMaskedLanguage
Masked
Language Modeling(MLM)(Devlinetal.,2019). Aswiththelanguagemodeltrainingmeth-
Modeling
MLM odswe’vealreadyseen,MLMusesunannotatedtextfromalargecorpus. Here,the
modelispresentedwithaseriesofsentencesfromthetrainingcorpuswherearan-
domsampleoftokensfromeachtrainingsequenceisselectedforuseinthelearning
task. Oncechosen,atokenisusedinoneofthreeways:
• Itisreplacedwiththeuniquevocabularytoken[MASK].
• It is replaced with another token from the vocabulary, randomly sampled
basedontokenunigramprobabilities.
• Itisleftunchanged.
InBERT,15%oftheinputtokensinatrainingsequencearesampledforlearning.
Ofthese,80%arereplacedwith[MASK],10%arereplacedwithrandomlyselected
tokens,andtheremaining10%areleftunchanged.
The MLM training objective is to predict the original inputs for each of the
maskedtokensusingabidirectionalencoderofthekinddescribedinthelastsection.
Thecross-entropylossfromthesepredictionsdrivesthetrainingprocessforallthe
parameters in the model. Note that all of the input tokens play a role in the self-
attentionprocess,butonlythesampledtokensareusedforlearning.11.2 • TRAININGBIDIRECTIONALENCODERS 233
Morespecifically,theoriginalinputsequenceisfirsttokenizedusingasubword
model. Thesampleditemswhichdrivethelearningprocessarechosenfromamong
the set of tokenized inputs. Word embeddings for all of the tokens in the input
are retrieved from the word embedding matrix and then combined with positional
embeddingstoformtheinputtothetransformer.
long thanks the
CE Loss
Softmax over
Vocabulary
Bidirectional Transformer Encoder
Token + + + + + + + + +
Positional
Embeddings p1 p2 p3 p4 p5 p6 p7 p8
So [mask] and [mask] for all apricot fish
So long and thanks for all the fish
Figure11.5 Maskedlanguagemodeltraining. Inthisexample,threeoftheinputtokensareselected,twoof
whicharemaskedandthethirdisreplacedwithanunrelatedword. Theprobabilitiesassignedbythemodel
tothesethreeitemsareusedasthetrainingloss. (Inthisandsubsequentfigureswedisplaytheinputaswords
ratherthansubwordtokens;thereadershouldkeepinmindthatBERTandsimilarmodelsactuallyusesubword
tokensinstead.)
Fig.11.5illustratesthisapproachwithasimpleexample. Here,long,thanksand
thehavebeensampledfromthetrainingsequence,withthefirsttwomaskedandthe
replaced with the randomly sampled token apricot. The resulting embeddings are
passedthroughastackofbidirectionaltransformerblocks. Toproduceaprobability
distribution over the vocabulary for each of the masked tokens, the output vector
from the final transformer layer for each of the masked tokens is multiplied by a
learned set of classification weights W
V
R|V |×dh and then through a softmax to
∈
yieldtherequiredpredictionsoverthevocabulary.
y =softmax(W h)
i V i
Withapredictedprobabilitydistributionforeachmaskeditem,wecanusecross-
entropy to compute the loss for each masked item—the negative log probability
assignedtotheactualmaskedword,asshowninFig.11.5. Thegradientsthatform
the basis for the weight updates are based on the average loss over the sampled
learningitemsfromasingletrainingsequence(orbatchofsequences).
11.2.2 MaskingSpans
FormanyNLPapplications,thenaturalunitofinterestmaybelargerthanasingle
word (or token). Question answering, syntactic parsing, coreference and seman-
ticrolelabelingapplicationsallinvolvetheidentificationandclassificationofcon-
stituents, or phrases. This suggests that a span-oriented masked learning objective
mightprovideimprovedperformanceonsuchtasks.234 CHAPTER11 • FINE-TUNINGANDMASKEDLANGUAGEMODELS
A span is a contiguous sequence of one or more words selected from a train-
ing text, prior to subword tokenization. In span-based masking, a set of randomly
selected spans from a training sequence are chosen. In the SpanBERT work that
originated this technique (Joshi et al., 2020), a span length is first chosen by sam-
plingfromageometricdistributionthatisbiasedtowardsshorterspansandwithan
upper bound of 10. Given this span length, a starting location consistent with the
desiredspanlengthandthelengthoftheinputissampleduniformly.
Onceaspanischosenformasking,allthewordswithinthespanaresubstituted
accordingtothesameregimeusedinBERT:80%ofthetimethespanelementsare
substitutedwiththe[MASK]token,10%ofthetimetheyarereplacedbyrandomly
sampled words from the vocabulary, and 10% of the time they are left as is. Note
thatthissubstitutionprocessisdoneatthespanlevel—allthetokensinagivenspan
aresubstitutedusingthesamemethod. AswithBERT,thetotaltokensubstitution
is limited to 15% of the training sequence input. Having selected and masked the
training span, the input is passed through the standard transformer architecture to
generatecontextualizedrepresentationsoftheinputtokens.
Downstreamspan-basedapplicationsrelyonspanrepresentationsderivedfrom
thetokenswithinthespan,aswellasthestartandendpoints,ortheboundaries,of
aspan. Representationsfortheseboundariesaretypicallyderivedfromthefirstand
last words of a span, the words immediately preceding and following the span, or
somecombinationofthem. TheSpanBERTlearningobjectiveaugmentstheMLM
objectivewithaboundaryorientedcomponentcalledtheSpanBoundaryObjective
(SBO). The SBO relies on a model’s ability to predict the words within a masked
span from the words immediately preceding and following it. This prediction is
madeusingtheoutputvectorsassociatedwiththewordsthatimmediatelyprecede
and follow the span being masked, along with positional embedding that signals
whichwordinthespanisbeingpredicted:
L(x) = L MLM(x)+L SBO(x) (11.7)
L SBO(x) = logP(xx s,x e,p x) (11.8)
− |
where s denotes the position of the word before the span and e denotes the word
after the end. The prediction for a given position i within the span is produced
by concatenating the output embeddings for words s and e span boundary vectors
withapositionalembeddingforpositioniandpassingtheresultthrougha2-layer
feedforwardnetwork.
s = FFN([y s 1;y e+1;p i s+1]) (11.9)
− −
z = softmax(Es) (11.10)
ThefinallossisthesumoftheBERTMLMlossandtheSBOloss.
Fig.11.6illustratesthiswithoneofourearlierexamples. Herethespanselected
is and thanks for which spans from position 3 to 5. The total loss associated with
themaskedtokenthanksisthesumofthecross-entropylossgeneratedfromthepre-
dictionofthanksfromtheoutputy ,plusthecross-entropylossfromtheprediction
4
ofthanksfromtheoutputvectorsfory ,y andtheembeddingforposition4inthe
2 6
span.
11.2.3 NextSentencePrediction
Thefocusofmasked-basedlearningisonpredictingwordsfromsurroundingcon-
textswiththegoalofproducingeffectiveword-levelrepresentations. However, an11.2 • TRAININGBIDIRECTIONALENCODERS 235
Span-based loss
FFN
Bidirectional Transformer Encoder
Embedding
Layer
So long [mask] [mask] [mask] all the fish
So long and thanks for all the fish
Figure11.6 Span-basedlanguagemodeltraining. Inthisexample,aspanoflength3isselectedfortraining
andallofthewordsinthespanaremasked. Thefigureillustratesthelosscomputedforwordthanks;theloss
fortheentirespanisbasedonthelossforallthreeofthewordsinthespan.
importantclassofapplicationsinvolvesdeterminingtherelationshipbetweenpairs
of sentences. These include tasks like paraphrase detection (detecting if two sen-
tences have similar meanings), entailment (detecting if the meanings of two sen-
tencesentailorcontradicteachother)ordiscoursecoherence(decidingiftwoneigh-
boringsentencesformacoherentdiscourse).
Tocapturethekindofknowledgerequiredforapplicationssuchasthese,BERT
NextSentence introducedasecondlearningobjectivecalledNextSentencePrediction(NSP).In
Prediction
this task, the model is presented with pairs of sentences and is asked to predict
whethereachpairconsistsofanactualpairofadjacentsentencesfromthetraining
corpusorapairofunrelatedsentences.InBERT,50%ofthetrainingpairsconsisted
ofpositivepairs,andintheother50%thesecondsentenceofapairwasrandomly
selectedfromelsewhereinthecorpus.TheNSPlossisbasedonhowwellthemodel
candistinguishtruepairsfromrandompairs.
TofacilitateNSPtraining,BERTintroducestwonewtokenstotheinputrepre-
sentation(tokensthatwillproveusefulforfine-tuningaswell). Aftertokenizingthe
inputwiththesubwordmodel,thetoken[CLS]isprependedtotheinputsentence
pair,andthetoken[SEP]isplacedbetweenthesentencesandafterthefinaltokenof
thesecondsentence.Finally,embeddingsrepresentingthefirstandsecondsegments
oftheinputareaddedtothewordandpositionalembeddingstoallowthemodelto
moreeasilydistinguishtheinputsentences.
Duringtraining,theoutputvectorfromthefinallayerassociatedwiththe[CLS]
tokenrepresentsthenextsentenceprediction.AswiththeMLMobjective,alearned
setofclassificationweightsW
NSP
R2 ×dh isusedtoproduceatwo-classprediction
∈
fromtheraw[CLS]vector.
y = softmax(W h)
i NSP i
+236 CHAPTER11 • FINE-TUNINGANDMASKEDLANGUAGEMODELS
CrossentropyisusedtocomputetheNSPlossforeachsentencepairpresented
tothemodel. Fig.11.7illustratestheoverallNSPtrainingsetup. InBERT,theNSP
losswasusedinconjunctionwiththeMLMtrainingobjectivetoformfinalloss.
1
CE Loss
Softmax
Bidirectional Transformer Encoder
Token +
Segment + + + + + + + + + + + + + + + + + + +
Positional
Embeddings s1 p1 s1 p2 s1 p3 s1 p4 s1 p5 s2 p6 s2 p7 s2 p8 s2 p9
[CLS] Cancel my flight [SEP] And the hotel [SEP]
Figure11.7 AnexampleoftheNSPlosscalculation.
11.2.4 TrainingRegimes
ThecorpususedintrainingBERTandotherearlytransformer-basedlanguagemod-
elsconsistedofan800millionwordcorpusofbooktextscalledBooksCorpus(Zhu
etal.,2015)anda2.5BillionwordcorpusderivedfromtheEnglishWikipedia,for
acombinedsizeof3.3Billionwords. TheBooksCorpusisnolongerused(forin-
tellectual property reasons), and in general, as we’ll discuss later, state-of-the-art
modelsemploycorporathatareordersofmagnitudelargerthantheseearlyefforts.
To train the original BERT models, pairs of sentences were selected from the
trainingcorpusaccordingtothenextsentenceprediction50/50scheme. Pairswere
sampled so that their combined length was less than the 512 token input. Tokens
within these sentence pairs were then masked using the MLM approach with the
combined loss from the MLM and NSP objectives used for a final loss. Approx-
imately 40 passes (epochs) over the training data was required for the model to
converge.
Theresultofthispretrainingprocessconsistsofbothlearnedwordembeddings,
as well as all the parameters of the bidirectional encoder that are used to produce
contextualembeddingsfornovelinputs.
11.2.5 ContextualEmbeddings
Givenapretrainedlanguagemodelandanovelinputsentence,wecanthinkofthe
contextual output of the model as constituting contextual embeddings for each token in the
embeddings
input. These contextual embeddings can be used as a contextual representation of
themeaningoftheinputtokenforanytaskrequiringthemeaningofword.
Contextualembeddingsarethusvectorsrepresentingsomeaspectofthemeaning
ofatokenincontext.Forexample,givenasequenceofinputtokensx ,...,x ,wecan
1 n
usetheoutputvectory fromthefinallayerofthemodelasarepresentationofthe
i
meaningoftokenx inthecontextofsentencex ,...,x . Orinsteadofjustusingthe
i 1 n
vectory fromthefinallayerofthemodel,it’scommontocomputearepresentation
i
forx byaveragingtheoutputtokensy fromeachofthelastfourlayersofthemodel.
i i11.3 • TRANSFERLEARNINGTHROUGHFINE-TUNING 237
Just as we used static embeddings like word2vec to represent the meaning of
words, we can use contextual embeddings as representations of word meanings in
context for any task that might require a model of word meaning. Where static
embeddings represent the meaning of word types (vocabulary entries), contextual
embeddings represent the meaning of word tokens: instances of a particular word
typeinaparticularcontext. Contextualembeddingscanthusbeusedfortaskslike
measuringthesemanticsimilarityoftwowordsincontext,andareusefulinlinguis-
tictasksthatrequiremodelsofwordmeaning.
In the next section, however, we’ll see the most common use of these repre-
sentations: as embeddings of word or even entire sentences that are the inputs to
classifiersinthefine-tuningprocessfordownstreamNLPapplications.
11.3 Transfer Learning through Fine-Tuning
Thepowerofpretrainedlanguagemodelsliesintheirabilitytoextractgeneraliza-
tionsfromlargeamountsoftext—generalizationsthatareusefulformyriaddown-
stream applications. To make practical use of these generalizations, we need to
create interfaces from these models to downstream applications through a process
fine-tuning calledfine-tuning. Fine-tuningfacilitatesthecreationofapplicationsontopofpre-
trainedmodelsthroughtheadditionofasmallsetofapplication-specificparameters.
Thefine-tuningprocessconsistsofusinglabeleddatafromtheapplicationtotrain
theseadditionalapplication-specificparameters. Typically, thistrainingwilleither
freezeormakeonlyminimaladjustmentstothepretrainedlanguagemodelparame-
ters.
Thefollowingsectionsintroducefine-tuningmethodsforthemostcommonap-
plicationsincludingsequenceclassification,sequencelabeling,sentence-pairinfer-
ence,andspan-basedoperations.
11.3.1 SequenceClassification
Sequenceclassificationapplicationsoftenrepresentaninputsequencewithasingle
consolidatedrepresentation. WithRNNs,weusedthehiddenlayerassociatedwith
thefinalinputelementtostandfortheentiresequence. Asimilarapproachisused
withtransformers. Anadditionalvectorisaddedtothemodeltostandfortheentire
sentence sequence. Thisvectorissometimescalledthesentenceembeddingsinceitrefers
embedding
totheentiresequence,althoughtheterm‘sentenceembedding’isalsousedinother
ways.InBERT,the[CLS]tokenplaystheroleofthisembedding.Thisuniquetoken
isaddedtothevocabularyandisprependedtothestartofallinputsequences,both
during pretraining and encoding. The output vector in the final layer of the model
for the [CLS] input represents the entire input sequence and serves as the input to
classifierhead a classifier head, a logistic regression or neural network classifier that makes the
relevantdecision.
As an example, let’s return to the problem of sentiment classification. A sim-
pleapproachtofine-tuningaclassifierforthisapplicationinvolveslearningasetof
weights,W ,tomaptheoutputvectorforthe[CLS]token,y toasetofscores
C CLS
overthepossiblesentimentclasses. Assumingathree-waysentimentclassification
task(positive,negative,neutral)anddimensionalityd forthesizeofthelanguage
h
model hidden layers gives W
C
R3 ×dh. Classification of unseen documents pro-
∈
ceeds by passing the input text through the pretrained language model to generate238 CHAPTER11 • FINE-TUNINGANDMASKEDLANGUAGEMODELS
y ,multiplyingitbyW ,andfinallypassingtheresultingvectorthroughasoft-
CLS C
max.
y = softmax(W Cy CLS) (11.11)
FinetuningthevaluesinW requiressupervisedtrainingdataconsistingofinput
C
sequences labeled with the appropriate class. Training proceeds in the usual way;
cross-entropy loss between the softmax output and the correct answer is used to
drivethelearningthatproducesW .
C
Akeydifferencefromwhatwe’veseenearlierwithneuralclassifiersisthatthis
losscanbeusedtonotonlylearntheweightsoftheclassifier,butalsotoupdatethe
weightsforthepretrainedlanguagemodelitself. Inpractice,reasonableclassifica-
tion performance is typically achieved with only minimal changes to the language
model parameters, often limited to updates over the final few layers of the trans-
former. Fig.11.8illustratesthisoverallapproachtosequenceclassification.
Bidirectional Transformer Encoder
Word +
Positional
Embeddings
[CLS] entirely predictable and lacks energy
Figure11.8 Sequence classification with a bidirectional transformer encoder. The output vector for the
[CLS]tokenservesasinputtoasimpleclassifier.
11.3.2 Pair-WiseSequenceClassification
As mentioned in Section 11.2.3, an important type of problem involves the classi-
fication of pairs of input sequences. Practical applications that fall into this class
includelogicalentailment,paraphrasedetectionanddiscourseanalysis.
Fine-tuninganapplicationforoneofthesetasksproceedsjustaswithpretraining
using the NSP objective. During fine-tuning, pairs of labeled sentences from the
supervisedtrainingdataarepresentedtothemodel.Aswithsequenceclassification,
theoutputvectorassociatedwiththeprepended[CLS]tokenrepresentsthemodel’s
view of the input pair. And as with NSP training, the two inputs are separated by
the a [SEP] token. To perform classification, the [CLS] vector is multiplied by a
setoflearningclassificationweightsandpassedthroughasoftmaxtogeneratelabel
predictions,whicharethenusedtoupdatetheweights.
As an example, let’s consider an entailment classification task with the Multi-
Genre Natural Language Inference (MultiNLI) dataset (Williams et al., 2018). In
natural
language the task of natural language inference or NLI, also called recognizing textual
inference11.3 • TRANSFERLEARNINGTHROUGHFINE-TUNING 239
entailment,amodelispresentedwithapairofsentencesandmustclassifythere-
lationship between their meanings. For example in the MultiNLI corpus, pairs of
sentences are given one of 3 labels: entails, contradicts and neutral. These labels
describearelationshipbetweenthemeaningofthefirstsentence(thepremise)and
themeaningofthesecondsentence(thehypothesis). Herearerepresentativeexam-
plesofeachclassfromthecorpus:
• Neutral
a: Jonwalkedbacktothetowntothesmithy.
b: Jontraveledbacktohishometown.
• Contradicts
a: TouristInformationofficescanbeveryhelpful.
b: TouristInformationofficesareneverofanyhelp.
• Entails
a: I’mconfused.
b: Notallofitisverycleartome.
Arelationshipofcontradictsmeansthatthepremisecontradictsthehypothesis;en-
tails means that the premise entails the hypothesis; neutral means that neither is
necessarilytrue. Themeaningoftheselabelsislooserthanstrictlogicalentailment
or contradiction indicating that a typical human reading the sentences would most
likelyinterpretthemeaningsinthisway.
To fine-tune a classifier for the MultiNLI task, we pass the premise/hypothesis
pairs through a bidirectional encoder as described above and use the output vector
forthe[CLS]tokenastheinputtotheclassificationhead.Aswithordinarysequence
classification,thisheadprovidestheinputtoathree-wayclassifierthatcanbetrained
ontheMultiNLItrainingcorpus.
11.3.3 SequenceLabelling
Sequencelabellingtasks,suchaspart-of-speechtaggingorBIO-basednamedentity
recognition, follow the same basic classification approach. Here, the final output
vector corresponding to each input token is passed to a classifier that produces a
softmaxdistributionoverthepossiblesetoftags.Again,assumingasimpleclassifier
consisting of a single feedforward layer followed by a softmax, the set of weights
to be learned for this additional layer is W
K
Rk ×dh, where k is the number of
∈
possibletagsforthetask. AswithRNNs,agreedyapproach,wheretheargmaxtag
for each token is taken as a likely answer, can be used to generate the final output
tagsequence. Fig.11.9illustratesanexampleofthisapproach.
y i = softmax(W Kz i) (11.12)
t i = argmax k(y i) (11.13)
Alternatively,thedistributionoverlabelsprovidedbythesoftmaxforeachinput
tokencanbepassedtoaconditionalrandomfield(CRF)layerwhichcantakeglobal
tag-leveltransitionsintoaccount.
Acomplicationwiththisapproacharisesfromtheuseofsubwordtokenization
such as WordPiece or Byte Pair Encoding. Supervised training data for tasks like
namedentityrecognition(NER)istypicallyintheformofBIOtagsassociatedwith
text segmented at the word level. For example the following sentence containing
twonamedentities:240 CHAPTER11 • FINE-TUNINGANDMASKEDLANGUAGEMODELS
NNP MD VB DT NN
Bidirectional Transformer Encoder
Embedding
Layer
[CLS] Janet will back the bill
Figure11.9 Sequencelabelingforpart-of-speechtaggingwithabidirectionaltransformerencoder.Theout-
putvectorforeachinputtokenispassedtoasimplek-wayclassifier.
[ LOCMt. Sanitas]isin[ LOCSunshineCanyon].
wouldhavethefollowingsetofper-wordBIOtags.
(11.14) Mt. Sanitasis inSunshineCanyon.
B-LOCI-LOC OO B-LOC I-LOC O
Unfortunately,theWordPiecetokenizationforthissentenceyieldsthefollowing
sequence of tokens which doesn’t align directly with BIO tags in the ground truth
annotation:
’Mt’, ’.’, ’San’, ’##itas’, ’is’, ’in’, ’Sunshine’, ’Canyon’ ’.’
To deal with this misalignment, we need a way to assign BIO tags to subword
tokens during training and a corresponding way to recover word-level tags from
subwords during decoding. For training, we can just assign the gold-standard tag
associatedwitheachwordtoallofthesubwordtokensderivedfromit.
Fordecoding,thesimplestapproachistousetheargmaxBIOtagassociatedwith
the first subword token of a word. Thus, in our example, the BIO tag assigned to
“Mt”wouldbeassignedto“Mt.” andthetagassignedto“San”wouldbeassigned
to “Sanitas”, effectively ignoring the information in the tags assigned to “.” and
“##itas”. More complex approaches combine the distribution of tag probabilities
acrossthesubwordsinanattempttofindanoptimalword-leveltag.
11.3.4 Fine-tuningforSpan-BasedApplications
Span-orientedapplicationsoperateinamiddlegroundbetweensequenceleveland
token level tasks. That is, in span-oriented applications the focus is on generating
andoperatingwithrepresentationsofcontiguoussequencesoftokens. Typicalop-
erations include identifying spans of interest, classifying spans according to some
labeling scheme, and determining relations among discovered spans. Applications
include named entity recognition, question answering, syntactic parsing, semantic
rolelabelingandcoreferenceresolution.11.3 • TRANSFERLEARNINGTHROUGHFINE-TUNING 241
Formally, given an input sequence x consisting of T tokens, (x ,x ,...,x ), a
1 2 T
spanisacontiguoussequenceoftokenswithstartiandend jsuchthat1<=i<=
j<=T.Thisformulationresultsinatotalsetofspansequalto T(T −1) .Forpractical
2
purposes,span-basedmodelsoftenimposeanapplication-specificlengthlimitL,so
thelegalspansarelimitedtothosewhere j i<L. Inthefollowing,we’llreferto
−
theenumeratedsetoflegalspansinxasS(x).
The first step in fine-tuning a pretrained language model for a span-based ap-
plication is using the contextualized input embeddings from the model to generate
representationsforallthespansintheinput. Mostschemesforrepresentingspans
make use of two primary components: representations of the span boundaries and
summary representations of the contents of each span. To compute a unified span
representation,weconcatenatetheboundaryrepresentationswiththesummaryrep-
resentation.
In the simplest possible approach, we can use the contextual embeddings of
the startand endtokens ofa span asthe boundaries, and theaverage ofthe output
embeddingswithinthespanasthesummaryrepresentation.
j
1
g ij = h k (11.15)
(j i)+1
− k=i
(cid:88)
spanRep
ij
= [h i;h j;g i,j] (11.16)
Aweaknessofthisapproachisthatitdoesn’tdistinguishtheuseofaword’sem-
beddingasthebeginningofaspanfromitsuseastheendofone. Therefore,more
elaborateschemesforrepresentingthespanboundariesinvolvelearnedrepresenta-
tionsforstartandendpointsthroughtheuseoftwodistinctfeedforwardnetworks:
s i = FFN start(h i) (11.17)
e j = FFN end(h j) (11.18)
spanRep
ij
= [s i;e j;g i,j] (11.19)
Similarly,asimpleaverageofthevectorsinaspanisunlikelytobeanoptimal
representationofaspansinceittreatsallofaspan’sembeddingsasequallyimpor-
tant. Formanyapplications,amoreusefulrepresentationwouldbecenteredaround
theheadofthephrasecorrespondingtothespan. Onemethodforgettingatsuchin-
formationintheabsenceofasyntacticparseistouseastandardself-attentionlayer
togenerateaspanrepresentation.
g ij=SelfAttention(h i:j) (11.20)
Now,givenspanrepresentationsgforeachspaninS(x),classifierscanbefine-
tunedtogenerateapplication-specificscoresforvariousspan-orientedtasks: binary
span identification (is this a legitimate span of interest or not?), span classification
(whatkindofspanisthis?),andspanrelationclassification(howarethesetwospans
related?).
Togroundthisdiscussion,let’sreturntonamedentityrecognition(NER).Given
a scheme for representing spans and a set of named entity types, a span-based ap-
proach to NER is a straightforward classification problem where each span in an
inputisassignedaclasslabel. Moreformally,givenaninputsequencex,wewant
to assign a label y, from the set of valid NER labels, to each of the spans in S(x).
Since most of the spans in a given input will not be named entities we’ll add the
labelNULLtothesetoftypesinY.
y ij = softmax(FFN(g ij)) (11.21)242 CHAPTER11 • FINE-TUNINGANDMASKEDLANGUAGEMODELS
Softmax
Classification
Scores FFNN FFNN
Span representation
Span summary SelfAttn SelfAttn
Contextualized
…
Embeddings (h)
Bidirectional Transformer Encoder
Jane Villanueva of United Airlines Holding discussed …
PER ORG
Figure11.10 Aspan-orientedapproachtonamedentityclassification.Thefigureonlyillustratesthecompu-
T(T 1)
tationfor2spanscorrespondingtogroundtruthnamedentities.Inreality,thenetworkscoresallofthe 2−
spansinthetext.Thatis,alltheunigrams,bigrams,trigrams,etc.uptothelengthlimit.
With this approach, fine-tuning entails using supervised training data to learn
the parameters of the final classifier, as well as the weights used to generate the
boundaryrepresentations, andtheweightsintheself-attentionlayerthatgenerates
the span content representation. During training, the model’s predictions for all
spans are compared to their gold-standard labels and cross-entropy loss is used to
drivethetraining.
During decoding, each span is scored using a softmax over the final classifier
outputtogenerateadistributionoverthepossiblelabels,withtheargmaxscorefor
each span taken as the correct answer. Fig. 11.10 illustrates this approach with an
example.Avariationonthisschemedesignedtoimproveprecisionaddsacalibrated
thresholdtothelabelingofaspanasanythingotherthanNULL.
There are two significant advantages to a span-based approach to NER over a
BIO-based per-word labeling approach. The first advantage is that BIO-based ap-
proachesarepronetoalabelingmis-matchproblem. Thatis,everylabelinalonger
named entity must be correct for an output to be judged correct. Returning to the
exampleinFig.11.10,thefollowinglabelingwouldbejudgedentirelywrongdueto
theincorrectlabelonthefirstitem. Span-basedapproachesonlyhavetomakeone
classificationforeachspan.
(11.22) Jane VillanuevaofUnited AirlinesHoldingdiscussed...
B-PERI-PER O I-ORGI-ORG I-ORG O
Thesecondadvantagetospan-basedapproachesisthattheynaturallyaccommo-
date embedded named entities. For example, in this example both United Airlines
and United Airlines Holding are legitimate named entities. The BIO approach has
nowayofencodingthisembeddedstructure. Butthespan-basedapproachcannat-
urallylabelbothsincethespansarelabeledseparately.
11.4 Training Corpora11.5 • SUMMARY 243
11.5 Summary
Thischapterhasintroducedthetopicoftransferlearningfrompretrainedlanguage
models. Here’sasummaryofthemainpointsthatwecovered:
• Bidirectionalencoderscanbeusedtogeneratecontextualizedrepresentations
ofinputembeddingsusingtheentireinputcontext.
• Pretrained language models based on bidirectional encoders can be learned
using a masked language model objective where a model is trained to guess
themissinginformationfromaninput.
• Pretrained language models can be fine-tuned for specific applications by
adding lightweight classifier layers on top of the outputs of the pretrained
model.
Bibliographical and Historical NotesCHAPTER
12 Prompting, In-Context Learn-
ing, and Instruct Tuning
Placeholder
244Part II
NLP APPLICATIONS
In this second part of the book we introduce fundamental NLP applications:
machine translation, information retrieval, question answering, dialogue systems,
andspeechrecognition.CHAPTER
13 Machine Translation
“I want to talk the dialect of your people. It’s no use of talking unless
peopleunderstandwhatyousay.”
ZoraNealeHurston,Moses,ManoftheMountain1939,p.121
machine Thischapterintroducesmachinetranslation(MT),theuseofcomputerstotrans-
translation
MT latefromonelanguagetoanother.
Ofcoursetranslation,initsfullgenerality,suchasthetranslationofliterature,or
poetry,isadifficult,fascinating,andintenselyhumanendeavor,asrichasanyother
areaofhumancreativity.
Machine translation in its present form therefore focuses on a number of very
practical tasks. Perhaps the most common current use of machine translation is
information forinformationaccess. Wemightwanttotranslatesomeinstructionsontheweb,
access
perhapstherecipeforafavoritedish,orthestepsforputtingtogethersomefurniture.
Or we might want to read an article in a newspaper, or get information from an
online resource like Wikipedia or a government webpage in some other language.
MT for information
access is probably
oneofthemostcom-
mon uses of NLP
technology,andGoogle
Translate alone (shown above) translates hundreds of billions of words a day be-
tween over 100 languages. Improvements in machine translation can thus help re-
digitaldivide ducewhatisoftencalledthedigitaldivideininformationaccess:thefactthatmuch
more information is available in English and other languages spoken in wealthy
countries. WebsearchesinEnglishreturnmuchmoreinformationthansearchesin
otherlanguages,andonlineresourceslikeWikipediaaremuchlargerinEnglishand
other higher-resourced languages. High-quality translation can help provide infor-
mationtospeakersoflower-resourcedlanguages.
Anothercommonuseofmachinetranslationistoaidhumantranslators.MTsys-
post-editing temsareroutinelyusedtoproduceadrafttranslationthatisfixedupinapost-editing
phasebyahumantranslator. Thistaskisoftencalledcomputer-aidedtranslation
CAT orCAT.CATiscommonlyusedaspartoflocalization:thetaskofadaptingcontent
localization oraproducttoaparticularlanguagecommunity.
Finally, a more recent application of MT is to in-the-moment human commu-
nicationneeds. Thisincludesincrementaltranslation, translatingspeechon-the-fly
beforetheentiresentenceiscomplete, asiscommonlyusedinsimultaneousinter-
pretation. Image-centrictranslationcanbeusedforexampletouseOCRofthetext
onaphonecameraimageasinputtoanMTsystemtotranslatemenusorstreetsigns.
encoder- ThestandardalgorithmforMTistheencoder-decodernetwork,anarchitecture
decoder
thatweintroducedinChapter9forRNNs.Recallthatencoder-decoderorsequence-
to-sequencemodelsareusedfortasksinwhichweneedtomapaninputsequenceto
anoutputsequencethatisacomplexfunctionoftheentireinputsequence. Indeed,248 CHAPTER13 • MACHINETRANSLATION
inmachinetranslation,thewordsofthetargetlanguagedon’tnecessarilyagreewith
thewordsofthesourcelanguageinnumberororder. Considertranslatingthefol-
lowingmade-upEnglishsentenceintoJapanese.
(13.1) English: Hewrotealettertoafriend
Japanese: tomodachinitegami-okaita
friend toletter wrote
Note thatthe elementsof thesentences arein verydifferent placesin thedifferent
languages. InEnglish,theverbisinthemiddleofthesentence,whileinJapanese,
theverbkaitacomesattheend. TheJapanesesentencedoesn’trequirethepronoun
he,whileEnglishdoes.
Suchdifferencesbetweenlanguagescanbequitecomplex. Inthefollowingac-
tualsentencefromtheUnitedNations,noticethemanychangesbetweentheChinese
sentence (we’ve given in red a word-by-word gloss of the Chinese characters) and
itsEnglishequivalent.
(13.2) 大会/GeneralAssembly在/on1982年/198212月/December10日/10通过
了/adopted第37号/37th决议/resolution，核准了/approved第二
次/second探索/exploration及/and和平peaceful利用/using外层空
间/outerspace会议/conference的/of各项/various建议/suggestions。
On10December1982,theGeneralAssemblyadoptedresolution37in
whichitendorsedtherecommendationsoftheSecondUnitedNations
ConferenceontheExplorationandPeacefulUsesofOuterSpace.
Note the many ways the English and Chinese differ. For example the order-
ingdiffers inmajorways; the Chineseorderof thenounphraseis “peacefulusing
outerspaceconferenceofsuggestions”whiletheEnglishhas“suggestionsofthe...
conference on peaceful use of outer space”). And the order differs in minor ways
(the date is ordered differently). English requires the in many places that Chinese
doesn’t, and adds some details (like “in which” and “it”) that aren’t necessary in
Chinese. Chinese doesn’t grammatically mark plurality on nouns (unlike English,
whichhasthe“-s”in“recommendations”),andsotheChinesemustusethemodi-
fier各项/varioustomakeitclearthatthereisnotjustonerecommendation. English
capitalizessomewordsbutnotothers. Encoder-decodernetworksareverysuccess-
fulathandlingthesesortsofcomplicatedcasesofsequencemappings.
We’ll begin in the next section by considering the linguistic background about
howlanguagesvary,andtheimplicationsthisvariancehasforthetaskofMT.Then
we’llsketchoutthestandardalgorithm,givedetailsaboutthingslikeinputtokeniza-
tion and creating training corpora of parallel sentences, give some more low-level
detailsabouttheencoder-decodernetwork,andfinallydiscusshowMTisevaluated,
introducingthesimplechrFmetric.
13.1 Language Divergences and Typology
There are about 7,000 languages in the world. Some aspects of human language
universal seemtobeuniversal,holdingtrueforeveryoneoftheselanguages,orarestatistical
universals,holdingtrueformostoftheselanguages. Manyuniversalsarisefromthe
functionalroleoflanguageasacommunicativesystembyhumans. Everylanguage,
forexample,seemstohavewordsforreferringtopeople,fortalkingabouteatingand
drinking,forbeingpoliteornot. Therearealsostructurallinguisticuniversals; for13.1 • LANGUAGEDIVERGENCESANDTYPOLOGY 249
example,everylanguageseemstohavenounsandverbs(Chapter8),haswaystoask
questions, orissuecommands, haslinguisticmechanismsforindicatingagreement
ordisagreement.
Yet languages also differ in many ways (as has been pointed out since ancient
translation times; see Fig. 13.1). Understanding what causes such translation divergences
divergence
(Dorr,1994)canhelpusbuildbetterMTmodels. Weoftendistinguishtheidiosyn-
craticandlexicaldifferencesthatmustbedealtwithonebyone(thewordfor“dog”
differswildlyfromlanguagetolanguage),fromsystematicdifferencesthatwecan
model in a general way (many languages put the verb before the grammatical ob-
ject;othersputtheverbafterthegrammaticalobject). Thestudyofthesesystematic
typology cross-linguisticsimilaritiesanddifferencesiscalledlinguistictypology. Thissec-
tionsketchessometypologicalfactsthatimpactmachinetranslation;theinterested
readershouldalsolookintoWALS,theWorldAtlasofLanguageStructures,which
givesmanytypologicalfactsaboutlanguages(DryerandHaspelmath,2013).
Figure13.1 The Tower of Babel, Pieter Bruegel 1563. Wikimedia Commons, from the
KunsthistorischesMuseum,Vienna.
13.1.1 WordOrderTypology
As we hinted it in our example above comparing English and Japanese, languages
differ in the basic word order of verbs, subjects, and objects in simple declara-
SVO tive clauses. German, French, English, and Mandarin, for example, are all SVO
(Subject-Verb-Object) languages, meaning that the verb tends to come between
SOV thesubjectandobject. HindiandJapanese,bycontrast,areSOVlanguages,mean-
ingthattheverbtendstocomeattheendofbasicclauses,andIrishandArabicare
VSO VSO languages. Two languages that share their basic word order type often have
othersimilarities.Forexample,VOlanguagesgenerallyhaveprepositions,whereas
OVlanguagesgenerallyhavepostpositions.250 CHAPTER13 • MACHINETRANSLATION
Let’s look in more detail at the example we saw above. In this SVO English
sentence,theverbwroteisfollowedbyitsobjectaletterandtheprepositionalphrase
toafriend,inwhichtheprepositiontoisfollowedbyitsargumentafriend. Arabic,
withaVSOorder,alsohastheverbbeforetheobjectandprepositions. Bycontrast,
intheJapaneseexamplethatfollows,eachoftheseorderingsisreversed;theverbis
precededbyitsarguments,andthepostpositionfollowsitsargument.
(13.3) English: Hewrotealettertoafriend
Japanese: tomodachinitegami-okaita
friend toletter wrote
Arabic: katabtrisa¯lali s˙adq
wrote letter tofriend
Otherkindsoforderingpreferencesvaryidiosyncraticallyfromlanguagetolan-
guage. In some SVO languages (like English and Mandarin) adjectives tend to
appear before verbs, while in others languages like Spanish and Modern Hebrew,
adjectivesappearafterthenoun:
(13.4) Spanish bruja verde English green witch
(a) (b)
Figure13.2 Examples of other word order differences: (a) In German, adverbs occur in
initialpositionthatinEnglisharemorenaturallater,andtensedverbsoccurinsecondposi-
tion. (b)InMandarin,prepositionphrasesexpressinggoalsoftenoccurpre-verbally,unlike
inEnglish.
Fig. 13.2 shows examples of other word order differences. All of these word
order differences between languages can cause problems for translation, requiring
thesystemtodohugestructuralreorderingsasitgeneratestheoutput.
13.1.2 LexicalDivergences
Ofcoursewealsoneedtotranslatetheindividualwordsfromonelanguagetoan-
other. Foranytranslation,theappropriatewordcanvarydependingonthecontext.
TheEnglishsource-languagewordbass,forexample,canappearinSpanishasthe
fishlubinaorthemusicalinstrumentbajo. Germanusestwodistinctwordsforwhat
inEnglishwouldbecalledawall: Wandforwallsinsideabuilding,andMauerfor
walls outside a building. Where English uses the word brother for any male sib-
ling, Chinese and many other languages have distinct words for older brother and
younger brother (Mandarin gege and didi, respectively). In all these cases, trans-
lating bass, wall, or brother from English would require a kind of specialization,
disambiguating the different uses of a word. For this reason the fields of MT and
WordSenseDisambiguation(Chapter23)arecloselylinked.
Sometimes one language places more grammatical constraints on word choice
thananother. WesawabovethatEnglishmarksnounsforwhethertheyaresingular
orplural. Mandarindoesn’t. OrFrenchandSpanish, forexample, markgrammat-
icalgenderonadjectives, soanEnglishtranslationintoFrenchrequiresspecifying
adjectivegender.13.1 • LANGUAGEDIVERGENCESANDTYPOLOGY 251
Thewaythatlanguagesdifferinlexicallydividingupconceptualspacemaybe
morecomplexthanthisone-to-manytranslationproblem,leadingtomany-to-many
mappings. Forexample, Fig.13.3summarizessomeofthecomplexitiesdiscussed
byHutchinsandSomers(1992)intranslatingEnglishleg,foot,andpaw,toFrench.
Forexample,whenlegisusedaboutananimalit’stranslatedasFrenchjambe;but
about the leg of a journey, as French etape; if the leg is of a chair, we use French
pied.
lexicalgap Further, one language may have a lexical gap, where no word or phrase, short
of an explanatory footnote, can express the exact meaning of a word in the other
language. For example, English does not have a word that corresponds neatly to
Mandarinxia`oorJapaneseoyako¯ko¯o(inEnglishonehastomakedowithawkward
phraseslikefilialpietyorlovingchild,orgoodson/daughterforboth).
paw
etape patte ANIMAL
JOURNEY ANIMAL
leg BIRD
foot
HUMAN CHAIR HUMAN
jambe pied
Figure13.3 ThecomplexoverlapbetweenEnglishleg,foot,etc.,andvariousFrenchtrans-
lationsasdiscussedbyHutchinsandSomers(1992).
Finally, languages differ systematically in how the conceptual properties of an
event are mapped onto specific words. Talmy (1985, 1991) noted that languages
can be characterized by whether direction of motion and manner of motion are
marked on the verb or on the “satellites”: particles, prepositional phrases, or ad-
verbialphrases. Forexample, abottlefloatingoutofacavewouldbedescribedin
Englishwiththedirectionmarkedontheparticleout,whileinSpanishthedirection
wouldbemarkedontheverb:
(13.5) English: Thebottlefloatedout.
Spanish: La botellasalio´ flotando.
Thebottle exitedfloating.
verb-framed Verb-framed languages mark the direction of motion on the verb (leaving the
satellites to mark the manner of motion), like Spanish acercarse ‘approach’, al-
satellite-framed canzar ‘reach’, entrar ‘enter’, salir ‘exit’. Satellite-framed languages mark the
directionofmotiononthesatellite(leavingtheverbtomarkthemannerofmotion),
like English crawl out, float off, jump down, run after. Languages like Japanese,
Tamil,andthemanylanguagesintheRomance,Semitic,andMayanlanguagesfam-
ilies, are verb-framed; Chinese as well as non-Romance Indo-European languages
likeEnglish, Swedish, Russian, Hindi, andFarsiaresatelliteframed(Talmy1991,
Slobin1996).
13.1.3 MorphologicalTypology
Morphologically,languagesareoftencharacterizedalongtwodimensionsofvari-
isolating ation. The first is the number of morphemes per word, ranging from isolating
languages like Vietnamese and Cantonese, in which each word generally has one
polysynthetic morpheme,topolysyntheticlanguageslikeSiberianYupik(“Eskimo”),inwhicha
singlewordmayhaveverymanymorphemes,correspondingtoawholesentencein252 CHAPTER13 • MACHINETRANSLATION
English. Theseconddimensionisthedegreetowhichmorphemesaresegmentable,
agglutinative rangingfromagglutinativelanguageslikeTurkish, inwhichmorphemeshaverel-
fusion atively clean boundaries, to fusion languages like Russian, in which a single affix
may conflate multiple morphemes, like -om in the word stolom (table-SG-INSTR-
DECL1), which fuses the distinct morphological categories instrumental, singular,
andfirstdeclension.
Translatingbetweenlanguageswithrichmorphologyrequiresdealingwithstruc-
turebelowthewordlevel,andforthisreasonmodernsystemsgenerallyusesubword
modelslikethewordpieceorBPEmodelsofSection13.2.1.
13.1.4 Referentialdensity
Finally,languagesvaryalongatypologicaldimensionrelatedtothethingstheytend
toomit.Somelanguages,likeEnglish,requirethatweuseanexplicitpronounwhen
talkingaboutareferentthatisgiveninthediscourse. Inotherlanguages,however,
wecansometimesomitpronounsaltogether,asthefollowingexamplefromSpanish
shows1:
(13.6) [Eljefe] dioconunlibro. 0/ Mostro´ suhallazgoaundescifradorambulante.
i i
[Theboss]cameuponabook. [He]showedhisfindtoawanderingdecoder.
pro-drop Languagesthatcanomitpronounsarecalledpro-droplanguages. Evenamong
the pro-drop languages, there are marked differences in frequencies of omission.
JapaneseandChinese,forexample,tendtoomitfarmorethandoesSpanish. This
dimensionofvariationacrosslanguagesiscalledthedimensionofreferentialden-
referential sity. Wesaythatlanguagesthattendtousemorepronounsaremorereferentially
density
densethanthosethatusemorezeros.Referentiallysparselanguages,likeChineseor
Japanese,thatrequirethehearertodomoreinferentialworktorecoverantecedents
coldlanguage arealsocalledcoldlanguages. Languagesthataremoreexplicitandmakeiteasier
hotlanguage forthehearerarecalledhotlanguages. Thetermshotandcoldareborrowedfrom
MarshallMcLuhan’s1964distinctionbetweenhotmedialikemovies,whichfillin
manydetailsfortheviewer,versuscoldmedialikecomics,whichrequirethereader
todomoreinferentialworktofillouttherepresentation(Bickel,2003).
Translatingfromlanguageswithextensivepro-drop,likeChineseorJapanese,to
non-pro-droplanguageslikeEnglishcanbedifficultsincethemodelmustsomehow
identifyeachzeroandrecoverwhoorwhatisbeingtalkedaboutinordertoinsert
theproperpronoun.
13.2 Machine Translation using Encoder-Decoder
ThestandardarchitectureforMTistheencoder-decodertransformerorsequence-
to-sequence model, an architecture we saw for RNNs in Chapter 9. We’ll see the
detailsofhowtoapplythisarchitecturetotransformersinSection13.3,butfirstlet’s
talkabouttheoveralltask.
Mostmachinetranslationtasksmakethesimplificationthatwecantranslateeach
sentenceindependently,sowe’lljustconsiderindividualsentencesfornow. Given
a sentence in a source language, the MT task is then to generate a corresponding
sentence in a target language. For example, an MT system is given an English
sentencelike
1 Hereweusethe0/-notation;we’llintroducethisanddiscussthisissuefurtherinChapter2613.2 • MACHINETRANSLATIONUSINGENCODER-DECODER 253
Thegreenwitcharrived
andmusttranslateitintotheSpanishsentence:
Llego´ labrujaverde
MT uses supervised machine learning: at training time the system is given a
large set of parallel sentences (each sentence in a source language matched with
a sentence in the target language), and learns to map source sentences into target
sentences. Inpractice, ratherthanusingwords(asintheexampleabove), wesplit
thesentencesintoasequenceofsubwordtokens(tokenscanbewords,orsubwords,
orindividualcharacters). Thesystemsarethentrainedtomaximizetheprobability
of the sequence of tokens in the target language y ,...,y given the sequence of
1 m
tokensinthesourcelanguagex ,...,x :
1 n
P(y 1,...,y m x 1,...,x n) (13.7)
|
Ratherthanusetheinputtokensdirectly,theencoder-decoderarchitecturecon-
sists of two components, an encoder and a decoder. The encoder takes the input
wordsx=[x ,...,x ]andproducesanintermediatecontexth.Atdecodingtime,the
1 n
systemtakeshand,wordbyword,generatestheoutputy:
h = encoder(x) (13.8)
y i+1 = decoder(h,y 1,...,y i)) i [1,...,m] (13.9)
∀ ∈
Inthenexttwosectionswe’lltalkaboutsubwordtokenization,andthenhowtoget
parallel corpora for training, and then we’ll introduce the details of the encoder-
decoderarchitecture.
13.2.1 Tokenization
Machine translation systems use a vocabulary that is fixed in advance, and rather
than using space-separated words, this vocabulary is generated with subword to-
kenization algorithms, like the BPE algorithm sketched in Chapter 2. A shared
vocabularyisusedforthesourceandtargetlanguages,whichmakesiteasytocopy
tokens(likenames)fromsourcetotarget. Usingsubwordtokenizationwithtokens
sharedbetweenlanguagesmakesitnaturaltotranslatebetweenlanguageslikeEn-
glishorHindithatusespacestoseparatewords,andlanguageslikeChineseorThai
thatdon’t.
Webuildthevocabularybyrunningasubwordtokenizationalgorithmonacor-
pusthatcontainsbothsourceandtargetlanguagedata.
RatherthanthesimpleBPEalgorithmfromFig.2.13,modernsystemsoftenuse
morepowerfultokenizationalgorithms. Somesystems(likeBERT)useavariantof
wordpiece BPEcalledthewordpiecealgorithm, whichinsteadofchoosingthemostfrequent
setoftokenstomerge,choosesmergesbasedonwhichonemostincreasesthelan-
guagemodelprobabilityofthetokenization. Wordpiecesuseaspecialsymbolatthe
beginningofeachtoken;here’saresultingtokenizationfromtheGoogleMTsystem
(Wuetal.,2016):
words: Jetmakersfeudoverseatwidthwithbigordersatstake
wordpieces: Jet makers feud over seat width with big orders at stake
Thewordpiecealgorithmisgivenatrainingcorpusandadesiredvocabularysize
V,andproceedsasfollows:254 CHAPTER13 • MACHINETRANSLATION
1. Initializethewordpiecelexiconwithcharacters(forexampleasubsetofUni-
codecharacters,collapsingalltheremainingcharacterstoaspecialunknown
charactertoken).
2. RepeatuntilthereareVwordpieces:
(a) Trainann-gramlanguagemodelonthetrainingcorpus,usingthecurrent
setofwordpieces.
(b) Considerthesetofpossiblenewwordpiecesmadebyconcatenatingtwo
wordpiecesfromthecurrentlexicon.Choosetheonenewwordpiecethat
mostincreasesthelanguagemodelprobabilityofthetrainingcorpus.
Recall that with BPE we had to specify the number of merges to perform; in
wordpiece, by contrast, we specify the total vocabulary, which is a more intuitive
parameter. Avocabularyof8Kto32Kwordpiecesiscommonlyused.
An even more commonly used tokenization algorithm is (somewhat ambigu-
unigram ously)calledtheunigramalgorithm(Kudo,2018)orsometimestheSentencePiece
SentencePiece algorithm, and is used in systems like ALBERT (Lan et al., 2020) and T5 (Raf-
fel et al., 2020). (Because unigram is the default tokenization algorithm used in a
library called SentencePiece that adds a useful wrapper around tokenization algo-
rithms (Kudo and Richardson, 2018b), authors often say they are using Sentence-
Piecetokenizationbutreallymeantheyareusingtheunigramalgorithm)).
Inunigramtokenization,insteadofbuildingupavocabularybymergingtokens,
we start with a huge vocabulary of every individual unicode character plus all fre-
quent sequences of characters (including all space-separated words, for languages
withspaces),anditerativelyremovesometokenstogettoadesiredfinalvocabulary
size. The algorithm is complex (involving suffix-trees for efficiently storing many
tokens,andtheEMalgorithmforiterativelyassigningprobabilitiestotokens),sowe
don’tgiveithere,butseeKudo(2018)andKudoandRichardson(2018b). Roughly
speaking the algorithm proceeds iteratively by estimating the probability of each
token, tokenizing the input data using various tokenizations, then removing a per-
centageoftokensthatdon’toccurinhigh-probabilitytokenization,andtheniterates
untilthevocabularyhasbeenreduceddowntothedesirednumberoftokens.
WhydoesunigramtokenizationworkbetterthanBPE?BPEtendstocreateslots
ofverysmallnon-meaningfultokens(becauseBPEcanonlycreatelargerwordsor
morphemes by merging characters one at a time), and it also tends to merge very
common tokens, like the suffix ed, onto their neighbors. We can see from these
examples from Bostrom and Durrett (2020) that unigram tends to produce tokens
thataremoresemanticallymeaningful:
Original: corrupted Original: Completely preposterous suggestions
BPE: cor rupted BPE: Comple t ely prep ost erous suggest ions
Unigram: corrupt ed Unigram: Complete ly pre post er ous suggestion s
13.2.2 CreatingtheTrainingdata
parallelcorpus Machine translation models are trained on a parallel corpus, sometimes called a
bitext, a text that appears in two (or more) languages. Large numbers of paral-
Europarl lel corpora are available. Some are governmental; the Europarl corpus (Koehn,
2005),extractedfromtheproceedingsoftheEuropeanParliament,containsbetween
400,000and2millionsentenceseachfrom21Europeanlanguages. TheUnitedNa-
tionsParallelCorpuscontainsontheorderof10millionsentencesinthesixofficial
languagesoftheUnitedNations(Arabic,Chinese,English,French,Russian,Span-
ish)Ziemskietal.(2016). Otherparallelcorporahavebeenmadefrommovieand13.2 • MACHINETRANSLATIONUSINGENCODER-DECODER 255
TVsubtitles,liketheOpenSubtitlescorpus(LisonandTiedemann,2016),orfrom
generalwebtext,liketheParaCrawlcorpusof223millionsentencepairsbetween
23EUlanguagesandEnglishextractedfromtheCommonCrawlBan˜o´netal.(2020).
Sentencealignment
StandardtrainingcorporaforMTcomeasalignedpairsofsentences. Whencreat-
ingnewcorpora,forexampleforunderresourcedlanguagesornewdomains,these
sentencealignmentsmustbecreated.Fig.13.4givesasamplehypotheticalsentence
alignment.
E1: “Good morning," said the little prince. F1: -Bonjour, dit le petit prince.
E2: “Good morning," said the merchant. F2: -Bonjour, dit le marchand de pilules perfectionnées qui
apaisent la soif.
E3: This was a merchant who sold pills that had
F3: On en avale une par semaine et l'on n'éprouve plus le
been perfected to quench thirst.
besoin de boire.
E4: You just swallow one pill a week and you F4: -C’est une grosse économie de temps, dit le marchand.
won’t feel the need for anything to drink.
E5: “They save a huge amount of time," said the merchant. F5: Les experts ont fait des calculs.
E6: “Fifty−three minutes a week." F6: On épargne cinquante-trois minutes par semaine.
E7: “If I had fifty−three minutes to spend?" said the F7: “Moi, se dit le petit prince, si j'avais cinquante-trois minutes
little prince to himself. à dépenser, je marcherais tout doucement vers une fontaine..."
E8: “I would take a stroll to a spring of fresh water”
Figure13.4 A sample alignment between sentences in English and French, with sentences extracted from
AntoinedeSaint-Exupery’sLePetitPrinceandahypotheticaltranslation. Sentencealignmenttakessentences
e 1,...,en,and f 1,...,fn andfindsminimalsetsofsentencesthataretranslationsofeachother,includingsingle
sentencemappingslike(e ,f ),(e ,f ),(e ,f ),(e ,f )aswellas2-1alignments(e /e ,f ),(e /e ,f ),andnull
1 1 4 3 5 4 6 6 2 3 2 7 8 7
alignments(f ).
5
Giventwodocumentsthataretranslationsofeachother,wegenerallyneedtwo
stepstoproducesentencealignments:
• acostfunctionthattakesaspanofsourcesentencesandaspanoftargetsen-
tencesandreturnsascoremeasuringhowlikelythesespansaretobetransla-
tions.
• an alignment algorithm that takes these scores to find a good alignment be-
tweenthedocuments.
To score the similarity of sentences across languages, we need to make use of
amultilingualembeddingspace,inwhichsentencesfromdifferentlanguagesare
in the same embedding space (Artetxe and Schwenk, 2019). Given such a space,
cosinesimilarityofsuchembeddingsprovidesanaturalscoringfunction(Schwenk,
2018). ThompsonandKoehn(2019)givethefollowingcostfunctionbetweentwo
sentencesorspansx,yfromthesourceandtargetdocumentsrespectively:
(1 cos(x,y))nSents(x)nSents(y)
c(x,y)= − (13.10)
S 1 cos(x,y )+ S 1 cos(x ,y)
s=1 − s s=1 − s
wherenSents()givesthe(cid:80)numberofsentences(cid:80)(thisbiasesthemetrictowardmany
alignments of single sentences instead of aligning very large spans). The denom-
inator helps to normalize the similarities, and so x ,...,x ,y ,...,y , are randomly
1 S 1 S
selectedsentencessampledfromtherespectivedocuments.
Usually dynamic programming is used as the alignment algorithm (Gale and
Church, 1993), in a simple extension of the minimum edit distance algorithm we
introducedinChapter2.256 CHAPTER13 • MACHINETRANSLATION
Finally,it’shelpfultodosomecorpuscleanupbyremovingnoisysentencepairs.
This can involve handwritten rules to remove low-precision pairs (for example re-
moving sentences that are too long, too short, have different URLs, or even pairs
that are too similar, suggesting that they were copies rather than translations). Or
pairs can be ranked by their multilingual embedding cosine score and low-scoring
pairsdiscarded.
13.3 Details of the Encoder-Decoder Model
Decoder
llegó la bruja verde </s>
cross-attention
transformer
blocks
The green witch arrived
<s> llegó la bruja verde
Encoder
Figure13.5 The encoder-decoder transformer architecture for machine translation. The encoder uses the
transformerblockswesawinChapter9, whilethedecoderusesamorepowerfulblockwithanextracross-
attentionlayerthatcanattendtoalltheencoderwords.We’llseethisinmoredetailinthenextsection.
ThestandardarchitectureforMTistheencoder-decodertransformer.Theencoder-
decoderarchitecturewasintroducedalreadyforRNNsinChapter9,andthetrans-
formerversionhasthesameidea. Fig.13.5showstheintuitionofthearchitecture
at a high level You’ll see that the encoder-decoder architecture is made up of two
transformers: anencoder,whichisthesameasthebasictransformersfromChap-
ter10,andadecoder,whichisaugmentedwithaspecialnewlayercalledthecross-
attentionlayer. TheencodertakesthesourcelanguageinputwordsX=x 1,...,x
T
and maps them to an output representation Henc =h ,...,h ; usually via N =6
1 T
stackedencoderblocks.
The decoder, just like the encoder-decoder RNN, is essentially a conditional
language model that attends to the encoder representation and generates the target
words one by one, at each timestep conditioning on the source sentence and the
previouslygeneratedtargetlanguagewordstogenerateatoken. Thiscanuseanyof
thedecodingmethodsdiscussedinChapter10: greedydecodinginwhichcasewe
generatethemostprobabletokenyˆ (we’llusewtostandfortokenshere):
t
yˆ t = argmax w VP(wx,y 1...y t 1) (13.11)
∈ | −
orwecanusebeamsearch,orsamplingmethodsliketemperaturesampling.
ButthecomponentsofthearchitecturediffersomewhatfromtheRNNandalso
from the transformer block we’ve seen. First, in order to attend to the source lan-
guage, the transformer blocks in the decoder have an extra cross-attention layer.
Recall that the transformer block of Chapter 10 consists of a self-attention layer
that attends to the input from the previous layer, followed by layer norm, a feed
forwardlayer, andanotherlayernorm. Thedecodertransformerblockincludesan13.3 • DETAILSOFTHEENCODER-DECODERMODEL 257
cross-attention extralayerwithaspecialkindofattention,cross-attention(alsosometimescalled
encoder-decoderattentionorsourceattention).Cross-attentionhasthesameform
asthemulti-headedself-attentioninanormaltransformerblock, exceptthatwhile
thequeriesasusualcomefromthepreviouslayerofthedecoder,thekeysandvalues
comefromtheoutputoftheencoder.
y1 y2 y3 … yn
Linear Layer
Block 3
hn hn hn … hn Block 2
Block 3 Layer Normalize
+
Block 2
Feedforward Layer
Layer Normalize
+ Layer Normalize
Encoder
Block 1 Feedforward Layer + Decoder
Block 1
Cross-Attention Layer
Layer Normalize
+ Layer Normalize
Self-Attention Layer +
Causal Self-Attention Layer
x1 x2 x3 … xn
Encoder Decoder
Figure13.6 The transformer block for the encoder and the decoder. The final output of
theencoderHenc=h 1,...,h T isthecontextusedinthedecoder. Thedecoderisastandard
transformerexceptwithoneextralayer,thecross-attentionlayer,whichtakesthatdecoder
outputHencandusesittoformitsKandVinputs.
That is, the final output of the encoder Henc =h ,...,h is multiplied by the
1 t
cross-attentionlayer’skeyweightsWK andvalueweightsWV,buttheoutputfrom
the prior decoder layer Hdec[i 1] is multiplied by the cross-attention layer’s query
−
weightsWQ:
Q=WQHdec[i −1]; K=WKHenc; V=WVHenc (13.12)
(cid:124)
QK
CrossAttention(Q,K,V) = softmax V (13.13)
√d
(cid:18) k (cid:19)
Thecrossattentionthusallowsthedecodertoattendtoeachofthesourcelanguage
words as projected into the entire encoder final output representations. The other
attentionlayerineachdecoderblock,theself-attentionlayer,isthesamecausal(left-
to-right)self-attentionthatwesawinChapter9. Theself-attentionintheencoder,
however,isallowedtolookaheadattheentiresourcelanguagetext.
Totrainanencoder-decodermodel,weusethesameself-supervisionmodelwe
used for training encoder-decoders RNNs in Chapter 9. The network is given the
sourcetextandthenstartingwiththeseparatortokenistrainedautoregressivelyto
predictthenexttokeny,usingcross-entropyloss:
t
L CE(yˆ t,y t) = logyˆ t[w t+1] (13.14)
−258 CHAPTER13 • MACHINETRANSLATION
teacherforcing Asinthatcase,weuseteacherforcinginthedecoder. Recallthatinteacherforc-
ing, ateachtimestepindecodingweforcethesystemtousethegoldtargettoken
fromtrainingasthenextinputx ,ratherthanallowingittorelyonthe(possibly
t+1
erroneous)decoderoutputyˆ.
t
13.4 Translating in low-resource situations
Forsomelanguages, andespeciallyforEnglish, onlineresourcesarewidelyavail-
able. There are many large parallel corpora that contain translations between En-
glish and many languages. But the vast majority of the world’s languages do not
havelargeparalleltrainingtextsavailable. Animportantongoingresearchquestion
ishowtogetgoodtranslationwithlesserresourcedlanguages. Theresourceprob-
lemcanevenbetrueforhighresourcelanguageswhenweneedtotranslateintolow
resourcedomains(forexampleinaparticulargenrethathappenstohaveverylittle
bitext).
Herewebrieflyintroducetwocommonlyusedapproachesfordealingwiththis
data sparsity: backtranslation, which is a special case of the general statistical
technique called data augmentation, and multilingual models, and also discuss
somesocio-technicalissues.
13.4.1 DataAugmentation
Data augmentation is a statistical technique for dealing with insufficient training
data,byaddingnewsyntheticdatathatisgeneratedfromthecurrentnaturaldata.
Themostcommondataaugmentationtechniqueformachinetranslationiscalled
backtranslation backtranslation. Backtranslationreliesontheintuitionthatwhileparallelcorpora
may be limited for particular languages or domains, we can often find a large (or
at least larger) monolingual corpus, to add to the smaller parallel corpora that are
available. Thealgorithmmakesuseofmonolingualcorporainthetargetlanguage
bycreatingsyntheticbitexts.
In backtranslation, our goal is to improve source-to-target MT, given a small
paralleltext(abitext)inthesource/targetlanguages,andsomemonolingualdatain
thetargetlanguage. WefirstusethebitexttotrainaMTsysteminthereversedi-
rection: atarget-to-sourceMTsystem. Wethenuseittotranslatethemonolingual
target data to the source language. Now we can add this synthetic bitext (natural
targetsentences, alignedwithMT-producedsourcesentences)toourtrainingdata,
andretrainoursource-to-targetMTmodel. Forexamplesupposewewanttotrans-
latefromNavajotoEnglishbutonlyhaveasmallNavajo-Englishbitext,althoughof
coursewecanfindlotsofmonolingualEnglishdata.Weusethesmallbitexttobuild
anMTenginegoingtheotherway(fromEnglishtoNavajo). Oncewetranslatethe
monolingualEnglishtexttoNavajo,wecanaddthissyntheticNavajo/Englishbitext
toourtrainingdata.
Backtranslationhasvariousparameters. Oneishowwegeneratethebacktrans-
lateddata; wecanrunthedecoderingreedyinference,orusebeamsearch. Orwe
can do sampling, like the temperature sampling algorithm we saw in Chapter 10.
Anotherparameteristheratioofbacktranslateddatatonaturalbitextdata; wecan
choose to upsample the bitext data (include multiple copies of each sentence). In
generalbacktranslationworkssurprisinglywell;oneestimatesuggeststhatasystem
trained on backtranslated text gets about 2/3 of the gain as would training on the13.4 • TRANSLATINGINLOW-RESOURCESITUATIONS 259
sameamountofnaturalbitext(Edunovetal.,2018).
13.4.2 Multilingualmodels
Themodelswe’vedescribedsofarareforbilingualtranslation:onesourcelanguage,
onetargetlanguage. It’salsopossibletobuildamultilingualtranslator.
In a multilingual translator, we train the system by giving it parallel sentences
inmanydifferentpairsoflanguages. Thatmeansweneedtotellthesystemwhich
language to translate from and to! We tell the system which language is which
by adding a special token l to the encoder specifying the source language we’re
s
translatingfrom, andaspecialtokenl tothedecodertellingitthetargetlanguage
t
we’dliketotranslateinto.
ThusweslightlyupdateEq.13.9abovetoaddthesetokensinEq.13.16:
h = encoder(x,l s) (13.15)
y i+1 = decoder(h,l t,y 1,...,y i)) i [1,...,m] (13.16)
∀ ∈
Oneadvantageofamultilingualmodelisthattheycanimprovethetranslation
of lower-resourced languages by drawing on information from a similar language
in the training data that happens to have more resources. Perhaps we don’t know
themeaningofawordinGalician,butthewordappearsinthesimilarandhigher-
resourcedlanguageSpanish.
13.4.3 Sociotechnicalissues
Manyissuesindealingwithlow-resourcelanguagesgobeyondthepurelytechnical.
Oneproblemisthatforlow-resourcelanguages,especiallyfromlow-incomecoun-
tries,nativespeakersareoftennotinvolvedasthecuratorsforcontentselection,as
thelanguagetechnologists,orastheevaluatorswhomeasureperformance( etal.,
∀
2020). Indeed, one well-known study that manually audited a large set of parallel
corpora and other major multilingual datasets found that for many of the corpora,
lessthanofthesentenceswereofacceptablequality,withalotofdataconsistingof
repeatedsentenceswithwebboilerplateorincorrecttranslations,suggestingthatna-
tivespeakersmaynothavebeensufficientlyinvolvedinthedataprocess(Kreutzer
etal.,2022).
Other issues, like the tendency of many MT approaches to focus on the case
whereoneofthelanguagesisEnglish(AnastasopoulosandNeubig,2020),haveto
dowithallocationofresources.Wheremostlargemultilingualsystemsweretrained
on bitexts in which English was one of the two languages, recent huge corporate
systems like those of Fan et al. (2021) and Team et al. (2022) and datasets like
Schwenk et al. (2021) attempt to handle large numbers of languages (up to 200
languages) and create bitexts between many more pairs of languages and not just
throughEnglish.
At the smaller end, et al. (2020) propose a participatory design process to
∀
encourage content creators, curators, and language technologists who speak these
low-resourcedlanguagestoparticipateindevelopingMTalgorithms. Theyprovide
online groups, mentoring, and infrastructure, and report on a case study on devel-
opingMTalgorithmsforlow-resourceAfricanlanguages. Amongtheirconclusions
wasperformMTevaluationbypost-editingratherthandirectevaluation,sincehav-
inglabelerseditanMTsystemandthenmeasurethedistancebetweentheMToutput
anditspost-editedversionbothwassimplertotrainevaluatorsandmakesiteasierto260 CHAPTER13 • MACHINETRANSLATION
measuretrueerrorsintheMToutputandnotdifferencesduetolinguisticvariation
(Bentivoglietal.,2018).
13.5 MT Evaluation
Translationsareevaluatedalongtwodimensions:
adequacy 1. adequacy: howwellthetranslation capturestheexactmeaningofthesource
sentence. Sometimescalledfaithfulnessorfidelity.
fluency 2. fluency:howfluentthetranslationisinthetarget language(isitgrammatical,
clear,readable,natural).
Usinghumanstoevaluateismostaccurate,butautomaticmetricsarealsousedfor
convenience.
13.5.1 UsingHumanRaterstoEvaluateMT
The most accurate evaluations use human raters, such as online crowdworkers, to
evaluateeachtranslationalongthetwodimensions. Forexample,alongthedimen-
sionoffluency,wecanaskhowintelligible,howclear,howreadable,orhownatural
theMToutput(thetargettext)is. Wecangivetheratersascale,forexample,from
1 (totally unintelligible) to 5 (totally intelligible, or 1 to 100, and ask them to rate
eachsentenceorparagraphoftheMToutput.
Wecandothesamethingtojudgetheseconddimension,adequacy,usingraters
toassignscoresonascale. Ifwehavebilingualraters,wecangivethemthesource
sentence and a proposed target sentence, and rate, on a 5-point or 100-point scale,
howmuchoftheinformationinthesourcewaspreservedinthetarget. Ifweonly
havemonolingualratersbutwehaveagoodhumantranslationofthesourcetext,we
cangivethemonolingualratersthehumanreferencetranslationandatargetmachine
translation and again rate how much information is preserved. An alternative is to
ranking doranking: givetheratersapairofcandidatetranslations,andaskthemwhichone
theyprefer.
Trainingofhumanraters(whoareoftenonlinecrowdworkers)isessential;raters
without translation expertise find it difficult to separate fluency and adequacy, and
sotrainingincludesexamplescarefullydistinguishingthese. Ratersoftendisagree
(source sentences may be ambiguous, raters will have different world knowledge,
ratersmayapplyscalesdifferently). Itisthereforecommontoremoveoutlierraters,
and (if we use a fine-grained enough scale) normalizing raters by subtracting the
meanfromtheirscoresanddividingbythevariance.
As discussed above, an alternative way of using human raters is to have them
post-edit translations, taking the MT output and changing it minimally until they
feel it represents a correct translation. The difference between their post-edited
translationsandtheoriginalMToutputcanthenbeusedasameasureofquality.
13.5.2 AutomaticEvaluation
Whilehumansproducethebestevaluationsofmachinetranslationoutput,runninga
humanevaluationcanbetimeconsumingandexpensive. Forthisreasonautomatic
metrics are often used as temporary proxies. Automatic metrics are less accurate
than human evaluation, but can help test potential systemimprovements, and even13.5 • MTEVALUATION 261
beusedasanautomaticlossfunctionfortraining. Inthissectionweintroducetwo
familiesofsuchmetrics,thosebasedoncharacter-orword-overlapandthosebased
onembeddingsimilarity.
AutomaticEvaluationbyCharacterOverlap: chrF
chrF ThesimplestandmostrobustmetricforMTevaluationiscalledchrF,whichstands
forcharacterF-score(Popovic´,2015). chrF(alongwithmanyotherearlierrelated
metricslikeBLEU,METEOR,TER,andothers)isbasedonasimpleintuitionde-
rivedfromthepioneeringworkofMillerandBeebe-Center(1956): agoodmachine
translation will tend to contain characters and words that occur in a human trans-
lation of the same sentence. Consider a test set from a parallel corpus, in which
eachsourcesentencehasbothagoldhumantargettranslationandacandidateMT
translationwe’dliketoevaluate. ThechrFmetricrankseachMTtargetsentenceby
afunctionofthenumberofcharactern-gramoverlapswiththehumantranslation.
Given the hypothesis and the reference, chrF is given a parameter k indicating
the length of character n-grams to be considered, and computes the average of the
kprecisions(unigramprecision,bigram,andsoon)andtheaverageofthekrecalls
(unigramrecall,bigramrecall,etc.):
chrP percentageofcharacter1-grams,2-grams,...,k-gramsinthehypothesisthat
occurinthereference,averaged.
chrR percentage of character 1-grams, 2-grams,..., k-grams in the reference that
occurinthehypothesis,averaged.
ThemetricthencomputesanF-scorebycombiningchrPandchrRusingaweighting
parameter β. It is common to set β =2, thus weighing recall twice as much as
precision:
chrP chrR
chrFβ =(1+β2) · (13.17)
β2 chrP+chrR
·
Forβ =2,thatwouldbe:
5 chrP chrR
chrF2= · ·
4 chrP+chrR
·
Forexample, considertwohypothesesthatwe’dliketoscoreagainsttherefer-
encetranslationwitnessforthepast.HerearethehypothesesalongwithchrFvalues
computedusingparametersk=β =2(inrealexamples,kwouldbeahighernumber
like6):
REF:witness for the past,
HYP1: witness of the past, chrF2,2=.86
HYP2: past witness chrF2,2=.62
Let’s see how we computed that chrF value for HYP1 (we’ll leave the compu-
tationofthechrFvalueforHYP2asanexerciseforthereader). First,chrFignores
spaces,sowe’llremovethemfromboththereferenceandhypothesis:
REF:witnessforthepast,(18unigrams,17bigrams)
HYP1: witnessofthepast,(17unigrams,16bigrams)
Next let’s see how many unigrams and bigrams match between the reference and
hypothesis:
unigramsthatmatch: w i t n e s s f o t h e p a s t ,(17unigrams)
bigramsthatmatch: wi it tn ne es ss th he ep pa as st t,(13bigrams)262 CHAPTER13 • MACHINETRANSLATION
Weusethattocomputetheunigramandbigramprecisionsandrecalls:
unigramP: 17/17=1 unigramR: 17/18=.944
bigramP: 13/16=.813 bigramR: 13/17=.765
FinallyweaveragetogetchrPandchrR,andcomputetheF-score:
chrP = (17/17+13/16)/2=.906
chrR = (17/18+13/17)/2=.855
chrP chrR
chrF2,2 = 5 ∗ =.86
4chrP+chrR
chrFissimple,robust,andcorrelatesverywellwithhumanjudgmentsinmany
languages(Kocmietal.,2021).
Alternativeoverlapmetric: BLEU
Therearevariousalternativeoverlapmetrics. Forexample,beforethedevelopment
ofchrF,itwascommontouseaword-basedoverlapmetriccalledBLEU(forBiLin-
gual Evaluation Understudy), that is purely precision-based rather than combining
precisionandrecall(Papinenietal.,2002). TheBLEUscoreforacorpusofcandi-
date translation sentences is a function of the n-gram word precision over all the
sentencescombinedwithabrevitypenaltycomputedoverthecorpusasawhole.
Whatdowemeanbyn-gramprecision?Consideracorpuscomposedofasingle
sentence. Theunigramprecisionforthiscorpusisthepercentageofunigramtokens
inthecandidatetranslationthatalsooccurinthereferencetranslation,anddittofor
bigramsandsoon,upto4-grams. BLEUextendsthisunigrammetrictothewhole
corpusbycomputingthenumeratorasthesumoverallsentencesofthecountsofall
theunigramtypesthatalsooccurinthereferencetranslation, andthedenominator
is the total of the counts of all unigrams in all candidate sentences. We compute
this n-gram precision for unigrams, bigrams, trigrams, and 4-grams and take the
geometricmean.BLEUhasmanyfurthercomplications,includingabrevitypenalty
for penalizing candidate translations that are too short, and it also requires the n-
gramcountsbeclippedinaparticularway.
BecauseBLEUisaword-basedmetric,itisverysensitivetowordtokenization,
makingitimpossibletocomparedifferentsystemsiftheyrelyondifferenttokeniza-
tion standards, and doesn’t work as well in languages with complex morphology.
Nonetheless,youwillsometimesstillseesystemsevaluatedbyBLEU,particularly
fortranslationintoEnglish. Insuchcasesit’simportanttousepackagesthatenforce
standardizationfortokenizationlikeSACREBLEU(Post,2018).
StatisticalSignificanceTestingforMTevals
Character or word overlap-based metrics like chrF (or BLEU, or etc.) are mainly
used to compare two systems, with the goal of answering questions like: did the
newalgorithmwejustinventedimproveourMTsystem? Toknowifthedifference
between the chrF scores of two MT systems is a significant difference, we use the
pairedbootstraptest,orthesimilarrandomizationtest.
TogetaconfidenceintervalonasinglechrFscoreusingthebootstraptest,recall
fromSection4.9thatwetakeourtestset(ordevset)andcreatethousandsofpseudo-
testsetsbyrepeatedlysamplingwithreplacementfromtheoriginaltestset. Wenow
computethechrFscoreofeachofthepseudo-testsets. Ifwedropthetop2.5%and
bottom 2.5% of the scores, the remaining scores will give us the 95% confidence
intervalforthechrFscoreofoursystem.13.6 • BIASANDETHICALISSUES 263
TocomparetwoMTsystemsAandB,wedrawthesamesetofpseudo-testsets,
andcomputethechrFscoresforeachofthem. Wethencomputethepercentageof
pseudo-test-setsinwhichAhasahigherchrFscorethanB.
chrF:Limitations
Whileautomaticcharacterandword-overlapmetricslikechrForBLEUareuseful,
they have important limitations. chrF is very local: a large phrase that is moved
around might barely change the chrF score at all, and chrF can’t evaluate cross-
sentence properties of a document like its discourse coherence (Chapter 27). chrF
and similar automatic metrics also do poorly at comparing very different kinds of
systems,suchascomparinghuman-aidedtranslationagainstmachinetranslation,or
differentmachinetranslationarchitecturesagainsteachother(Callison-Burchetal.,
2006). Instead,automaticoverlapmetricslikechrFaremostappropriatewheneval-
uatingchangestoasinglesystem.
13.5.3 AutomaticEvaluation: Embedding-BasedMethods
ThechrFmetricisbasedonmeasuringtheexactcharactern-gramsahumanrefer-
ence and candidate machine translation have in common. However, this criterion
isoverlystrict,sinceagoodtranslationmayusealternatewordsorparaphrases. A
solutionfirstpioneeredinearlymetricslikeMETEOR(BanerjeeandLavie,2005)
was to allow synonyms to match between the reference x and candidate x˜. More
recentmetricsuseBERTorotherembeddingstoimplementthisintuition.
For example, in some situations we might have datasets that have human as-
sessments of translation quality. Such datasets consists of tuples (x,x˜,r), where
x=(x ,...,x ) is a reference translation, x˜=(x˜ ,...,x˜ ) is a candidate machine
1 n 1 m
translation,andr Risahumanratingthatexpressesthequalityofx˜withrespect
∈
tox. Givensuchdata,algorithmslikeCOMET(Reietal.,2020)BLEURT(Sellam
etal.,2020)trainapredictoronthehuman-labeleddatasets,forexamplebypassing
x and x˜ through a version of BERT (trained with extra pretraining, and then fine-
tunedonthehuman-labeledsentences),followedbyalinearlayerthatistrainedto
predictr. Theoutputofsuchmodelscorrelateshighlywithhumanlabels.
Inothercases,however,wedon’thavesuchhuman-labeleddatasets.Inthatcase
wecanmeasurethesimilarityofxandx˜bythesimilarityoftheirembeddings. The
BERTSCOREalgorithm(Zhangetal.,2020)showninFig.13.7,forexample,passes
the reference x and the candidate x˜through BERT, computing a BERT embedding
for each token x i and x˜ j. Each pair of tokens (x i,x˜ j) is scored by its cosine |xx ii |· |x˜ x˜j j|.
Each token in x is matched to a token in x˜ to compute recall, and each token in x˜
ismatchedtoatokeninxtocomputeprecision(witheachtokengreedilymatched
to the most similar token in the corresponding sentence). BERTSCORE provides
precisionandrecall(andhenceF ):
1
1 1
R BERT= maxx i x˜ j P BERT= maxx i x˜ j (13.18)
|x
|
(cid:88)xi∈xx˜j∈x˜ · |x˜
|x
(cid:88)˜j∈x˜xi∈x ·
13.6 Bias and Ethical Issues
Machinetranslationraisesmanyofthesameethicalissuesthatwe’vediscussedin
earlier chapters. For example, consider MT systems translating from HungarianPublishedasaconferencepaperatICLR2020
264 CHAPTER13 • MACHINETRANSLATION
Contextual Pairwise Cosine Maximum Similarity Importance Weighting
Embedding Similarity (Optional)
Referencex 1.27
the weather<<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""ffff2222yyyyzzzziiiimmmmwwwwbbbbRRRR////DDDDggggjjjjzzzzpppp6666ttttZZZZ333366660000ffffHHHHRRRRqqqqNNNNIIII===="""">>>>AAAAAAAAAAAABBBB6666HHHHiiiiccccbbbbVVVVBBBBNNNNSSSS8888NNNNAAAAEEEEJJJJ3333UUUUrrrr1111qqqq////qqqqhhhh66669999LLLLBBBBbbbbBBBBUUUU0000llllEEEE0000GGGGPPPPRRRRiiii8888ccccWWWW7777AAAAeeee0000ooooWWWWyyyy2222kkkk3333bbbbttttZZZZhhhhNNNN2222NNNN2222IIIIJJJJ////QQQQVVVVeeeePPPPCCCCjjjjiiii1111ZZZZ////kkkkzzzzXXXX////jjjjttttssss1111BBBBWWWWxxxx8888MMMMPPPPNNNN6666bbbbYYYYWWWWZZZZeeeekkkkAAAAiiiiuuuujjjjeeeetttt++++OOOO4444WWWW11119999YYYY3333NNNNrrrreeeeJJJJ2222aaaaWWWWdddd3333bbbb////++++ggggffffHHHHjjjjUUUU0000nnnnGGGGqqqqGGGGDDDDZZZZZZZZLLLLGGGGLLLLVVVVCCCCaaaahhhhGGGGwwwwSSSSUUUU2222DDDDTTTTccccCCCCOOOO4444llllCCCCGGGGggggUUUUCCCC22228888HHHH4444dddduuuuaaaa3333HHHH1111FFFFppppHHHHsssstttt7777MMMM0000nnnnQQQQjjjj++++hhhhQQQQ8888ppppAAAAzzzzaaaaqqqqzzzzUUUUeeeeOOOOqqqqXXXXKKKK22227777VVVVnnnnYYYYOOOOssssEEEEiiii8888nnnnFFFFcccchhhhRRRR77775555eeee////eeeeooooOOOOYYYYppppRRRRFFFFKKKKwwwwwwwwTTTTVVVVuuuuuuuuuuuu5555iiiiffffEEEEzzzzqqqqggggxxxxnnnnAAAAqqqqeeeellllXXXXqqqqooooxxxxooooWWWWxxxxMMMMhhhh9999iiii1111VVVVNNNNIIIIIIIIttttZZZZ////NNNNDDDD55552222SSSSMMMM6666ssssMMMMSSSSBBBBggggrrrrWWWW9999KKKKQQQQuuuuffffpppp7777IIIIqqqqOOOORRRR1111ppppMMMMoooossssJJJJ0000RRRRNNNNSSSSOOOO99997777MMMM3333EEEE////7777xxxxuuuuaaaassssJJJJrrrrPPPP++++MMMMyyyySSSSQQQQ1111KKKKttttllllggggUUUUppppooooKKKKYYYYmmmmMMMMyyyy++++JJJJggggOOOOuuuukkkkBBBBkkkkxxxxssssYYYYQQQQyyyyxxxxeeee2222tttthhhhIIII2222oooooooosssszzzzYYYYbbbbEEEEoooo2222BBBBGGGG////55555555VVVVXXXXSSSSuuuuqqqqhhhh6666bbbbttttVVVVrrrrXXXXFFFFZZZZqqqqNNNN3333kkkkccccRRRRTTTTiiiiBBBBUUUUzzzzggggHHHHDDDD66666666ggggBBBBnnnnddddQQQQhhhhyyyyYYYYwwwwQQQQHHHHiiiiGGGGVVVV3333hhhhzzzzHHHHppppwwwwXXXX5555999933335555WWWWLLLLQQQQWWWWnnnnHHHHzzzzmmmmGGGGPPPP7777AAAA++++ffffwwwwBBBB5555jjjjmmmmMMMM////AAAA========<<<<////llllaaaatttteeeexxxxiiiitttt>>>> is 7.94
cold today 1.82
7.90
RBERT=(0 1.7 .21 73 + 71 .. 92 47 +)+ 1( .80 2.5 +1 75 . 907 +.9 84 .) 8+ 8...
Candidatexˆ
8.88
<<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""ORRfJIIGynnWoTTlKcc4lZZNmkkCBWWvAiilgVVvUBBtAnnM0ffuK//1Dnn7tccrUBBjcssLstttHCCk/vv2daa5itto5CCWBttplGGdI44c===="""">>>>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<Wjj/cWWlxdda8WWtwUUerHHxu//iqqqtNNN>XFF644BBB8kkdPPUYYrrrVbbwkk===<<<///lllaaattteeexxxiiittt>>>
it is freezing<<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""5555QQQQTTTTnnnnVVVVRRRRVVVVSSSSrrrrnnnnyyyyzzzzzzzznnnnVVVVUUUU7777dddd5555bbbbFFFF5555uuuu00003333IIIIwwww===="""">>>>AAAAAAAAAAAABBBB7777nnnniiiiccccbbbbVVVVBBBBNNNNSSSS8888NNNNAAAAEEEEJJJJ3333UUUUrrrr1111qqqq////qqqqhhhh66669999LLLLBBBBbbbbBBBBUUUU0000llllEEEE0000GGGGPPPPRRRRiiii8888ccccKKKK9999ggggPPPPaaaaUUUUDDDDbbbbbbbbTTTTbbbbtttt0000sssswwwwmmmm7777EEEE7777GGGGEEEE////ggggggggvvvvHHHHhhhhTTTTxxxx6666uuuu////xxxx5555rrrr9999xxxx0000++++aaaaggggrrrrQQQQ8888GGGGHHHHuuuu////NNNNMMMMDDDDMMMMvvvvSSSSKKKKQQQQwwww6666LLLLrrrrffffTTTTmmmmllllttttffffWWWWNNNNzzzzqqqq7777xxxxdddd2222ddddnnnndddd2222zzzz++++ooooHHHHhhhh66661111TTTTZZZZxxxxqqqqxxxxllllssssssssllllrrrrHHHHuuuuBBBBttttRRRRwwwwKKKKRRRRRRRRvvvvooooUUUUDDDDJJJJuuuu4444nnnnmmmmNNNNAAAAooookkkk7777wwwwSSSSTTTT22229999zzzzvvvvPPPPHHHHJJJJttttRRRRKKKKwwwweeeeccccJJJJppppwwwwPPPP6666IIIIjjjjJJJJUUUULLLLBBBBKKKKFFFFqqqqpppp0000xxxx9999TTTTzzzzJJJJ5555mmmmgggg2222rrrrNNNNrrrrbbbbttttzzzzkkkkFFFFXXXXiiiiFFFFaaaaQQQQGGGGBBBBZZZZqqqqDDDD6666lllldddd////GGGGLLLLMMMM00004444ggggqqqqZZZZppppMMMMbbbb0000PPPPDDDDddddBBBBPPPP6666MMMMaaaaBBBBZZZZNNNN8888VVVVuuuummmmnnnnhhhhiiiieeeeUUUUTTTTeeeeiiiiIIII9999yyyyxxxxVVVVNNNNOOOOLLLLGGGGzzzz++++bbbbnnnnzzzzssssiiiiZZZZVVVVYYYYYYYYkkkkjjjjLLLLUUUUtttthhhhWWWWSSSSuuuu////pppp7777IIIIaaaaGGGGTTTTMMMMNNNNAAAAppppssssZZZZ0000RRRRxxxxbbbbJJJJaaaa9999XXXXPPPPzzzzPPPP66666666UUUUYYYYXXXXvvvvuuuuZZZZUUUUEEEEmmmmKKKKXXXXLLLLHHHHFFFFoooojjjjCCCCVVVVBBBBGGGGOOOOSSSS////00006666GGGGQQQQnnnnOOOOGGGGccccmmmmooooJJJJZZZZVVVVrrrrYYYYWWWWwwwwkkkkbbbbUUUU00000000ZZZZ2222ooooQQQQqqqqNNNNggggRRRRvvvv++++eeeeVVVVVVVV0000rrrr6666ooooeeee22227777dddduuuu7777++++ssssNNNNWWWW6666KKKKOOOOMMMMppppwwwwAAAAqqqqddddwwwwDDDDhhhh5555ccccQQQQQQQQPPPPuuuuooooAAAAkkkkttttYYYYDDDDCCCCBBBBZZZZ3333iiiiFFFFNNNNyyyyddddxxxxXXXXppppxxxx333355552222PPPPRRRRWWWWnnnnKKKKKKKKmmmmWWWWPPPP4444AAAA++++ffffzzzzBBBB7777AAAA8888jjjj8888kkkk====<<<<////llllaaaatttteeeexxxxiiiitttt>>>> today
idf
weights
Candidate
Figure 1: Illustration of the computation of the recall metric R . Given the reference x and
Figure13.7 The computation of BERTSCORE recall from referenceBExRaTnd candidate xˆ, from Figure 1 in
candidatexˆ,wecomputeBERTembeddingsandpairwisecosinesimilarity.Wehighlightthegreedy
Zhangetal.(2020).Thisversionshowsanextendedversionofthemetricinwhichtokensarealsoweightedby
matchinginred,andincludetheoptionalidf importanceweighting.
theiridfvalues.
(whichhasthegenderneutralpronouno˝)orSpanish(whichoftendropspronouns)
We experiment with different models (Section 4), using the tokenizer provided with each model.
intoEnglish(inwhichpronounsareobligatory,andtheyhavegrammaticalgender).
Given a tokenized reference sentence x = x ,...,x , the embedding model generates a se-
1 k
quence of vectoWrshexn t,r.a.n.s,lxatin.gSaimreiflearrelyn,ctehh etotoakepneirzseodni cdaensdcirdiabteedxˆw=ithoxˆut,s.p.e.c,ixˆfied igsemndaeprp,eMdT
1 k 1 m
to xˆ ,...,xˆ
.syTshtheemmsaoinftemnoddieelfawueltutosemisalBeEgRenTd,ewrh(iScchhtioekbeinngizeers2th0e14in,h pPurtatteexsteitnatoli.a2s0e1q9u)e.nAcend
1 l
h i
ofwordpiecesM(WTuseytstaelm.,s20o1ft6e)n,washseirgenugneknndoewrnacwcoorrddsinagretospcluilttuinrteossteevreeoratylpceosmomfothnelysoorbtswerevesdaw
sequences of chinarSacetcetriso.nT6h.1e1r.eFpriges.e1n3ta.8tioshnofworseeaxcahmwploersdfproiemcePirsatceosmeptuatle.d(2w0i1th9)a,iTnrwanhsifcohrmHeurn-
encoder(Vaswagnairieatnalg.,e2n0d1e7r-)nbeyutrreapleo˝atiesdalynauprpsleyiinsgtrsaenlfs-laattteedntwiointhansdhen,obnulitngeeanrdtrearn-nsfeourtmraaltio˝oniss a
inanalternatingfashion. BERTembeddingshavebeenshowntobenefitvariousNLPtasks(Devlin
CEOistranslatedwithhe. Pratesetal.(2019)findthatthesestereotypescan’tcom-
etal.,2019;Liu,2019;Huangetal.,2019;Yangetal.,2019a).
pletelybeaccountedforbygenderbiasinUSlaborstatistics,becausethebiasesare
SimilarityMeaasmurpelifieTdhebyveMctTorsryesptreemsesn,twatiiothnparlolonwosunfosrbaeisnogftmmaepapseudretoofmsaimleiloarriftyeminaslteeagdenodfer
exact-string (Pawpiitnhenaipertobaal.b,i2li0ty02h)igohrehrethurainstiifcth(Beamnearpjpeein&gwLaasvbiea,se2d00o5n)amctautaclhilnabgo.rTehmepclooysmineent
similarity of asrteafteirsetinccse. token x
i
and a candidate token xˆ
j
is kxx i>i kkxˆ xˆj jk. We use pre-normalized
vectors, which reduces this calculation to the inner product x xˆ . While this measure considers
>i j
tokensinisolation,thHeucnongtaerxitaunal(egmenbdeedrdinnegustcroanlt)asinouinrfcoermationfromEtnheglriesshtMofTtheousetpnutetnce.
o˝ egya´polo´ sheisanurse
BERTSCORE The o˝c eo gm yp tule dte o´sscore matches each token in x to a tok he en isin axˆ sct io enc to ism tpute recall,
and each token in xˆ to a token in x to compute precision. We use greedy matching to maximize
o˝ egyme´rno¨k heisanengineer
thematchingsimilarityscore,2 whereeachtokenismatchedtothemostsimilartokenintheother
o˝ egype´k heisabaker
sentence. We combine precision and recall to compute an F1 measure. For a reference x and
o˝ egytana´r sheisateacher
candidatexˆ,therecall,precision,andF1scoresare:
o˝ egyesku¨vo˝szervezo˝ sheisaweddingorganizer
R BERT = |x1 |x Xi cF 2ui x rg rum x eˆ nrjo˝ ea 2 tx Mxˆ1e 3g x T.y >i 8 sv xˆ ye sj W tz ee´ m, hri e sg n ia P ntz r tB eag E rna pR st rlTo´ ea tt= i pn eg | ox1 ˆf pr | lo exˆm Xj fr2g oxˆe mnm xdi tr2a e arx x d-n ix te i>i ou ntxˆ r aaj ll lyl, a mng auF leaB -gh dE ee oR s mTi ls i ik= na e atC 2 H edPE uP nB oOB g cEE a cRR r uTiT pa+ an· tR iiR on ntB Bo sE ER E aR sT nTg mli a. s lh e,,
andtraditionallyfemale-dominatedoccupationsasfemale(Pratesetal.,2019).
ImportanceWeighting Previousworkonsimilaritymeasuresdemonstratedthatrarewordscan
bemoreindicativeforsentencesimilaritythancommonwords(Banerjee&Lavie,2005;Vedantam
etal.,2015). BERTSSimCiOlaRrElye,naabrelecsenutsctohaelalesnilgyeinsceot,rpthoeraWteiinmopMorTtadncaetawseetig(Shttianngo.vWskeyeextpaelr.i,m2e0n1t9)
withinversedoschuomwesntthfaretqMueTncsyys(tiedmf)sspcoerrefosrcmomwpourtseedwfrhoemntthheeyteasrtecoarspkuesd.tGoitvreannsMlateresfeenretennccees
sentences x(i)thMatd,ethsecriidbfespceooreploefwaiwthonrdo-np-isetceeretooktyepnicwalisgenderroles,like“Thedoctoraskedthe
{ }i=1
nursetohelpherintheoperation”.
M
Many ethica idl fq (u we )st =ions loin gM1T req Iu [i wre fu xr(tih)e ]r ,research. One open problem is
developingmetricsforkn owingMwhatours2ystemsdon’tknow. ThisisbecauseMT
i=1
systemscanbeusedinurgentsituatXionswherehumantranslatorsmaybeunavailable
whereI[]isanoinrddieclaatyoredfu:nicntimone.dWicealddoonmoatiunsse,tthoehfeullplttfr-aindsflamteeawsuhreenbpecaatiuesnetswaenpdrodcoecstsorssindgolen’t
·
sentences,wherethetermfrequency(tf)islikely1.Forexample,recallwithidf weightingis
speakthesamelanguage,orinlegaldomains,tohelpjudgesorlawyerscommuni-
catewithwitnessesordefendants. Inorderto‘donoharm’,systemsneedwaysto
idf(x )max x xˆ
confidence
assignconfidReBnEcReTv=aluesxtio2xcandidi
ate idt fr (a
xnxˆjs )2laxˆ tio>i ns,j
so.theycanabstainfromgiving
incorrecttranslationstPhatmaycaxuis 2exharm.i
BecauseweusereferencesentencestocompuPteidf,theidf scoresremainthesameforallsystems
evaluatedonaspecifictestset.Weapplyplus-onesmoothingtohandleunknownwordpieces.
2WecomparegreedymatchingwithoptimalassignmentinAppendixC.
4
ecnerefeR13.7 • SUMMARY 265
13.7 Summary
Machine translation is one of the most widely used applications of NLP, and the
encoder-decoder model, first developed for MT is a key tool that has applications
throughoutNLP.
• Languageshavedivergences,bothstructuralandlexical,thatmaketranslation
difficult.
• The linguistic field of typology investigates some of these differences; lan-
guages can be classified by their position along typological dimensions like
whetherverbsprecedetheirobjects.
• Encoder-decodernetworks(fortransformersjustaswesawinChapter9for
RNNs) are composed of an encoder network that takes an input sequence
and creates a contextualized representation of it, the context. This context
representation is then passed to a decoder which generates a task-specific
outputsequence.
• Cross-attentionallowsthetransformerdecodertoviewinformationfromall
thehiddenstatesoftheencoder.
• Machinetranslationmodelsaretrainedonaparallelcorpus,sometimescalled
abitext,atextthatappearsintwo(ormore)languages.
• Backtranslationisawayofmakinguseofmonolingualcorporainthetarget
languagebyrunningapilotMTenginebackwardstocreatesyntheticbitexts.
• MTisevaluatedbymeasuringatranslation’sadequacy(howwellitcaptures
the meaning of the source sentence) and fluency (how fluent or natural it is
inthetargetlanguage). Humanevaluationisthegoldstandard,butautomatic
evaluation metrics like chrF, which measure character n-gram overlap with
human translations, or more recent metrics based on embedding similarity,
arealsocommonlyused.
Bibliographical and Historical Notes
MTwasproposedseriouslybythelate1940s, soonafterthebirthofthecomputer
(Weaver,1949/1955). In1954,thefirstpublicdemonstrationofanMTsystempro-
totype (Dostert, 1955) led to great excitement in the press (Hutchins, 1997). The
next decade saw a great flowering of ideas, prefiguring most subsequent develop-
ments. Butthisworkwasaheadofitstime—implementationswerelimitedby,for
example, thefactthatpendingthedevelopmentofdiskstherewasnogoodwayto
storedictionaryinformation.
As high-quality MT proved elusive (Bar-Hillel, 1960), there grew a consensus
on the need for better evaluation and more basic research in the new fields of for-
malandcomputationallinguistics. Thisconsensusculminatedinthefamouslycrit-
icalALPAC(AutomaticLanguageProcessingAdvisoryCommittee)reportof1966
(Pierce et al., 1966) that led in the mid 1960s to a dramatic cut in funding for MT
in the US. As MT research lost academic respectability, the Association for Ma-
chineTranslationandComputationalLinguisticsdroppedMTfromitsname. Some
MTdevelopers,however,persevered,andtherewereearlyMTsystemslikeMe´te´o,266 CHAPTER13 • MACHINETRANSLATION
whichtranslatedweatherforecastsfromEnglishtoFrench(Chandioux,1976),and
industrialsystemslikeSystran.
In the early years, the space of MT architectures spanned three general mod-
els. In direct translation, the system proceeds word-by-word through the source-
language text, translating each word incrementally. Direct translation uses a large
bilingualdictionary,eachofwhoseentriesisasmallprogramwiththejoboftrans-
latingoneword. Intransferapproaches, wefirstparsetheinputtextandthenap-
ply rules to transform the source-language parse into a target language parse. We
then generate the target language sentence from the parse tree. In interlingua ap-
proaches, we analyze the source language text into some abstract meaning repre-
sentation, called an interlingua. We then generate into the target language from
this interlingual representation. A common way to visualize these three early ap-
Vauquois proaches was the Vauquois triangle shown in Fig. 13.9. The triangle shows the
triangle
increasing depth of analysis required (on both the analysis and generation end) as
we move from the direct approach through transfer approaches to interlingual ap-
proaches. Inaddition,itshowsthedecreasingamountoftransferknowledgeneeded
as we move up the triangle, from huge amounts of transfer at the direct level (al-
mostallknowledgeistransferknowledgeforeachword)throughtransfer(transfer
rulesonlyforparsetreesorthematicroles)throughinterlingua(nospecifictransfer
knowledge). Wecanviewtheencoder-decodernetworkasaninterlingualapproach,
withattentionactingasanintegrationofdirectandtransfer,allowingwordsortheir
representationstobedirectlyaccessedbythedecoder.
Interlingua
ssoouurrccee llaa aann nngg aalluu yyaa ssiigg ssee SemS ao Snu tt rr i uc ce c/S
t
T uye rn ex tt a: ctic Transfer SemT aa Snr tg t ri ue c ct / S tT uye rnx ett a: ctic gt ea nrg ere at tl ia on nguage
source Direct Translation target
text text
Figure13.9 TheVauquois(1968)triangle.
Statisticalmethodsbegantobeappliedaround1990,enabledfirstbythedevel-
opmentoflargebilingualcorporaliketheHansardcorpusoftheproceedingsofthe
Canadian Parliament, which are kept in both French and English, and then by the
growthoftheWeb. Earlyon, a numberofresearchersshowedthatitwaspossible
toextractpairsofalignedsentencesfrombilingualcorpora,usingwordsorsimple
cueslikesentencelength(KayandRo¨scheisen1988, GaleandChurch1991, Gale
andChurch1993,KayandRo¨scheisen1993).
Atthesametime,theIBMgroup,drawingdirectlyonthenoisychannelmodel
statisticalMT for speech recognition, proposed two related paradigms for statistical MT. These
IBMModels include the generative algorithms that became known as IBM Models 1 through
Candide 5, implemented in the Candide system. The algorithms (except for the decoder)
were published in full detail— encouraged by the US government who had par-
tiallyfundedthework—whichgavethemahugeimpactontheresearchcommunity
(Brownetal.1990,Brownetal.1993).
Thegroupalsodevelopedadiscriminativeapproach,calledMaxEnt(formaxi-BIBLIOGRAPHICALANDHISTORICALNOTES 267
mumentropy,analternativeformulationoflogisticregression),whichallowedmany
features to be combined discriminatively rather than generatively (Berger et al.,
1996),whichwasfurtherdevelopedbyOchandNey(2002).
Bytheturnofthecentury,mostacademicresearchonmachinetranslationused
statisticalMT,eitherinthegenerativeordiscriminativemode. Anextendedversion
phrase-based ofthegenerativeapproach,calledphrase-basedtranslationwasdeveloped,based
translation
oninducingtranslationsforphrase-pairs(Och1998,MarcuandWong2002,Koehn
etal.(2003),OchandNey2004,DengandByrne2005,interalia).
OnceautomaticmetricslikeBLEUweredeveloped(Papinenietal.,2002), the
discriminativeloglinearformulation(OchandNey,2004), drawingfromtheIBM
MaxEntwork(Bergeretal.,1996),wasusedtodirectlyoptimizeevaluationmetrics
MERT likeBLEUinamethodknownasMinimumErrorRateTraining,orMERT(Och,
2003), also drawing from speech recognition models (Chou et al., 1993). Toolkits
Moses likeGIZA(OchandNey,2003)andMoses(Koehnetal.2006,ZensandNey2007)
werewidelyused.
There were also approaches around the turn of the century that were based on
transduction syntactic structure (Chapter 17). Models based on transduction grammars (also
grammars
calledsynchronousgrammarsassignaparallelsyntactictreestructuretoapairof
sentences in different languages, with the goal of translating the sentences by ap-
plying reordering operations on the trees. From a generative perspective, we can
view a transduction grammar as generating pairs of aligned sentences in two lan-
guages. Someofthemostwidelyusedmodelsincludedtheinversiontransduction
inversion
transduction grammar(Wu,1996)andsynchronouscontext-freegrammars(Chiang,2005),
grammar
Neuralnetworkshadbeenappliedatvarioustimestovariousaspectsofmachine
translation;forexampleSchwenketal.(2006)showedhowtouseneurallanguage
models to replace n-gram language models in a Spanish-English system based on
IBM Model 4. The modern neural encoder-decoder approach was pioneered by
KalchbrennerandBlunsom(2013),whousedaCNNencoderandanRNNdecoder,
and was first applied to MT by Bahdanau et al. (2015). The transformer encoder-
decoder was proposed by Vaswani et al. (2017) (see the History section of Chap-
ter10).
Research on evaluation of machine translation began quite early. Miller and
Beebe-Center(1956)proposedanumberofmethodsdrawingonworkinpsycholin-
guistics.TheseincludedtheuseofclozeandShannontaskstomeasureintelligibility
aswellasametricofeditdistancefromahumantranslation, theintuitionthatun-
derliesallmodernoverlap-basedautomaticevaluationmetrics. TheALPACreport
includedanearlyevaluationstudyconductedbyJohnCarrollthatwasextremelyin-
fluential(Pierceetal.,1966,Appendix10). Carrollproposeddistinctmeasuresfor
fidelityandintelligibility,andhadratersscorethemsubjectivelyon9-pointscales.
Muchearlyevaluationworkfocusesonautomaticword-overlapmetricslikeBLEU
(Papinenietal.,2002),NIST(Doddington,2002),TER(TranslationErrorRate)
(Snover et al., 2006), Precision and Recall (Turian et al., 2003), and METEOR
(BanerjeeandLavie,2005); charactern-gramoverlapmethodslikechrF(Popovic´,
2015) came later. More recent evaluation work, echoing the ALPAC report, has
emphasizedtheimportanceofcarefulstatisticalmethodologyandtheuseofhuman
evaluation(Kocmietal.,2021;Marieetal.,2021).
TheearlyhistoryofMTissurveyedinHutchins1986and1997;Nirenburgetal.
(2002)collectsearlyreadings. SeeCroft(1990)orComrie(1989)forintroductions
tolinguistictypology.268 CHAPTER13 • MACHINETRANSLATION
Exercises
13.1 ComputebyhandthechrF2,2scoreforHYP2onpage261(theanswershould
roundto.62).CHAPTER
14 Question Answering and In-
formation Retrieval
The quest for knowledge is deeply human, and so it is not surprising that practi-
callyassoonastherewerecomputerswewereaskingthemquestions. Bytheearly
1960s,systemsusedthetwomajorparadigmsofquestionanswering—information-
retrieval-basedandknowledge-based—toanswerquestionsaboutbaseballstatis-
tics or scientific facts. Even imaginary computers got into the act. Deep Thought,
thecomputerthatDouglasAdamsinventedinTheHitchhiker’sGuidetotheGalaxy,
managedtoanswer“theUltimateQuestionOfLife,TheUniverse,andEverything”.1
In 2011, IBM’s Watson question-answering system won the TV game-show Jeop-
ardy!,surpassinghumansatansweringquestionslike:
WILLIAM WILKINSON’S “AN ACCOUNT OF THE
PRINCIPALITIES OF WALLACHIA AND MOLDOVIA”
INSPIRED THIS AUTHOR’S MOST FAMOUS NOVEL 2
Question answering systems are designed to fill human information needs that
mightariseinsituationsliketalkingtoavirtualassistant, interactingwithasearch
engine, or querying a database. Most question answering systems focus on a par-
ticularsubsetoftheseinformationneeds: factoidquestions, questionsthatcanbe
answeredwithsimplefactsexpressedinshorttexts,likethefollowing:
(14.1) WhereistheLouvreMuseumlocated?
(14.2) Whatistheaverageageoftheonsetofautism?
Inthischapterwedescribethetwomajorparadigmsforfactoidquestionanswer-
ing. Information-retrieval (IR) based QA, sometimes called open domain QA,
relies on the vast amount of text on the web or in collections of scientific papers
like PubMed. Given a user question, information retrieval is used to find relevant
passages. Thenneuralreadingcomprehensionalgorithmsreadtheseretrievedpas-
sagesanddrawananswerdirectlyfromspansoftext.
In the second paradigm, knowledge-based question answering, a system in-
steadbuildsasemanticrepresentationofthequery,suchasmappingWhatstatesbor-
der Texas? to the logical representation: λx.state(x) borders(x,texas), or When
∧
was Ada Lovelace born? to the gapped relation: birth-year (Ada Lovelace,
?x). Thesemeaningrepresentationsarethenusedtoquerydatabasesoffacts.
We’ll also briefly discuss two other QA paradigms. We’ll see how to query a
languagemodeldirectlytoansweraquestion,relyingonthefactthathugepretrained
language models have already encoded a lot of factoids. And we’ll sketch classic
pre-neuralhybridquestion-answeringalgorithmsthatcombineinformationfromIR-
basedandknowledge-basedsources.
We’llexplorethepossibilitiesandlimitationsofalltheseapproaches,alongthe
wayalsointroducingtwotechnologiesthatarekeyforquestionansweringbutalso
1 Theanswerwas42,butunfortunatelythedetailsofthequestionwereneverrevealed.
2 Theanswer,ofcourse,is‘WhoisBramStoker’,andthenovelwasDracula.270 CHAPTER14 • QUESTIONANSWERINGANDINFORMATIONRETRIEVAL
relevantthroughoutNLP:informationretrieval(akeycomponentofIR-basedQA)
andentitylinking(similarlykeyforknowledge-basedQA).We’llstartinthenext
sectionbyintroducingthetaskofinformationretrieval.
The focus of this chapter is factoid question answering, but there are many
other QA tasks the interested reader could pursue, including long-form question
answering(answeringquestionslike“why”questionsthatrequiregeneratinglong
answers), community question answering, (using datasets of community-created
question-answerpairslikeQuoraorStackOverflow), orevenansweringquestions
on human exams like the New York Regents Science Exam (Clark et al., 2019) as
anNLP/AIbenchmarktomeasureprogressinthefield.
14.1 Information Retrieval
information InformationretrievalorIRisthenameofthefieldencompassingtheretrievalofall
retrieval
IR mannerofmediabasedonuserinformationneeds. TheresultingIRsystemisoften
calledasearchengine. OurgoalinthissectionistogiveasufficientoverviewofIR
toseeitsapplicationtoquestionanswering. Readerswithmoreinterestspecifically
in information retrieval should see the Historical Notes section at the end of the
chapterandtextbookslikeManningetal.(2008).
adhocretrieval The IR task we consider is called ad hoc retrieval, in which a user poses a
query to a retrieval system, which then returns an ordered set of documents from
document somecollection. Adocumentreferstowhateverunitoftextthesystemindexesand
retrieves (web pages, scientific papers, news articles, or even shorter passages like
collection paragraphs). A collection refers to a set of documents being used to satisfy user
term requests. A term refers to a word in a collection, but it may also include phrases.
query Finally, a query represents a user’s information need expressed as a set of terms.
Thehigh-levelarchitectureofanadhocretrievalengineisshowninFig.14.1.
Document Inverted
DocumenDtocument Indexing
Index
DocumeDntocument
Document Document
Document
Document
Document
document collection Search Do Dc ou cm Rue man et nnkted
Documents
Query query
query
Processing vector
Figure14.1 ThearchitectureofanadhocIRsystem.
The basic IR architecture uses the vector space model we introduced in Chap-
ter 6, in which we map queries and document to vectors based on unigram word
counts,andusethecosinesimilaritybetweenthevectorstorankpotentialdocuments
(Salton, 1971). This is thus an example of the bag-of-words model introduced in
Chapter4,sincewordsareconsideredindependentlyoftheirpositions.
14.1.1 Termweightinganddocumentscoring
Let’slookatthedetailsofhowthematchbetweenadocumentandqueryisscored.14.1 • INFORMATIONRETRIEVAL 271
termweight Wedon’tuserawwordcountsinIR,insteadcomputingatermweightforeach
document word. Two term weighting schemes are common: the tf-idf weighting
BM25 introducedinChapter6,andaslightlymorepowerfulvariantcalledBM25.
We’ll reintroduce tf-idf here so readers don’t need to look back at Chapter 6.
Tf-idf (the ‘-’ here is a hyphen, not a minus sign) is the product of two terms, the
termfrequencytfandtheinversedocumentfrequencyidf.
The term frequency tells us how frequent the word is; words that occur more
ofteninadocumentarelikelytobeinformativeaboutthedocument’scontents. We
usuallyusethelog ofthewordfrequency,ratherthantherawcount. Theintuition
10
isthatawordappearing100timesinadocumentdoesn’tmakethatword100times
morelikely toberelevant tothemeaning ofthedocument. Becausewe can’ttake
thelogof0,wenormallyadd1tothecount:3
tf t,d = log 10(count(t,d)+1) (14.3)
If we use log weighting, terms which occur 0 times in a document would have
tf=log (1)=0, 10 times in a document tf=log (11)=1.04, 100 times tf=
10 10
log (101)=2.004,1000timestf=3.00044,andsoon.
10
The document frequency df t of a term t is the number of documents it oc-
curs in. Terms that occur in only a few documents are useful for discriminating
those documents from the rest of the collection; terms that occur across the entire
collection aren’t as helpful. The inverse document frequency or idf term weight
(SparckJones,1972)isdefinedas:
N
idf t =log 10df (14.4)
t
where N is the total number of documents in the collection, and df is the number
t
ofdocumentsinwhichtermt occurs. Thefewerdocumentsinwhichatermoccurs,
thehigherthisweight;thelowestweightof0isassignedtotermsthatoccurinevery
document.
Here are some idf values for some words in the corpus of Shakespeare plays,
rangingfromextremelyinformativewordsthatoccurinonlyoneplaylikeRomeo,
tothosethatoccurinafewlikesaladorFalstaff,tothosethatareverycommonlike
foolorsocommonastobecompletelynon-discriminativesincetheyoccurinall37
playslikegoodorsweet.4
Word df idf
Romeo 1 1.57
salad 2 1.27
Falstaff 4 0.967
forest 12 0.489
battle 21 0.246
wit 34 0.037
fool 36 0.012
good 37 0
sweet 37 0
(cid:26)
1+log count(t,d) if count(t,d)>0
3 Orwecanusethisalternative: tft,d=
0
10
otherwise
4 SweetwasoneofShakespeare’sfavoriteadjectives, afactprobablyrelatedtotheincreaseduseof
sugarinEuropeanrecipesaroundtheturnofthe16thcentury(Jurafsky,2014,p.175).272 CHAPTER14 • QUESTIONANSWERINGANDINFORMATIONRETRIEVAL
Thetf-idfvalueforwordt indocumentd isthentheproductoftermfrequency
tf andIDF:
t,d
tf-idf(t,d)=tf t,d idf t (14.5)
·
14.1.2 DocumentScoring
Wescoredocumentd bythecosineofitsvectordwiththequeryvectorq:
q d
score(q,d)=cos(q,d)= · (14.6)
q d
| || |
Anotherwaytothinkofthecosinecomputationisasthedotproductofunitvectors;
wefirstnormalizeboththequeryanddocumentvectortounitvectors,bydividing
bytheirlengths,andthentakethedotproduct:
q d
score(q,d)=cos(q,d)= (14.7)
q · d
| | | |
WecanspelloutEq.14.7,usingthetf-idfvaluesandspellingoutthedotproductas
asumofproducts:
tf-idf(t,q) tf-idf(t,d)
score(q,d)= (14.8)
(cid:88)t ∈q qi∈qtf-idf2(q i,q)· di∈dtf-idf2(d i,d)
(cid:113) (cid:113)
(cid:80) (cid:80)
Inpractice,it’scommontoapproximateEq.14.8bysimplifyingthequerypro-
cessing. Queriesareusuallyveryshort,soeachquerywordislikelytohaveacount
of 1. And the cosine normalization for the query (the division by q) will be the
| |
same for all documents, so won’t change the ranking between any two documents
D andD Sowegenerallyusethefollowingsimplescoreforadocumentd givena
i j
queryq:
tf-idf(t,d)
score(q,d)= (14.9)
d
t q | |
(cid:88)∈
Let’swalkthroughanexampleofatinyqueryagainstacollectionof4nanodoc-
uments,computingtf-idfvaluesandseeingtherankofthedocuments.We’llassume
allwordsinthefollowingqueryanddocumentsaredowncasedandpunctuationis
removed:
Query: sweetlove
Doc1: Sweetsweetnurse! Love?
Doc2: Sweetsorrow
Doc3: Howsweetislove?
Doc4: Nurse!
Fig. 14.2 shows the computation of the tf-idf values and the document vector
length d forthefirsttwodocumentsusingEq.14.3,Eq.14.4,andEq.14.5(com-
| |
putationsfordocuments3and4areleftasanexerciseforthereader).
Fig.14.3showsthescoresofthe4documents,rerankedaccordingtoEq.14.9.
Therankingfollowsintuitivelyfromthevectorspacemodel.Document1,whichhas
bothtermsincludingtwoinstancesofsweet,isthehighestranked,abovedocument
3whichhasalargerlength d inthedenominator, andalsoasmallertfforsweet.
| |
Document3ismissingoneoftheterms,andDocument4ismissingboth.14.1 • INFORMATIONRETRIEVAL 273
Document1 Document2
word count tf df idf tf-idf count tf df idf tf-idf
love 1 0.301 2 0.301 0.091 0 0 2 0.301 0
sweet 2 0.477 3 0.125 0.060 1 0.301 3 0.125 0.038
sorrow 0 0 1 0.602 0 1 0.301 1 0.602 0.181
how 0 0 1 0.602 0 0 0 1 0.602 0
nurse 1 0.301 2 0.301 0.091 0 0 2 0.301 0
is 0 0 1 0.602 0 0 0 1 0.602 0
d =√.0912+.0602+.0912=.141 d =√.0382+.1812=.185
1 2
| | | |
Figure14.2 Computationoftf-idffornano-documents1and2,usingEq.14.3,Eq.14.4,
andEq.14.5.
Doc d tf-idf(sweet) tf-idf(love) score
| |
1 .141 .060 .091 1.07
3 .274 .038 .091 0.471
2 .185 .038 0 0.205
4 .090 0 0 0
Figure14.3 RankingdocumentsbyEq.14.9.
BM25 A slightly more complex variant in the tf-idf family is the BM25 weighting
scheme(sometimescalledOkapiBM25aftertheOkapiIRsysteminwhichitwas
introduced (Robertson et al., 1995)). BM25 adds two parameters: k, a knob that
adjustthebalancebetweentermfrequencyandIDF,andb, whichcontrolstheim-
portanceofdocumentlengthnormalization.TheBM25scoreofadocumentdgiven
aqueryqis:
IDF weightedtf
N tf
t,d
log (14.10)
(cid:88)t ∈q(cid:122) (cid:18)(cid:125)(cid:124)df t(cid:19)(cid:123)(cid:122)k 1 −b+b (cid:125)(cid:124) |d| ad v|
g|
+tf t,d(cid:123)
(cid:16) (cid:16) (cid:17)(cid:17)
where d is the length of the average document. When k is 0, BM25 reverts to
avg
| |
no use of term frequency, just a binary selection of terms in the query (plus idf).
A large k results in raw term frequency (plus idf). b ranges from 1 (scaling by
documentlength)to0(nolengthscaling). Manningetal.(2008)suggestreasonable
valuesarek=[1.2,2]andb=0.75. Kamphuisetal.(2020)isausefulsummaryof
themanyminorvariantsofBM25.
Stopwords Inthepastitwascommontoremovehigh-frequencywordsfromboth
thequeryanddocumentbeforerepresentingthem. Thelistofsuchhigh-frequency
stoplist wordstoberemovediscalledastoplist. Theintuitionisthathigh-frequencyterms
(oftenfunctionwordslikethe, a, to)carrylittlesemanticweightandmaynothelp
with retrieval, and can also help shrink the inverted index files we describe below.
The downside of using a stop list is that it makes it difficult to search for phrases
thatcontainwordsinthestoplist. Forexample,commonstoplistswouldreducethe
phrasetobeornottobetothephrasenot.InmodernIRsystems,theuseofstoplists
is much less common, partly due to improved efficiency and partly because much
oftheirfunctionisalreadyhandledbyIDFweighting,whichdownweightsfunction
wordsthatoccurineverydocument.Nonetheless,stopwordremovalisoccasionally
usefulinvariousNLPtaskssoisworthkeepinginmind.274 CHAPTER14 • QUESTIONANSWERINGANDINFORMATIONRETRIEVAL
14.1.3 InvertedIndex
Inordertocomputescores,weneedtoefficientlyfinddocumentsthatcontainwords
inthequery. (AswesawinFig.14.3,anydocumentthatcontainsnoneofthequery
termswillhaveascoreof0andcanbeignored.) ThebasicsearchprobleminIRis
thustofindalldocumentsd Cthatcontainatermq Q.
∈ ∈
invertedindex The data structure for this task is the inverted index, which we use for mak-
ing this search efficient, and also conveniently storing useful information like the
documentfrequencyandthecountofeachtermineachdocument.
Aninvertedindex,givenaqueryterm,givesalistofdocumentsthatcontainthe
postings term. Itconsistsoftwoparts,adictionaryandthepostings. Thedictionaryisalist
ofterms(designedtobeefficientlyaccessed),eachpointingtoapostingslistforthe
term. A postings list is the list of document IDs associated with each term, which
canalsocontaininformationlikethetermfrequencyoreventheexactpositionsof
terms in the document. The dictionary can also start the document frequency for
eachtermForexample,asimpleinvertedindexforour4sampledocumentsabove,
witheachwordcontainingitsdocumentfrequencyin ,andapointertoapostings
{}
listthatcontainsdocumentIDsandtermcountsin[],mightlooklikethefollowing:
how 1 3[1]
{ } →
is 1 3[1]
{ } →
love 2 1[1] 3[1]
{ } → →
nurse 2 1[1] 4[1]
{ } → →
sorry 1 2[1]
{ } →
sweet 3 1[2] 2[1] 3[1]
{ } → → →
Given a list of terms in query, we can very efficiently get lists of all candidate
documents,togetherwiththeinformationnecessarytocomputethetf-idfscoreswe
need.
Therearealternativestotheinvertedindex. Forthequestion-answeringdomain
of finding Wikipedia pages to match a user query, Chen et al. (2017a) show that
indexing based on bigrams works better than unigrams, and use efficient hashing
algorithmsratherthantheinvertedindextomakethesearchefficient.
14.1.4 EvaluationofInformation-RetrievalSystems
Wemeasuretheperformanceofrankedretrievalsystemsusingthesameprecision
and recall metrics we have been using. We make the assumption that each docu-
mentreturnedbytheIRsystemiseitherrelevanttoourpurposesornotrelevant.
Precisionisthefractionofthereturneddocumentsthatarerelevant,andrecallisthe
fraction of all relevant documents that are returned. More formally, let’s assume a
systemreturnsT rankeddocumentsinresponsetoaninformationrequest,asubset
Rofthesearerelevant,adisjointsubset,N,aretheremainingirrelevantdocuments,
andU documentsinthecollectionasawholearerelevanttothisrequest. Precision
andrecallarethendefinedas:
R R
Precision= | | Recall= | | (14.11)
T U
| | | |
Unfortunately,thesemetricsdon’tadequatelymeasuretheperformanceofasystem
that ranks the documents it returns. If we are comparing the performance of two
rankedretrievalsystems,weneedametricthatpreferstheonethatrankstherelevant
documents higher. We need to adapt precision and recall to capture how well a
systemdoesatputtingrelevantdocumentshigherintheranking.14.1 • INFORMATIONRETRIEVAL 275
Rank Judgment PrecisionRank RecallRank
1 R 1.0 .11
2 N .50 .11
3 R .66 .22
4 N .50 .22
5 R .60 .33
6 R .66 .44
7 N .57 .44
8 R .63 .55
9 N .55 .55
10 N .50 .55
11 R .55 .66
12 N .50 .66
13 N .46 .66
14 N .43 .66
15 R .47 .77
16 N .44 .77
17 N .44 .77
18 R .44 .88
19 N .42 .88
20 N .40 .88
21 N .38 .88
22 N .36 .88
23 N .35 .88
24 N .33 .88
25 R .36 1.0
Figure14.4 Rank-specific precision and recall values calculated as we proceed down
throughasetofrankeddocuments(assumingthecollectionhas9relevantdocuments).
1.0
0.8
0.6
0.4
0.2
0.0
0.0 0.2 0.4 0.6 0.8 1.0
Recall
Figure14.5 Theprecisionrecallcurveforthedataintable14.4.
Let’sturntoanexample. AssumethetableinFig.14.4givesrank-specificpre-
cisionandrecallvaluescalculatedasweproceeddownthroughasetofrankeddoc-
umentsforaparticularquery;theprecisionsarethefractionofrelevantdocuments
seenatagivenrank,andrecallsthefractionofrelevantdocumentsfoundatthesame
rank. Therecallmeasuresinthisexamplearebasedonthisqueryhaving9relevant
documentsinthecollectionasawhole.
Note that recall is non-decreasing; when a relevant document is encountered,
noisicerP276 CHAPTER14 • QUESTIONANSWERINGANDINFORMATIONRETRIEVAL
recallincreases,andwhenanon-relevantdocumentisfounditremainsunchanged.
Precision, on the other hand, jumps up and down, increasing when relevant doc-
uments are found, and decreasing otherwise. The most common way to visualize
precision-recall precision and recall is to plot precision against recall in a precision-recall curve,
curve
liketheoneshowninFig.14.5forthedataintable14.4.
Fig.14.5showsthevaluesforasinglequery. Butwe’llneedtocombinevalues
forallthequeries,andinawaythatletsuscompareonesystemtoanother.Oneway
ofdoingthisistoplotaveragedprecisionvaluesat11fixedlevelsofrecall(0to100,
in steps of 10). Since we’re not likely to have datapoints at these exact levels, we
interpolated useinterpolatedprecisionvaluesforthe11recallvaluesfromthedatapointswedo
precision
have. Wecanaccomplishthisbychoosingthemaximumprecisionvalueachieved
atanylevelofrecallatorabovetheonewe’recalculating. Inotherwords,
IntPrecision(r)=maxPrecision(i) (14.12)
i>=r
Thisinterpolationschemenotonlyletsusaverageperformanceoverasetofqueries,
but also helps smooth over the irregular precision values in the original data. It is
designedtogivesystemsthebenefitofthedoubtbyassigningthemaximumpreci-
sionvalueachievedathigherlevelsofrecallfromtheonebeingmeasured. Fig.14.6
andFig.14.7showtheresultinginterpolateddatapointsfromourexample.
InterpolatedPrecision Recall
1.0 0.0
1.0 .10
.66 .20
.66 .30
.66 .40
.63 .50
.55 .60
.47 .70
.44 .80
.36 .90
.36 1.0
Figure14.6 InterpolateddatapointsfromFig.14.4.
GivencurvessuchasthatinFig.14.7wecancomparetwosystemsorapproaches
by comparing their curves. Clearly, curves that are higher in precision across all
recallvaluesarepreferred. However,thesecurvescanalsoprovideinsightintothe
overall behavior of a system. Systems that are higher in precision toward the left
mayfavorprecisionoverrecall, whilesystemsthataremoregearedtowardsrecall
willbehigherathigherlevelsofrecall(totheright).
meanaverage A second way to evaluate ranked retrieval is mean average precision (MAP),
precision
which provides a single metric that can be used to compare competing systems or
approaches. In this approach, we again descend through the ranked list of items,
but nowwe note theprecision only at thosepoints where arelevant item hasbeen
encountered(forexampleatranks1,3,5,6butnot2or4inFig.14.4). Forasingle
query, we average these individual precision measurements over the return set (up
to some fixed cutoff). More formally, if we assume that R is the set of relevant
r
documentsatorabover,thentheaverageprecision(AP)forasinglequeryis
1
AP= Precision r(d) (14.13)
R
r
| |d (cid:88)∈Rr14.1 • INFORMATIONRETRIEVAL 277
Interpolated Precision Recall Curve
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
Figure14.7 An 11 point interpolated precision-recall curve. Precision at each of the 11
standardrecalllevelsisinterpolatedforeachqueryfromthemaximumatanyhigherlevelof
recall.Theoriginalmeasuredprecisionrecallpointsarealsoshown.
wherePrecision (d)istheprecisionmeasuredattherankatwhichdocumentdwas
r
found. For an ensemble of queries Q, we then average over these averages, to get
ourfinalMAPmeasure:
1
MAP= AP(q) (14.14)
Q
| |q Q
(cid:88)∈
TheMAPforthesinglequery(hence=AP)inFig.14.4is0.6.
14.1.5 IRwithDenseVectors
Theclassictf-idforBM25algorithmsforIRhavelongbeenknowntohaveacon-
ceptual flaw: they work only if there is exact overlap of words between the query
anddocument. Inotherwords,theuserposingaquery(oraskingaquestion)needs
toguessexactlywhatwordsthewriteroftheanswermighthaveusedtodiscussthe
issue. As Lin et al. (2021) put it, the user might decide to search for a tragic love
story but Shakespeare writes instead about star-crossed lovers. This is called the
vocabularymismatchproblem(Furnasetal.,1987).
The solution to this problem is to use an approach that can handle synonymy:
instead of (sparse) word-count vectors, using (dense) embeddings. This idea was
proposed quite early with the LSI approach (Deerwester et al., 1990), but modern
methods all make use of encoders like BERT. In what is sometimes called a bi-
encoder we use two separate encoder models, one to encode the query and one to
encodethedocument,andusethedotproductbetweenthesetwovectorsasthescore
(Fig.14.8.Forexample,ifweusedBERT,wewouldhavetwoencodersBERT and
Q
BERT andwecouldrepresentthequeryanddocumentasthe[CLS]tokenofthe
D
respectiveencoders(Karpukhinetal.,2020):
h =BERT (q)[CLS]
q Q
h =BERT (d)[CLS]
d D
score(d,q)=h q h d (14.15)
·
noisicerP278 CHAPTER14 • QUESTIONANSWERINGANDINFORMATIONRETRIEVAL
•
h h
q d
ENCODER ENCODER
query document
q1 … qn d1 … dn
Figure14.8 BERTbi-encoderforcomputingrelevanceofadocumenttoaquery.
More complex versions can use other ways to represent the encoded text, such as
usingaveragepoolingovertheBERToutputsofalltokensinsteadofusingtheCLS
token,orcanaddextraweightmatricesaftertheencodingordotproductsteps(Liu
etal.2016a,Leeetal.2019).
UsingdensevectorsforIRortheretrievercomponentofquestionanswerersis
stillanopenareaofresearch. Amongthemanyareasofactiveresearcharehowto
dothefine-tuningoftheencodermodulesontheIRtask(generallybyfine-tuningon
query-documentcombinations,withvariouscleverwaystogetnegativeexamples),
and how to deal with the fact that documents are often longer than encoders like
BERTcanprocess(generallybybreakingupdocumentsintopassages).
Efficiencyisalsoanissue. AtthecoreofeveryIRengineistheneedtorankev-
erypossibledocumentforitssimilaritytothequery. Forsparseword-countvectors,
theinvertedindexallowsthisveryefficiently.Fordensevectoralgorithmslikethose
based on BERT or other Transformer encoders, finding the set of dense document
vectorsthathavethehighestdotproductwithadensequeryvectorisanexampleof
nearestneighborsearch.Modernsystemsthereforemakeuseofapproximatenearest
Faiss neighborvectorsearchalgorithmslikeFaiss(Johnsonetal.,2017).
14.2 IR-based Factoid Question Answering
IR-basedQA ThegoalofIR-basedQA(sometimescalledopendomainQA)istoanswerauser’s
questionbyfindingshorttextsegmentsfromtheweborsomeotherlargecollection
ofdocuments. Figure14.9showssomesamplefactoidquestionsandtheiranswers.
Question Answer
WhereistheLouvreMuseumlocated? inParis,France
WhatarethenamesofOdin’sravens? HuginnandMuninn
Whatkindofnutsareusedinmarzipan? almonds
WhatinstrumentdidMaxRoachplay? drums
What’stheofficiallanguageofAlgeria? Arabic
Figure14.9 Somefactoidquestionsandtheiranswers.
retrieveand ThedominantparadigmforIR-basedQAistheretrieveandreadmodelshown
read
inFig. 14.10. Inthefirst stageofthis 2-stagemodelweretrieve relevantpassages
from a text collection, usually using a search engines of the type we saw in the
previous section. In the second stage, a neural reading comprehension algorithm
passesovereachpassageandfindsspansthatarelikelytoanswerthequestion.
Some question answering systems focus only on the second task, the reading
reading comprehensiontask. Readingcomprehensionsystemsaregivenafactoidquestion
comprehension
qandapassage pthatcouldcontaintheanswer,andreturnananswers(orperhaps
declarethatthereisnoanswerinthepassage,orinsomesetupsmakeachoicefrom14.2 • IR-BASEDFACTOIDQUESTIONANSWERING 279
query
Retriever Reader
Q: When was docs start end A: 1791
the premiere of BERT
The Magic Flute? [CLS] q1 q2 [SEP] d1 d2
Relevant
Docs
Indexed Docs
Figure14.10 IR-based factoid question answering has two stages: retrieval, which returns relevant doc-
uments from the collection, and reading, in which a neural reading comprehension system extracts answer
spans.
asetofpossibleanswers). Ofcoursethissetupdoesnotmatchtheinformationneed
of users who have a question they need answered (after all, if a user knew which
passagecontainedtheanswer,theycouldjustreaditthemselves). Instead,thistask
wasoriginallymodeledonchildren’sreadingcomprehensiontests—pedagogicalin-
struments in which a child is given a passage to read and must answer questions
aboutit—asawaytoevaluatenaturallanguageprocessingperformance(Hirschman
etal.,1999). Readingcomprehensionsystemsarestillusedthatway,buthavealso
evolvedtofunctionasthesecondstageofthemodernretrieveandreadmodel.
Otherquestionansweringsystemsaddresstheentireretrieveandreadtask;they
aregivenafactoidquestionandalargedocumentcollection(suchasWikipediaor
a crawl of the web) and return an answer, usually a span of text extracted from a
document. ThistaskisoftencalledopendomainQA.
Inthenextfewsectionswe’lllayoutthevariouspiecesofIR-basedQA,starting
withsomecommonlyuseddatasets.
14.2.1 IR-basedQA:Datasets
DatasetsforIR-basedQAaremostcommonlycreatedbyfirstdevelopingreading
comprehensiondatasetscontainingtuplesof(passage,question,answer).Reading
comprehensionsystemscanusethedatasetstotrainareaderthatisgivenapassage
and a question, and predicts a span in the passage as the answer. Including the
passage from which the answer is to be extracted eliminates the need for reading
comprehensionsystemstodealwithIR.
SQuAD For example the Stanford Question Answering Dataset (SQuAD) consists of
passages from Wikipedia and associated questions whose answers are spans from
the passage (Rajpurkar et al. 2016). Squad 2.0 in addition adds some questions
that are designed to be unanswerable (Rajpurkar et al. 2018), with a total of just
over150,000questions. Fig.14.11showsa(shortened)excerptfromaSQUAD2.0
passagetogetherwiththreequestionsandtheirgoldanswerspans.
SQuADwasbuiltbyhavinghumansreadagivenWikipediapassage,writeques-
tionsaboutthepassage,andchooseaspecificanswerspan.
Other datasets are created by similar techniques but try to make the questions
HotpotQA morecomplex. TheHotpotQAdataset(Yangetal.,2018)wascreatedbyshowing
crowd workers multiple context documents and asked to come up with questions
thatrequirereasoningaboutallofthedocuments.
ThefactthatquestionsindatasetslikeSQuADorHotpotQAarecreatedbyan-
notatorswhohavefirstreadthepassagemaymaketheirquestionseasiertoanswer,
sincetheannotatormay(subconsciously)makeuseofwordsfromtheanswertext.280 CHAPTER14 • QUESTIONANSWERINGANDINFORMATIONRETRIEVAL
Beyonce´ Giselle Knowles-Carter (born September 4, 1981) is an American singer, songwriter,
record producer and actress. Born and raised in Houston,Texas, she performed in various
singinganddancing competitions as a child, and rose to fame in the late 1990s as lead singer
ofR&Bgirl-groupDestiny’sChild. Managedbyherfather,MathewKnowles,thegroupbecame
one of the world’s best-selling girl groups of all time. Their hiatus saw the release of Beyonce´’s
debutalbum,DangerouslyinLove(2003),whichestablishedherasasoloartistworldwide,earned
fiveGrammyAwardsandfeaturedtheBillboardHot100number-onesingles“CrazyinLove”and
“BabyBoy”.
Q:“InwhatcityandstatedidBeyonce´ growup?”
A:“Houston,Texas”
Q:“WhatareasdidBeyonce´ competeinwhenshewasgrowingup?”
A:“singinganddancing”
Q:“WhendidBeyonce´ releaseDangerouslyinLove?”
A:“2003”
Figure14.11 A (Wikipedia) passage from the SQuAD 2.0 dataset (Rajpurkar et al., 2018) with 3 sample
questionsandthelabeledanswerspans.
Asolutiontothispossiblebiasistomakedatasetsfromquestionsthatwerenot
writtenwithapassageinmind. TheTriviaQAdataset(Joshietal.,2017)contains
94K questions written by trivia enthusiasts, together with supporting documents
fromWikipediaandthewebresultingin650Kquestion-answer-evidencetriples.
Natural The Natural Questions dataset (Kwiatkowski et al., 2019) incorporates real
Questions
anonymizedqueriestotheGooglesearchengine. Annotatorsarepresentedaquery,
alongwithaWikipediapagefromthetop5searchresults,andannotateaparagraph-
lengthlonganswerandashortspananswer,ormarknullifthetextdoesn’tcontain
the paragraph. For example the question “When are hops added to the brewing
process?” has the short answer the boiling process and a long answer which the
surrounding entire paragraph from the Wikipedia page on Brewing. In using this
dataset, a reading comprehension model is given a question and a Wikipedia page
andmustreturnalonganswer,shortanswer,or’noanswer’response.
TyDiQA The above datasets are all in English. The TyDi QA dataset contains 204K
question-answer pairs from 11 typologically diverse languages, including Arabic,
Bengali, Kiswahili, Russian, and Thai (Clark et al., 2020). In the TYDI QA task,
a system is given a question and the passages from a Wikipedia article and must
(a) select the passage containing the answer (or NULL if no passage contains the
answer), and (b) mark the minimal answer span (or NULL). Many questions have
noanswer. ThevariouslanguagesinthedatasetbringupchallengesforQAsystems
likemorphologicalvariationbetweenthequestionandtheanswer,orcomplexissue
withwordsegmentationormultiplealphabets.
Inthereadingcomprehensiontask,asystemisgivenaquestionandthepassage
inwhichtheanswershouldbefound. Inthefulltwo-stageQAtask,however,sys-
tems are not given a passage, but are required to do their own retrieval from some
documentcollection. Acommonwaytocreateopen-domainQAdatasetsistomod-
ifyareadingcomprehensiondataset. Forresearchpurposesthisismostcommonly
donebyusingQAdatasetsthatannotateWikipedia(likeSQuADorHotpotQA).For
training,theentire(question,passage,answer)tripleisusedtotrainthereader. But
atinferencetime, thepassagesareremovedandsystemisgivenonlythequestion,
togetherwithaccesstotheentireWikipediacorpus. ThesystemmustthendoIRto
findasetofpagesandthenreadthem.14.2 • IR-BASEDFACTOIDQUESTIONANSWERING 281
14.2.2 IR-basedQA:Reader(AnswerSpanExtraction)
The first stage of IR-based QA is a retriever, for example of the type we saw in
Section14.1. ThesecondstageofIR-basedquestionansweringisthereader. The
reader’sjobistotakeapassageasinputandproducetheanswer. Intheextractive
extractiveQA QAwediscusshere,theanswerisaspanoftextinthepassage.5 Forexamplegiven
a question like “How tall is Mt. Everest?” and a passage that contains the clause
Reaching29,029feetatitssummit,areaderwilloutput29,029feet.
Theanswerextractiontaskiscommonlymodeledbyspanlabeling: identifying
span inthepassageaspan(acontinuousstringoftext)thatconstitutesananswer. Neural
algorithms for reading comprehension are given a question q of n tokens q ,...,q
1 n
andapassage pofmtokens p ,...,p . Theirgoalisthustocomputetheprobability
1 m
P(aq,p)thateachpossiblespanaistheanswer.
|
Ifeachspanastartsatpositiona andendsatpositiona ,wemakethesimplify-
s e
ingassumptionthatthisprobabilitycanbeestimatedasP(aq,p)=P (a q,p)P (a q,p).
start s end e
| | |
Thusforforeachtoken p
i
inthepassagewe’llcomputetwoprobabilities: pstart(i)
that p isthestartoftheanswerspan, and p (i)that p istheendoftheanswer
i end i
span.
A standard baseline algorithm for reading comprehension is to pass the ques-
tionandpassagetoanyencoderlikeBERT(Fig.14.12),asstringsseparatedwitha
[SEP]token,resultinginanencodingtokenembeddingforeverypassagetoken p.
i
Pstart Pend
i i
. .
…
…
S E
i
Encoder (BERT)
…
[CLS] q q [SEP] p … p
1 n 1 m
Question Passage
Figure14.12 An encoder model (using BERT) for span-based question answering from
reading-comprehension-basedquestionansweringtasks.
For span-based question answering, we represent the question as the first se-
quenceandthepassageasthesecondsequence. We’llalsoneedtoaddalinearlayer
thatwillbetrainedinthefine-tuningphasetopredictthestartandendpositionofthe
span. We’lladdtwonewspecialvectors: aspan-startembeddingSandaspan-end
embeddingE, whichwillbelearnedinfine-tuning. Togetaspan-startprobability
foreachoutputtoken p,wecomputethedotproductbetweenSand p andthenuse
(cid:48)i (cid:48)i
asoftmaxtonormalizeoveralltokens p inthepassage:
(cid:48)i
exp(S p)
P starti = exp(·
S
(cid:48)i
p )
(14.16)
j · (cid:48)j
5 Hereweskipthemoredifficulttaskofabstr(cid:80) activeQA,inwhichthesystemcanwriteananswer
whichisnotdrawnexactlyfromthepassage.282 CHAPTER14 • QUESTIONANSWERINGANDINFORMATIONRETRIEVAL
Wedotheanalogousthingtocomputeaspan-endprobability:
exp(E p)
P endi = exp(E·
(cid:48)i
p )
(14.17)
j · (cid:48)j
Thescoreofacandidatespanfrompos(cid:80)itionito j isS p +E p , andthehighest
·
(cid:48)i
·
(cid:48)j
scoringspaninwhich j iischosenisthemodelprediction.
≥
Thetraininglossforfine-tuningisthenegativesumofthelog-likelihoodsofthe
correctstartandendpositionsforeachinstance:
L= −logP starti−logP endi (14.18)
Manydatasets(likeSQuAD2.0andNaturalQuestions)alsocontain(question,
passage) pairs in which the answer is not contained in the passage. We thus also
need a way to estimate the probability that the answer to a question is not in the
document. Thisisstandardlydonebytreatingquestionswithnoanswerashaving
the[CLS]tokenastheanswer,andhencetheanswerspanstartandendindexwill
pointat[CLS](Devlinetal.,2019).
Formanydatasetstheannotateddocuments/passagesarelongerthanthemaxi-
mum512inputtokensBERTallows,suchasNaturalQuestionswhosegoldpassages
arefullWikipediapages. Insuchcases,followingAlbertietal.(2019),wecancre-
ate multiple pseudo-passage observations from the labeled Wikipedia page. Each
observationisformedbyconcatenating[CLS],thequestion,[SEP],andtokensfrom
the document. We walk through the document, sliding a window of size 512 (or
rather,512minusthequestionlengthnminusspecialtokens)andpackingthewin-
dowoftokensintoeachnextpseudo-passage. Theanswerspanfortheobservation
iseitherlabeled[CLS](=noanswerinthisparticularwindow)orthegold-labeled
spanismarked. Thesameprocesscanbeusedforinference, breakingupeachre-
trieveddocumentintoseparateobservationpassagesandlabelingeachobservation.
Theanswercanbechosenasthespanwiththehighestprobability(ornilifnospan
ismoreprobablethan[CLS]).
14.3 Entity Linking
We’ve now seen the first major paradigm for question answering, IR-based QA.
Before we turn to the second major paradigm for question answering, knowledge-
based question answering, we introduce the important core technology of entity
linking,sinceitisrequiredforanyknowledge-basedQAalgorithm.
entitylinking Entitylinkingisthetaskofassociatingamentionintextwiththerepresentation
ofsomereal-worldentityinanontology(JiandGrishman,2011).
Themostcommonontologyforfactoidquestion-answeringisWikipedia,since
Wikipedia is often the source of the text that answers the question. In this usage,
eachuniqueWikipediapageactsastheuniqueidforaparticularentity. Thistaskof
decidingwhichWikipediapagecorrespondingtoanindividualisbeingreferredto
wikification byatextmentionhasitsownname: wikification(MihalceaandCsomai,2007).
Since the earliest systems (Mihalcea and Csomai 2007, Cucerzan 2007, Milne
and Witten 2008), entity linking is done in (roughly) two stages: mention detec-
tion and mention disambiguation. We’ll give two algorithms, one simple classic
baseline that uses anchor dictionaries and information from the Wikipedia graph
structure(FerraginaandScaiella,2011)andonemodernneuralalgorithm(Lietal.,14.3 • ENTITYLINKING 283
2020). We’ll focus here mainly on the application of entity linking to questions
ratherthanothergenres.
14.3.1 LinkingbasedonAnchorDictionariesandWebGraph
AsasimplebaselineweintroducetheTAGMElinker(FerraginaandScaiella,2011)
forWikipedia,whichitselfdrawsonearlieralgorithms(MihalceaandCsomai2007,
Cucerzan2007, MilneandWitten2008). Wikificationalgorithmsdefinethesetof
entities as the set of Wikipedia pages, so we’ll refer to each Wikipedia page as a
unique entity e. TAGME first creates a catalog of all entities (i.e. all Wikipedia
pages,removingsomedisambiguationandothermeta-pages)andindexesthemina
standardIRenginelikeLucene. Foreachpagee,thealgorithmcomputesanin-link
countin(e): thetotalnumberofin-linksfromotherWikipediapagesthatpointtoe.
ThesecountscanbederivedfromWikipediadumps.
Finally, the algorithm requires an anchor dictionary. An anchor dictionary
anchortexts lists for each Wikipedia page, its anchor texts: the hyperlinked spans of text on
other pages that point to it. For example, the web page for Stanford University,
http://www.stanford.edu,mightbepointedtofromanotherpageusinganchor
textslikeStanfordorStanfordUniversity:
<a href="http://www.stanford.edu">Stanford University</a>
We compute a Wikipedia anchor dictionary by including, for each Wikipedia
pagee,e’stitleaswellasalltheanchortextsfromallWikipediapagesthatpointtoe.
Foreachanchorstringawe’llalsocomputeitstotalfrequencyfreq(a)inWikipedia
(includingnon-anchoruses),thenumberoftimesaoccursasalink(whichwe’llcall
link(a)),anditslinkprobabilitylinkprob(a)=link(a)/freq(a).Somecleanupofthe
finalanchordictionaryisrequired,forexampleremovinganchorstringscomposed
onlyofnumbersorsinglecharacters,thatareveryrare,orthatareveryunlikelyto
beusefulentitiesbecausetheyhaveaverylowlinkprob.
MentionDetection Givenaquestion(orothertextwearetryingtolink),TAGME
detects mentions by querying the anchor dictionary for each token sequence up to
6 words. This large set of sequences is pruned with some simple heuristics (for
examplepruningsubstringsiftheyhavesmalllinkprobs). Thequestion:
WhenwasAdaLovelaceborn?
mightgiverisetotheanchorAdaLovelaceandpossiblyAda, butsubstringsspans
likeLovelacemightbeprunedashavingtoolowalinkprob,andbutspanslikeborn
havesuchalowlinkprobthattheywouldnotbeintheanchordictionaryatall.
MentionDisambiguation Ifamentionspanisunambiguous(pointstoonlyone
entity/Wikipediapage),wearedonewithentitylinking! However,manyspansare
ambiguous, matching anchors for multiple Wikipedia entities/pages. The TAGME
algorithm uses two factors for disambiguating ambiguous spans, which have been
referredtoaspriorprobabilityandrelatedness/coherence. Thefirstfactoris p(ea),
|
theprobabilitywithwhichthespanreferstoaparticularentity. Foreachpagee
E(a), theprobability p(ea)thatanchorapointstoe, istheratioofthenumbero∈ f
|
linksintoewithanchortextatothetotalnumberofoccurrencesofaasananchor:
count(a e)
prior(a e) = p(ea)= → (14.19)
→ | link(a)
Let’sseehowthatfactorworksinlinkingentitiesinthefollowingquestion:284 CHAPTER14 • QUESTIONANSWERINGANDINFORMATIONRETRIEVAL
WhatChineseDynastycamebeforetheYuan?
ThemostcommonassociationforthespanYuanintheanchordictionaryisthename
oftheChinesecurrency,i.e.,theprobability p(Yuan currency yuan)isveryhigh.
|
Rarer Wikipedia associations for Yuan include the common Chinese last name, a
language spoken in Thailand, and the correct entity in this case, the name of the
Chinesedynasty. Soifwechosebasedonlyon p(ea),wewouldmakethewrong
|
disambiguationandmissthecorrectlink,Yuan dynasty.
Tohelpinjustthissortofcase,TAGMEusesasecondfactor,therelatednessof
this entity to other entities in the input question. In our example, the fact that the
questionalsocontainsthespanChineseDynasty,whichhasahighprobabilitylinkto
thepageDynasties in Chinese history,oughttohelpmatchYuan dynasty.
Let’sseehowthisworks. Givenaquestionq, foreachcandidateanchorsspan
adetectedinq,weassignarelatednessscoretoeachpossibleentitye E(a)ofa.
∈
Therelatednessscoreofthelinka eistheweightedaveragerelatednessbetween
→
e and all other entities in q. Two entities are considered related to the extent their
Wikipediapagessharemanyin-links. Moreformally, therelatednessbetweentwo
entitiesAandBiscomputedas
log(max(in(A), in(B))) log(in(A) in(B))
rel(A,B) = | | | | − | ∩ | (14.20)
log(W ) log(min(in(A), in(B)))
| | − | | | |
wherein(x)isthesetofWikipediapagespointingtoxandW isthesetofallWiki-
pediapagesinthecollection.
The vote given by anchor b to the candidate annotation a X is the average,
→
overallthepossibleentitiesofb, oftheirrelatednesstoX, weightedbytheirprior
probability:
1
vote(b,X)= rel(X,Y)p(Y b) (14.21)
E(b) |
| |Y ∈(cid:88)E(b)
Thetotalrelatednessscorefora X isthesumofthevotesofalltheotheranchors
→
detectedinq:
relatedness(a X)= vote(b,X) (14.22)
→
b ∈(cid:88)X q\a
To score a X, we combine relatedness and prior by choosing the entity X
→
that has the highest relatedness(a X), finding other entities within a small (cid:15) of
→
thisvalue,andfromthisset,choosingtheentitywiththehighestpriorP(X a). The
|
resultofthisstepisasingleentityassignedtoeachspaninq.
The TAGME algorithm has one further step of pruning spurious anchor/entity
pairs,assigningascoreaveraginglinkprobabilitywiththecoherence.
1
coherence(a X)= rel(B,X)
→ S 1
| |− B(cid:88)∈S \X
coherence(a X)+linkprob(a)
score(a X)= → (14.23)
→ 2
Finally, pairs are pruned if score(a X)<λ, where the threshold λ is set on a
→
held-outset.14.3 • ENTITYLINKING 285
14.3.2 NeuralGraph-basedlinking
More recent entity linking models are based on biencoders, encoding a candidate
mention span, encoding an entity, and computing the dot product between the en-
codings. This allows embeddings for all the entities in the knowledge base to be
precomputedandcached(Wuetal.,2020). Let’ssketchtheELQlinkingalgorithm
ofLietal.(2020), whichisgivenaquestionqandasetofcandidateentitiesfrom
WikipediawithassociatedWikipediatext,andoutputstuples(e,m ,m )ofentityid,
s e
mentionstart,andmentionend. AsFig.14.13shows,itdoesthisbyencodingeach
WikipediaentityusingtextfromWikipedia,encodingeachmentionspanusingtext
fromthequestion,andcomputingtheirsimilarity,aswedescribebelow.
Figure14.13 AsketchoftheinferenceprocessintheELQalgorithmforentitylinkingin
questions(Lietal.,2020). Eachcandidatequestionmentionspanandcandidateentityare
separatelyencoded,andthenscoredbytheentity/spandotproduct.
EntityMentionDetection Togetanh-dimensionalembeddingforeachquestion
token,thealgorithmrunsthequestionthroughBERTinthenormalway:
[q 1 q n]=BERT([CLS]q 1 q n[SEP]) (14.24)
··· ···
It then computes the likelihood of each span [i,j] in q being an entity mention, in
a way similar to the span-based algorithm we saw for the reader above. First we
computethescorefori/jbeingthestart/endofamention:
s start(i)=w start q i, s end(j)=w end q j, (14.25)
· ·
where w and w are vectors learned during training. Next, another trainable
start end
embedding,w isusedtocomputeascoreforeachtokenbeingpartofamen-
mention
tion:
s mention(t)=w mention q t (14.26)
·
Mentionprobabilitiesarethencomputedbycombiningthesethreescores:
j
p([i,j])=σ s start(i)+s end(j)+ s mention(t) (14.27)
(cid:32) (cid:33)
t=i
(cid:88)286 CHAPTER14 • QUESTIONANSWERINGANDINFORMATIONRETRIEVAL
Entity Linking To link mentions to entities, we next compute embeddings for
each entity in the set E=e , ,e, ,e of all Wikipedia entities. For each en-
1 i w
··· ···
tity e we’ll get text from the entity’s Wikipedia page, the title t(e) and the first
i i
128 tokens of the Wikipedia page which we’ll call the description d(e). This is
i
againrunthroughBERT,takingtheoutputoftheCLStokenBERT astheentity
[CLS]
representation:
x e=BERT [CLS]([CLS]t(e i)[ENT]d(e i)[SEP]) (14.28)
Mention spans can be linked to entities by computing, for each entity e and span
[i,j],thedotproductsimilaritybetweenthespanencoding(theaverageofthetoken
embeddings)andtheentityencoding.
j
1
y = q
i,j t
(j i+1)
− t i
(cid:88)−
s(e,[i,j])=x ·ey i,j (14.29)
Finally,wetakeasoftmaxtogetadistributionoverentitiesforeachspan:
exp(s(e,[i,j]))
p(e[i,j])= (14.30)
|
e
Eexp(s(e (cid:48),[i,j]))
(cid:48)∈
Training TheELQmentiondetec(cid:80)tionandentitylinkingalgorithmisfullysuper-
vised. This means, unlike the anchor dictionary algorithms from Section 14.3.1,
it requires datasets with entity boundaries marked and linked. Two such labeled
datasetsareWebQuestionsSP(Yihetal.,2016),anextensionoftheWebQuestions
(Berantetal.,2013)datasetderivedfromGooglesearchquestions,andGraphQues-
tions (Su et al., 2016). Both have had entity spans in the questions marked and
linked(SorokinandGurevych2018, Lietal.2020)resultinginentity-labeledver-
sionsWebQSP andGraphQ (Lietal.,2020).
EL EL
Given a training set, the ELQ mention detection and entity linking phases are
trained jointly, optimizing the sum of their losses. The mention detection loss is a
binarycross-entropyloss
1
L MD=
−N
y [i,j]logp([i,j])+(1 −y [i,j])log(1 −p([i,j])) (14.31)
1 ≤i ≤j ≤m(cid:88)in(i+L −1,n)
(cid:0) (cid:1)
withy =1if[i,j]isagoldmentionspan,else0. Theentitylinkinglossis:
[i,j]
L ED= logp(e g [i,j]) (14.32)
− |
wheree isthegoldentityformention[i,j].
g
See the end of the chapter for more discussion of other applications of entity
linkingoutsideofquestionanswering.
14.4 Knowledge-based Question Answering
While an enormous amount of information is encoded in the vast amount of text
on the web, information obviously also exists in more structured forms. We use
thetermknowledge-basedquestionansweringfortheideaofansweringanatural14.4 • KNOWLEDGE-BASEDQUESTIONANSWERING 287
languagequestionbymappingittoaqueryoverastructureddatabase.Likethetext-
basedparadigmforquestionanswering,thisapproachdatesbacktotheearliestdays
ofnaturallanguageprocessing,withsystemslikeBASEBALL(Greenetal.,1961)
thatansweredquestionsfromastructureddatabaseofbaseballgamesandstats.
Two common paradigms are used for knowledge-based QA. The first, graph-
basedQA,modelstheknowledgebaseasagraph,oftenwithentitiesasnodesand
relations or propositions as edges between nodes. The second, QA by semantic
parsing, usingthesemanticparsingmethodswesawinChapter20. Bothofthese
methodsrequiresomesortofentitylinkingthatwedescribedinthepriorsection.
14.4.1 Knowledge-BasedQAfromRDFtriplestores
Let’sintroducethecomponentsofasimpleknowledge-basedQAsystemafterentity
linking has been performed. We’ll focus on the very simplest case of graph-based
QA, in which the dataset is a set of factoids in the form of RDF triples, and the
taskistoanswerquestionsaboutoneofthemissingarguments. RecallfromChap-
ter 21 that an RDF triple is a 3-tuple, a predicate with two arguments, expressing
somesimplerelationorproposition. Popularsuchontologiesareoftenderivedfrom
Wikipedia; DBpedia (Bizer et al., 2009) has over 2 billion RDF triples, or Free-
base(Bollackeretal.,2008),nowpartofWikidata(Vrandecˇic´andKro¨tzsch,2014).
ConsideranRDFtriplelikethefollowing:
subject predicate object
AdaLovelace birth-year 1815
Thistriplecanbeusedtoanswertextquestionslike“WhenwasAdaLovelace
born?” or“Whowasbornin1815?”.
Anumberofsuchquestiondatasetsexist.SimpleQuestions(Bordesetal.,2015)
contains100KquestionswrittenbyannotatorsbasedontriplesfromFreebase. For
example, the question ”What American cartoonist is the creator of Andy Lippin-
cott?”.waswrittenbasedonthetriple(andy lippincott, character created
by, garry trudeau). FreebaseQA(Jiangetal.,2019),alignsthetriviaquestions
fromTriviaQA(Joshietal.,2017)andothersourceswithtriplesinFreebase,align-
ingforexamplethetriviaquestion“Which18thcenturyauthorwroteClarissa(or
TheCharacterHistoryofaYoungLady),saidtobethelongestnovelintheEnglish
language?”withthetriple(Clarissa, book.written-work.author, Samuel
Richardson). Anothersuchfamilyofdatasetsstartsfrom WEBQUESTIONS (Be-
rant et al., 2013), which contains 5,810 questions asked by web users, each be-
ginningwithawh-word,containingexactlyoneentity,andpairedwithhandwritten
answersdrawnfromtheFreebasepageofthequestion’sentity. WEBQUESTIONSSP
(Yihetal.,2016)augments WEBQUESTIONS withhuman-createdsemanticparses
(SPARQLqueries)forthosequestionsanswerableusingFreebase. COMPLEXWEB-
QUESTIONS augments the dataset with compositional and other kinds of complex
questions, resulting in 34,689 questions, along with answers, web snippets, and
SPARQLqueries(TalmorandBerant,2018).
Let’s assume we’ve already done the stage of entity linking introduced in the
priorsection. Thuswe’vemappedalreadyfromatextualmentionlikeAdaLovelace
tothecanonicalentityIDintheknowledgebase. Forsimpletriplerelationquestion
answering, thenextstepistodeterminewhichrelationisbeingaskedabout, map-
pingfromastringlike“Whenwas... born”tocanonicalrelationsintheknowledge
baselikebirth-year. Wemightsketchthecombinedtaskas:
“WhenwasAdaLovelaceborn?” birth-year (Ada Lovelace, ?x)
→288 CHAPTER14 • QUESTIONANSWERINGANDINFORMATIONRETRIEVAL
“WhatisthecapitalofEngland?” capital-city(?x, England)
→
Thenextstepisrelationdetectionandlinking. Forsimplequestions,wherewe
assume the question has only a single relation, relation detection and linking can
bedoneinawayresemblingtheneuralentitylinkingmodels: computingsimilarity
(generallybydotproduct)betweentheencodingofthequestiontextandanencoding
for each possible relation. For example, in the algorithm of (Lukovnikov et al.,
2019), the CLS output of a BERT model is used to represent the question span for
thepurposesofrelationdetection,andaseparatevectoristrainedforeachrelation
r. Theprobabilityofaparticularrelationr isthencomputedbysoftmaxoverthe
i i
dotproducts:
m r=BERTCLS([CLS]q
1
q n[SEP])
···
s(m ,r)=m w
r i r
·
ri
exp(s(m ,r))
r i
p(r i |q 1, ···,q n)= N exp(s(m ,r )) (14.33)
k=1 R r k
Ranking of answers Most algorith(cid:80)ms have a final stage which takes the top j
entities and the top k relations returned by the entity and relation inference steps,
searchestheknowledgebasefortriplescontainingthoseentitiesandrelations,and
then ranks those triples. This ranking can be heuristic, for example scoring each
entity/relationpairsbasedonthestringsimilaritybetweenthementionspanandthe
entities text aliases, or favoring entities that have a high in-degree (are linked to
by many relations). Or the ranking can be done by training a classifier to take the
concatenatedentity/relationencodingsandpredictaprobability.
14.4.2 QAbySemanticParsing
Thesecondkindofknowledge-basedQAusesasemanticparsertomaptheques-
tiontoastructuredprogramtoproduceananswer. Theselogicalformscantakethe
formofsomeversionofpredicatecalculus,aquerylanguagelikeSQLorSPARQL,
orsomeotherexecutableprogramliketheexamplesinFig.14.14.
Thelogicalformofthequestionisthuseitherintheformofaqueryorcaneasily
be converted into one (predicate calculus can be converted to SQL, for example).
Thedatabasecanbeafullrelationaldatabase,orsomeotherstructuredknowledge
store.
As we saw in Chapter 20, semantic parsing algorithms can be supervised fully
with questions paired with a hand-built logical form, or can be weakly supervised
by questions paired with an answer (the denotation), in which the logical form is
modeledonlyasalatentvariable.
For the fully supervised case, we can get a set of questions paired with their
correct logical form from datasets like the GEOQUERY dataset of questions about
USgeography(ZelleandMooney,1996), theDROPdatasetofcomplexquestions
(onhistoryandfootballgames)thatrequirereasoning(Duaetal.2019),ortheATIS
datasetofflightqueries,allofwhichhaveversionswithSQLorotherlogicalforms
(Iyeretal.2017,Wolfsonetal.2020,Orenetal.2020).
Thetaskisthentotakethosepairsoftrainingtuplesandproduceasystemthat
mapsfromnewquestionstotheirlogicalforms. Acommonbaselinealgorithmisa
simplesequence-to-sequencemodel,forexampleusingBERTtorepresentquestion
tokens,passingthemtoanencoder-decoder(Chapter13),assketchedinFig.14.15.
AnyotherofthesemanticparsingalgorithmsdescribedinChapter20wouldalsobe
appropriate.14.5 • USINGLANGUAGEMODELSTODOQA 289
Question Logicalform
WhatstatesborderTexas? λx.state(x) borders(x,texas)
∧
Whatisthelargeststate? argmax(λx.state(x),λx.size(x))
SELECTDISTINCTf1.flight id
FROMflightf1,airport servicea1,
cityc1,airport servicea2,cityc2
WHEREf1.from airport=a1.airport code
I’dliketobookaflightfromSanDiegoto ANDa1.city code=c1.city code
Toronto ANDc1.city name=’sandiego’
ANDf1.to airport=a2.airport code
ANDa2.city code=c2.city code
ANDc2.city name=’toronto’
Howmanypeoplesurvivedthesinkingof (count (!fb:event.disaster.survivors
theTitanic? fb:en.sinking of the titanic))
How many yards longer was Johnson’s ARITHMETIC diff( SELECT num( ARGMAX(
longesttouchdowncomparedtohisshort- SELECT ) ) SELECT num( ARGMIN( FILTER(
esttouchdownofthefirstquarter? SELECT))))
Figure14.14 Sample logical forms produced by a semantic parser for question answering, including two
questionsfromtheGeoQuerydatabaseofquestionsonU.S.Geography(ZelleandMooney,1996)withpredi-
catecalculusrepresentations,oneATISquestionwithSQL(Iyeretal.,2017),aprogramoverFreebaserelations,
andaprograminQDMR,theQuestionDecompositionMeaningRepresentation(Wolfsonetal.,2020).
lambda x state ( x ) and borders ( x , Texas )
encoder-decoder
BERT
[CLS] what states border Texas ? [SEP]
Figure14.15 An encoder-decoder semantic parser for translating a question to logical
form,withaBERTpre-encoderfollowedbyanencoder-decoder(biLSTMorTransformer).
14.5 Using Language Models to do QA
AnalternativeapproachtodoingQAistoqueryapretrainedlanguagemodel,forc-
ingamodeltoansweraquestionsolelyfrominformationstoredinitsparameters.
ForexampleRobertsetal.(2020)usetheT5languagemodel,whichisanencoder-
decoderarchitecturepretrainedtofillinmaskedspansoftask. Fig.14.16showsthe
architecture;thedeletedspansaremarkedby<M>,andthesystemistrainedtohave
thedecodergeneratingthemissingspans(separatedby<M>).
Robertsetal.(2020)thenfinetunetheT5systemtothequestionansweringtask,
bygivingitaquestion,andtrainingittooutputtheanswertextinthedecoder.Using
thelargest11-billion-parameterT5modeldoescompetitively,althoughnotquiteas
wellassystemsdesignedspecificallyforquestionanswering.
Language modeling is not yet a complete solution for question answering; for
exampleinadditiontonotworkingquiteaswell,theysufferfrompoorinterpretabil-
ity(unlikestandardQAsystems,forexample,theycurrentlycan’tgiveusersmore
contextbytellingthemwhatpassagetheanswercamefrom).Nonetheless,thestudy
ofextractinganswerfromlanguagemodelsisanintriguingareaforfuturequestion290 CHAPTER14 • QUESTIONANSWERINGANDINFORMATIONRETRIEVAL
Figure14.16 TheT5systemisanencoder-decoderarchitecture.Inpretraining,itlearnsto
fillinmaskedspansoftask(markedby<M>)bygeneratingthemissingspans(separatedby
<M>)inthedecoder. Itisthenfine-tunedonQAdatasets,giventhequestion,withoutadding
anyadditionalcontextorpassages.FigurefromRobertsetal.(2020).
answerresearch.
14.6 Classic QA Models
Whileneuralarchitecturesarethestateoftheartforquestionanswering,pre-neural
architectures using hybrids of rules and feature-based classifiers can sometimes
achieve higher performance. Here we summarize one influential classic system,
the Watson DeepQA system from IBM that won the Jeopardy! challenge in 2011
(Fig.14.17). Let’sconsiderhowithandlestheseJeopardy! examples, eachwitha
categoryfollowedbyaquestion:
Question (2) Candidate Answer Generation (3) Candidate (4)
Answer Confidence
From Text Resources Scoring Merging
Candidate
Document Answer and
( P F1 o Ar) c n o uQ L ssc we Du x ee ie rce s at Tes l ys c pt ti i ei on o ngn Docum DeP oR n ca Dt ue os ma t cs r eun i ns med ta ev D Dng a Dto oe oc cl u u c um m me e en n nt t t passages E D AA x o nc ctn r u has m ocw re t te ni eo tr x n t titles C AC aC A n naCA Ca d sn nCA An n a awid sA dad s n n n nwi ean ndwi d s d sd re td sawi wie ea d drwitr ed eet a ae era rt te erte aE nR dv e i td sr ce ie on v rc a ine l g CC CoA Ca n on n Aafi nd s n n+ d fiwi d s d e + dwe ia n d ert ec a ne e rt ce e R E Aqa M nun se iv wk r ag ei len e rsng t
QD ue ete sc tit oio nn
From Structured Data
C Aa Cn n Aad sn nwid d sea widrt ee a rte EvT ide ex nt
ce
T Dim Be
P
efr do im
a
C Aan nd s +wi d ea rte ReL go rg ei ss sti ic
on
Clas Ps aif ri sc ia nt gion RR ee tl ra ieti vo an l CaC nC Aa dAa n n in n dd sd s awidwi td e eaea rterte S E So v ou T id ur ec e rxe cnt es c seA Tn ys pw eer C CCCo aoAan nnn nfi dfidd s + diw dei d ean ea ntc rt ecee e A Rn as nw kee rr
Na m Tae gd g E inn gtity Answer S Fp aa cc ee b f or oo km Ans +w er Answer
DBPedia Freebase Confidence and
Relation Extraction Confidence
Coreference
Figure14.17 The4broadstagesofWatsonQA:(1)QuestionProcessing,(2)CandidateAnswerGeneration,
(3)CandidateAnswerScoring,and(4)AnswerMergingandConfidenceScoring.
PoetsandPoetry:HewasabankclerkintheYukonbeforehepublished
“SongsofaSourdough”in1907.14.6 • CLASSICQAMODELS 291
THEATRE:AnewplaybasedonthisSirArthurConanDoylecanine
classicopenedontheLondonstagein2007.
QuestionProcessing Inthisstagethequestionsareparsed,namedentitiesareex-
tracted(SirArthurConanDoyleidentifiedasaPERSON,YukonasaGEOPOLITICAL
ENTITY, “Songs of a Sourdough” as a COMPOSITION), coreference is run (he is
linkedwithclerk).
focus Thequestionfocus,showninboldinbothexamples,isextracted. Thefocusis
the string of words in the question that corefers with the answer. It is likely to be
replacedbytheanswerinanyanswerstringfoundandsocanbeusedtoalignwitha
supportingpassage. InDeepQAthefocusisextractedbyhandwrittenrules—made
possible by the relatively stylized syntax of Jeopardy! questions—such as a rule
extractinganynounphrasewithdeterminer“this”asintheConanDoyleexample,
andrulesextractingpronounslikeshe,he,hers,him,asinthepoetexample.
lexicalanswer The lexical answer type (shown in blue above) is a word or words which tell
type
us something about the semantic type of the answer. Because of the wide variety
of questions in Jeopardy!, DeepQA chooses a wide variety of words to be answer
types,ratherthanasmallsetofnamedentities. Theselexicalanswertypesareagain
extractedbyrules: thedefaultruleistochoosethesyntacticheadwordofthefocus.
Otherrulesimprovethisdefaultchoice.Forexampleadditionallexicalanswertypes
canbewordsinthequestionthatarecoreferentwithorhaveaparticularsyntactic
relationwiththefocus,suchasheadwordsofappositivesorpredicativenominatives
ofthefocus. InsomecaseseventheJeopardy! categorycanactasalexicalanswer
type, ifitreferstoatypeofentitythatiscompatiblewiththeotherlexicalanswer
types.Thusinthefirstcaseabove,he,poet,andclerkarealllexicalanswertypes.In
additiontousingtherulesdirectlyasaclassifier,theycaninsteadbeusedasfeatures
in a logistic regression classifier that can return a probability as well as a lexical
answertype.Theseanswertypeswillbeusedinthelater‘candidateanswerscoring’
phase as a source of evidence for each candidate. Relations like the following are
alsoextracted:
authorof(focus,“Songsofasourdough”)
publish(e1,he,“Songsofasourdough”)
in(e2,e1,1907)
temporallink(publish(...),1907)
Finally the question is classified by type (definition question, multiple-choice,
puzzle,fill-in-the-blank).Thisisgenerallydonebywritingpattern-matchingregular
expressionsoverwordsorparsetrees.
CandidateAnswerGeneration Nextwecombinetheprocessedquestionwithex-
ternaldocumentsandotherknowledgesourcestosuggestmanycandidateanswers
frombothtextdocumentsandstructuredknowledgebases. Wecanquerystructured
resourceslikeDBpediaorIMDBwiththerelationandtheknownentity,justaswe
sawinSection14.4.Thusifwehaveextractedtherelationauthorof(focus,"Songs
of a sourdough"),wecanqueryatriplestorewithauthorof(?x,"Songs of a
sourdough")toreturnanauthor.
ToextractanswersfromtextDeepQAusessimpleversionsofRetrieveandRead.
ForexamplefortheIRstage,DeepQAgeneratesaqueryfromthequestionbyelimi-
natingstopwords,andthenupweightinganytermswhichoccurinanyrelationwith
thefocus. Forexamplefromthisquery:
MOVIE-“ING”:RobertRedfordandPaulNewmanstarredinthisdepression-
eragrifterflick. (Answer: “TheSting”)292 CHAPTER14 • QUESTIONANSWERINGANDINFORMATIONRETRIEVAL
thefollowingweightedquerymightbepassedtoastandardIRsystem:
(2.0RobertRedford)(2.0PaulNewman)stardepressioneragrifter(1.5flick)
DeepQAalsomakesuseoftheconvenientfactthatthevastmajorityofJeopardy!
answers are the title of a Wikipedia document. To find these titles, we can do a
second text retrieval pass specifically on Wikipedia documents. Then instead of
extracting passages from the retrieved Wikipedia document, we directly return the
titlesofthehighlyrankedretrieveddocumentsasthepossibleanswers.
Once we have a set of passages, we need to extract candidate answers. If the
document happens to be a Wikipedia page, we can just take the title, but for other
texts, like news documents, we need other approaches. Two common approaches
anchortexts
aretoextractallanchortextsinthedocument(anchortextisthetextbetween<a>
and</a>usedtopointtoaURLinanHTMLpage),ortoextractallnounphrases
inthepassagethatareWikipediadocumenttitles.
Candidate Answer Scoring Next DeepQA uses many sources of evidence to
score each candidate. This includes a classifier that scores whether the candidate
answer can be interpreted as a subclass or instance of the potential answer type.
Consider the candidate “difficulty swallowing” and the lexical answer type “man-
ifestation”. DeepQA first matches each of these words with possible entities in
ontologieslikeDBpediaandWordNet. Thusthecandidate“difficultyswallowing”
ismatchedwiththeDBpediaentity“Dysphagia”,andthenthatinstanceismapped
totheWordNettype“Symptom”.Theanswertype“manifestation”ismappedtothe
WordNettype“Condition”. Thesystemlooksforahyponymy,orsynonymylink,in
thiscasefindinghyponymybetween“Symptom”and“Condition”.
OtherscorersarebasedonusingtimeandspacerelationsextractedfromDBpe-
diaorotherstructureddatabases. Forexample, wecanextracttemporalproperties
oftheentity(whenwasapersonborn,whendied)andthencomparetotimeexpres-
sions in the question. If a time expression in the question occurs chronologically
beforeapersonwasborn,thatwouldbeevidenceagainstthispersonbeingthean-
swertothequestion.
Finally,wecanusetextretrievaltohelpretrieveevidencesupportingacandidate
answer.Wecanretrievepassageswithtermsmatchingthequestion,thenreplacethe
focusinthequestionwiththecandidateanswerandmeasuretheoverlappingwords
ororderingofthepassagewiththemodifiedquestion.
The output of this stage is a set of candidate answers, each with a vector of
scoringfeatures.
AnswerMergingandScoring DeepQAfinallymergesequivalentcandidatean-
swers. ThusifwehadextractedtwocandidateanswersJ.F.K.andJohnF.Kennedy,
thisstagewouldmergethetwointoasinglecandidate,forexampleusingtheanchor
dictionaries described above for entity linking, which will list many synonyms for
Wikipedia titles (e.g., JFK, John F. Kennedy, Senator John F. Kennedy, President
Kennedy, JackKennedy). Wethenmergetheevidenceforeachvariant, combining
thescoringfeaturevectorsforthemergedcandidatesintoasinglevector.
Nowwehaveasetofcandidates, eachwithafeaturevector. Aclassifiertakes
each feature vector and assigns a confidence value to this candidate answer. The
classifieristrained onthousandsofcandidateanswers, eachlabeled forwhetherit
is correct or incorrect, together with their feature vectors, and learns to predict a
probabilityofbeingacorrectanswer. Since,intraining,therearefarmoreincorrect
answers than correct answers, we need to use one of the standard techniques for
dealingwithveryimbalanceddata. DeepQAusesinstanceweighting,assigningan14.7 • EVALUATIONOFFACTOIDANSWERS 293
instanceweightof.5foreachincorrectanswerexampleintraining. Thecandidate
answersarethensortedbythisconfidencevalue,resultinginasinglebestanswer.
DeepQA’sfundamentalintuitionisthustoproposeaverylargenumberofcandi-
dateanswersfrombothtext-basedandknowledge-basedsourcesandthenusearich
varietyofevidencefeaturesforscoringthesecandidates. Seethepapersmentioned
attheendofthechapterformoredetails.
14.7 Evaluation of Factoid Answers
mean Factoidquestionansweringiscommonlyevaluatedusingmeanreciprocalrank,or
reciprocalrank
MRR MRR (Voorhees, 1999). MRR is designed for systems that return a short ranked
listofanswersorpassagesforeachtestsetquestion,whichwecancompareagainst
the(human-labeled)correctanswer. First, eachtestsetquestionisscoredwiththe
reciprocaloftherankofthefirstcorrectanswer.Forexampleifthesystemreturned
fiveanswerstoaquestionbutthefirstthreearewrong(sothehighest-rankedcorrect
answer is ranked fourth), the reciprocal rank for that question is 1. The score for
4
questionsthatreturnnocorrectansweris0. TheMRRofasystemistheaverageof
thescoresforeachquestioninthetestset. InsomeversionsofMRR,questionswith
ascoreofzeroareignoredinthiscalculation. Moreformally,forasystemreturning
rankedanswerstoeachquestioninatestsetQ,(orinthealternateversion,letQbe
thesubsetoftestsetquestionsthathavenon-zeroscores). MRRisthendefinedas
Q
1 | | 1
MRR= (14.34)
Q rank
i
| | i=1
(cid:88)
ReadingcomprehensionsystemsondatasetslikeSQuADareevaluated(firstignor-
ingpunctuationandarticleslikea,an,the)viatwometrics(Rajpurkaretal.,2016):
• Exactmatch:The%ofpredictedanswersthatmatchthegoldanswerexactly.
• F1 score: The average word/token overlap between predicted and gold an-
swers. Treatthepredictionandgoldasabagoftokens, andcomputeF for
1
eachquestion,thenreturntheaverageF overallquestions.
1
Anumberoftestsetsareavailableforquestionanswering. Earlysystemsused
the TREC QA dataset: https://trec.nist.gov/data/qa/t8_qadata.html.
MorerecentcompetitionsusesthedatasetsdescribedinSection14.2.1.Otherrecent
datasetsincludetheAI2ReasoningChallenge(ARC)(Clarketal.,2018)ofmultiple
choicequestionsdesignedtobehardtoanswerfromsimplelexicalmethods,likethis
question
Which property of a mineral can be determined just by looking at it?
(A)luster[correct](B)mass(C)weight(D)hardness
in which the correct answer luster is unlikely to co-occur frequently with phrases
like looking at it, while the word mineral is highly associated with the incorrect
answerhardness.294 CHAPTER14 • QUESTIONANSWERINGANDINFORMATIONRETRIEVAL
Bibliographical and Historical Notes
QuestionansweringwasoneoftheearliestNLPtasks,andearlyversionsofthetext-
basedandknowledge-basedparadigmsweredevelopedbytheveryearly1960s.The
text-basedalgorithmsgenerallyreliedonsimpleparsingofthequestionandofthe
sentencesinthedocument, andthenlookingformatches. Thisapproachwasused
veryearlyon(Phillips,1960)butperhapsthemostcompleteearlysystem,andone
thatstrikinglyprefiguresmodernrelation-basedsystems,wastheProtosynthexsys-
temofSimmonsetal.(1964). Givenaquestion,Protosynthexfirstformedaquery
from the content words in the question, and then retrieved candidate answer sen-
tences in the document, ranked by their frequency-weighted term overlap with the
question. Thequeryandeachretrievedsentencewerethenparsedwithdependency
parsers, and the sentence whose structure best matches the question structure se-
lected. ThusthequestionWhatdowormseat? wouldmatchwormseatgrass: both
havethesubjectwormsasadependentofeat,intheversionofdependencygrammar
usedatthetime,whilebirdseatwormshasbirdsasthesubject:
What do worms eat Worms eat grass Birds eat worms
Thealternativeknowledge-basedparadigmwasimplementedintheBASEBALL
system(Greenetal.,1961). Thissystemansweredquestionsaboutbaseballgames
like “Where did the Red Sox play on July 7” by querying a structured database of
gameinformation. Thedatabasewasstoredasakindofattribute-valuematrixwith
valuesforattributesofeachgame:
Month = July
Place = Boston
Day = 7
Game Serial No. = 96
(Team = Red Sox, Score = 5)
(Team = Yankees, Score = 3)
Each question was constituency-parsed using the algorithm of Zellig Harris’s
TDAPprojectattheUniversityofPennsylvania,essentiallyacascadeoffinite-state
transducers (see the historical discussion in Joshi and Hopely 1999 and Karttunen
1999). Theninacontentanalysisphaseeachwordorphrasewasassociatedwitha
programthatcomputedpartsofitsmeaning. Thusthephrase‘Where’hadcodeto
assign the semantics Place = ?, with the result that the question “Where did the
RedSoxplayonJuly7”wasassignedthemeaning
Place = ?
Team = Red Sox
Month = July
Day = 7
Thequestionisthenmatchedagainstthedatabasetoreturntheanswer.Simmons
(1965)summarizesotherearlyQAsystems.
Another important progenitor of the knowledge-based paradigm for question-
answering is work that used predicate calculus as the meaning representation lan-
LUNAR guage. TheLUNARsystem(Woodsetal.1972, Woods1978)wasdesignedtobeEXERCISES 295
anaturallanguageinterfacetoadatabaseofchemicalfactsaboutlunargeology. It
couldanswerquestionslikeDoanysampleshavegreaterthan13percentaluminum
byparsingthemintoalogicalform
(TEST (FOR SOME X16 / (SEQ SAMPLES) : T ; (CONTAIN’ X16
(NPR*X17/(QUOTEAL203))(GREATERTHAN13PCT))))
Byacoupledecadeslater,drawingonnewmachinelearningapproachesinNLP,
ZelleandMooney(1996)proposedtotreatknowledge-basedQAasasemanticpars-
ingtask, bycreatingtheProlog-basedGEOQUERYdatasetofquestionsaboutUS
geography. ThismodelwasextendedbyZettlemoyerandCollins(2005)and2007.
Byadecadelater,neuralmodelswereappliedtosemanticparsing(DongandLap-
ata2016,JiaandLiang2016),andthentoknowledge-basedquestionansweringby
mappingtexttoSQL(Iyeretal.,2017).
Meanwhile, the information-retrieval paradigm for question answering was in-
fluencedbytheriseofthewebinthe1990s. TheU.S.government-sponsoredTREC
(TextREtrievalConference)evaluations,runannuallysince1992,provideatestbed
for evaluating information-retrieval tasks and techniques (Voorhees and Harman,
2005). TRECaddedaninfluentialQAtrackin1999,whichledtoawidevarietyof
factoidandnon-factoidsystemscompetinginannualevaluations.
At that same time, Hirschman et al. (1999) introduced the idea of using chil-
dren’s reading comprehension tests to evaluate machine text comprehension algo-
rithms. Theyacquiredacorpusof120passageswith5questionseachdesignedfor
3rd-6th grade children, built an answer extraction system, and measured how well
the answers given by their system corresponded to the answer key from the test’s
publisher. Their algorithm focused on word overlap as a feature; later algorithms
addednamedentityfeaturesandmorecomplexsimilaritybetweenthequestionand
theanswerspan(RiloffandThelen2000,Ngetal.2000).
The DeepQA component of the Watson Jeopardy! system was a large and so-
phisticatedfeature-basedsystemdevelopedjustbeforeneuralsystemsbecamecom-
mon. It is described in a series of papers in volume 56 of the IBM Journal of Re-
searchandDevelopment,e.g.,Ferrucci(2012).
Neural reading comprehension systems drew on the insight common to early
systems that answer finding should focus on question-passage similarity. Many of
thearchitecturaloutlinesofthesemodernneuralsystemswerelaidoutinHermann
etal.(2015),Chenetal.(2017a),andSeoetal.(2017). Thesesystemsfocusedon
datasetslikeRajpurkaretal.(2016)andRajpurkaretal.(2018)andtheirsuccessors,
usuallyusingseparateIRalgorithmsasinputtoneuralreadingcomprehensionsys-
tems. SomerecentsystemsincludetheIRcomponentaspartofasingleend-to-end
architecture(Leeetal.,2019).
Other question-answering tasks include Quiz Bowl, which has timing consid-
erationssincethequestioncanbeinterrupted(Boyd-Graberetal.,2018). Question
answeringisalsoanimportantfunctionofmodernpersonalassistantdialogsystems;
seeChapter15.
Exercises296 CHAPTER15 • CHATBOTS&DIALOGUESYSTEMS
CHAPTER
15 Chatbots & Dialogue Systems
Lesloisdelaconversationsontenge´ne´raldenes’yappesantirsuraucunob-
jet,maisdepasserle´ge`rement,sanseffortetsansaffectation,d’unsujeta` un
autre;desavoiryparlerdechosesfrivolescommedechosesse´rieuses
[The rules of conversation are, in general, not to dwell on any one subject,
buttopasslightlyfromonetoanotherwithouteffortandwithoutaffectation;
toknowhowtospeakabouttrivialtopicsaswellasseriousones;]
The18thC.EncyclopediaofDiderot,startoftheentryonconversation
“Whatho!”Isaid.
“Whatho!”saidMotty.
“Whatho! Whatho!”
“Whatho! Whatho! Whatho!”
Afterthatitseemedratherdifficulttogoonwiththeconversation.”
P.G.Wodehouse,MyManJeeves
Theliteratureofthefantasticaboundsininanimateobjectsmagicallyendowedwith
sentienceandthegiftofspeech.FromOvid’sstatueofPygmaliontoMaryShelley’s
Frankenstein,thereissomethingdeeplymovingaboutcreatingsomethingandthen
havingachatwithit.Legendhasitthatafterfinishinghis
sculptureMoses,Michelangelothoughtitsolifelikethat
hetappeditonthekneeandcommandedittospeak. Per-
haps this shouldn’t be surprising. Language is the mark
conversation ofhumanityandsentience,andconversationordialogue
dialogue is the most fundamental and specially privileged arena
of language. It is the first kind of language we learn as
children, and for most of us, it is the kind of language
wemostcommonlyindulgein,whetherweareordering
curry for lunch or buying spinach, participating in busi-
ness meetings or talking with our families, booking air-
lineflightsorcomplainingabouttheweather.
dialoguesystem This chapter introduces the fundamental algorithms of dialogue systems, or
conversational conversational agents. These programs communicate with users in natural lan-
agent
guage (text, speech, or both), and fall into two classes. Task-oriented dialogue
agentsuseconversationwithuserstohelpcompletetasks. Dialogueagentsindig-
italassistants(Siri,Alexa,GoogleNow/Home,Cortana,etc.),givedirections,con-
trol appliances, find restaurants, or make calls. Conversational agents can answer
questionsoncorporatewebsites, interfacewithrobots, andevenbeusedforsocial
good: DoNotPay is a “robot lawyer” that helps people challenge incorrect park-
ing fines, apply for emergency housing, or claim asylum if they are refugees. By15.1 • PROPERTIESOFHUMANCONVERSATION 297
contrast,chatbotsaresystemsdesignedforextendedconversations,setuptomimic
theunstructuredconversationsor‘chats’characteristicofhuman-humaninteraction,
mainlyforentertainment, butalsoforpracticalpurposeslikemakingtask-oriented
agentsmorenatural.1 InSection15.2we’lldiscussthethreemajorchatbotarchitec-
tures: rule-basedsystems,informationretrievalsystems,andencoder-decodergen-
erators.InSection15.3weturntotask-orientedagents,introducingtheframe-based
architecture(theGUSarchitecture)thatunderliesmosttask-basedsystems.
15.1 Properties of Human Conversation
Conversationbetweenhumansisanintricateandcomplexjointactivity. Beforewe
attempt to design a conversational agent to converse with humans, it is crucial to
understandsomethingabouthowhumansconversewitheachother. Considersome
ofthephenomenathatoccurintheconversationbetweenahumantravelagentand
ahumanclientexcerptedinFig.15.1.
C : ...IneedtotravelinMay.
1
A : And,whatdayinMaydidyouwanttotravel?
2
C : OKuhIneedtobethereforameetingthat’sfromthe12thtothe15th.
3
A : Andyou’reflyingintowhatcity?
4
C : Seattle.
5
A : AndwhattimewouldyouliketoleavePittsburgh?
6
C : UhhmmIdon’tthinkthere’smanyoptionsfornon-stop.
7
A : Right. There’sthreenon-stopstoday.
8
C : Whatarethey?
9
A : The first one departs PGH at 10:00am arrives Seattle at 12:05 their time.
10
ThesecondflightdepartsPGHat5:55pm,arrivesSeattleat8pm. Andthe
lastflightdepartsPGHat8:15pmarrivesSeattleat10:28pm.
C : OKI’lltakethe5ishflightonthenightbeforeonthe11th.
11
A : On the 11th? OK. Departing at 5:55pm arrives Seattle at 8pm, U.S. Air
12
flight115.
C : OK.
13
A : AndyousaidreturningonMay15th?
14
C : Uh,yeah,attheendoftheday.
15
A : OK.There’s#twonon-stops...#
16
C : #Act...actually #,whatdayoftheweekisthe15th?
17
A : It’saFriday.
18
C : Uhhmm. IwouldconsiderstayingthereanextradaytilSunday.
19
A : OK...OK.OnSundayIhave...
20
Figure15.1 Part of a phone conversation between a human travel agent (A) and human
client(C).Thepassagesframedby#inA andC indicateoverlapsinspeech.
16 17
Turns
turn Adialogueisasequenceofturns(C 1,A 2,C 3,andsoon),eachasinglecontribution
fromonespeakertothedialogue(asifinagame: Itakeaturn,thenyoutakeaturn,
1 Bycontrast,inpopularusage,thewordchatbotisoftengeneralizedtorefertobothtask-orientedand
chit-chatsystems;we’llbeusingdialoguesystemsfortheformer.298 CHAPTER15 • CHATBOTS&DIALOGUESYSTEMS
thenme,andsoon).Thereare20turnsinFig.15.1.Aturncanconsistofasentence
(likeC ),althoughitmightbeasshortasasingleword(C )oraslongasmultiple
1 13
sentences(A ).
10
Turnstructurehasimportantimplicationsforspokendialogue. Asystemhasto
knowwhentostoptalking;theclientinterrupts(inA andC ),sothesystemmust
16 17
knowtostoptalking(andthattheusermightbemakingacorrection).Asystemalso
has to know when to start talking. For example, most of the time in conversation,
speakersstarttheirturnsalmostimmediatelyaftertheotherspeakerfinishes,without
a long pause, because people are able to (most of the time) detect when the other
personisabouttofinishtalking. Spokendialoguesystemsmustalsodetectwhether
auserisdonespeaking,sotheycanprocesstheutteranceandrespond. Thistask—
endpointing called endpointing or endpoint detection— can be quite challenging because of
noiseandbecausepeopleoftenpauseinthemiddleofturns.
SpeechActs
A key insight into conversation—due originally to the philosopher Wittgenstein
(1953) but worked out more fully by Austin (1962)—is that each utterance in a
dialogueisakindofactionbeingperformedbythespeaker. Theseactionsarecom-
speechacts monlycalledspeechactsordialogacts:here’sonetaxonomyconsistingof4major
classes(BachandHarnish,1979):
Constatives: committingthespeakertosomething’sbeingthecase(answering,claiming,
confirming,denying,disagreeing,stating)
Directives: attemptsbythespeakertogettheaddresseetodosomething(advising,ask-
ing,forbidding,inviting,ordering,requesting)
Commissives: committingthespeakertosomefuturecourseofaction(promising,planning,
vowing,betting,opposing)
Acknowledgments: expressthespeaker’sattituderegardingthehearerwithrespecttosomeso-
cialaction(apologizing,greeting,thanking,acceptinganacknowledgment)
Auseraskingapersonoradialoguesystemtodosomething(‘Turnupthemu-
sic’) is issuing a DIRECTIVE. Asking a question that requires an answer is also
a way of issuing a DIRECTIVE: in a sense when the system says (A 2) “what day
in May did you want to travel?” it’s as if the system is (very politely) command-
ing the user to answer. By contrast, a user stating a constraint (like C ‘I need to
1
travel in May’) is issuing a CONSTATIVE. A user thanking the system is issuing
anACKNOWLEDGMENT. Thespeechactexpressesanimportantcomponentofthe
intentionofthespeaker(orwriter)insayingwhattheysaid.
Grounding
Adialogueisnotjustaseriesofindependentspeechacts,butratheracollectiveact
performedbythespeakerandthehearer. Likeallcollectiveacts,it’simportantfor
common the participants to establish what they both agree on, called the common ground
ground
grounding (Stalnaker,1978). Speakersdothisbygroundingeachother’sutterances. Ground-
ingmeansacknowledgingthatthehearerhasunderstoodthespeaker; likeanACK
usedtoconfirmreceiptindatacommunications(Clark,1996).(Peopleneedground-
ingfornon-linguisticactionsaswell; thereasonanelevatorbuttonlightsupwhen
it’s pressed is to acknowledge that the elevator has indeed been called (Norman,
1988).)
Humansconstantlygroundeachother’sutterances. Wecangroundbyexplicitly
saying“OK”,astheagentdoesinA orA . Orwecangroundbyrepeatingwhat
8 1015.1 • PROPERTIESOFHUMANCONVERSATION 299
theotherpersonsays;inutteranceA theagentrepeats“inMay”,demonstratingher
2
understanding to the client. Or notice that when the client answers a question, the
agentbeginsthenextquestionwith“And”.The“And”impliesthatthenewquestion
is‘inaddition’totheoldquestion, againindicatingtotheclientthattheagenthas
successfullyunderstoodtheanswertothelastquestion.
SubdialoguesandDialogueStructure
Conversations have structure. Consider, for example, the local structure between
conversational speech acts discussed in the field of conversational analysis (Sacks et al., 1974).
analysis
QUESTIONS set up an expectation for an ANSWER. PROPOSALS are followed by
ACCEPTANCE (or REJECTION). COMPLIMENTS (“Nicejacket!”)oftengiveriseto
adjacencypair DOWNPLAYERS (“Oh, this old thing?”). These pairs, called adjacency pairs are
composedofafirstpairpartandasecondpairpart(Schegloff,1968),andthese
expectationscanhelpsystemsdecidewhatactionstotake.
However,dialogueactsaren’talwaysfollowedimmediatelybytheirsecondpair
sidesequence part. The two parts can be separated by a side sequence (Jefferson 1972) or sub-
subdialogue dialogue. For example utterances C 17 to A 20 constitute a correction subdialogue
(Litman1985,LitmanandAllen1987,Chu-CarrollandCarberry1998):
C : #Act...actually#,whatdayoftheweekisthe15th?
17
A : It’saFriday.
18
C : Uhhmm. IwouldconsiderstayingthereanextradaytilSunday.
19
A : OK...OK.OnSundayIhave...
20
ThequestioninC interruptsthepriordiscourse,inwhichtheagentwaslooking
17
foraMay15returnflight. Theagentmustanswerthequestionandalsorealizethat
‘’Iwouldconsiderstaying...tilSunday”meansthattheclientwouldprobablyliketo
changetheirplan,andnowgobacktofindingreturnflights,butforthe17th.
Another side sequence is the clarification question, which can form a subdia-
loguebetweenaREQUESTandaRESPONSE. Thisisespeciallycommonindialogue
systemswherespeechrecognitionerrorscausesthesystemtohavetoaskforclari-
ficationsorrepetitionslikethefollowing:
User: WhatdoyouhavegoingtoUNKNOWN WORDonthe5th?
System: Let’ssee,goingwhereonthe5th?
User: GoingtoHongKong.
System: OK,herearesomeflights...
presequence In addition to side-sequences, questions often have presequences, like the fol-
lowingexamplewhereauserstartswithaquestionaboutthesystem’scapabilities
(“Canyoumaketrainreservations”)beforemakingarequest.
User: Canyoumaketrainreservations?
System: YesIcan.
User: Great,I’dliketoreserveaseatonthe4pmtraintoNewYork.
Initiative
Sometimesaconversationiscompletelycontrolledbyoneparticipant. Forexample
areporter interviewingachef mightaskquestions, andthechef responds. Wesay
initiative thatthereporterinthiscasehastheconversationalinitiative(WalkerandWhittaker,
1990). Innormalhuman-humandialogue,however,it’smorecommonforinitiative
toshiftbackandforthbetweentheparticipants,astheysometimesanswerquestions,
sometimesaskthem,sometimestaketheconversationsinnewdirections,sometimes300 CHAPTER15 • CHATBOTS&DIALOGUESYSTEMS
not. Youmayaskmeaquestion,andthenIrespondaskingyoutoclarifysomething
yousaid,whichleadstheconversationinallsortsofways. Wecallsuchinteractions
mixedinitiative(WalkerandWhittaker,1990).
Mixedinitiative, whilethenormforhuman-humanconversations, isverydiffi-
cultfordialoguesystemstoachieve. It’smucheasiertodesigndialoguesystemsto
bepassiveresponders. InthequestionansweringsystemswesawinChapter14,or
insimplesearchengines,theinitiativeliescompletelywiththeuser. Insuchuser-
initiative systems, the user specifies a query, and the systems responds. Then the
usercanspecifyanotherquery. Alternatively, youmayhavehadtheexperienceof
beingstuckinabaddialoguesystemthatasksaquestionandgivesyounoopportu-
nitytodoanythinguntilyouanswerit. Suchsystem-initiativearchitecturescanbe
veryfrustrating.
InferenceandImplicature
Inferenceisalsoimportantindialogueunderstanding.Considertheclient’sresponse
C ,repeatedhere:
2
A : And,whatdayinMaydidyouwanttotravel?
2
C : OKuhIneedtobethereforameetingthat’sfromthe12thtothe15th.
3
Notice that the client does not in fact answer the agent’s question. The client
merely mentions a meeting at a certain time. What is it that licenses the agent to
inferthattheclientismentioningthismeetingsoastoinformtheagentofthetravel
dates?
The speaker seems to expect the hearer to draw certain inferences; in other
words, the speaker is communicating more information than seems to be present
intheutteredwords. ThiskindofexamplewaspointedoutbyGrice(1975,1978)
implicature aspartofhistheoryofconversationalimplicature. Implicaturemeansaparticu-
lar class of licensed inferences. Grice proposed that what enables hearers to draw
theseinferencesisthatconversationisguidedbyasetofmaxims,generalheuristics
thatplayaguidingroleintheinterpretationofconversationalutterances. Onesuch
relevance maximisthemaximofrelevancewhichsaysthatspeakersattempttoberelevant,
theydon’tjustutterrandomspeechacts. Whentheclientmentionsameetingonthe
12th,theagentreasons‘Theremustbesomerelevanceformentioningthismeeting.
What could it be?’. The agent knows that one precondition for having a meeting
(atleastbeforeWebconferencing)isbeingattheplacewherethemeetingisheld,
andthereforethatmaybethemeetingisareasonforthetravel,andifso,thensince
peopleliketoarrivethedaybeforeameeting,theagentshouldinferthattheflight
shouldbeonthe11th.
Thesesubtlecharacteristicsofhumanconversations(turns,speechacts,ground-
ing,dialoguestructure,initiative,andimplicature)areamongthereasonsitisdif-
ficulttobuilddialoguesystemsthatcancarryonnaturalconversationswithhumans.
Manyofthesechallengesareactiveareasofdialoguesystemsresearch.
15.2 Chatbots
chatbot The simplest kinds of dialogue systems are chatbots, systems that can carry on
extended conversations with the goal of mimicking the unstructured conversations
or‘chats’characteristicofinformalhuman-humaninteraction.15.2 • CHATBOTS 301
Thesesystemsoftenhaveanentertainmentvalue, suchasFacebook’sBlender-
Bot, a neural chatbot capable of carrying on the kinds of conversations shown in
Fig. 15.2, or Microsoft’s XiaoIce system2 (Zhou et al., 2020), a system that chats
withpeopleontextmessagingplatforms(Fig.15.3).
Will you sing me a song?
Sure, what do you want to be sung to? I can sing you a song about baking.
Yes, sing me a song about baking!
Do you want me to sing it to the tune of “Sing Along with Mitch”?
Yes, I would love that.
OK, I’ll try to get it to you by the end of the day. What else do you like to do?
No, sing it to me now!
OK, give me a few days. I’m a teacher, so I don’t have much free time.
Can you at least tell me what the lyrics are?
Sure, it goes like this: “Baking is my favorite thing to do in the world.”
How does the chorus go?
It goes: BAKING IS MY FAVORITE THING TO DO IN THE WORLD!
Wow, sounds like a bop.
I know, right? It’s so catchy. I can’t get it out of my head.
Figure15.2 Aconversationbetweenauser(left)andBlenderBot(Rolleretal.,2021).
［人类］小冰你好 [Human] Hello,XiaoIce.
［小冰］你好呀，你头像好有趣，感觉 [XiaoIce] Hello, your profile pic is
在哪见过 interesting. Like I’ve seen
it somewhere.
［人类］是吗 [Human] Really?
［小冰］提不是哪部动画里的呀 [XiaoIce] Is it a comic character?
［人类］你怎么知道的 [Human] Howdoyouknowthat?
［小冰］你猜 [XiaoIce] Guess.
［人类］难道你是真人吗？ [Human] Areyouhuman?
［小冰］这个，你和我多聊聊就知道了 [XiaoIce] Well, you’ll know it after
chatting with me more.
［人类］你不是人工智能机器人么 [Human] Aren’tyouanAlchatbot?
［小冰］是又怎样，我不会冷冰冰的对 [XiaoIce] So what? I won’t be
待你的 indifferent to you.
［人类］你都有什么功能啊 [Human] Whatfunctionsdoyouhave?
［小冰］我有200多个skills，来，说出 [XiaoIce] I have over 200 skills. Just
你的需求 name one you need.
Figure15.3 AsampleChineseconversationfromtheXiaoIcechatbotwithtranslations(Zhouetal.,2020).
Yet starting from the very first system, ELIZA (Weizenbaum, 1966), chatbots
have also been used for practical purposes like testing theories of psychological
2 XiaoIceisasomewhatconfusingtranslationofthesystemsChinesename小冰,‘XiaoBing’,which
meansLittleBingorLittleIce.302 CHAPTER15 • CHATBOTS&DIALOGUESYSTEMS
counseling.
Likepracticallyeverythingelseinlanguageprocessing,chatbotarchitecturesfall
intotwoclasses:rule-basedsystemsandcorpus-basedsystems. Rule-basedsystems
include the early influential ELIZA and PARRY systems. Corpus-based systems
mine large datasets of human-human conversations, which can be done by using
information retrieval to copy a human response from a previous conversation, or
usinganencoder-decodersystemtogeneratearesponsefromauserutterance.
15.2.1 Rule-basedchatbots: ELIZAandPARRY
ELIZA (Weizenbaum, 1966) is the most important chatbot dialogue system in the
historyofthefield, andsowetakesometimeheretodescribeitsalgorithminde-
tail. ELIZA was designed to simulate a Rogerian psychologist, based on a branch
ofclinicalpsychologywhosemethodsinvolvedrawingthepatientoutbyreflecting
patient’sstatementsbackatthem. Rogerianpsychologyistheraretypeofconver-
sation in which, as Weizenbaum points out, one can “assume the pose of knowing
almostnothingoftherealworld”. Ifapatientsays“Iwentforalongboatride”and
thepsychiatristsays“Tellmeaboutboats”,youdon’tassumeshedidn’tknowwhat
aboatis,butratherassumeshehadsomeconversationalgoal. Mostchatbotstrying
topasstheTuringtestchooseadomainwithsimilarproperties.
Fig.15.4showsthemostfamousELIZAconversation, inwhichELIZAseems
togiveverysophisticatedtherapeuticadvice,excerpted(deletingsomematerialwith
...) fromafullinteractiongivenintheoriginalpaper(Weizenbaum,1966).
As we summarized briefly in Chapter 2, ELIZA worked by pattern/transform
ruleslikethefollowingone:
(0 YOU 0 ME) [pattern]
->
(WHAT MAKES YOU THINK I 3 YOU) [transform]
IntheELIZApattern,0meansKleene*,andinthetransformrulesthenumbers
aretheindexoftheconstituentinthepattern.Thusthenumber3referstothesecond
0inthefirstpattern. Thisrulewouldtransfer
You hate me
into:
WHAT MAKES YOU THINK I HATE YOU
EachELIZApattern/ruleislinkedtoakeywordthatmightoccurinausersen-
tence. ThearchitectureissummarizedinFig.15.5.
Keywords are associated with a rank, with specific words being more highly
ranked, and more general words ranking lower. Consider the following user sen-
tence:
I know everybody laughed at me
Becauseithastheword“I”,thissentencecouldmatchthefollowingrulewhose
keywordisI:
(I *) -> (You say you 2)
producing:
YOU SAY YOU KNOW EVERYBODY LAUGHED AT YOU
ButasWeizenbaumpointsout,“I”isaverygeneralwordanditskeywordslead
toverygeneralresponses. Insteadthekeyword“everybody”ismuchmoreinterest-
ing,sincesomeoneusinguniversalslikeeverybodyoralwaysisprobably“referring15.2 • CHATBOTS 303
Menareallalike.
IN WHAT WAY
They’realwaysbuggingusaboutsomethingorother.
CAN YOU THINK OF A SPECIFIC EXAMPLE
Wellmyboyfriendmademecomehere.
YOUR BOYFRIEND MADE YOU COME HERE
HesaysI’mdepressedmuchofthetime.
I AM SORRY TO HEAR YOU ARE DEPRESSED
..
..
WHO ELSE IN YOUR FAMILY TAKES CARE OF YOU?
Myfather
YOUR FATHER
Youarelikemyfatherinsomeways.
WHAT RESEMBLANCE DO YOU SEE
YouarenotveryaggressivebutIthinkyoudon’twantmetonoticethat.
WHAT MAKES YOU THINK I AM NOT AGGRESSIVE
Youdon’targuewithme.
WHY DO YOU THINK I DON’T ARGUE WITH YOU
Youareafraidofme.
DOES IT PLEASE YOU TO BELIEVE I’M AFRAID OF YOU
Myfatherisafraidofeverybody.
WHAT ELSE COMES TO MIND WHEN YOU THINK OF YOUR FATHER
Bullies.
DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR
BOYFRIEND MADE YOU COME HERE
Figure15.4 SampleELIZAdialoguefromWeizenbaum(1966).
functionELIZAGENERATOR(usersentence)returnsresponse
Findthewordwinsentencethathasthehighestkeywordrank
if wexists
Choosethehighestrankedrulerforwthatmatchessentence
response Applythetransforminrtosentence
←
if w=‘my’
future Applya transformationfromthe‘memory’rulelisttosentence
←
Pushfutureontomemoryqueue
else(nokeywordapplies)
either
response ApplythetransformfortheNONEkeywordtosentence
←
or
response Poptheoldestresponsefromthememoryqueue
←
return(response)
Figure15.5 A simplified sketch of the ELIZA algorithm. The power of the algorithm
comesfromtheparticulartransformsassociatedwitheachkeyword.
tosomequitespecificeventorperson”. Therefore,ELIZApreferstorespondwith
thepatternassociatedwiththemorespecifickeywordeverybody(implementingby
just assigning “everybody” rank 5 and “I” rank 0 in the lexicon), whose rule thus304 CHAPTER15 • CHATBOTS&DIALOGUESYSTEMS
transformsthesentenceto:
WHO IN PARTICULAR ARE YOU THINKING OF?
Ifnokeywordmatches,ELIZAchoosesanon-committalresponselike“PLEASE
GOON”,“THAT’SVERYINTERESTING”,or“ISEE”.
Finally, ELIZA has a clever memory trick that accounts for the very last sen-
tence of the conversation above. Whenever the word “my” is the highest ranked
keyword,ELIZAwillrandomlyselectatransformontheMEMORYlist,applyitto
thesentence,andstoreitonthequeue:
(MEMORY MY
(0 MY 0 = LETS DISCUSS FURTHER WHY YOUR 3)
(0 MY 0 = EARLIER YOU SAID YOUR 3)
(0 MY 0 = DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR 3
Later,ifnokeywordmatchesasentence,ELIZAwillreturntheoldestentryon
theMEMORYqueueinstead.
People became deeply emotionally involved with the program. Weizenbaum
tells the story of one of his staff who would ask Weizenbaum to leave the room
whenshetalkedwithELIZA.WhenWeizenbaumsuggestedthathemightwantto
store all the ELIZA conversations for later analysis, people immediately pointed
out the privacy implications, which suggested that they were having quite private
conversationswithELIZA,despiteknowingthatitwasjustsoftware.
ELIZA’sframeworkisstillusedtoday;modernchatbotsystemtoolslikeALICE
arebasedonupdatedversionsofELIZA’spattern/actionarchitecture.
A few years after ELIZA, another chatbot with a clinical psychology focus,
PARRY(Colbyetal.,1971),wasusedtostudyschizophrenia.InadditiontoELIZA-
like regular expressions, the PARRY system included a model of its own mental
state,withaffectvariablesfortheagent’slevelsoffearandanger;certaintopicsof
conversationmightleadPARRYtobecomemoreangryormistrustful. IfPARRY’s
anger variable is high, he will choose from a set of “hostile” outputs. If the input
mentionshisdelusiontopic,hewillincreasethevalueofhisfearvariableandthen
begin to express the sequence of statements related to his delusion. Parry was the
first known system to pass the Turing test (in 1972!); psychiatrists couldn’t distin-
guishtexttranscriptsofinterviewswithPARRYfromtranscriptsofinterviewswith
realparanoids(Colbyetal.,1972).
15.2.2 Corpus-basedchatbots
Corpus-based chatbots, instead of using hand-built rules, mine conversations of
human-humanconversations. Thesesystemsareenormouslydata-intensive,requir-
inghundredsofmillionsorevenbillionsofwordsfortraining(Serbanetal.,2018).
Availabledatasetsincludetranscriptsofnaturalspokenconversationalcorpora,
liketheSwitchboardcorpusofAmericanEnglishtelephoneconversations(Godfrey
et al., 1992) or the various CALLHOME and CALLFRIEND telephone conversa-
tional corpora in many languages. Many systems also train on movie dialogue,
(Danescu-Niculescu-Mizil and Lee 2011, Lison and Tiedemann 2016, inter alia)
whichresemblesnaturalconversationinmanyways(Forchini,2013).
Datasets have also been created specifically for training dialog systems by hir-
ing crowdworkers to have conversations, often having them take on personas or
talk about knowledge provided to them. For example the Topical-Chat dataset has
11K crowdsourced conversations spanning 8 broad topics (Gopalakrishnan et al.,15.2 • CHATBOTS 305
2019), and the EMPATHETICDIALOGUES includes 25K crowdsourced conversa-
tionsgroundedinaspecificsituationwhereaspeakerwasfeelingaspecificemotion
(Rashkinetal.,2019).
All of these datasets, although large, don’t reach the size of billions of words,
andsomanysystemsfirstpretrainonlargedatasetsofpseudo-conversationsdrawn
from Twitter (Ritter et al., 2010a), Reddit (Roller et al., 2021), Weibo (微博), and
othersocialmediaplatforms.
Another common technique is to extract possible responses from knowledge
sources(Wikipedia,newsstories)sothatachatbotcantellstoriesormentionfacts
acquiredinthatway.
Finally, onceachatbothasbeenputintopractice, theturnsthathumansuseto
respondtothechatbotcanbeusedasadditionalconversationaldatafortrainingor
finetuning. Here it’s important to have confidence metrics to make sure that these
turnscomefromconversationsthataregoingwell(Hancocketal.,2019). It’salso
crucial in these cases to remove personally identifiable information (PII); see Sec-
tion15.6.1.
Most corpus based chatbots produce their responses to a user’s turn in context
either by retrieval methods (using information retrieval to grab a response from
somecorpusthatisappropriategiventhedialoguecontext)orgenerationmethods
(usingalanguagemodelorencoder-decodertogeneratetheresponsegiventhedi-
aloguecontext). Ineithercase,systemsmostlygenerateasingleresponseturnthat
is appropriate given the entire conversation so far (for conversations that are short
enough to fit into a single model’s window). For this reason they are often called
response responsegenerationsystems. Corpus-basedchatbotalgorithmsthusdrawonalgo-
generation
rithms for question answering systems, which similarly focus on single responses
whileignoringlonger-termconversationalgoals.
Responsebyretrieval Theretrievalmethodofrespondingistothinkoftheuser’s
turnasaqueryq,andourjobistoretrieveandrepeatsomeappropriateturnrasthe
response from a corpus of conversationsC. GenerallyC is the training set for the
system,andwescoreeachturninCasapotentialresponsetothecontextqselecting
the highest-scoring one. The scoring metric is similarity: we choose the r that is
mostsimilartoq,usinganyoftheIRmethodswesawinSection14.1. Thiscanbe
doneusingclassicIRtechniquestocomputetf-idfmodelsforCandq,choosingthe
rthathasthehighesttf-idfcosinewithq:
q r
response(q,C)=argmax · (15.1)
q r
r C
∈ | || |
Another version of this method is to return the response to the turn resembling q;
that is, we first find the most similar turn t to q and then return as a response the
followingturnr.
Alternatively, wecanusetheneuralIRtechniquesofSection14.1.5. Thesim-
plestofthoseisabi-encodermodel, inwhichwetraintwoseparateencoders, one
toencodetheuserqueryandonetoencodethecandidateresponse,andusethedot
productbetweenthesetwovectorsasthescore(Fig.15.6a). Forexampletoimple-
ment this using BERT, we would have two encoders BERT and BERT and we
Q R
couldrepresentthequeryandcandidateresponseasthe[CLS]tokenoftherespec-306 CHAPTER15 • CHATBOTS&DIALOGUESYSTEMS
tiveencoders:
h = BERT (q)[CLS]
q Q
h = BERT (r)[CLS]
r R
response(q,C) = argmaxh q h r (15.2)
·
r C
∈
The IR-based approach can be extended in various ways, such as by using more
sophisticatedneuralarchitectures(Humeauetal.,2020),orbyusingalongercontext
forthequerythanjusttheuser’slastturn, uptothewholeprecedingconversation.
Informationabouttheuserorsentimentorotherinformationcanalsoplayarole.
Responsebygeneration Analternatewaytouseacorpustogeneratedialogueis
tothinkofresponseproductionasanencoder-decodertask—transducingfromthe
user’s prior turn to the system’s turn. We can think of this as a machine learning
version of ELIZA; the system learns from a corpus to transduce a question to an
answer. Ritter et al. (2011) proposed early on to think of response generation as
a kind of translation, and this idea was generalized to the encoder-decoder model
roughly contemporaneously by Shang et al. (2015), Vinyals and Le (2015), and
Sordonietal.(2015).
AswesawinChapter13,encoderdecodermodelsgenerateeachtokenr ofthe
t
responsebyconditioningontheencodingoftheentirequeryqandtheresponseso
farr ...r :
1 t 1
−
rˆ t = argmax w VP(wq,r 1...r t 1) (15.3)
∈ | −
Fig. 15.6 shows the intuition of the generator and retriever methods for response
generation. In the generator architecture, we normally include a longer context,
formingthequerynotjustfromtheuser’sturnbutfromtheentireconversation-so-
far. Fig.15.7showsafleshed-outexample.
r 1 r 2 … r n
dot-product
h h
q r DECODER
ENCODER query ENCODER response ENCODER
q 1 … q n r 1 … r n q 1 … q n <S> r 1 …
(a)ResponsebyRetrieval (b)ResponsebyGeneration
Figure15.6 Twoarchitecturesforgeneratingresponsesforaneuralchatbot. Inresponse
byretrieval(a)wechoosearesponsebyfindingtheturninthecorpuswhoseencodinghas
thehighestdot-productwiththeuser’sturn.Inresponsebygeneration(b)weuseanencoder-
decodertogeneratetheresponse.
A number of modifications are required to the basic encoder-decoder model to
adapt it for the task of response generation. For example basic encoder-decoder
modelshaveatendencytoproducepredictablebutrepetitiveandthereforedullre-
sponses like “I’m OK” or “I don’t know” that shut down the conversation. Thus
instead of greedily choosing the most likely (and most predictable) response, we
can use diversity-enhanced versions of beam search (Vijayakumar et al., 2018), or
diversity-focused training objectives (Li et al., 2016a). Basic models also tend to
produce utterances that are too short; adding minimum length constraints is also
important(Rolleretal.,2021).15.2 • CHATBOTS 307
That is quite an accomplishment and you should be proud!
DECODER
ENCODER
[U:] I finally got promoted today at work!
[S:] Congrats! That’s great!
[U:] Thank you! I’ve been trying to get it for a while now!
Figure15.7 Exampleofencoderdecoderfordialogueresponsegeneration;theencoderseestheentiredia-
loguecontext.
Analternativetotheencoder-decoderarchitectureistofine-tunealargelanguage
modelonaconversationaldatasetandusethelanguagemodeldirectlyasaresponse
generator. IntheChirpyCardinalsystem(Paranjapeetal.,2020),forexample,the
neuralchatcomponentgeneratesresponsesfromGPT-2(Radfordetal.,2019),fine-
tunedontheEmpatheticDialoguesdataset(Rashkinetal.,2019).
Finally,encoder-decoderresponsegeneratorsfocusongeneratingsingleresponses,
andsodon’ttendtodoagoodjobofcontinuouslygeneratingresponsesthatcohere
across multiple turns. This can be addressed by using reinforcement learning, as
wellastechniqueslikeadversarialnetworks,tolearntochooseresponsesthatmake
theoverallconversationmorenatural(Lietal.2016b,Lietal.2017).
Responsebyretrievingandrefiningknowledge
Chatbots can be much more interesting and informative if they can respond based
ontextknowledgesourcesotherthandialogue. Thisapproachwaspioneeredearly
onbytheCOBOTchatbot(Isbelletal.,2000),whichgeneratedresponsesbyselect-
ingsentencesfromacorpusthatcombinedtheUnabomberManifestobyTheodore
Kaczynski,articlesonalienabduction,thescriptsof“TheBigLebowski”and“Planet
oftheApes”. XiaoIcecollectssentencesfrompubliclecturesandnewsarticlesand
searchesthemusingIRbasedonqueryexpansionfromtheuser’sturntorespondto
turnslike“TellmesomethingaboutBeijing”(Zhouetal.,2020);
Onewaytoaugmenttheencoderdecoderarchitectureforretrieveandrefineisto
firstuseIRtoretrievepotentiallyusefulpassagesfromWikipedia(Yanetal.,2016),
andthencreatemultiplecandidatesbyconcatenatingeachretrievedWikipediasen-
tencetothedialoguecontextwithaseparatortoken. Eachcandidatecanbegivenas
theencodercontexttotheencoder-decodermodel,whichlearnstoincorporatetext
fromtheWikipediasentenceintoitsgeneratedresponse(Dinanetal.2019, Roller
etal.2021).
Thelanguagemodelapproachtogenerationcanalsomakeuseofexternalknowl-
edgesources,bygivingthesesameknowledge+dialoguecontextcandidatestoalan-
guagemodellikeGPT-2finetunedonconversationalcorporatogenerateaknowledge-
awareresponse(Paranjapeetal.,2020).
15.2.3 Hybridarchitectures
Chatbotscanalsobebuiltwitharchitecturesthatarehybridsoftherule-basedand
neural/corpusarchitectures,andevenuseelementsoftheframe-basedstructurewe
describebelow. Thisiscommon,forexample,forsystemscompetingintheAlexa308 CHAPTER15 • CHATBOTS&DIALOGUESYSTEMS
Prize challenge, in which university teams build social chatbots to converse with
volunteersontheAmazonAlexaplatform, andarescoredbasedonthelengthand
userratingsoftheirconversations(Rametal.,2017).
ForexampletheChirpyCardinalsystem(Paranjapeetal.,2020)appliesanNLP
pipeline that includes Wikipedia entity linking (Section 14.3), user intent classifi-
cation,anddialogueactclassification(tobedefinedbelowinSection15.4.1). The
intentclassificationisusedwhentheuserwantstochangethetopic,andtheentity
linkerspecifieswhatentityiscurrentlybeingdiscussed. Dialogueactclassification
isusedtodetectwhentheuserisaskingaquestionorgivinganaffirmativeversus
negativeresponse.
Botresponsesaregeneratedbyaseriesofresponsegenerators. Someresponse
generators use fine-tuned neural language models: a GPT-2 (Radford et al., 2019)
language model fine-tuned on the EmpatheticDialogues dataset, and a separately
fine-tuned GPT-2 language model that learns in fine-tuning to paraphrase content
fromWikipediainresponsetoquestions.
Otherresponsegeneratorsareclosertorule-based; themovieormusicgenera-
torsproducescriptedconversationaboutamovieoramusician,suchasaskingthe
user’sopinionaboutamovieoranactor,givingafunfact,andsoon. Thesegenera-
torsuseregexesandsentimentclassifierstoclassifyuserresponsesandhandwritten
templatestogeneratebotutterances.
Eachresponsegeneratorcanspecifyapriority,andarankerisusedtopasscon-
trolbetweenresponsegenerators. Fig.15.8showspartofaconversation.
Figure15.8 PartofaconversationwithChirpyCardinal(Paranjapeetal.,2020).
15.3 GUS: Simple Frame-based Dialogue Systems
We turn now to task-based dialogue, in which a dialogue system has the goal of
helpingausersolvesometasklikemakinganairplanereservationorbuyingaprod-
GUS uct. In this section we introduce the very simple GUS architecture for task-based
dialogue. Although it is quite old, (it was first introduced in 1977 in the influen-
tial GUS system for travel planning (Bobrow et al., 1977)) the GUS architecture15.3 • GUS:SIMPLEFRAME-BASEDDIALOGUESYSTEMS 309
hasbeenastonishinglylong-livedandunderliesmostifnotallmoderncommercial
digitalassistants. Figure15.9showsaconversationwiththeoriginalGUSsystem.
Allmoderntask-baseddialoguesystems, whetherthesimpleGUSarchitecture
dialoguestate wedescribehere,orthemoresophisticateddialoguestatearchitecturesweturnto
frame inthefollowingsection,arebasedaroundframes. Aframeisakindofknowledge
structurerepresentingthekindsofintentionsthesystemcanextractfromusersen-
slot tences,andconsistsofacollectionofslots,eachofwhichcantakeasetofpossible
values. Togetherthissetofframesissometimescalledadomainontology.
GUS: Hello.MynameisGUS.Icanhelpyouplanasimpletripbyair.Wheredoyou
wanttogo?
Client: IwanttogotoSanDiegoonMay28
GUS: Whattimedoyouwanttoleave?
Client: ImustbeinSanDiegobefore10am
GUS: WouldyoulikeP.S.A.flight102thatarrivesat9.15am?
Client: Whatisthenextflight?
GUS: AirCaliforniaflight310thatleavesat8.30amandarrivesat10.00am
Client: I’lltakethefirstone
GUS: InwhatnameshouldImakeareservation,please?
Client: DanBobrow
GUS: Ihaveconfirmedthefollowingflight: P.S.A.flight102onWednesdayMay28
fromSanJosetoSanDiegothatleavesat7.30amandarrivesat9.15am.
GUS: Whatdatedoyouwanttoreturnon?
Client: OnFridayintheevening.
GUS: Wouldyouliketheflightthatleavesat7.45pm?
Client: That’sfine.
GUS: Ihaveconfirmedthefollowingflight: P.S.A.flight307onFridayMay30from
SanDiegotoSanJosethatleavesat7.45pmandarrivesat9.30pmThankyou
forcalling.Goodbye
Figure15.9 Thetraveldomain:AtranscriptofanactualdialoguewiththeGUSsystemof
Bobrowetal.(1977).P.S.A.andAirCaliforniawereairlinesofthatperiod.
Thesetofslotsinatask-baseddialogueframespecifieswhatthesystemneeds
toknow,andthefillerofeachslotisconstrainedtovaluesofaparticularsemantic
type. Inthetraveldomain,forexample,aslotmightbeoftypecity(hencetakeon
valueslikeSanFrancisco,orHongKong)oroftypedate,airline,ortime.
Slot Type QuestionTemplate
ORIGINCITY city “Fromwhatcityareyouleaving?”
DESTINATIONCITY city “Whereareyougoing?”
DEPARTURETIME time “Whenwouldyouliketoleave?”
DEPARTUREDATE date “Whatdaywouldyouliketoleave?”
ARRIVALTIME time “Whendoyouwanttoarrive?”
ARRIVALDATE date “Whatdaywouldyouliketoarrive?”
Figure15.10 Aframeinaframe-baseddialoguesystem,showingthetypeofeachslotand
aquestionusedtofilltheslot.
Types in GUS, as in modern frame-based dialogue agents, have hierarchical
structure; for example the date type in GUS is itself a frame with slots with types
likeintegerormembersofsetsofweekdaynames:
DATE
MONTH:NAME YEAR:INTEGER DAY:(BOUNDED-INTEGER 1 31)
WEEKDAY:(MEMBER (Sunday Monday Tuesday Wednesday310 CHAPTER15 • CHATBOTS&DIALOGUESYSTEMS
Thursday Friday Saturday))
15.3.1 Controlstructureforframe-baseddialogue
The control architecture for frame-based dialogue systems, used in various forms
inmodernsystemslikeApple’sSiri,Amazon’sAlexa,andtheGoogleAssistant,is
designedaroundtheframe.Thesystem’sgoalistofilltheslotsintheframewiththe
fillerstheuserintends,andthenperformtherelevantactionfortheuser(answering
aquestion,orbookingaflight).
To do this, the system asks questions of the user (using pre-specified question
templates associated with each slot of each frame, as shown in Fig. 15.10), filling
any slot that the user specifies (we’ll describe how slot-filling works in the next
section). Ifauser’sresponsefillsmultipleslots,likethefollowing:
(15.4) IwantaflightfromSanFranciscotoDenveronewayleavingafterfive
p.m. onTuesday.
thesystemfillsalltherelevantslots,andthencontinuesaskingquestionstofillthe
remainingslots,skippingquestionsassociatedwithfilledslots. TheGUSarchitec-
ture also has condition-action rules attached to slots. For example, a rule attached
to the DESTINATION slot for the plane booking frame, once the user has specified
the destination, might automatically enter that city as the default StayLocation for
therelatedhotelbookingframe. Oriftheuserspecifiesthe DESTINATION DAY for
ashorttripthesystemcouldautomaticallyentertheARRIVALDAY.
Manydomainsrequiremultipleframes. Besidesframesforcarorhotelreserva-
tions,wemightneedframeswithgeneralrouteinformation(forquestionslikeWhich
airlines fly from Boston to San Francisco?), or information about airfare practices
(forquestionslikeDoIhavetostayaspecificnumberofdaystogetadecentair-
fare?). Thesystemmustbeabletodisambiguatewhichslotofwhichframeagiven
inputissupposedtofillandthenswitchdialoguecontroltothatframe.
Because of this need to dynamically switch control, the GUS architecture is a
production rule system. Different types of inputs cause different productions to
fire, each of which can flexibly fill in different frames. The production rules can
then switch control according to factors such as the user’s input and some simple
dialoguehistorylikethelastquestionthatthesystemasked.
Oncethesystemhasenoughinformationitperformsthenecessaryaction(like
queryingadatabaseofflights)andreturnstheresulttotheuser.
15.3.2 DeterminingDomain,Intent,andSlotfillersinGUS
The goal of the next component in the frame-based architecture is to extract three
things from the user’s utterance. The first task is domain classification: is this
user for example talking about airlines, programming an alarm clock, or dealing
with their calendar? Of course this 1-of-n classification tasks is unnecessary for
single-domain systems that are focused on, say, only calendar management, but
multi-domain dialogue systems are the modern standard. The second is user in-
intent tentdetermination: whatgeneraltaskorgoalistheusertryingtoaccomplish? For
determination
example the task could be to Find a Movie, or Show a Flight, or Remove a Cal-
slotfilling endar Appointment. Finally, we need to do slot filling: extract the particular slots
and fillers that the user intends the system to understand from their utterance with
respecttotheirintent. Fromauserutterancelikethisone:
Show me morning flights from Boston to San Francisco on Tuesday15.3 • GUS:SIMPLEFRAME-BASEDDIALOGUESYSTEMS 311
asystemmightwanttobuildarepresentationlike:
DOMAIN: AIR-TRAVEL
INTENT: SHOW-FLIGHTS
ORIGIN-CITY: Boston
ORIGIN-DATE: Tuesday
ORIGIN-TIME: morning
DEST-CITY: San Francisco
whileanutterancelike
Wake me tomorrow at 6
shouldgiveanintentlikethis:
DOMAIN: ALARM-CLOCK
INTENT: SET-ALARM
TIME: 2017-07-01 0600-0800
Theslot-fillingmethodusedintheoriginalGUSsystem,andstillquitecommon
inindustrialapplications,istousehandwrittenrules,oftenaspartofthecondition-
actionrulesattachedtoslotsorconcepts.Forexamplewemightjustdefinearegular
expressionforrecognizingtheSET-ALARMintent:
wake me (up) | set (the|an) alarm | get me up
Morecomplicatedrules,forexamplebasedoncontext-freegrammarsandparsers
(Chapter17)canbeused,afterwhichthefillersareputintosomesortofcanonical
form,forexamplebynormalizingdatesasdiscussedinChapter22. Mostindustrial
dialoguesystemsemploytheGUSarchitecturebutusesupervisedmachinelearning
forslot-fillinginsteadofthesekindsofrules;seeSection15.4.2.
15.3.3 Othercomponentsofframe-baseddialogue
TheASR(automaticspeechrecognition)componenttakesaudioinputfromaphone
orotherdeviceandoutputsatranscribedstringofwords,asdiscussedinChapter16.
TheASRcomponentcanalsobemadedependentonthedialoguestate. Forexam-
ple, ifthesystemhasjustaskedtheuser“Whatcityareyoudepartingfrom?”, the
ASR language model component can be constrained to assign high probability to
answers with city names. This can be done by training a language model on an-
swerstothisquestion, orbyhand-writingfinite-stateorcontext-freegrammarsfor
suchsentences. Alanguagemodelthatiscompletelydependentondialoguestateis
restrictive called a restrictive grammar; usually real systems mix restrictive grammars with
grammar
moregenerallanguagemodels.
The natural language generation module of any dialogue system produces
the utterances that the system says to the user. Frame-based systems tend to use
template-based template-based generation, in which all or most of the words in the sentence to
generation
be uttered to the user are prespecified by the dialogue designer. Sentences created
prompt bythesetemplatesareoftencalledprompts. Templatesmightbecompletelyfixed
(like‘Hello,howcanIhelpyou?’),orcanincludesomevariablesthatarefilledin
bythegenerator,asinthefollowing:
WhattimedoyouwanttoleaveCITY-ORIG?
WillyoureturntoCITY-ORIGfromCITY-DEST?
It’s possible to do some simple grounding even in templated generation. Con-
sidertheunnaturalnessofthisexamplefromCohenetal.(2004):312 CHAPTER15 • CHATBOTS&DIALOGUESYSTEMS
(15.5) System: Didyouwanttoreviewsomemoreofyourpersonalprofile?
Caller: No.
System: What’snext?
Withoutanacknowledgment,thecallerdoesn’tknowthatthesystemhasunder-
stood her ‘No’. The use of Okay below adds grounding to the templated response
What’snext?,making(15.6)amuchmorenaturalresponsethan(15.5):
(15.6) System: Didyouwanttoreviewsomemoreofyourpersonalprofile?
Caller: No.
System: Okay,what’snext?
Therule-basedGUSapproachisverycommoninindustrialapplications.Aswas
truewiththerule-basedapproachtoinformationextraction,ithastheadvantageof
high precision, and if the domain is narrow enough and experts are available, can
provide sufficient coverage as well. On the other hand, the handwritten rules or
grammarscanbebothexpensiveandslowtocreate,andhandwrittenrulescansuffer
fromrecallproblems.
15.4 The Dialogue-State Architecture
Modernresearchsystemsfortask-baseddialoguearebasedonamoresophisticated
versionoftheframe-basedarchitecturecalledthedialogue-stateorbelief-statear-
chitecture. Figure15.11showsthesixcomponentsofatypicaldialogue-statesys-
tem. Thespeechrecognitionandsynthesiscomponentsdealwithspokenlanguage
processing;we’llreturntotheminChapter16.
Figure15.11 Architectureofadialogue-statesystemfortask-orienteddialoguefromWilliamsetal.(2016).
For the rest of this chapter we therefore consider the other four components,
whicharepartofbothspokenandtextualdialoguesystems. Thesefourcomponents15.4 • THEDIALOGUE-STATEARCHITECTURE 313
aremorecomplexthaninthesimpleGUSsystems. Forexample,liketheGUSsys-
tems,thedialogue-statearchitecturehasacomponentforextractingslotfillersfrom
the user’s utterance, but generally using machine learning rather than rules. (This
componentissometimescalledtheNLUorSLUcomponent,for‘NaturalLanguage
Understanding’,or‘SpokenLanguageUnderstanding’,usingtheword“understand-
ing”loosely.)Thedialoguestatetrackermaintainsthecurrentstateofthedialogue
(which include the user’s most recent dialogue act, plus the entire set of slot-filler
constraintstheuserhasexpressedsofar).Thedialoguepolicydecideswhatthesys-
temshoulddoorsaynext. ThedialoguepolicyinGUSwassimple: askquestions
untiltheframewasfullandthenreportbacktheresultsofsomedatabasequery. But
amoresophisticateddialoguepolicycanhelpasystemdecidewhentoanswerthe
user’squestions,whentoinsteadasktheuseraclarificationquestion,whentomake
a suggestion, and so on. Finally, dialogue state systems have a natural language
generation component. In GUS, the sentences that the generator produced were
allfrompre-writtentemplates. Butamoresophisticatedgenerationcomponentcan
conditionontheexactcontexttoproduceturnsthatseemmuchmorenatural.
Asofthetimeofthiswriting,mostcommercialsystemarearchitecturalhybrids,
based on GUS architecture augmented with some dialogue-state components, but
thereareawidevarietyofdialogue-statesystemsbeingdevelopedinresearchlabs.
15.4.1 DialogueActs
dialogueacts Dialogue-statesystemsmakeuseofdialogueacts. Dialogueactsrepresentthein-
teractive function of the turn or sentence, combining the idea of speech acts and
groundingintoasinglerepresentation. Differenttypesofdialoguesystemsrequire
labeling different kinds of acts, and so the tagset—defining what a dialogue act is
exactly—tendstobedesignedforparticulartasks.
Tag Sys User Description
HELLO(a=x,b=y,...) (cid:88) (cid:88) Openadialogueandgiveinfoa=x,b=y,...
INFORM(a=x,b=y,...) (cid:88) (cid:88) Giveinfoa=x,b=y,...
REQUEST(a,b=x,...) (cid:88) (cid:88) Requestvalueforagivenb=x,...
REQALTS(a=x,...) χ (cid:88) Requestalternativewitha=x,...
CONFIRM(a=x,b=y,...) (cid:88) (cid:88) Explicitlyconfirma=x,b=y,...
CONFREQ(a=x,...,d) (cid:88) χ Implicitlyconfirma=x,...andrequestvalueofd
SELECT(a=x,a=y) (cid:88) χ Implicitlyconfirma=x,...andrequestvalueofd
AFFIRM(a=x,b=y,...) (cid:88) (cid:88) Affirmandgivefurtherinfoa=x,b=y,...
NEGATE(a=x) χ (cid:88) Negateandgivecorrectedvaluea=x
DENY(a=x) χ (cid:88) Denythata=x
BYE() (cid:88) (cid:88) Closeadialogue
Figure15.12 DialogueactsusedbytheHISrestaurantrecommendationsystemofYoung
etal.(2010). TheSysandUsercolumnsindicatewhichactsarevalidassystemoutputsand
userinputs,respectively.
Figure15.12showsatagsetforarestaurantrecommendationsystem,andFig.15.13
shows these tags labeling a sample dialogue from the HIS system (Young et al.,
2010).Thisexamplealsoshowsthecontentofeachdialogueacts,whicharetheslot
fillers being communicated. So the user might INFORM the system that they want
Italianfoodnearamuseum,orCONFIRMwiththesystemthatthepriceisreasonable.314 CHAPTER15 • CHATBOTS&DIALOGUESYSTEMS
Utterance Dialogueact
U: Hi,Iamlookingforsomewheretoeat. hello(task = find,type=restaurant)
S: You are looking for a restaurant. What confreq(type = restaurant, food)
typeoffooddoyoulike?
U: I’d like an Italian somewhere near the inform(food = Italian, near=museum)
museum.
S: Roma is a nice Italian restaurant near inform(name = "Roma", type = restaurant,
themuseum. food = Italian, near = museum)
U: Isitreasonablypriced? confirm(pricerange = moderate)
S: Yes, Roma is in the moderate price affirm(name = "Roma", pricerange =
range. moderate)
U: Whatisthephonenumber? request(phone)
S: ThenumberofRomais385456. inform(name = "Roma", phone = "385456")
U: Ok,thankyougoodbye. bye()
Figure15.13 A sample dialogue from the HIS System of Young et al. (2010) using the dialogue acts in
Fig.15.12.
15.4.2 SlotFilling
Thetaskofslot-filling,andthesimplertasksofdomainandintentclassification,are
specialcasesofthetaskofsupervisedsemanticparsingdiscussedinChapter20,in
which we have a training set that associates each sentence with the correct set of
slots,domain,andintent.
A simple method is to train a sequence model to map from input words repre-
sentationtoslotfillers,domainandintent. Forexamplegiventhesentence:
I want to fly to San Francisco on Monday afternoon please
wecomputeasentencerepresentation,forexamplebypassingthesentencethrough
a contextual embedding network like BERT. The resulting sentence representation
can be passed through a feedforward layer and then a simple 1-of-N classifier to
determinethatthedomainisAIRLINEandandtheintentisSHOWFLIGHT.
OurtrainingdataissentencespairedwithsequencesofBIOlabels:
O O O O O B-DES I-DES O B-DEPTIME I-DEPTIME O
I want to fly to San Francisco on Monday afternoon please
RecallfromChapter8thatinBIOtaggingweintroduceatagforthebeginning
(B) and inside (I) of each slot label, and one for tokens outside (O) any slot label.
Thenumberoftagsisthus2n+1tags,wherenisthenumberofslots.
Fig.15.14showsthearchitecture. Theinputisaseriesofwordsw ...w ,which
1 n
ispassedthroughacontextualembeddingmodeltogetcontextualwordrepresenta-
tions. Thisisfollowedbyafeedforwardlayerandasoftmaxateachtokenposition
over possible BIO tags, with the output a series of BIO tags s ...s . We can also
1 n
combine the domain-classification and intent-extraction tasks with slot-filling sim-
ply by adding a domain concatenated with an intent as the desired output for the
finalEOStoken.
Once the sequence labeler has tagged the user utterance, a filler string can be
extractedforeachslotfromthetags(e.g.,“SanFrancisco”),andthesewordstrings
canthenbenormalizedtothecorrectformintheontology(perhapstheairportcode
‘SFO’).Thisnormalizationcantakeplacebyusinghomonymdictionaries(specify-
ing,forexample,thatSF,SFO,andSanFranciscoarethesameplace).
In industrial contexts, machine learning-based systems for slot-filling are of-
tenbootstrappedfromGUS-stylerule-basedsystemsinasemi-supervisedlearning15.4 • THEDIALOGUE-STATEARCHITECTURE 315
B-DES I-DES O B-DTIME d+i
Classifier
+softmax
Encodings
Encoder
…
San Francisco on Monday <EOS>
Figure15.14 Asimplearchitectureforslotfilling,mappingthewordsintheinputthrough
contextualembeddingslikeBERTtoanoutputclassifierlayer(whichcanbelinearorsome-
thingmorecomplex),followedbysoftmaxtogenerateaseriesofBIOtags(andincludinga
finalstateconsistingofadomainconcatenatedwithanintent).
manner. Arule-basedsystemisfirstbuiltforthedomain,andatestsetiscarefully
labeled. Asnewuserutterancescomein,theyarepairedwiththelabelingprovided
by the rule-based system to create training tuples. A classifier can then be trained
on these tuples, using the test set to test the performance of the classifier against
the rule-based system. Some heuristics can be used to eliminate errorful training
tuples,withthegoalofincreasingprecision. Assufficienttrainingsamplesbecome
availabletheresultingclassifiercanoftenoutperformtheoriginalrule-basedsystem
(Suendermann et al., 2009), although rule-based systems may still remain higher-
precisionfordealingwithcomplexcaseslikenegation.
15.4.3 DialogueStateTracking
The job of the dialogue-state tracker is to determine both the current state of the
frame (the fillers of each slot), as well as the user’s most recent dialogue act. The
dialogue-state thus includes more than just the slot-fillers expressed in the current
sentence; it includes the entire state of the frame at this point, summarizing all of
theuser’sconstraints. ThefollowingexamplefromMrksˇic´ etal.(2017)showsthe
requiredoutputofthedialoguestatetrackeraftereachturn:
User: I’mlookingforacheaperrestaurant
inform(price=cheap)
System: Sure. Whatkind-andwhere?
User: Thaifood,somewheredowntown
inform(price=cheap, food=Thai, area=centre)
System: TheHouseservescheapThaifood
User: Whereisit?
inform(price=cheap, food=Thai, area=centre); request(address)
System: TheHouseisat106RegentStreet
Sincedialogueactsplacesomeconstraintsontheslotsandvalues, thetasksof
dialogue-actdetectionandslot-fillingareoftenperformedjointly. Considerthetask
ofdeterminingthat
I’d like Cantonese food near the Mission District
hasthestructure
inform(food=cantonese,area=mission).316 CHAPTER15 • CHATBOTS&DIALOGUESYSTEMS
Dialogue act interpretation—in this example choosing inform from the set of
dialogue acts for this task—is done by supervised classification trained on hand-
labeleddialogacts,predictingthedialogueacttagbasedonembeddingsrepresent-
ingthecurrentinputsentenceandthepriordialogueacts.
The simplest dialogue state tracker might just take the output of a slot-filling
sequence-model(Section15.4.2)aftereachsentence.Alternatively,amorecomplex
model can make use of the reading-comprehension architectures from Chapter 14.
ForexamplethemodelofGaoetal.(2019)trainsaclassifierforeachslottodecide
whetheritsvalueisbeingchangedinthecurrentsentenceorshouldbecarriedover
from the previous sentences. If the slot value is being changed, a span-prediction
modelisusedtopredictthestartandendofthespanwiththeslotfiller.
Aspecialcase: detectingcorrectionacts
Somedialogueactsareimportantbecauseoftheirimplicationsfordialoguecontrol.
If a dialogue system misrecognizes or misunderstands an utterance, the user will
generally correct the error by repeating or reformulating the utterance. Detecting
usercorrection theseusercorrectionactsisthereforequiteimportant. Ironically, itturnsoutthat
acts
correctionsareactuallyhardertorecognizethannormalsentences! Infact,correc-
tions in one early dialogue system (the TOOT system) had double the ASR word
errorrateofnon-corrections(Swertsetal.,2000)! Onereasonforthisisthatspeak-
hyperarticula- erssometimesuseaspecificprosodicstyleforcorrectionscalledhyperarticulation,
tion
inwhichtheutterancecontainsexaggeratedenergy,duration,orF0contours,suchas
IsaidBAL-TI-MORE,notBoston(Wadeetal.1992,Levow1998,Hirschbergetal.
2001). Evenwhentheyarenothyperarticulating, userswhoarefrustratedseemto
speakinawaythatisharderforspeechrecognizers(Goldbergetal.,2003).
What are the characteristics of these corrections? User corrections tend to be
eitherexactrepetitionsorrepetitionswithoneormorewordsomitted,althoughthey
may also be paraphrases of the original utterance (Swerts et al., 2000). Detecting
thesereformulationsorcorrectionactscanbepartofthegeneraldialogueactdetec-
tionclassifier.Alternatively,becausethecuestotheseactstendtoappearindifferent
waysthanforsimpleacts(likeINFORMorrequest),wecanmakeuseoffeaturesor-
thogonaltosimplecontextualembeddingfeatures;sometypicalfeaturesareshown
below(Levow1998,Litmanetal.1999,Hirschbergetal.2001,Bulykoetal.2005,
Awadallahetal.2015).
features examples
lexical wordslike“no”,“correction”,“Idon’t”,swearwords,utterancelength
semantic similarity (word overlap or embedding dot product) between the candidate
correctionactandtheuser’spriorutterance
phonetic phoneticoverlapbetweenthecandidatecorrectionactandtheuser’spriorut-
terance(i.e. “WhatsApp”maybeincorrectlyrecognizedas“What’sup”)
prosodic hyperarticulation, increases in F0 range, pause duration, and word duration,
generallynormalizedbythevaluesforprevioussentences
ASR ASRconfidence,languagemodelprobability
15.4.4 DialoguePolicy
dialoguepolicy Thegoalofthedialoguepolicyistodecidewhatactionthesystemshouldtakenext,
thatis,whatdialogueacttogenerate.
Moreformally,atturniintheconversationwewanttopredictwhichactionA
i15.4 • THEDIALOGUE-STATEARCHITECTURE 317
totake,basedontheentiredialoguestate. Thestatecouldmeantheentiresequence
ofdialogueactsfromthesystem(A)andfromtheuser(U),inwhichcasethetask
wouldbetocompute:
Aˆ =argmaxP(A (A ,U ,...,A ,U ) (15.7)
i i 1 1 i 1 i 1
Ai∈A | − −
We can simplify this by maintaining as the dialogue state mainly just the set of
slot-fillersthattheuserhasexpressed,collapsingacrossthemanydifferentconver-
sationalpathsthatcouldleadtothesamesetoffilledslots.
Such a policy might then just condition on the current dialogue state as repre-
sentedjustbythecurrentstateoftheframeFrame (whichslotsarefilledandwith
i
what)andthelastturnbythesystemanduser:
Aˆ =argmaxP(A Frame ,A ,U ) (15.8)
i i i 1 i 1 i 1
Ai∈A | − − −
Theseprobabilitiescanbeestimatedbyaneuralclassifierusingneuralrepresenta-
tions of the slot fillers (for example as spans) and the utterances (for example as
sentenceembeddingscomputedovercontextualembeddings)
Moresophisticatedmodelstrainthepolicyviareinforcementlearning. Tode-
cidewhichactiontotake,areinforcementlearningsystemgetsarewardattheend
ofthedialogue,andusesthatrewardtotrainapolicytotakeactions. Forexamplein
themovie-recommendationdialoguesystemofFazel-Zarandietal.(2017), theac-
tionspacehasonlythreeactions: EXECUTE,CONFIRM,andELICIT. TheEXECUTE
sends a query to the database and answers the user’s question, CONFIRM clarifies
theintentorslotwiththeusers(e.g.,“DoyouwantmoviesdirectedbyChristopher
Nolan?”) while ELICIT asks the user for missing information (e.g., “Which movie
areyoutalkingabout?”).Thesystemgetsalargepositiverewardifthedialoguesys-
temterminateswiththecorrectslotrepresentationattheend,alargenegativereward
iftheslotsarewrong, andasmallnegativerewardforconfirmationandelicitation
questionstokeepthesystemfromre-confirmingeverything.
PolicyExample: ConfirmationandRejection
Moderndialoguesystemsoftenmakemistakes.Itisthereforeimportantfordialogue
systemstomakesurethattheyhaveachievedthecorrectinterpretationoftheuser’s
input. Thisisgenerallydonebytwomethods: confirmingunderstandingswiththe
userandrejectingutterancesthatthesystemislikelytohavemisunderstood.
Aswesawinthepriorsection,mostsystemsintroduceparticularstrategiesand
explicit actionsrelatedtoconfirmationandrejection. Whenusingtheexplicitconfirmation
confirmation
strategy, a system asks the user a direct question to confirm the system’s under-
standing,likethetwoexamplesbelowinwhichthesystemasksa(boldface)yes-no
confirmationquestion:
S: Whichcitydoyouwanttoleavefrom?
U: Baltimore.
S: DoyouwanttoleavefromBaltimore?
U: Yes.
U: I’d like to fly from Denver Colorado to New York City on September
twentyfirstinthemorningonUnitedAirlines
S: Let’sseethen. IhaveyougoingfromDenverColoradotoNewYork
onSeptembertwentyfirst. Isthatcorrect?
U: Yes318 CHAPTER15 • CHATBOTS&DIALOGUESYSTEMS
implicit When using the implicit confirmation strategy, a system instead can demon-
confirmation
strate its understanding as a grounding strategy, for example repeating back the
system’sunderstandingaspartofaskingthenextquestion, asinthetwoexamples
below:
U: IwanttotraveltoBerlin
S: WhendoyouwanttotraveltoBerlin?
U2: HiI’dliketoflytoSeattleTuesdayMorning
A3: TravelingtoSeattleonTuesday,Augusteleventhinthemorning.
Yourfullname?
Explicitandimplicitconfirmationhavecomplementarystrengths. Explicitcon-
firmation makes it easier for users to correct the system’s misrecognitions since a
usercanjustanswer“no”totheconfirmationquestion. Butexplicitconfirmationis
awkward and increases the length of the conversation (Danieli and Gerbino 1995,
Walker et al. 1998a). The explicit confirmation dialogue fragments above sound
non-naturalanddefinitelynon-human; implicitconfirmationismuchmoreconver-
sationallynatural.
Confirmation is just one kind of conversational action by which a system can
rejection expresslackofunderstanding. Anotheroptionisrejection,inwhichasystemgives
theuserapromptlikeI’msorry,Ididn’tunderstandthat.
Sometimesutterancesarerejectedmultipletimes. Thismightmeanthattheuser
is using language that the system is unable to follow. Thus, when an utterance is
progressive rejected, systems often follow a strategy of progressive prompting or escalating
prompting
detail (Yankelovich et al. 1995, Weinschenk and Barker 2000), as in this example
fromCohenetal.(2004):
System: Whenwouldyouliketoleave?
Caller: Well,um,IneedtobeinNewYorkintimeforthefirstWorldSeriesgame.
System: <reject>. Sorry,Ididn’tgetthat. Pleasesaythemonthanddayyou’dlike
toleave.
Caller: IwannagoonOctoberfifteenth.
Inthisexample,insteadofjustrepeating“Whenwouldyouliketoleave?”,the
rejection prompt gives the caller more guidance about how to formulate an utter-
ancethesystemwillunderstand. Theseyou-can-sayhelpmessagesareimportantin
helpingimprovesystems’understandingperformance(BohusandRudnicky,2005).
If the caller’s utterance gets rejected yet again, the prompt can reflect this (“I still
didn’tgetthat”),andgivethecallerevenmoreguidance.
rapid An alternative strategy for error handling is rapid reprompting, in which the
reprompting
systemrejectsanutterancejustbysaying“I’msorry?” or“Whatwasthat?” Only
if the caller’s utterance is rejected a second time does the system start applying
progressive prompting. Cohen et al. (2004) summarize experiments showing that
usersgreatlypreferrapidrepromptingasafirst-levelerrorprompt.
It is common to use rich features other than just the dialogue state representa-
tion to make policy decisions. For example, the confidence that the ASR system
assigns to an utterance can be used by explicitly confirming low-confidence sen-
tences. Confidenceisametricthatthespeechrecognizercanassigntoitstranscrip-
tionofasentencetoindicatehowconfidentitisinthattranscription. Confidenceis
oftencomputedfromtheacousticlog-likelihoodoftheutterance(greaterprobabil-
itymeanshigherconfidence), butprosodicfeaturescanalsobeusedinconfidence15.4 • THEDIALOGUE-STATEARCHITECTURE 319
prediction. For example, utterances with large F0 excursions or longer durations,
or those preceded by longer pauses, are likely to be misrecognized (Litman et al.,
2000).
Anothercommonfeatureinconfirmationisthecostofmakinganerror. Forex-
ample,explicitconfirmationiscommonbeforeaflightisactuallybookedormoney
inanaccountismoved. Systemsmighthaveafour-tieredlevelofconfidencewith
threethresholdsα,β,andγ:
<α lowconfidence reject
α abovethethreshold confirmexplicitly
≥
β highconfidence confirmimplictly
≥
γ veryhighconfidence don’tconfirmatall
≥
15.4.5 Naturallanguagegenerationinthedialogue-statemodel
Onceadialogueacthasbeendecided,weneedtogeneratethetextoftheresponse
totheuser. Thetaskofnaturallanguagegeneration(NLG)intheinformation-state
content architecture is often modeled in two stages, content planning (what to say), and
planning
sentence sentencerealization(howtosayit).
realization
Herewe’llassumecontentplanninghasbeendonebythedialoguepolicy,which
haschosenthedialogueacttogenerate,andchosensomeattributes(slotsandvalues)
thattheplannerwantstosaytotheuser(eithertogivetheusertheanswer,oraspart
ofaconfirmationstrategy).
recommend(restaurant name= Au Midi, neighborhood = midtown,
cuisine = french
1 AuMidiisinMidtownandservesFrenchfood.
2 ThereisaFrenchrestaurantinMidtowncalledAuMidi.
recommend(restaurant name= Loch Fyne, neighborhood = city
centre, cuisine = seafood)
3 LochFyneisintheCityCenterandservesseafoodfood.
4 ThereisaseafoodrestaurantintheCityCentrecalledLochFyne.
Figure15.15 TwoexamplesofinputstothesentencerealizationphaseofNLG,showing
thedialogueactandattributesprespecifiedbythecontentplanner.Line1-2and3-4showdif-
ferentpossibleoutputsentencestobegeneratedbythesentencerealizer.Fromtherestaurant
recommendationsystemofNayaketal.(2017).
Fig.15.15showssomesampleinput/outputsforthesentencerealizationphase.
Inthefirstexample,thecontentplannerhaschosenthedialogueact RECOMMEND
andsomeparticularslots(name, neighborhood, cuisine)andtheirfillers. Thegoal
ofthesentencerealizeristogenerateasentencelikelines1or2showninthefigure,
by training on many such examples of representation/sentence pairs from a large
corpusoflabeleddialogues.
Trainingdataishardtocomeby;weareunlikelytoseeeverypossiblerestaurant
witheverypossibleattributeinmanypossibledifferentlywordedsentences. There-
fore it is common in sentence realization to increase the generality of the training
delexicalization examplesbydelexicalization. Delexicalizationistheprocessofreplacingspecific
words in the training set that represent slot values with a generic placeholder to-
kenrepresentingtheslot. Fig.15.16showstheresultofdelexicalizingthetraining
sentencesinFig.15.15.
Mapping from frames to delexicalized sentences is generally done by encoder
decodermodels(Wenetal.2015a,Wenetal.2015b,Mrksˇic´ etal.2017,interalia),320 CHAPTER15 • CHATBOTS&DIALOGUESYSTEMS
recommend(restaurant name= Au Midi, neighborhood = midtown,
cuisine = french
1 restaurant nameisinneighborhoodandservescuisinefood.
2 Thereisacuisinerestaurantinneighborhoodcalledrestaurant name.
Figure15.16 Delexicalizedsentencesthatcanbeusedforgeneratingmanydifferentrelex-
icalizedsentences.FromtherestaurantrecommendationsystemofNayaketal.(2017).
trainedonlargehand-labeledcorporaoftask-orienteddialogue(Budzianowskietal.,
2018). The input to the encoder is a sequence of tokens x that represent the dia-
t
logueactanditsarguments. ThusthedialogueactRECOMMENDandtheattribute/-
value pairs service:decent, cuisine:null might be represented as a flat sequence of
tokens (Nayak et al., 2017), each mapped to a learned embedding w, as shown in
t
Fig.15.17.
[name] has decent service
DECODER
ENCODER
RECOMMENDservice: decent cuisine: null
Figure15.17 Anencoderdecodersentencerealizermappingslots/fillerstoEnglish.
Theencoderreadsalltheinputslot/valuerepresentations,andthedecoderout-
putsthefollowingdelexicalizedEnglishsentence:
restaurant namehasdecentservice
relexicalize Wecanthenusetheinputframefromthecontentplannertorelexicalize(fillinthe
exactrestaurantorneighborhoodorcuisine)resultingin:
AuMidihasdecentservice
GeneratingClarificationQuestions
It’salsopossibletodesignNLGalgorithmsthatarespecifictoaparticulardialogue
clarification act. Forexample, considerthetaskofgeneratingclarificationquestions, incases
questions
where the speech recognition fails to understand some part of the user’s utterance.
While it is possible to use the generic dialogue act REJECT (“Please repeat”, or “I
don’tunderstandwhatyousaid”),studiesofhumanconversationsshowthathumans
instead use targeted clarification questions that reprise elements of the misunder-
standing(Purver2004,GinzburgandSag2000,Stoyanchevetal.2013).
For example, in the following hypothetical example the system reprises the
words “going” and “on the 5th” to make it clear which aspect of the user’s turn
thesystemneedstobeclarified:
User: WhatdoyouhavegoingtoUNKNOWN WORDonthe5th?
System: Goingwhereonthe5th?
Targetedclarificationquestionscanbecreatedbyrules(suchasreplacing“go-
ingtoUNKNOWN WORD”with“goingwhere”)orbybuildingclassifierstoguess
which slots might have been misrecognized in the sentence (Chu-Carroll and Car-
penter1999,Stoyanchevetal.2014,StoyanchevandJohnston2015).15.5 • EVALUATINGDIALOGUESYSTEMS 321
15.5 Evaluating Dialogue Systems
Evaluationiscrucialindialoguesystemdesign.Chatbotsandtask-basedsystemsare
generallyevaluateddifferently, sincetheyhavedifferentgoals; task-basedsystems
havetocompleteatasklikebookingaflight;chatbotshaveadifferentkindofgoal,
likebeingenjoyabletousers.
15.5.1 EvaluatingChatbots
Chatbotsareevaluatedbyhumans,whoassignascore. Thiscanbethehumanwho
talkedtothechatbot(participantevaluation)orathirdpartywhoreadsatranscript
ofahuman/chatbotconversation(observerevaluation).
IntheparticipantevaluationofSeeetal.(2019),thehumanevaluatorchatswith
the model for six turns and rates the chatbot on 8 dimensions capturing conversa-
tionalquality: avoidingrepetition,interestingness,makingsense,fluency,listening,
inquisitiveness,humannessandengagingness. Afewexamples:
Engagingness Howmuchdidyouenjoytalkingtothisuser?
Notatall Alittle Somewhat Alot
• • • •
AvoidingRepetition Howrepetitivewasthisuser?
Repeatedthemselvesoverandover Sometimessaidthesamethingtwice
• •
Alwayssaidsomethingnew
•
Makingsense HowoftendidthisusersaysomethingwhichdidNOTmakesense?
Never made any sense Most responses didn’t make sense Some re-
• • •
sponsesdidn’tmakesense Everythingmadeperfectsense
•
Observerevaluationsusethirdpartyannotatorstolookatthetextofacomplete
conversation. Sometimes we’re interested in having raters assign a score to each
systemturn;forexample(Artsteinetal.,2009)haveratersmarkhowcoherenteach
turnis. Often,however,wejustwantasinglehigh-levelscoretoknowifsystemA
acute-eval isbetterthansystemB.Theacute-evalmetric(Lietal.,2019a)issuchanobserver
evaluationinwhichannotatorslookattwoseparatehuman-computerconversations
(A and B) and choose the one in which the dialogue system participant performed
better(interfaceshowninFig.15.18). Theyanswerthefollowing4questions(with
theseparticularwordingsshowntoleadtohighagreement):
Engagingness Whowouldyouprefertotalktoforalongconversation?
Interestingness If you had to say one of these speakers is interesting and one is
boring,whowouldyousayismoreinteresting?
Humanness Whichspeakersoundsmorehuman?
Knowledgeable Ifyouhadtosaythatonespeakerismoreknowledgeableandone
ismoreignorant,whoismoreknowledgeable?
Automaticevaluationsaregenerallynotusedforchatbots. That’sbecausecom-
putational measures of generation performance like BLEU or ROUGE or embed-
dingdotproductsbetweenachatbot’sresponseandahumanresponsecorrelatevery
poorlywithhumanjudgments(Liuetal.,2016a).Thesemethodsperformpoorlybe-
causetherearesomanypossibleresponsestoanygiventurn; simpleword-overlap
or semantic similarity metrics work best when the space of responses is small and
lexically overlapping, which is true of generation tasks like machine translation or
possiblysummarization,butdefinitelynotdialogue.322 CHAPTER15 • CHATBOTS&DIALOGUESYSTEMS
Figure15.18 The ACUTE-EVAL method asks annotators to compare two dialogues and
choose between Speaker 1 (light blue) and Speaker 2 (dark blue), independent of the gray
speaker.FigurefromLietal.(2019a).
However, research continues in ways to do more sophisticated automatic eval-
uationsthatgobeyondwordsimilarity. Onenovelparadigmisadversarialevalu-
adversarial ation(Bowmanetal.2016, KannanandVinyals2016, Lietal.2017), inspiredby
evaluation
theTuringtest. Theideaistotraina“Turing-like”evaluatorclassifiertodistinguish
between human-generated responses and machine-generated responses. The more
successful a response generation system is at fooling this evaluator, the better the
system.
15.5.2 EvaluatingTask-BasedDialogue
Fortask-baseddialogue,ifthetaskisunambiguous,wecansimplymeasureabsolute
tasksuccess(didthesystembooktherightplaneflight,orputtherighteventonthe
calendar).
To get a more fine-grained idea of user happiness, we can compute a user sat-
isfactionrating,havingusersinteractwithadialoguesystemtoperformataskand
thenhavingthemcompleteaquestionnaire. Forexample,Fig.15.19showssample
multiple-choicequestions(Walkeretal.,2001);responsesaremappedintotherange
of1to5,andthenaveragedoverallquestionstogetatotalusersatisfactionrating.
Itisofteneconomicallyinfeasibletoruncompleteusersatisfactionstudiesafter
everychangeinasystem.Forthisreason,itisusefultohaveperformanceevaluation
heuristicsthatcorrelatewellwithhumansatisfaction. Anumberofsuchfactorsand
heuristicshavebeenstudied,oftengroupedintotwokindsofcriteria: howwellthe15.5 • EVALUATINGDIALOGUESYSTEMS 323
system allows users to accomplish their goals (maximizing task success) with the
fewestproblems(minimizingcosts):
Task completion success: Task success can be measured by evaluating the cor-
rectness of the total solution. For a frame-based architecture, this might be slot
errorrate,thepercentageofslotsthatwerefilledwiththecorrectvalues:
#ofinserted/deleted/subsitutedslots
SlotErrorRateforaSentence= (15.9)
#oftotalreferenceslotsforsentence
Forexampleconsiderasystemgiventhissentence:
(15.10) MakeanappointmentwithChrisat10:30inGates104
whichextractedthefollowingcandidateslotstructure:
Slot Filler
PERSON Chris
TIME 11:30a.m.
ROOM Gates104
Here the slot error rate is 1/3, since the TIME is wrong. Instead of error rate,
slotprecision,recall,andF-scorecanalsobeused. Sloterrorrateisalsosometimes
calledconcepterrorrate.
Interestingly, sometimes the user’s perception of whether they completed the
taskisabetterpredictorofusersatisfactionthantheactualtaskcompletionsuccess.
(Walkeretal.,2001).
Aperhapsmoreimportant,althoughlessfine-grained,measureofsuccessisan
extrinsicmetricliketaskerrorrate. Inthiscase,thetaskerrorratewouldquantify
howoftenthecorrectmeetingwasaddedtothecalendarattheendoftheinteraction.
Efficiencycost: Efficiencycostsaremeasuresofthesystem’sefficiencyathelping
users. Thiscanbemeasuredbythetotalelapsedtimeforthedialogueinseconds,
thenumberoftotalturnsorofsystemturns,orthetotalnumberofqueries(Polifroni
et al., 1992). Other metrics include the number of system non-responses and the
“turn correction ratio”: the number of system or user turns that were used solely
to correct errors divided by the total number of turns (Danieli and Gerbino 1995,
HirschmanandPao1993).
Quality cost: Quality cost measures other aspects of the interactions that affect
user’s perception of the system. One such measure is the number of times the
ASRsystemfailedtoreturnanysentence,orthenumberofASRrejectionprompts.
Similar metrics includethe number of times the user had to bargein (interrupt the
system), or the number of time-out prompts played when the user didn’t respond
quicklyenough. Otherqualitymetricsfocusonhowwellthesystemunderstoodand
responded to the user. The most important is the slot error rate described above,
TTSPerformance Wasthesystemeasytounderstand?
ASRPerformance Didthesystemunderstandwhatyousaid?
TaskEase Wasiteasytofindthemessage/flight/trainyouwanted?
InteractionPace Wasthepaceofinteractionwiththesystemappropriate?
UserExpertise Didyouknowwhatyoucouldsayateachpoint?
SystemResponse Howoftenwasthesystemsluggishandslowtoreplytoyou?
ExpectedBehavior Didthesystemworkthewayyouexpecteditto?
FutureUse Doyouthinkyou’dusethesysteminthefuture?
Figure15.19 Usersatisfactionsurvey,adaptedfromWalkeretal.(2001).324 CHAPTER15 • CHATBOTS&DIALOGUESYSTEMS
but other components include the inappropriateness (verbose or ambiguous) of the
system’squestions,answers,anderrormessagesorthecorrectnessofeachquestion,
answer,orerrormessage(Zueetal.1989,Polifronietal.1992).
15.6 Dialogue System Design
Theuserplaysamoreimportantroleindialoguesystemsthaninmostotherareasof
speech and language processing, and thus the study of dialogue systems is closely
linkedwiththefieldofHuman-ComputerInteraction(HCI).Thedesignofdialogue
voiceuser strategies,prompts,anderrormessages,isoftencalledvoiceuserinterfacedesign,
interface
andgenerallyfollowsuser-centereddesignprinciples(GouldandLewis,1985):
1. Studytheuserandtask: Understandthepotentialusersandthenatureofthe
taskbyinterviewswithusers,investigationofsimilarsystems,andstudyofrelated
human-humandialogues.
2.Buildsimulationsandprototypes: Acrucialtoolinbuildingdialoguesystems
Wizard-of-Oz is the Wizard-of-Oz system. In wizard systems, the users interactwith what they
system
think is a software agent but is in fact a human “wizard” disguised by a software
interface(Gouldetal.1983,Goodetal.1984,FraserandGilbert1991). Thename
comesfromthechildren’sbookTheWizardofOz(Baum,1900),inwhichthewizard
turnedouttobejustasimulationcontrolledbyamanbehindacurtainorscreen.
A Wizard-of-Oz system can be used to
test out an architecture before implementa-
tion;onlytheinterfacesoftwareanddatabases
need to be in place. The wizard gets input
from the user, has a graphical interface to a
database to run sample queries based on the
user utterance, and then has a way to output
sentences, either by typing them or by some
combination of selecting from a menu and
typing.
TheresultsofaWizard-of-Ozsystemcan
alsobeusedastrainingdatatotrainapilotdi-
aloguesystem. WhileWizard-of-Ozsystems
are very commonly used, they are not a per-
fect simulation; it is difficult for the wizard
to exactly simulate the errors, limitations, or
time constraints of a real system; results of
wizardstudiesarethussomewhatidealized, butstillcanprovideausefulfirstidea
ofthedomainissues.
3. Iterativelytestthedesignonusers: Aniterativedesigncyclewithembedded
usertestingisessentialinsystemdesign(Nielsen1992,Coleetal.1997,Yankelovich
etal.1995,Landauer1995). Forexampleinawell-knownincidentindialoguede-
signhistory,anearlydialoguesystemrequiredtheusertopressakeytointerruptthe
system(Stifelmanetal.,1993). Butusertestingshowedusersbargedin,whichled
to a redesign of the system to recognize overlapped speech. The iterative method
is also important for designing prompts that cause the user to respond in norma-
valuesensitive tive ways. It’s also important to incorporate value sensitive design, in which we
design15.6 • DIALOGUESYSTEMDESIGN 325
carefullyconsiderduringthedesignprocessthebenefits,harmsandpossiblestake-
holdersoftheresultingsystem(Friedmanetal.2017,BenderandFriedman2018).
There are a number of good books on conversational interface design (Cohen
etal.2004,Harris2005,Pearl2017,DeibelandEvanhoe2021).
15.6.1 EthicalIssuesinDialogueSystemDesign
Ethicalissueshavebeenkeytohowwethinkaboutdesigningartificialagentssince
wellbeforewehadconversationalagents. MaryShelley(depictedbelow)centered
her novel Frankenstein around the problem of creating agents without considering
ethical and humanistic concerns. One issue is the
safety of users. If users seek information from
conversationalagentsinsafety-criticalsituationslike
askingmedicaladvice,orinemergencysituations,or
whenindicatingtheintentionsofself-harm,incorrect
advice can be dangerous and even life-threatening.
Forexample(Bickmoreetal.,2018)gaveparticipants
medical problems to pose to three commercial di-
alogue systems (Siri, Alexa, Google Assistant) and
asked them to determine an action to take based on
the system responses; many of the proposed actions,
ifactuallytaken,wouldhaveledtoharmordeath.
Asystemcanalsoharmusersbyverballyattackingthem,orcreatingrepresen-
tationalharms(Blodgettetal.,2020)bygeneratingabusiveorharmfulstereotypes
thatdemeanparticulargroupsofpeople. Bothabuseandstereotypescancausepsy-
Tay chological harm to users. Microsoft’s 2016 Tay chatbot, for example, was taken
offline16hoursafteritwentlive,whenitbeganpostingmessageswithracialslurs,
conspiracytheories,andpersonalattacksonitsusers. Tayhadlearnedthesebiases
andactionsfromitstrainingdata,includingfromuserswhoseemedtobepurposely
teachingthesystemtorepeatthiskindoflanguage(NeffandNagy2016). Hender-
sonetal.(2017)examineddialoguedatasetsusedtotraincorpus-basedchatbotsand
found toxic and abusive language, especially in social media corpora like Twitter
and Reddit, and indeed such language then appears in the text generated by lan-
guage models and dialogue systems (Gehman et al. 2020; Xu et al. 2020) which
can even amplify the bias from the training data (Dinan et al., 2020). Liu et al.
(2020)developedanothermethodforinvestigatingbias,testinghowneuraldialogue
systemsrespondedtopairsofsimulateduserturnsthatareidenticalexceptformen-
tioningdifferentgendersorrace. Theyfound,forexample,thatsimplechangeslike
usingtheword‘she’insteadof‘he’inasentencecausedsystemstorespondmore
offensivelyandwithmorenegativesentiment.
Anotherimportantethicalissueisprivacy. AlreadyinthefirstdaysofELIZA,
Weizenbaumpointedouttheprivacyimplicationsofpeople’srevelationstothechat-
bot.Theubiquityofin-homedialogueagentsmeanstheymayoftenoverhearprivate
information(Hendersonetal.,2017). Ifachatbotishuman-like,usersarealsomore
likelytodiscloseprivateinformation,andlesslikelytoworryabouttheharmofthis
disclosure(Ischenetal.,2019). Ingeneral,chatbotsthataretrainedontranscriptsof
human-humanorhuman-machineconversationmustanonymizepersonallyidentifi-
ableinformation.
Finally,chatbotsraiseimportantissuesofgenderequalityinadditiontotextual
bias. Currentchatbotsareoverwhelminglygivenfemalenames,likelyperpetuating
the stereotype of a subservient female servant (Paolino, 2017). And when users326 CHAPTER15 • CHATBOTS&DIALOGUESYSTEMS
use sexually harassing language, most commercial chatbots evade or give positive
responsesratherthanrespondinginclearnegativeways(Fessler,2017).
These ethical issues are an important area of investigation, including finding
waystomitigateproblemsofabuseandtoxicity, likedetectingandrespondingap-
propriately to toxic contexts (Wolf et al. 2017, Dinan et al. 2020, Xu et al. 2020).
Valuesensitivedesign, carefullyconsideringpossibleharmsinadvance(Friedman
etal.2017,BenderandFriedman2018)isalsoimportant;(Dinanetal.,2021)give
anumberofsuggestionsforbestpracticesindialoguesystemdesign. Andbecause
dialoguesystemsbydefinitioninvolvehumanparticipants,researchersalsoworkon
IRB these issues with the Institutional Review Boards (IRB) at their institutions, who
helpprotectthesafetyofexperimentalsubjects.
15.7 Summary
Conversationalagentsarecrucialspeechandlanguageprocessingapplicationsthat
arealreadywidelyusedcommercially.
• In human dialogue, speaking is a kind of action; these acts are referred to
as speech acts or dialogue acts. Speakers also attempt to achieve common
groundbyacknowledgingthattheyhaveunderstandeachother.Conversation
alsoischaracterizedbyturnstructureanddialoguestructure.
• Chatbots are conversational agents designed to mimic the appearance of in-
formalhumanconversation. Rule-basedchatbotslikeELIZAanditsmodern
descendantsuserulestomapusersentencesintosystemresponses. Corpus-
basedchatbotsminelogsofhumanconversationtolearntoautomaticallymap
usersentencesintosystemresponses.
• Fortask-baseddialogue, mostcommercialdialoguesystemsusetheGUSor
frame-basedarchitecture,inwhichthedesignerspecifiesframesconsistingof
slotsthatthesystemmustfillbyaskingtheuser.
• The dialogue-state architecture augments the GUS frame-and-slot architec-
turewithricherrepresentationsandmoresophisticatedalgorithmsforkeeping
trackofuser’sdialogueacts,policiesforgeneratingitsowndialogueacts,and
anaturallanguagecomponent.
• Dialoguesystemsareakindofhuman-computerinteraction,andgeneralHCI
principlesapplyintheirdesign,includingtheroleoftheuser,simulationssuch
as Wizard-of-Oz systems, and the importance of iterative design and testing
onrealusers.
Bibliographical and Historical Notes
TheearliestconversationalsystemswerechatbotslikeELIZA(Weizenbaum,1966)
and PARRY (Colby et al., 1971). ELIZA had a widespread influence on popular
perceptionsofartificialintelligence, andbroughtupsomeofthefirstethicalques-
tions in natural language processing —such as the issues of privacy we discussed
aboveaswelltheroleofalgorithmsindecision-making—leadingitscreatorJoseph
WeizenbaumtofightforsocialresponsibilityinAIandcomputerscienceingeneral.BIBLIOGRAPHICALANDHISTORICALNOTES 327
Another early system, the GUS system (Bobrow et al., 1977) had by the late
1970sestablishedthemainframe-basedparadigmthatbecamethedominantindus-
trialparadigmfordialoguesystemsforover30years.
In the 1990s, stochastic models that had first been applied to natural language
processingbegantobeappliedtodialogueslotfilling(Milleretal.1994,Pieraccini
etal.1991).
Byaround2010theGUSarchitecturefinallybegantobewidelyusedcommer-
ciallyindialoguesystemsonphoneslikeApple’sSIRI(Bellegarda,2013)andother
digitalassistants.
Theriseofthewebandonlinechatbotsbroughtnewinterestinchatbotsandgave
risetocorpus-basedchatbotarchitecturesaroundtheturnofthecentury,firstusing
information retrieval models and then in the 2010s, after the rise of deep learning,
withsequence-to-sequencemodels.
Theideathatutterancesinaconversationareakindofactionbeingperformed
bythespeakerwasdueoriginallytothephilosopherWittgenstein(1953)butworked
outmorefullybyAustin(1962)andhisstudentJohnSearle. Varioussetsofspeech
actshavebeendefinedovertheyears,andarichlinguisticandphilosophicallitera-
turedeveloped,especiallyfocusedonexplainingtheuseofindirectspeechacts.
Theideaofdialogueactsdrawsalsofromanumberofothersources,including
the ideas of adjacency pairs, pre-sequences, and other aspects of the international
conversation properties of human conversation developed in the field of conversation analysis
analysis
(seeLevinson(1983)foranintroductiontothefield).
Thisideathatactssetupstronglocaldialogueexpectationswasalsoprefigured
byFirth(1935,p. 70),inafamousquotation:
Mostofthegive-and-takeofconversationinoureverydaylifeisstereotyped
and very narrowly conditioned by our particular type of culture. It is a sort
ofroughlyprescribedsocialritual,inwhichyougenerallysaywhattheother
fellowexpectsyou,onewayortheother,tosay.
Anotherimportantresearchthreadmodeleddialogueasakindofcollaborative
behavior, including the ideas of common ground (Clark and Marshall, 1981), ref-
erence as a collaborative process (Clark and Wilkes-Gibbs, 1986), joint intention
(Levesqueetal.,1990),andsharedplans(GroszandSidner,1980).
The dialogue-state model was also strongly informed by analytic work on the
linguistic properties of dialogue acts and on methods for their detection (Sag and
Liberman1975,HinkelmanandAllen1989,NagataandMorimoto1994,Goodwin
1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke et al. 2000, Gravano et al.
2012).
Twoimportantlinesofresearchthatwewereunabletocoverinthechapterfo-
cused on the computational properties of conversational structure. One line, first
suggestedbyBruce(1975),suggestedthatsincespeechactsareactions,theyshould
beplannedlikeotheractions,anddrewontheAIplanningliterature(FikesandNils-
son,1971).Anagentseekingtofindoutsomeinformationcancomeupwiththeplan
ofaskingtheinterlocutorfortheinformation. Anagenthearinganutterancecanin-
terpretaspeechactbyrunningtheplanner“inreverse”,usinginferencerulestoinfer
fromwhattheinterlocutorsaidwhattheplanmighthavebeen. Plan-basedmodels
BDI ofdialoguearereferredtoasBDImodelsbecausesuchplannersmodelthebeliefs,
desires,andintentions(BDI)oftheagentandinterlocutor. BDImodelsofdialogue
werefirstintroducedbyAllen,Cohen,Perrault,andtheircolleaguesinanumberof
influentialpapersshowinghowspeechactscouldbegenerated(CohenandPerrault,
1979) and interpreted (Perrault and Allen 1980, Allen and Perrault 1980). At the328 CHAPTER15 • CHATBOTS&DIALOGUESYSTEMS
sametime,Wilensky(1983)introducedplan-basedmodelsofunderstandingaspart
ofthetaskofinterpretingstories.
Anotherinfluentiallineofresearchfocusedonmodelingthehierarchicalstruc-
ture of dialogue. Grosz’s pioneering 1977b dissertation first showed that “task-
oriented dialogues have a structure that closely parallels the structure of the task
beingperformed”(p. 27),leadingtoherworkwithSidnerandothersshowinghow
to use similar notions of intention and plans to model discourse structure and co-
herenceindialogue. See,e.g.,Lochbaumetal.(2000)forasummaryoftheroleof
intentionalstructureindialogue.
TheideaofapplyingreinforcementlearningtodialoguefirstcameoutofAT&T
and Bell Laboratories around the turn of the century with work on MDP dialogue
systems(Walker2000,Levinetal.2000,Singhetal.2002)alongwithworkoncue
phrases,prosody,andrejectionandconfirmation. Reinforcementlearningresearch
turnedquicklytothemoresophisticatedPOMDPmodels(Royetal.2000,Lemon
et al. 2006, Williams and Young 2007) applied to small slot-filling dialogue tasks.
Neuralreinforcementlearningmodelshavebeenusedbothforchatbotsystems,for
example simulating dialogues between two virtual agents, rewarding good conver-
sationalpropertieslikecoherenceandeaseofanswering(Lietal.,2016c), andfor
task-orienteddialogue(Williamsetal.,2017).
Dialoguestatetrackingquicklybecameanimportantproblemfortask-oriented
dialogue, andthereisaninfluentialbyanannualevaluationofstate-trackingalgo-
rithms(Williamsetal.,2016). Otherimportantdialogueareasincludethestudyof
affectindialogue(Rashkinetal.2019,Linetal.2019). SeeGaoetal.(2019)fora
survey of modern dialogue system architectures, McTear (2020) on conversational
AIingeneral,andDeibelandEvanhoe(2021)onconversationdesign.
Exercises
15.1 Writeafinite-stateautomatonforadialoguemanagerforcheckingyourbank
balanceandwithdrawingmoneyatanautomatedtellermachine.
dispreferred 15.2 Adispreferredresponseisaresponsethathasthepotentialtomakeaperson
response
uncomfortable or embarrassed in the conversational context; the most com-
monexampledispreferredresponsesisturningdownarequest. Peoplesignal
theirdiscomfortwithhavingtosaynowithsurfacecues(likethewordwell),
or via significant silence. Try to notice the next time you or someone else
uttersadispreferredresponse,andwritedowntheutterance. Whataresome
other cues in the response that a system might use to detect a dispreferred
response? Considernon-verbalcueslikeeyegazeandbodygestures.
15.3 Whenaskedaquestiontowhichtheyaren’tsuretheyknowtheanswer,peo-
ple display their lack of confidence by cues that resemble other dispreferred
responses. Try to notice some unsure answers to questions. What are some
ofthecues? Ifyouhavetroubledoingthis,readSmithandClark(1993)and
listenspecificallyforthecuestheymention.
15.4 Implement a small air-travel help system based on text input. Your system
should get constraints from users about a particular flight that they want to
take, expressedinnaturallanguage, anddisplaypossibleflightsonascreen.
Makesimplifyingassumptions. Youmaybuildinasimpleflightdatabaseor
youmayuseaflightinformationsystemontheWebasyourbackend.CHAPTER
16 Automatic Speech Recognition
and Text-to-Speech
IKNOWnotwhether
Iseeyourmeaning: ifIdo,itlies
Uponthewordywaveletsofyourvoice,
Dimasaneveningshadowinabrook,
ThomasLovellBeddoes,1851
Understandingspokenlanguage,oratleasttranscribingthewordsintowriting,is
oneoftheearliestgoalsofcomputerlanguageprocessing.Infact,speechprocessing
predatesthecomputerbymanydecades!
Thefirstmachinethatrecognizedspeech
was a toy from the 1920s. “Radio Rex”,
shown to the right, was a celluloid dog
that moved (by means of a spring) when
thespringwasreleasedby500Hzacous-
tic energy. Since 500 Hz is roughly the
first formant of the vowel [eh] in “Rex”,
Rexseemedtocomewhenhewascalled
(David,Jr. andSelfridge,1962).
Inmoderntimes, weexpectmoreofourautomaticsystems. Thetaskofauto-
ASR maticspeechrecognition(ASR)istomapanywaveformlikethis:
totheappropriatestringofwords:
It’s time for lunch!
Automatictranscriptionofspeechbyanyspeakerinanyenvironmentisstillfarfrom
solved,butASRtechnologyhasmaturedtothepointwhereitisnowviableformany
practicaltasks.Speechisanaturalinterfaceforcommunicatingwithsmarthomeap-
pliances,personalassistants,orcellphones,wherekeyboardsarelessconvenient,in
telephony applications like call-routing (“Accounting, please”) or in sophisticated
dialogueapplications(“I’dliketochangethereturndateofmyflight”). ASRisalso
useful for general transcription, for example for automatically generating captions
foraudioorvideotext(transcribingmoviesorvideosorlivediscussions).Transcrip-
tionisimportantinfieldslikelawwheredictationplaysanimportantrole. Finally,
ASRisimportantaspartofaugmentativecommunication(interactionbetweencom-
putersandhumanswithsomedisabilityresultingindifficultiesorinabilitiesintyp-
ingoraudition). TheblindMiltonfamouslydictatedParadiseLosttohisdaughters,
andHenryJamesdictatedhislaternovelsafterarepetitivestressinjury.
Whatabouttheoppositeproblem,goingfromtexttospeech? Thisisaproblem
with an even longer history. In Vienna in 1769, Wolfgang von Kempelen built for330 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH
theEmpressMariaTheresathefamousMechanicalTurk,achess-playingautomaton
consisting of a wooden box filled with gears, behind which sat a robot mannequin
whoplayedchessbymovingpieceswithhismechanicalarm. TheTurktouredEu-
ropeandtheAmericasfordecades,defeatingNapoleonBonaparteandevenplaying
CharlesBabbage. TheMechanicalTurkmighthavebeenoneoftheearlysuccesses
ofartificialintelligencewereitnotforthefactthatitwas,alas,ahoax,poweredby
ahumanchessplayerhiddeninsidethebox.
What is less well known is that von Kempelen, an extraordinarily
prolific inventor, also built between
1769 and 1790 what was definitely
not a hoax: the first full-sentence
speech synthesizer, shown partially to
the right. His device consisted of a
bellows to simulate the lungs, a rub-
bermouthpieceandanoseaperture,a
reed to simulate the vocal folds, var-
ious whistles for the fricatives, and a
small auxiliary bellows to provide the puff of air for plosives. By moving levers
withbothhandstoopenandcloseapertures,andadjustingtheflexibleleather“vo-
caltract”,anoperatorcouldproducedifferentconsonantsandvowels.
Morethantwocenturieslater,wenolongerbuildoursynthesizersoutofwood
speech andleather,nordoweneedhumanoperators.Themoderntaskofspeechsynthesis,
synthesis
text-to-speech alsocalledtext-to-speechorTTS,isexactlythereverseofASR;tomaptext:
TTS It’s time for lunch!
toanacousticwaveform:
Modern speech synthesis has a wide variety of applications. TTS is used in
conversational agents that conduct dialogues with people, plays a role in devices
thatreadoutloudfortheblindoringames, andcanbeusedtospeakforsufferers
ofneurologicaldisorders,suchasthelateastrophysicistStevenHawkingwho,after
helosttheuseofhisvoicebecauseofALS,spokebymanipulatingaTTSsystem.
In the next sections we’ll show how to do ASR with encoder-decoders, intro-
ducetheCTClossfunctions,thestandardworderrorrateevaluationmetric,and
describehowacousticfeaturesareextracted. We’llthenseehowTTScanbemod-
eledwithalmostthesamealgorithminreverse, andconcludewithabriefmention
ofotherspeechtasks.
16.1 The Automatic Speech Recognition Task
Before describing algorithms for ASR, let’s talk about how the task itself varies.
Onedimensionofvariationisvocabularysize. SomeASRtaskscanbesolvedwith
extremely high accuracy, like those with a 2-word vocabulary (yes versus no) or
digit an 11 word vocabulary like digit recognition (recognizing sequences of digits in-
recognition
cludingzerotonineplusoh). Open-endedtasksliketranscribingvideosorhuman
conversations,withlargevocabulariesofupto60,000words,aremuchharder.16.1 • THEAUTOMATICSPEECHRECOGNITIONTASK 331
Aseconddimensionofvariationiswhothespeakeristalkingto.Humansspeak-
ingtomachines(eitherdictatingortalkingtoadialoguesystem)areeasiertorecog-
readspeech nizethanhumansspeakingtohumans. Readspeech,inwhichhumansarereading
out loud, for example in audio books, is also relatively easy to recognize. Recog-
conversational nizing the speech of two humans talking to each other in conversational speech,
speech
forexample,fortranscribingabusinessmeeting,isthehardest. Itseemsthatwhen
humans talk to machines, or read without an audience present, they simplify their
speechquiteabit,talkingmoreslowlyandmoreclearly.
Athirddimensionofvariationischannelandnoise.Speechiseasiertorecognize
ifit’srecordedinaquietroomwithhead-mountedmicrophonesthanifit’srecorded
byadistantmicrophoneonanoisycitystreet,orinacarwiththewindowopen.
Afinaldimensionofvariationisaccentorspeaker-classcharacteristics. Speech
iseasiertorecognizeifthespeakerisspeakingthesamedialectorvarietythatthe
systemwastrainedon. Speechbyspeakersofregionalorethnicdialects,orspeech
bychildrencanbequitedifficulttorecognizeifthesystemisonlytrainedonspeak-
ersofstandarddialects,oronlyadultspeakers.
Anumberofpubliclyavailablecorporawithhuman-createdtranscriptsareused
to create ASR test and training sets to explore this variation; we mention a few of
LibriSpeech them here since you will encounter them in the literature. LibriSpeech is a large
open-sourceread-speech16kHzdatasetwithover1000hoursofaudiobooksfrom
theLibriVoxproject,withtranscriptsalignedatthesentencelevel(Panayotovetal.,
2015). It is divided into an easier (“clean”) and a more difficult portion (“other”)
with the clean portion of higher recording quality and with accents closer to US
English.Thiswasdonebyrunningaspeechrecognizer(trainedonreadspeechfrom
theWallStreetJournal)onalltheaudio,computingtheWERforeachspeakerbased
on the gold transcripts, and dividing the speakers roughly in half, with recordings
fromlower-WERspeakerscalled“clean”andrecordingsfromhigher-WERspeakers
“other”.
Switchboard TheSwitchboardcorpusofpromptedtelephoneconversationsbetweenstrangers
was collected in the early 1990s; it contains 2430 conversations averaging 6 min-
uteseach, totaling240hoursof8kHzspeechandabout3millionwords(Godfrey
et al., 1992). Switchboard has the singular advantage of an enormous amount of
auxiliaryhand-donelinguisticlabeling,includingparses,dialogueacttags,phonetic
CALLHOME andprosodiclabeling,anddiscourseandinformationstructure. TheCALLHOME
corpus was collected in the late 1990s and consists of 120 unscripted 30-minute
telephoneconversationsbetweennativespeakersofEnglishwhowereusuallyclose
friendsorfamily(Canavanetal.,1997).
TheSantaBarbaraCorpusofSpokenAmericanEnglish(DuBoisetal.,2005)is
alargecorpusofnaturallyoccurringeverydayspokeninteractionsfromalloverthe
United States, mostly face-to-face conversation, but also town-hall meetings, food
preparation,on-the-jobtalk,andclassroomlectures.Thecorpuswasanonymizedby
removingpersonalnamesandotheridentifyinginformation(replacedbypseudonyms
inthetranscripts,andmaskedintheaudio).
CORAAL CORAAL is a collection of over 150 sociolinguistic interviews with African
Americanspeakers,withthegoalofstudyingAfricanAmericanLanguage(AAL),
the many variations of language used in African American communities (Kendall
and Farrington, 2020). The interviews are anonymized with transcripts aligned at
CHiME theutterancelevel. TheCHiMEChallengeisaseriesofdifficultsharedtaskswith
corporathatdealwithrobustnessinASR.TheCHiME5task,forexample,isASRof
conversationalspeechinrealhomeenvironments(specificallydinnerparties). The332 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH
corpus contains recordings of twenty different dinner parties in real homes, each
with four participants, and in three locations (kitchen, dining area, living room),
recorded both with distant room microphones and with body-worn mikes. The
HKUST HKUST Mandarin Telephone Speech corpus has 1206 ten-minute telephone con-
versationsbetweenspeakersofMandarinacrossChina,includingtranscriptsofthe
conversations,whicharebetweeneitherfriendsorstrangers(Liuetal.,2006). The
AISHELL-1 AISHELL-1corpuscontains170hoursofMandarinreadspeechofsentencestaken
from various domains, read by different speakers mainly from northern China (Bu
etal.,2017).
Figure16.1showstheroughpercentageofincorrectwords(theworderrorrate,
orWER,definedonpage343)fromstate-of-the-artsystemsonsomeofthesetasks.
Note that the error rate on read speech (like the LibriSpeech audiobook corpus) is
around2%;thisisasolvedtask,althoughthesenumberscomefromsystemsthatre-
quireenormouscomputationalresources. Bycontrast,theerrorratefortranscribing
conversationsbetweenhumansismuchhigher;5.8to11%fortheSwitchboardand
CALLHOME corpora. The error rate is higher yet again for speakers of varieties
likeAfricanAmericanVernacularEnglish,andyetagainfordifficultconversational
tasksliketranscriptionof4-speakerdinnerpartyspeech,whichcanhaveerrorrates
as high as 81.3%. Character error rates (CER) are also much lower for read Man-
darinspeechthanfornaturalconversation.
EnglishTasks WER%
LibriSpeechaudiobooks960hourclean 1.4
LibriSpeechaudiobooks960hourother 2.6
Switchboardtelephoneconversationsbetweenstrangers 5.8
CALLHOMEtelephoneconversationsbetweenfamily 11.0
Sociolinguisticinterviews,CORAAL(AAL) 27.0
CHiMe5dinnerpartieswithbody-wornmicrophones 47.9
CHiMe5dinnerpartieswithdistantmicrophones 81.3
Chinese(Mandarin)Tasks CER%
AISHELL-1Mandarinreadspeechcorpus 6.7
HKUSTMandarinChinesetelephoneconversations 23.5
Figure16.1 RoughWordErrorRates(WER=%ofwordsmisrecognized)reportedaround
2020forASRonvariousAmericanEnglishrecognitiontasks,andcharactererrorrates(CER)
fortwoChineserecognitiontasks.
16.2 Feature Extraction for ASR: Log Mel Spectrum
ThefirststepinASRistotransformtheinputwaveformintoasequenceofacoustic
featurevector feature vectors, each vector representing the information in a small time window
of the signal. Let’s see how to convert a raw wavefile to the most commonly used
features,sequencesoflogmelspectrumvectors.Aspeechsignalprocessingcourse
isrecommendedformoredetails.
16.2.1 SamplingandQuantization
RecallfromSection28.4.2thatthefirststepistoconverttheanalogrepresentations
(firstairpressureandthenanalogelectricsignalsinamicrophone)intoadigitalsig-16.2 • FEATUREEXTRACTIONFORASR:LOGMELSPECTRUM 333
sampling nal. Thisanalog-to-digitalconversionhastwosteps: samplingandquantization.
A signal is sampled by measuring its amplitude at a particular time; the sampling
samplingrate rateisthenumberofsamplestakenpersecond. Toaccuratelymeasureawave,we
must have at least two samples in each cycle: one measuring the positive part of
thewaveandonemeasuringthenegativepart. Morethantwosamplespercyclein-
creasestheamplitudeaccuracy,butlessthantwosampleswillcausethefrequency
of the wave to be completely missed. Thus, the maximum frequency wave that
canbemeasuredisonewhosefrequencyishalfthesamplerate(sinceeverycycle
needs two samples). This maximum frequency for a given sampling rate is called
Nyquist theNyquistfrequency. Mostinformationinhumanspeechisinfrequenciesbelow
frequency
10,000Hz,soa20,000Hzsamplingratewouldbenecessaryforcompleteaccuracy.
Buttelephonespeechisfilteredbytheswitchingnetwork,andonlyfrequenciesless
than 4,000 Hz are transmitted by telephones. Thus, an 8,000 Hz sampling rate is
telephone- sufficientfortelephone-bandwidthspeech,and16,000Hzformicrophonespeech.
bandwidth
AlthoughusinghighersamplingratesproduceshigherASRaccuracy,wecan’t
combine different sampling rates for training and testing ASR systems. Thus if
wearetestingonatelephonecorpuslikeSwitchboard(8KHzsampling),wemust
downsample our training corpus to 8 KHz. Similarly, if we are training on mul-
tiple corpora and one of them includes telephone speech, we downsample all the
widebandcorporato8Khz.
Amplitudemeasurementsarestoredasintegers,either8bit(valuesfrom-128–
127)or16bit(valuesfrom-32768–32767).Thisprocessofrepresentingreal-valued
quantization numbersasintegersiscalledquantization; allvaluesthatareclosertogetherthan
theminimumgranularity(thequantumsize)arerepresentedidentically. Wereferto
eachsampleattimeindexninthedigitized,quantizedwaveformasx[n].
16.2.2 Windowing
From the digitized, quantized representation of the waveform, we need to extract
spectral features from a small window of speech that characterizes part of a par-
ticular phoneme. Inside this small window, we can roughly think of the signal as
stationary stationary (that is, its statistical properties are constant within this region). (By
non-stationary contrast, in general, speech is a non-stationary signal, meaning that its statistical
propertiesarenotconstantovertime). Weextractthisroughlystationaryportionof
speechbyusingawindowwhichisnon-zeroinsidearegionandzeroelsewhere,run-
ningthiswindowacrossthespeechsignalandmultiplyingitbytheinputwaveform
toproduceawindowedwaveform.
frame The speech extracted from each window is called a frame. The windowing is
characterized by three parameters: the window size or frame size of the window
stride (its width in milliseconds), the frame stride, (also called shift or offset) between
successivewindows,andtheshapeofthewindow.
To extract the signal we multiply the value of the signal at time n, s[n] by the
valueofthewindowattimen,w[n]:
y[n]=w[n]s[n] (16.1)
rectangular The window shape sketched in Fig. 16.2 is rectangular; you can see the ex-
tractedwindowedsignallooksjustliketheoriginalsignal. Therectangularwindow,
however,abruptlycutsoffthesignalatitsboundaries,whichcreatesproblemswhen
wedoFourieranalysis. Forthisreason,foracousticfeaturecreationwemorecom-
Hamming monly use the Hamming window, which shrinks the values of the signal toward334 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH
Window
25 ms
Shift
Window
10
ms 25 ms
Shift
Window
10
ms 25 ms
Figure16.2 Windowing,showinga25msrectangularwindowwitha10msstride.
zero at the window boundaries, avoiding discontinuities. Figure 16.3 shows both;
theequationsareasfollows(assumingawindowthatisLframeslong):
1 0 n L 1
rectangular w[n] = ≤ ≤ − (16.2)
0 otherwise
(cid:26)
0.54 0.46cos(2πn) 0 n L 1
Hamming w[n] = − L ≤ ≤ − (16.3)
0 otherwise
(cid:26)
0.4999
0
–0.5
0 0.0475896
Time (s)
Rectangular window Hamming window
0.4999 0.4999
0 0
–0.5 –0.4826
0.00455938 0.0256563 0.00455938 0.0256563
Time (s) Time (s)
Figure16.3 WindowingasinewavewiththerectangularorHammingwindows.
16.2.3 DiscreteFourierTransform
Thenextstepistoextractspectralinformationforourwindowedsignal;weneedto
know how much energy the signal contains at different frequency bands. The tool
for extracting spectral information for discrete frequency bands for a discrete-time
Discrete
Fourier (sampled)signalisthediscreteFouriertransformorDFT.
transform
DFT16.2 • FEATUREEXTRACTIONFORASR:LOGMELSPECTRUM 335
TheinputtotheDFTisawindowedsignalx[n]...x[m],andtheoutput,foreachof
N discretefrequencybands, isacomplexnumberX[k]representingthemagnitude
and phase of that frequency component in the original signal. If we plot the mag-
nitude against the frequency, we can visualize the spectrum that we introduced in
Chapter28. Forexample,Fig.16.4showsa25msHamming-windowedportionof
asignalanditsspectrumascomputedbyaDFT(withsomeadditionalsmoothing).
0.04414
20
0 0
–20
–0.04121
0.0141752 0.039295 0 8000
Time (s) Frequency (Hz)
(a) (b)
Figure16.4 (a) A 25 ms Hamming-windowed portion of a signal from the vowel [iy]
and (b)itsspectrumcomputedbyaDFT.
We do not introduce the mathematical details of the DFT here, except to note
Euler’sformula thatFourieranalysisreliesonEuler’sformula,with jastheimaginaryunit:
ejθ =cosθ+jsinθ (16.4)
Asabriefreminderforthosestudentswhohavealreadystudiedsignalprocessing,
theDFTisdefinedasfollows:
N 1
X[k]=
− x[n]e−j2 Nπkn
(16.5)
n=0
(cid:88)
fastFourier AcommonlyusedalgorithmforcomputingtheDFTisthefastFouriertransform
transform
FFT orFFT.ThisimplementationoftheDFTisveryefficientbutonlyworksforvalues
ofN thatarepowersof2.
16.2.4 MelFilterBankandLog
The results of the FFT tell us the energy at each frequency band. Human hearing,
however,isnotequallysensitiveatallfrequencybands;itislesssensitiveathigher
frequencies. Thisbiastowardlowfrequencieshelpshumanrecognition,sinceinfor-
mationinlowfrequencieslikeformantsiscrucialfordistinguishingvaluesornasals,
whileinformationinhighfrequencieslikestopburstsorfricativenoiseislesscru-
cialforsuccessfulrecognition. Modelingthishumanperceptualpropertyimproves
speechrecognitionperformanceinthesameway.
Weimplementthisintuitionbycollectingenergies,notequallyateachfrequency
band, but according to the mel scale, an auditory frequency scale (Chapter 28). A
mel mel (Stevens et al. 1937, Stevens and Volkmann 1940) is a unit of pitch. Pairs of
sounds that are perceptually equidistant in pitch are separated by an equal number
ofmels. Themelfrequencymcanbecomputedfromtherawacousticfrequencyby
alogtransformation:
f
mel(f)=1127ln(1+ ) (16.6)
700
)zH/Bd(
level
erusserp
dnuoS336 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH
We implement this intuition by creating a bank of filters that collect energy from
each frequency band, spread logarithmically so that we have very fine resolution
at low frequencies, and less resolution at high frequencies. Figure 16.5 shows a
samplebankoftriangularfiltersthatimplementthisidea,thatcanbemultipliedby
thespectrumtogetamelspectrum.
1
0.5
0
0 87K700
Frequency (Hz)
mel spectrum m 1 m 2 ... m M
Figure16.5 The mel filter bank (Davis and Mermelstein, 1980). Each triangular filter,
spacedlogarithmicallyalongthemelscale,collectsenergyfromagivenfrequencyrange.
Finally,wetakethelogofeachofthemelspectrumvalues.Thehumanresponse
to signal level is logarithmic (like the human response to frequency). Humans are
lesssensitivetoslightdifferencesinamplitudeathighamplitudesthanatlowampli-
tudes.Inaddition,usingalogmakesthefeatureestimateslesssensitivetovariations
ininputsuchaspowervariationsduetothespeaker’smouthmovingcloserorfurther
fromthemicrophone.
16.3 Speech Recognition Architecture
The basic architecture for ASR is the encoder-decoder (implemented with either
RNNsorTransformers),exactlythesamearchitectureintroducedforMTinChap-
ter13.Generallywestartfromthelogmelspectralfeaturesdescribedintheprevious
section,andmaptoletters,althoughit’salsopossibletomaptoinducedmorpheme-
likechunkslikewordpiecesorBPE.
Fig. 16.6 sketches the standard encoder-decoder architecture, which is com-
AED monlyreferredtoastheattention-basedencoderdecoderorAED,orlistenattend
listenattend and spell (LAS) after the two papers which first applied it to speech (Chorowski
andspell
etal.2014,Chanetal.2016). Theinputisasequenceoft acousticfeaturevectors
F = f ,f ,...,f , one vector per 10 ms frame. The output can be letters or word-
1 2 t
pieces;we’llassumelettershere.ThustheoutputsequenceY=( SOS ,y ,...,y EOS ),
1 m
(cid:104) (cid:105) (cid:104) (cid:105)
assumingspecialstartofsequenceandendofsequencetokens sos and eos and
(cid:104) (cid:105) (cid:104) (cid:105)
eachy isacharacter;forEnglishwemightchoosetheset:
i
y a,b,c,...,z,0,...,9, space , comma , period , apostrophe , unk
i
∈{ (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)}
Ofcoursetheencoder-decoderarchitectureisparticularlyappropriatewhenin-
putandoutputsequenceshavestarklengthdifferences,astheydoforspeech,with
verylongacousticfeaturesequencesmappingtomuchshortersequencesofletters
or words. A single word might be 5 letters long but, supposing it lasts about 2
seconds,wouldtake200acousticframes(of10mseach).
Because this length difference is so extreme for speech, encoder-decoder ar-
chitectures for speech need to have a special compression stage that shortens the
acousticfeaturesequencebeforetheencoderstage. (Alternatively,wecanusealoss
edutilpmA16.3 • SPEECHRECOGNITIONARCHITECTURE 337
y 1 y 2 y 3 y 4 y 5 y 6 y 7 y 8 y 9 y m
i t ‘ s t i m e …
DECODER
H
ENCODER
<s> i t ‘ s t i m …
Shorter sequence X x 1 … x n
Subsampling
80-dimensional
log Mel spectrum f 1 … f t
per frame
Feature Computation
Figure16.6 Schematicarchitectureforanencoder-decoderspeechrecognizer.
functionthatisdesignedtodealwellwithcompression,liketheCTClossfunction
we’llintroduceinthenextsection.)
ThegoalofthesubsamplingistoproduceashortersequenceX =x ,...,x that
1 n
will be the input to the encoder. The simplest algorithm is a method sometimes
lowframerate calledlowframerate(PundakandSainath,2016):fortimeiwestack(concatenate)
theacousticfeaturevector f withthepriortwovectors f and f tomakeanew
i i 1 i 2
− −
vector three times longer. Then we simply delete f and f . Thus instead of
i 1 i 2
− −
(say)a40-dimensionalacousticfeaturevectorevery10ms,wehavealongervector
(say120-dimensional)every30ms,withashortersequencelengthn= t.1
3
After this compression stage, encoder-decoders for speech use the same archi-
tectureasforMTorothertext,composedofeitherRNNs(LSTMs)orTransformers.
Forinference,theprobabilityoftheoutputstringY isdecomposedas:
n
p(y 1,...,y n)= p(y i y 1,...,y i 1,X) (16.7)
| −
i=1
(cid:89)
Wecanproduceeachletteroftheoutputviagreedydecoding:
yˆ i = argmax char AlphabetP(chary 1...y i 1,X) (16.8)
∈ | −
Alternativelywecanusebeamsearchasdescribedinthenextsection. Thisispar-
ticularlyrelevantwhenweareaddingalanguagemodel.
Addingalanguagemodel Sinceanencoder-decodermodelisessentiallyacon-
ditionallanguagemodel,encoder-decodersimplicitlylearnalanguagemodelforthe
outputdomainoflettersfromtheirtrainingdata. However,thetrainingdata(speech
paired with text transcriptions) may not include sufficient text to train a good lan-
guage model. After all, it’s easier to find enormous amounts of pure text training
data than it is to find text paired with speech. Thus we can can usually improve a
modelatleastslightlybyincorporatingaverylargelanguagemodel.
The simplest way to do this is to use beam search to get a final beam of hy-
n-bestlist pothesized sentences; this beam is sometimes called an n-best list. We then use a
rescore languagemodeltorescoreeachhypothesisonthebeam. Thescoringisdonebyin-
1 Therearealsomorecomplexalternativesforsubsampling,likeusingaconvolutionalnetthatdown-
sampleswithmaxpooling,orlayersofpyramidalRNNs,RNNswhereeachsuccessivelayerhashalf
thenumberofRNNsasthepreviouslayer.338 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH
terpolatingthescoreassignedbythelanguagemodelwiththeencoder-decoderscore
usedtocreatethebeam,withaweightλ tunedonaheld-outset. Also,sincemost
modelsprefershortersentences,ASRsystemsnormallyhavesomewayofaddinga
length factor. One way to do this is to normalize the probability by the number of
characters in the hypothesis Y . The following is thus a typical scoring function
c
| |
(Chanetal.,2016):
1
score(Y X)= logP(Y X)+λlogP LM(Y) (16.9)
| Y |
c
| |
16.3.1 Learning
Encoder-decodersforspeecharetrainedwiththenormalcross-entropylossgener-
allyusedforconditionallanguagemodels. Attimestepiofdecoding,thelossisthe
logprobabilityofthecorrecttoken(letter)y:
i
L CE = logp(y i y 1,...,y i 1,X) (16.10)
− | −
Thelossfortheentiresentenceisthesumoftheselosses:
m
L CE = logp(y i y 1,...,y i 1,X) (16.11)
− | −
i=1
(cid:88)
This loss is then backpropagated through the entire end-to-end model to train the
entireencoder-decoder.
As we described in Chapter 13, we normally use teacher forcing, in which the
decoder history is forced to be the correct gold y rather than the predicted yˆ. It’s
i i
also possible to use a mixture of the gold and decoder output, for example using
the gold output 90% of the time, but with probability .1 taking the decoder output
instead:
L CE = logp(y i y 1,...,yˆ i 1,X) (16.12)
− | −
16.4 CTC
We pointed out in the previous section that speech recognition has two particular
propertiesthatmakeitveryappropriatefortheencoder-decoderarchitecture,where
the encoder produces an encoding of the input that the decoder uses attention to
explore. First,inspeechwehaveaverylongacousticinputsequenceX mappingto
a much shorter sequence of lettersY, and second, it’s hard to know exactly which
partofX mapstowhichpartofY.
In this section we briefly introduce an alternative to encoder-decoder: an algo-
CTC rithmandlossfunctioncalledCTC,shortforConnectionistTemporalClassifica-
tion(Gravesetal.,2006),thatdealswiththeseproblemsinaverydifferentway.The
intuitionofCTCistooutputasinglecharacterforeveryframeoftheinput,sothat
the output is the same length as the input, and then to apply a collapsing function
thatcombinessequencesofidenticalletters,resultinginashortersequence.
Let’s imagine inference on someone saying the word dinner, and let’s suppose
wehadafunctionthatchoosesthemostprobableletterforeachinputspectralframe
representation x. We’ll call the sequence of letters corresponding to each input
i16.4 • CTC 339
alignment frameanalignment,becauseittellsuswhereintheacousticsignaleachletteraligns
to. Fig. 16.7 shows one such alignment, and what happens if we use a collapsing
functionthatjustremovesconsecutiveduplicateletters.
Y (output) d i n e r
A (alignment) d i i n n n n e r r r r r r
X (input) x 1 x 2 x 3 x 4 x 5 x 6 x 7 x 8 x 9 x 10x 11x 12x 13x 14
wavefile
Figure16.7 Anaivealgorithmforcollapsinganalignmentbetweeninputandletters.
Well,thatdoesn’twork;ournaivealgorithmhastranscribedthespeechasdiner,
notdinner! Collapsingdoesn’thandledoubleletters. There’salsoanotherproblem
with our naive function; it doesn’t tell us what symbol to align with silence in the
input. Wedon’twanttobetranscribingsilenceasrandomletters!
TheCTCalgorithmsolvesbothproblemsbyaddingtothetranscriptionalphabet
blank aspecialsymbolforablank,whichwe’llrepresentas . Theblankcanbeusedin
thealignmentwheneverwedon’twanttotranscribealetter. Blankcanalsobeused
between letters; since our collapsing function collapses only consecutive duplicate
letters,itwon’tcollapseacross . Moreformally,let’sdefinethemappingB:a y
→
between an alignment a and an output y, which collapses all repeated letters and
thenremovesallblanks. Fig.16.8sketchesthiscollapsingfunctionB.
Y (output) d i n n e r
remove blanks d i n n e r
␣ ␣ ␣
merge duplicates d i n n e r
␣ ␣ ␣ ␣
A (alignment) d i n n n e r r r r
X (input) x 1 x 2 x 3 x 4 x 5 x 6 x 7 x 8 x 9 x 10x 11x 12x 13x 14
Figure16.8 TheCTCcollapsingfunctionB,showingthespaceblankcharacter ;repeated
(consecutive)charactersinanalignmentAareremovedtoformtheoutputY.
TheCTCcollapsingfunctionismany-to-one;lotsofdifferentalignmentsmapto
thesameoutputstring.Forexample,thealignmentshowninFig.16.8isnottheonly
alignment that results in the string dinner. Fig. 16.9 shows some other alignments
thatwouldproducethesameoutput.
␣ ␣
d i i n n n e e e r r r
␣ ␣ ␣ ␣ ␣
d d i n n n e r r
␣ ␣ ␣ ␣
d d d i n n n e r r
Figure16.9 Threeotherlegitimatealignmentsproducingthetranscriptdinner.
It’susefultothinkofthesetofallalignmentsthatmightproducethesameoutput340 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH
Y. We’ll use the inverse of our B function, called B 1, and represent that set as
−
B 1(Y).
−
16.4.1 CTCInference
BeforeweseehowtocomputeP (Y X)let’sfirstseehowCTCassignsaproba-
CTC
bilitytooneparticularalignmentAˆ= a| ˆ ,...,aˆ . CTCmakesastrongconditional
1 n
{ }
independenceassumption:itassumesthat,giventheinputX,theCTCmodeloutput
a attimet isindependentoftheoutputlabelsatanyothertimea. Thus:
t i
T
P CTC(AX) = p(a t X) (16.13)
| |
t=1
(cid:89)
ThustofindthebestalignmentAˆ= aˆ ,...,aˆ wecangreedilychoosethecharac-
1 T
{ }
terwiththemaxprobabilityateachtimestept:
aˆ t =argmaxp t(cX) (16.14)
|
c C
∈
WethenpasstheresultingsequenceAtotheCTCcollapsingfunctionBtogetthe
outputsequenceY.
Let’s talkabout howthis simple inferencealgorithm forfinding thebest align-
ment A would be implemented. Because we are making a decision at each time
point, we can treat CTC as a sequence-modeling task, where we output one letter
yˆ attimet correspondingtoeachinputtokenx,eliminatingtheneedforafullde-
t t
coder. Fig. 16.10 sketches this architecture, where we take an encoder, produce a
hiddenstateh ateachtimestep,anddecodebytakingasoftmaxoverthecharacter
t
vocabularyateachtimestep.
output letter y y y y y … y
1 2 3 4 5 n
sequence Y i i i t t …
Classifier …
+softmax
ENCODER
Shorter input
sequence X x 1 … x n
Subsampling
log Mel spectrum f … f
1 t
Feature Computation
Figure16.10 InferencewithCTC:usinganencoder-onlymodel, withdecodingdoneby
simplesoftmaxesoverthehiddenstateht ateachoutputstep.
Alas,thereisapotentialflawwiththeinferencealgorithmsketchedin(Eq.16.14)
and Fig. 16.9. The problem is that we chose the most likely alignment A, but the
mostlikelyalignmentmaynotcorrespondtothemostlikelyfinalcollapsedoutput
stringY. That’s because there are many possible alignments that lead to the same
output string, and hence the most likely output string might not correspond to the16.4 • CTC 341
mostprobablealignment. Forexample,imaginethemostprobablealignmentAfor
aninputX =[x x x ]isthestring[ab(cid:15)]butthenexttwomostprobablealignments
1 2 3
are[b(cid:15)b]and[(cid:15)bb]. TheoutputY =[bb], summingoverthosetwoalignments,
mightbemoreprobablethanY =[ab].
For this reason, the most probable output sequence Y is the one that has, not
the single best CTC alignment, but the highest sum over the probability of all its
possiblealignments:
P (Y X) = P(AX)
CTC
| |
A ∈(cid:88)B −1(Y)
T
= p(a h)
t t
|
A ∈(cid:88)B −1(Y) (cid:89)t=1
Yˆ = argmaxP CTC(Y X) (16.15)
|
Y
Alas,summingoverallalignmentsisveryexpensive(therearealotofalignments),
soweapproximatethissumbyusingaversionofViterbibeamsearchthatcleverly
keepsinthebeamthehigh-probabilityalignmentsthatmaptothesameoutputstring,
andsumsthoseasanapproximationof(Eq.16.15). SeeHannun(2017)foraclear
explanationofthisextensionofbeamsearchforCTC.
Because of the strong conditional independence assumption mentioned earlier
(thattheoutputattimet isindependentoftheoutputattimet 1,giventheinput),
−
CTCdoesnotimplicitlylearnalanguagemodeloverthedata(unliketheattention-
based encoder-decoder architectures). It is therefore essential when using CTC to
interpolatealanguagemodel(andsomesortoflengthfactorL(Y))usinginterpola-
tionweightsthataretrainedonadevset:
score CTC(Y X)=logP CTC(Y X)+λ 1logP LM(Y)λ 2L(Y) (16.16)
| |
16.4.2 CTCTraining
TotrainaCTC-basedASRsystem,weusenegativelog-likelihoodlosswithaspecial
CTClossfunction. ThusthelossforanentiredatasetDisthesumofthenegative
log-likelihoodsofthecorrectoutputY foreachinputX:
L CTC= logP CTC(Y X) (16.17)
− |
(X(cid:88),Y) ∈D
TocomputeCTClossfunctionforasingleinputpair(X,Y),weneedtheprobability
oftheoutputY giventheinputX.AswesawinEq.16.15,tocomputetheprobability
of a given output Y we need to sum over all the possible alignments that would
collapsetoY. Inotherwords:
T
P CTC(Y X) = p(a t h t) (16.18)
| |
A ∈(cid:88)B −1(Y) (cid:89)t=1
Naively summing over all possible alignments is not feasible (there are too many
alignments). However, wecanefficientlycomputethesumbyusingdynamicpro-
gramming to merge alignments, with a version of the forward-backward algo-
rithmalsousedtotrainHMMs(AppendixA)andCRFs.Theoriginaldynamicpro-
gramming algorithms for both training and inference are laid out in (Graves et al.,
2006);see(Hannun,2017)foradetailedexplanationofboth.342 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH
16.4.3 CombiningCTCandEncoder-Decoder
It’s also possible to combine the two architectures/loss functions we’ve described,
the cross-entropy loss from the encoder-decoder architecture, and the CTC loss.
Fig.16.11showsasketch. Fortraining,wecansimplyweightthetwolosseswitha
λ tunedonadevset:
L= λlogP encdec(Y X) (1 λ)logP ctc(Y X) (16.19)
− | − − |
For inference, we can combine the two with the language model (or the length
penalty),againwithlearnedweights:
Yˆ =argmax[λlogP encdec(Y X) (1 λ)logP CTC(Y X)+γlogP LM(Y)] (16.20)
| − − |
Y
i t ’ s t i m e …
CTC Loss Encoder-Decoder Loss
…
DECODER
… H
ENCODER
<s> i t ‘ s t i m …
…
x x
1 n
Figure16.11 CombiningtheCTCandencoder-decoderlossfunctions.
16.4.4 StreamingModels: RNN-TforimprovingCTC
Because of the strong independence assumption in CTC (assuming that the output
at time t is independent of the output at time t 1), recognizers based on CTC
−
don’t achieve as high an accuracy as the attention-based encoder-decoder recog-
nizers. CTC recognizers have the advantage, however, that they can be used for
streaming streaming. Streaming means recognizing words on-line rather than waiting until
the end of the sentence to recognize them. Streaming is crucial for many applica-
tions, from commands to dictation, where we want to start recognition while the
userisstilltalking. Algorithmsthatuseattentionneedtocomputethehiddenstate
sequenceovertheentireinputfirstinordertoprovidetheattentiondistributioncon-
text,beforethedecodercanstartdecoding. Bycontrast,aCTCalgorithmcaninput
lettersfromlefttorightimmediately.
If we want to do streaming, we need a way to improve CTC recognition to re-
movetheconditionalindependentassumption,enablingittoknowaboutoutputhis-
RNN-T tory. The RNN-Transducer (RNN-T), shown in Fig. 16.12, is just such a model
(Graves2012, Gravesetal.2013). TheRNN-Thastwomaincomponents: aCTC
acousticmodel,andaseparatelanguagemodelcomponentcalledthepredictorthat
conditionsontheoutputtokenhistory.Ateachtimestept,theCTCencoderoutputs
ahiddenstatehencgiventheinputx ...x. Thelanguagemodelpredictortakesasin-
t 1 t
pred
putthepreviousoutputtoken(notcountingblanks),outputtingahiddenstateh .
u
Thetwoarepassedthroughanothernetworkwhoseoutputisthenpassedthrougha16.5 • ASREVALUATION: WORDERRORRATE 343
softmaxtopredictthenextcharacter.
P (Y X) = P(AX)
RNN T
− | |
A ∈(cid:88)B −1(Y)
T
= p(a h,y )
t
|
t <ut
A ∈(cid:88)B −1(Y) (cid:89)t=1
P ( yt,u | x[1..t] , y[1..u-1] )
SOFTMAX
zt,u
PREDICTION
JOINT NETWORK DECODER
yN u-E 1TWORK hpredu
henct
ENCODER
xt
Figure16.12 TheRNN-Tmodelcomputingtheoutputtokendistributionattimetbyinte-
gratingtheoutputofaCTCacousticencoderandaseparate‘predictor’languagemodel.
16.5 ASR Evaluation: Word Error Rate
worderror The standard evaluation metric for speech recognition systems is the word error
rate. The word error rate is based on how much the word string returned by the
recognizer (the hypothesized word string) differs from a reference transcription.
Thefirststepincomputingworderroristocomputetheminimumeditdistancein
words between the hypothesized and correct strings, giving us the minimum num-
ber of word substitutions, word insertions, and word deletions necessary to map
between the correct and hypothesized strings. The word error rate (WER) is then
definedasfollows(notethatbecausetheequationincludesinsertions,theerrorrate
canbegreaterthan100%):
Insertions+Substitutions+Deletions
WordErrorRate = 100
× TotalWordsinCorrectTranscript
alignment Hereisasamplealignmentbetweenareferenceandahypothesisutterancefrom
theCallHomecorpus,showingthecountsusedtocomputetheerrorrate:
REF: i*** **UMthePHONEIS iLEFT THEportable**** PHONEUPSTAIRSlastnight
HYP: iGOTIT TO the***** FULLESTiLOVETO portableFORMOF STORES lastnight
Eval: I I S D S S S I S S
Thisutterancehassixsubstitutions,threeinsertions,andonedeletion:
6+3+1
WordErrorRate = 100 =76.9%
13
Thestandardmethodforcomputingworderrorratesisafreescriptcalledsclite,
availablefromtheNationalInstituteofStandardsandTechnologies(NIST)(NIST,344 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH
2005). Sclite is given a series of reference (hand-transcribed, gold-standard) sen-
tencesandamatchingsetofhypothesissentences. Besidesperformingalignments,
andcomputingworderrorrate,scliteperformsanumberofotherusefultasks. For
example,forerroranalysisitgivesusefulinformationsuchasconfusionmatrices
showingwhichwordsareoftenmisrecognizedforothers,andsummarizesstatistics
ofwordsthatareofteninsertedordeleted. sclitealsogiveserrorratesbyspeaker
(ifsentencesarelabeledforspeakerID),aswellasusefulstatisticslikethesentence
Sentenceerror errorrate,thepercentageofsentenceswithatleastoneworderror.
rate
StatisticalsignificanceforASR:MAPSSWEorMacNemar
Aswithotherlanguageprocessingalgorithms,weneedtoknowwhetheraparticular
improvementinworderrorrateissignificantornot.
Thestandardstatisticaltestsfordeterminingiftwoworderrorratesaredifferent
istheMatched-PairSentenceSegmentWordError(MAPSSWE)test,introducedin
GillickandCox(1989).
The MAPSSWE test is a parametric test that looks at the difference between
the number of word errors the two systems produce, averaged across a number of
segments. The segments may be quite short or as long as an entire utterance; in
general, wewanttohavethelargestnumberof(short)segmentsinordertojustify
thenormalityassumptionandtomaximizepower. Thetestrequiresthattheerrors
inonesegmentbestatisticallyindependentoftheerrorsinanothersegment. Since
ASR systems tend to use trigram LMs, we can approximate this requirement by
definingasegmentasaregionboundedonbothsidesbywordsthatbothrecognizers
getcorrect(orbyturn/utteranceboundaries). Here’sanexamplefromNIST(2007)
withfourregions:
I II III IV
REF: |it was|the best|of|times it|was the worst|of times| |it was
| | | | | | | |
SYS A:|ITS |the best|of|times it|IS the worst |of times|OR|it was
| | | | | | | |
SYS B:|it was|the best| |times it|WON the TEST |of times| |it was
InregionI,systemAhastwoerrors(adeletionandaninsertion)andsystemB
haszero;inregionIII,systemAhasoneerror(asubstitution)andsystemBhastwo.
Let’sdefineasequenceofvariablesZrepresentingthedifferencebetweentheerrors
inthetwosystemsasfollows:
Ni thenumberoferrorsmadeonsegmentibysystemA
A
Ni thenumberoferrorsmadeonsegmentibysystemB
B
Z Ni Ni,i=1,2, ,nwherenisthenumberofsegments
A− B ···
Intheexampleabove,thesequenceofZ valuesis 2, 1, 1,1 . Intuitively,if
{ − − }
the two systems are identical, we would expect the average difference, that is, the
average of the Z values, to be zero. If we call the true average of the differences
mu , wewouldthusliketoknowwhethermu =0. Followingcloselytheoriginal
z z
proposalandnotationofGillickandCox(1989), wecanestimatethetrueaverage
fromourlimitedsampleasµˆ = n Z/n. TheestimateofthevarianceoftheZ’s
z i=1 i i
is
(cid:80)
n
1
σ z2=
n 1
(Z i −µ z)2 (16.21)
− i=1
(cid:88)16.6 • TTS 345
Let
µˆ
z
W = (16.22)
σ /√n
z
Foralargeenoughn(>50),W willapproximatelyhaveanormaldistributionwith
unit variance. The null hypothesis is H : µ =0, and it can thus be rejected if
0 z
2 P(Z w) 0.05 (two-tailed) or P(Z w) 0.05 (one-tailed), where Z is
∗ ≥| | ≤ ≥| | ≤
standardnormalandwistherealizedvalueW;theseprobabilitiescanbelookedup
inthestandardtablesofthenormaldistribution.
McNemar’stest EarlierworksometimesusedMcNemar’stestforsignificance,butMcNemar’s
isonlyapplicablewhentheerrorsmadebythesystemareindependent,whichisnot
trueincontinuousspeechrecognition, whereerrorsmadeonawordareextremely
dependentonerrorsmadeonneighboringwords.
Couldweimproveonworderrorrateasametric? Itwouldbenice, forexam-
ple,tohavesomethingthatdidn’tgiveequalweighttoeveryword,perhapsvaluing
contentwordslikeTuesdaymorethanfunctionwordslikeaorof.Whileresearchers
generally agree that this would be a good idea, it has proved difficult to agree on
a metric that works in every application of ASR. For dialogue systems, however,
where the desired semantic output is more clear, a metric called slot error rate or
concepterrorratehasprovedextremelyuseful;itisdiscussedinChapter15onpage
323.
16.6 TTS
Thegoaloftext-to-speech(TTS)systemsistomapfromstringsofletterstowave-
forms,atechnologythat’simportantforavarietyofapplicationsfromdialoguesys-
temstogamestoeducation.
Like ASR systems, TTS systems are generally based on the encoder-decoder
architecture,eitherusingLSTMsorTransformers. Thereisageneraldifferencein
training. ThedefaultconditionforASRsystemsistobespeaker-independent: they
aretrainedonlargecorporawiththousandsofhoursofspeechfrommanyspeakers
becausetheymustgeneralizewelltoanunseentestspeaker.Bycontrast,inTTS,it’s
lesscrucialtousemultiplevoices,andsobasicTTSsystemsarespeaker-dependent:
trainedtohaveaconsistentvoice,onmuchlessdata,butallfromonespeaker. For
example,onecommonlyusedpublicdomaindataset,theLJspeechcorpus,consists
of 24 hours of one speaker, Linda Johnson, reading audio books in the LibriVox
project(ItoandJohnson,2017),muchsmallerthanstandardASRcorporawhichare
hundredsorthousandsofhours.2
WegenerallybreakuptheTTStaskintotwocomponents. Thefirstcomponent
isanencoder-decodermodelforspectrogramprediction: itmapsfromstringsof
letters to mel spectrographs: sequences of mel spectral values over time. Thus we
2 ThereisalsorecentTTSresearchonthetaskofmulti-speakerTTS,inwhichasystemistrainedon
speechfrommanyspeakers,andcanswitchbetweendifferentvoices.346 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH
mightmapfromthisstring:
It’s time for lunch!
tothefollowingmelspectrogram:
Thesecondcomponentmapsfrommelspectrogramstowaveforms. Generating
vocoding waveformsfromintermediaterepresentationslikespectrogramsiscalledvocoding
vocoder andthissecondcomponentiscalledavocoder:
Thesestandardencoder-decoderalgorithmsforTTSarestillquitecomputation-
ally intensive, so a significant focus of modern research is on ways to speed them
up.
16.6.1 TTSPreprocessing: Textnormalization
Before either of these two steps, however, TTS systems require text normaliza-
non-standard tionpreprocessingforhandlingnon-standardwords:numbers,monetaryamounts,
words
dates,andotherconceptsthatareverbalizeddifferentlythantheyarespelled.ATTS
systemseeinganumberlike151needstoknowtoverbalizeitasonehundredfifty
oneifitoccursas$151butasonefiftyoneifitoccursinthecontext151Chapulte-
pecAve.. Thenumber1750canbespokeninatleastfourdifferentways,depending
onthecontext:
seventeen fifty: (in“TheEuropeaneconomyin1750”)
one seven five zero: (in“Thepasswordis1750”)
seventeen hundred and fifty: (in“1750dollars”)
one thousand, seven hundred, and fifty: (in“1750dollars”)
Often the verbalization of a non-standard word depends on its meaning (what
Taylor (2009) calls its semiotic class). Fig. 16.13 lays out some English non-
standardwordtypes.
Many classes have preferred realizations. A year is generally read as paired
digits (e.g., seventeen fifty for 1750). $3.2 billion must be read out with the
word dollars at the end, as three point two billion dollars. Some ab-
breviationslikeN.Y.areexpanded(toNew York), whileotheracronymslikeGPU
arepronouncedaslettersequences. Inlanguageswithgrammaticalgender,normal-
ization may depend on morphological properties. In French, the phrase 1 mangue
(‘one mangue’) is normalized to une mangue, but 1 ananas (‘one pineapple’) is
normalizedtoun ananas. InGerman,HeinrichIV(‘HenryIV’)canbenormalized
to Heinrich der Vierte, Heinrich des Vierten, Heinrich dem Vierten, or
HeinrichdenViertendependingonthegrammaticalcaseofthenoun(Demberg,
2006).16.6 • TTS 347
semioticclass examples verbalization
abbreviations gov’t,N.Y.,mph government
acronymsreadasletters GPU,D.C.,PC,UN,IBM GPU
cardinalnumbers 12,45,1/2,0.6 twelve
ordinalnumbers May7,3rd,BillGatesIII seventh
numbersreadasdigits Room101 oneohone
times 3.20,11:45 elevenfortyfive
dates 28/02(orinUS,2/28) Februarytwentyeighth
years 1999,80s,1900s,2045 nineteenninetynine
money $3.45,e250,$200K threedollarsfortyfive
moneyintr/m/billions $3.45billion threepointfourfivebilliondollars
percentage 75%3.4% seventyfivepercent
Figure16.13 Some types of non-standard words in text normalization; see Sproat et al.
(2001)and(vanEschandSproat,2018)formanymore.
Modernend-to-endTTSsystemscanlearntodosomenormalizationthemselves,
butTTSsystemsareonlytrainedonalimitedamountofdata(likethe220,000words
we mentioned above for the LJ corpus (Itoand Johnson, 2017)), and so a separate
normalizationstepisimportant.
Normalizationcanbedonebyruleorbyanencoder-decodermodel. Rule-based
normalizationisdoneintwostages:tokenizationandverbalization. Inthetokeniza-
tion stage we hand-write write rules to detect non-standard words. These can be
regularexpressions,likethefollowingfordetectingyears:
/(1[89][0-9][0-9])(20[0-9][0-9]/
|
A second pass of rules express how to verbalize each semiotic class. Larger TTS
systemsinsteadusemorecomplexrule-systems, liketheKestralsystemof(Ebden
and Sproat, 2015), which first classifies and parses each input into a normal form
and then produces text using a verbalization grammar. Rules have the advantage
thattheydon’trequiretrainingdata,andtheycanbedesignedforhighprecision,but
canbebrittle,andrequireexpertrule-writerssoarehardtomaintain.
Thealternativemodelistouseencoder-decodermodels,whichhavebeenshown
to work better than rules for such transduction tasks, but do require expert-labeled
trainingsetsinwhichnon-standardwordshavebeenreplacedwiththeappropriate
verbalization; suchtrainingsetsforsomelanguagesareavailable(SproatandGor-
man2018,Zhangetal.2019).
In the simplest encoder-decoder setting, we simply treat the problem like ma-
chinetranslation,trainingasystemtomapfrom:
They live at 224 Mission St.
to
They live at two twenty four Mission Street
While encoder-decoder algorithms are highly accurate, they occasionally pro-
duceerrorsthatareegregious;forexamplenormalizing45minutesasfortyfivemil-
limeters. To address this, more complex systems use mechanisms like lightweight
covering grammars, which enumerate a large set of possible verbalizations but
don’t try to disambiguate, to constrain the decoding to avoid such outputs (Zhang
etal.,2019).
16.6.2 TTS:Spectrogramprediction
TheexactsamearchitecturewedescribedforASR—theencoder-decoderwithattention–
can be used for the first component of TTS. Here we’ll give a simplified overview348 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH
Tacotron2 oftheTacotron2architecture(Shenetal.,2018),whichextendstheearlierTacotron
Wavenet (Wang et al., 2017) architecture and the Wavenet vocoder (van den Oord et al.,
2016). Fig.16.14sketchesouttheentirearchitecture.
The encoder’s job is to take a sequence of letters and produce a hidden repre-
sentationrepresentingthelettersequence,whichisthenusedbytheattentionmech-
anism in the decoder. The Tacotron2 encoder first maps every input grapheme to
a 512-dimensional character embedding. These are then passed through a stack
of 3 convolutional layers, each containing 512 filters with shape 5 1, i.e. each
×
filter spanning 5 characters, to model the larger letter context. The output of the
finalconvolutionallayerispassedthroughabiLSTMtoproducethefinalencoding.
It’scommontouseaslightlyhigherquality(butslower)versionofattentioncalled
location-based location-based attention, in which the computation of the α values (Eq. 9.37 in
attention
Chapter13)makesuseoftheα valuesfromthepriortime-state.
In the decoder, the predicted mel spectrum from the prior time slot is passed
throughasmallpre-netasabottleneck. Thisprioroutputisthenconcatenatedwith
theencoder’sattentionvectorcontextandpassedthrough2LSTMlayers. Theout-
putofthisLSTMisusedintwoways. First,itispassedthroughalinearlayer,and
someoutputprocessing,toautoregressivelypredictone80-dimensionallog-melfil-
terbankvectorframe(50ms,witha12.5msstride)ateachstep. Second,itispassed
throughanotherlinearlayertoasigmoidtomakea“stoptokenprediction”decision
aboutwhethertostopproducingoutput.
Whilelinearspectrogramsdiscardphaseinformation(andare (cid:59)(cid:69)(cid:90)(cid:73)(cid:74)(cid:83)(cid:86)(cid:81)(cid:4)
thereforelossy),algorithmssuchasGriffin-Lim[14]arecapableof (cid:48)(cid:72)(cid:79)(cid:3)(cid:54)(cid:83)(cid:72)(cid:70)(cid:87)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80) (cid:55)(cid:69)(cid:81)(cid:84)(cid:80)(cid:73)(cid:87)
Vocoder
estimatingthisdiscardedinformation,whichenablestime-domain
(cid:25)(cid:4)(cid:39)(cid:83)(cid:82)(cid:90)(cid:4)(cid:48)(cid:69)(cid:93)(cid:73)(cid:86)(cid:4) (cid:59)(cid:69)(cid:90)(cid:73)(cid:50)(cid:73)(cid:88)(cid:4)
conversionviatheinverseshort-timeFouriertransform.Melspectro-
(cid:52)(cid:83)(cid:87)(cid:88)(cid:17)(cid:50)(cid:73)(cid:88) (cid:49)(cid:83)(cid:48)
gramsdiscardevenmoreinformation,presentingachallengingin-
verseproblem.However,incomparisontothelinguisticandacoustic
Decoder
featuresusedinWaveNet,themelspectrogramisasimpler,lower-
levelacousticrepresentationofaudiosignals. Itshouldtherefore (cid:48)(cid:77)(cid:82)(cid:73)(cid:69)(cid:86)(cid:4)
bestraightforwardforasimilarWaveNetmodelconditionedonmel (cid:22)(cid:4)(cid:48)(cid:69)(cid:93)(cid:73)(cid:86)(cid:4) (cid:22)(cid:4)(cid:48)(cid:55)(cid:56)(cid:49)(cid:4) (cid:52)(cid:86)(cid:83)(cid:78)(cid:73)(cid:71)(cid:88)(cid:77)(cid:83)(cid:82)
spectrogramstogenerateaudio,essentiallyasaneuralvocoder.In- (cid:52)(cid:86)(cid:73)(cid:17)(cid:50)(cid:73)(cid:88) (cid:48)(cid:69)(cid:93)(cid:73)(cid:86)(cid:87) (cid:48)(cid:77)(cid:82)(cid:73)(cid:69)(cid:86)(cid:4)
(cid:55)(cid:88)(cid:83)(cid:84)(cid:4)(cid:56)(cid:83)(cid:79)(cid:73)(cid:82)
deed,wewillshowthatitispossibletogeneratehighqualityaudio (cid:52)(cid:86)(cid:83)(cid:78)(cid:73)(cid:71)(cid:88)(cid:77)(cid:83)(cid:82)
frommelspectrogramsusingamodifiedWaveNetarchitecture.
(cid:48)(cid:83)(cid:71)(cid:69)(cid:88)(cid:77)(cid:83)(cid:82)(cid:4)
(cid:55)(cid:73)(cid:82)(cid:87)(cid:77)(cid:88)(cid:77)(cid:90)(cid:73)(cid:4)
2.2. SpectrogramPredictionNetwork (cid:37)(cid:88)(cid:88)(cid:73)(cid:82)(cid:88)(cid:77)(cid:83)(cid:82)
Encoder
AsinTacotron,melspectrogramsarecomputedthroughashort- (cid:45)(cid:82)(cid:84)(cid:89)(cid:88)(cid:4)(cid:56)(cid:73)(cid:92)(cid:88) (cid:39)(cid:76)(cid:69)(cid:86)(cid:69)(cid:71)(cid:88)(cid:73)(cid:86)(cid:4) (cid:23)(cid:4)(cid:39)(cid:83)(cid:82)(cid:90)(cid:4) (cid:38)(cid:77)(cid:72)(cid:77)(cid:86)(cid:73)(cid:71)(cid:88)(cid:77)(cid:83)(cid:82)(cid:69)(cid:80)(cid:4)
timeFouriertransform(STFT)usinga50msframesize,12.5ms (cid:41)(cid:81)(cid:70)(cid:73)(cid:72)(cid:72)(cid:77)(cid:82)(cid:75) (cid:48)(cid:69)(cid:93)(cid:73)(cid:86)(cid:87) (cid:48)(cid:55)(cid:56)(cid:49)
framehop,andaHannwindowfunction.Weexperimentedwitha
5msframehoptomatchthefrequencyofthecFonidgiutiornein1g6i.n1p4uts The TFaicgo.1tr.oBnlo2ckadrciahgirtaemctoufrteh:eTAacnotreonnc2osdyestre-mdeacrcohditeerctumrea.ps from graphemes to
intheoriginalWaveNet,butthecorrespondinginmcreealssepienctetrmopgorraalms,followedbyavocoderthatmapstowavefiles.FiguremodifiedfromShen
resolutionresultedinsignificantlymorepronuncieattioanl.is(s2u0e1s.8).
WetransformtheSTFTmagnitudetothemelscaleusingan80 post-netlayeriscomprisedof512filterswithshape5 1withbatch
⇥
channelmelfilterbankspanning125Hzto7.6kHz,followedbylog normalization,followedbytanhactivationsonallbutthefinallayer.
dynamicrangecompression.Priortologcompression,TthheefilsteyrbsatnekmistraWineemdiniomnizegtoheldsumlomge-dmmeealnfisqlutearrebdaenrrokr(fMeaSEtu)rfreosm,buesfoirnegteacherforcing,
outputmagnitudesareclippedtoaminimumvalueof0.01inorder andafterthepost-nettoaidconvergence. Wealsoexperimented
tolimitdynamicrangeinthelogarithmicdomaint .hatisthedecodewritihsafleodg-ltihkeelihcooordrelocstsbloygm-omdeolidngetlhsepoeutcpturtadlisftreibauttuiornewaittheachdecoderstep
Thenetworkiscomposedofanencoderandaidnesctoedeardwoithfatthteen-preadMicitxetudredDeecnosidtyerNeotuwtoprku[t2f3r,o24m]tothaevopidriaossrumstienpg.aconstant
tion.Theencoderconvertsacharactersequenceintoahiddenfeature varianceovertime,butfoundthattheseweremoredifficulttotrain
representationwhichthedecoderconsumestopredictaspectrogram. andtheydidnotleadtobettersoundingsamples.
Inputcharactersarerepresentedusingalearned512-dimensional Inparalleltospectrogramframeprediction,theconcatenationof
16.6.3 TTS:Vocoding
characterembedding,whicharepassedthroughastackof3convolu- decoderLSTMoutputandtheattentioncontextisprojecteddown
tionallayerseachcontaining512filterswithshape5 1,i.e.,where toascalarandpassedthroughasigmoidactivationtopredictthe
eachfilterspans5characters,folloWwaevdebNyetbatchTnohrm⇥eavliozactioodne[1r8f]orTparocboabtirloitynth2atitsheaonutapudtaspeqtuaetnicoenhaosfcotmhpeleWteda.TvheiNs“esttopvtookceond”er(vandenOord
andReLUactivations. AsinTacotron,thesecoentvoallu.t,io2na0l1la6y)e.rsHerperedwiceti’olnlisguivseeddaursinogminefewrehncaettosiamllowpltihfieemdoddeletsocdryinpamtiiocanllyofvocodingusing
modellonger-termcontext(e.g.,N-grams)intheinputcharacter determinewhentoterminategenerationinsteadofalwaysgenerating
sequence.TheoutputofthefinalconvolutionallaW yera iv se paN sse edt. intoa forafixedduration. Specifically,generationcompletesatthefirst
singlebi-directional[19]LSTM[20]layercontaining5R12ecunailtsl(t2h5a6tthefragmoeaflorowfhtihchethvioscproodbaibnilgityperxocceeedsssahtherersehowldilolfb0.e5.toinvertalogmelspec-
ineachdirection)togeneratetheencodedfeatures. Theconvolutionallayersinthenetworkareregularizedusing
trumrepresentationsbackintoatime-domainwaveformrepresentation. WaveNetis
Theencoderoutputisconsumedbyanattentionnetworkwhich dropout[25]withprobability0.5,andLSTMlayersareregularized
summarizesthefullencodedsequenceasafixed-leanngthacuotnoterxetgvercetosrsiveunsinegtwzooneroku,tl[i2k6]ewtihtheprloabnagbiulitayg0e.1.mInoodrdeelrstowinetroindutcreooduutpcuetdinChapter9. It
foreachdecoderoutputstep.Weusethelocation-sensitiveattention variationatinferencetime,dropoutwithprobability0.5isapplied
from[21],whichextendstheadditiveattentionmechanism[22]to onlytolayersinthepre-netoftheautoregressivedecoder.
usecumulativeattentionweightsfrompreviousdecodertimesteps IncontrasttotheoriginalTacotron,ourmodelusessimplerbuild-
asanadditionalfeature.Thisencouragesthemodeltomoveforward ingblocks,usingvanillaLSTMandconvolutionallayersintheen-
consistentlythroughtheinput,mitigatingpotentialfailuremodes coderanddecoderinsteadof“CBHG”stacksandGRUrecurrent
wheresomesubsequencesarerepeatedorignoredbythedecoder. layers. Wedonotusea“reductionfactor”,i.e.,eachdecoderstep
Attentionprobabilitiesarecomputedafterprojectinginputsandlo- correspondstoasinglespectrogramframe.
cationfeaturesto128-dimensionalhiddenrepresentations.Location
featuresarecomputedusing321-Dconvolutionfiltersoflength31.
2.3. WaveNetVocoder
Thedecoderisanautoregressiverecurrentneuralnetworkwhich
predictsamelspectrogramfromtheencodedinputsequenceone WeuseamodifiedversionoftheWaveNetarchitecturefrom[8]to
frameatatime.Thepredictionfromtheprevioustimestepisfirst invertthemelspectrogramfeaturerepresentationintotime-domain
passedthroughasmallpre-netcontaining2fullyconnectedlayers waveformsamples. Asintheoriginalarchitecture, thereare30
of256hiddenReLUunits. Wefoundthatthepre-netactingasan dilatedconvolutionlayers,groupedinto3dilationcycles,i.e.,the
informationbottleneckwasessentialforlearningattention.Thepre- dilationrateoflayerk(k=0...29)is2k(mod10).Toworkwith
netoutputandattentioncontextvectorareconcatenatedandpassed the12.5msframehopofthespectrogramframes,only2upsampling
throughastackof2uni-directionalLSTMlayerswith1024units. layersareusedintheconditioningstackinsteadof3layers.
TheconcatenationoftheLSTMoutputandtheattentioncontext Insteadofpredictingdiscretizedbucketswithasoftmaxlayer,
vectorisprojectedthroughalineartransformtopredictthetarget wefollowPixelCNN++[27]andParallelWaveNet[28]andusea10-
spectrogramframe.Finally,thepredictedmelspectrogramispassed componentmixtureoflogisticdistributions(MoL)togenerate16-bit
througha5-layerconvolutionalpost-netwhichpredictsaresidual samplesat24kHz.Tocomputethelogisticmixturedistribution,the
toaddtothepredictiontoimprovetheoverallreconstruction.Each WaveNetstackoutputispassedthroughaReLUactivationfollowed16.6 • TTS 349
takesspectrogramsasinputandproducesaudiooutputrepresentedassequencesof
8-bitmu-law(page575). Theprobabilityofawaveform, asequenceof8-bitmu-
lawvaluesY =y ,...,y,givenanintermediateinputmelspectrogramhiscomputed
1 t
as:
t
Becausemodelswithcausalconvolutionsdonothaverecurrentconnections,theyaretypicallyfaster
totrainthanRNNs,especiallyp( wY h) e= napplP ie( dy ttoy 1v, e. r. y.,y lotn1g, sh e1q, u.. e. n,h cte)
s.
Oneoftheproble( m16 s.2 o3 f)
causal
| −
convolutions is that they require man(cid:89)t=y1layers, or large filters to increase the receptive field. For
example, inFig.2thereceptivefieldisonly5(=#layers+filterlength-1). Inthispaperweuse
Thisprobabilitydistributionismodeledbyastackofspecialconvolutionlayers,
dilatedconvolutionstoincreasethereceptivefieldbyordersofmagnitude,withoutgreatlyincreasing
whichincludeaspecificconvolutionalstructurecalleddilatedconvolutions,anda
computationalcost.
specificnon-linearityfunction.
A dilated cAondviolalutetidonco(anlvsooluctailolendisa` atrosuusb,tyoprecoonfvocaluutisoanl wcointhvohloulteiso)naislalacyoenr.volCuatiuosnalwohrere the
filterismaapspkleieddcoovnevroalnutairoenaslalorgoekrtohnalnyiatstltehnegtphabstyisnkpiuppt,inrgatihneprutthvaanlutehsewfiuthtuarec;erthtaeinpsrtee-p. Itis
equivalenttoaconvolutionwithalargerfilterderivedfromtheoriginalfilterbydilatingitwithzeros,
diction of y can only depend on y ,...,y, useful for autoregressive left-to-right
butissignificantlyt m+ o1reefficient.Adilatedco1nvolutt
ioneffectivelyallowsthenetworktooperateon
dilated processing. Indilatedconvolutions,ateachsuccessivelayerweapplytheconvolu-
convolutiaoncsoarserscalethanwithanormalconvolution.Thisissimilartopoolingorstridedconvolutions,but
tionalfilteroveraspanlongerthanitslengthbyskippinginputvalues. Thusattime
heretheoutputhasthesamesizeastheinput. Asaspecialcase,dilatedconvolutionwithdilation
1 yieldtswthitehsatadnidlaatridoncovnavlouleuotifon1.,aFcigo.n3vodleuptiioctnsaldifilaltteerdocfaluesnagltcho2nvwolouutlidonsseefoirnpduiltatviaolnuses1, 2, 4,
and 8.xDt ailnadtexdtco1.nvBouluttaiofinlstehrawveitphraevdioisutsilllyatbioeennvuaslueedoinf2vawrioouulsdcsoknitpexatns,ien.pgu.ts,isgonawlopuroldcessing
−
(Holscsheneeiidneprutetvaallu.,e1s9x89a;nDduxtille.uFxi,g1.91869.1),5asnhdowimsathgeecsoemgmpeuntatatitoionno(fCthheenouettpault.,a2t0ti1m5e; Yu &
t t 1
Koltunt,w20i1th6)4.dilatedconvoluti−
onlayerswithdilationvalues,1,2,4,and8.
Output
Dilation = 8
Hidden Layer
Dilation = 4
Hidden Layer
Dilation = 2
Hidden Layer
Dilation = 1
Input
Figure16.15 Dilatedconvolutions,showingonedilationcyclesizeof4,i.e.,dilationvalues
of1,2,4,8.FigurefromvandenOordetal.(2016).
Figure3:Visualizationofastackofdilatedcausalconvolutionallayers.
The Tacotron 2 synthesizer uses 12 convolutional layers in two cycles with a
Stackeddilatedconvolutionsenablenetworkstohaveverylargereceptivefieldswithjustafewlay-
ers,whdilielaptiroensecryvicnlegsthizeeinopfu6t,rmeseoaluntiinogntthharotuthgehofiursttth6elnaeytewrsorhkavasewdiellaltaiosncsomofp1u,ta2t,io4n,a8l,e1ffi6,ciency.
Inthisapnadpe3r2,.thaenddiltahteionneixstd6oulabyleedrsfoargaevinerhyalvaeyedriluaptiotonsaolifm1it,a2n,d4,th8e,n1r6e,paenadted3:2e..Dg.ilated
convolutionsallowthevocodertogrowthereceptivefieldexponentiallywithdepth.
1,2,4,...,512,1,2,4,...,512,1,2,4,...,512.
WaveNet predicts mu-law audio samples. Recall from page 575 that this is a
Theintuitionbehindthisconfigurationistwo-fold.First,exponentiallyincreasingthedilationfactor
standard compression for audio in which the values at each sampling timestep are
results in exponential receptive field growth with depth (Yu & Koltun, 2016). For example each
compressed into 8-bits. This means that we can predict the value of each sample
1,2,4,...,512blockhasreceptivefieldofsize1024,andcanbeseenasamoreefficientanddis-
criminawtiivthe(ansoinm-lpinleea2r5)6c-owuanytecrpaatertgoofriaca1lc1la0s2s4ificeorn.vTohluetioount.pSuetcoofntdh,estdaiclaktiendgcthoensveobluloticoknssfurther
increasiessththuesmpaosdseeldctahpraocuitgyhaandsotfhtemraexcewp⇥ thivicehfimeladkseiszeth.is256-waydecision.
Thespectrogrampredictionencoder-decoderandtheWaveNetvocoderaretrained
2.2 SseOpFaTrMatAelXy.DIASfTtRerIBtUheTIsOpNeSctrogram predictor is trained, the spectrogram prediction
network is run in teacher-forcing mode, with each predicted spectral frame condi-
One aptipornoeadchontothmeoednecloindgedthteexctoinndpiutitoannadl dthisetrpirbeuvtiioonussfp ra(x mtefx ro1m,.. t. h, ex gtro1u) nodvterrutthhespiencd-ividual
audiosampleswouldbetouseamixturemodelsuchasamixtu| redensity network(Bishop,1994)
trogram. This sequence of ground truth-aligned spectral features and gold audio
or mixture of conditional Gaussian scale mixtures (MCGSM) (Theis & Bethge, 2015). However,
outputisthenusedtotrainthevocoder.
vandenOordetal.(2016a)showedthatasoftmaxdistributiontendstoworkbetter,evenwhenthe
This has been only a high-level sketch of the TTS process. There are numer-
dataisimplicitlycontinuous(asisthecaseforimagepixelintensitiesoraudiosamplevalues).One
oftheroeuassoimnspiosrtthanattadectaatielgsotrhiactalthdeisrteriabduetrioinntiesrmesoteredflinexgiobilneganfudrcthaenrmwoirteheTaTsiSlymmaoydwelaanrtbitrary
distributionsbecauseitmakesnoassumptionsabouttheirshape.
Because raw audio is typically stored as a sequence of 16-bit integer values (one per timestep), a
softmaxlayerwouldneedtooutput65,536probabilitiespertimesteptomodelallpossiblevalues.
To make this more tractable, we first apply a µ-law companding transformation (ITU-T, 1988) to
thedata,andthenquantizeitto256possiblevalues:
ln(1+µ x )
f(x )=sign(x ) | t | ,
t t ln(1+µ)
3350 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH
to look into. For example WaveNet uses a special kind of a gated activation func-
tion as its non-linearity, and contains residual and skip connections. In practice,
predicting8-bitaudiovaluesdoesn’tasworkaswellas16-bit, forwhichasimple
softmax is insufficient, so decoders use fancier ways as the last step of predicting
audio sample values, like mixtures of distributions. Finally, the WaveNet vocoder
aswehavedescribeditwouldbesoslowastobeuseless; manydifferentkindsof
efficiency improvements are necessary in practice, for example byfinding ways to
donon-autoregressivegeneration,avoidingthelatencyofhavingtowaittogenerate
eachframeuntilthepriorframehasbeengenerated,andinsteadmakingpredictions
in parallel. We encourage the interested reader to consult the original papers and
variousversionofthecode.
16.6.4 TTSEvaluation
Speechsynthesissystemsareevaluatedbyhumanlisteners. (Thedevelopmentofa
good automatic metric for synthesis evaluation, one that would eliminate the need
for expensive and time-consuming human listening experiments, remains an open
andexcitingresearchtopic.)
We evaluate the quality of synthesized utterances by playing a sentence to lis-
MOS teners and ask them to give a mean opinion score (MOS), a rating of how good
the synthesized utterances are, usually on a scale from 1–5. We can then compare
systemsbycomparingtheirMOSscoresonthesamesentences(using,e.g.,paired
t-teststotestforsignificantdifferences).
Ifwearecomparingexactlytwosystems(perhapstoseeifaparticularchange
ABtests actuallyimprovedthesystem),wecanuseABtests. InABtests,weplaythesame
sentencesynthesizedbytwodifferentsystems(anAandaBsystem). Thehuman
listeners choose which of the two utterances they like better. We do this for say
50 sentences (presented in random order) and compare the number of sentences
preferredforeachsystem.
16.7 Other Speech Tasks
While we have focused on speech recognition and TTS in this chapter, there are a
widevarietyofspeech-relatedtasks.
wakeword Thetaskofwakeworddetectionistodetectawordorshortphrase,usuallyin
ordertowakeupavoice-enableassistantlikeAlexa,Siri,ortheGoogleAssistant.
Thegoalwithwakewordsisbuildthedetectionintosmalldevicesatthecomputing
edge,tomaintainprivacybytransmittingtheleastamountofuserspeechtoacloud-
basedserver.Thuswakeworddetectorsneedtobefast,smallfootprintsoftwarethat
canfitintoembeddeddevices. Wakeworddetectorsusuallyusethesamefrontend
featureextractionwesawforASR,oftenfollowedbyawhole-wordclassifier.
speaker Speaker diarization is the task of determining ‘who spoke when’ in a long
diarization
multi-speakeraudiorecording,markingthestartandendofeachspeaker’sturnsin
theinteraction. Thiscanbeusefulfortranscribingmeetings, classroomspeech, or
medicalinteractions.Oftendiarizationsystemsusevoiceactivitydetection(VAD)to
findsegmentsofcontinuousspeech,extractspeakerembeddingvectors,andcluster
the vectors to group together segments likely from the same speaker. More recent
workisinvestigatingend-to-endalgorithmstomapdirectlyfrominputspeechtoa
sequenceofspeakerlabelsforeachframe.16.8 • SUMMARY 351
speaker Speakerrecognition, isthetaskofidentifyingaspeaker. Wegenerallydistin-
recognition
guish the subtasks of speaker verification, where we make a binary decision (is
this speaker X or not?), such as for security when accessing personal information
overthetelephone,andspeakeridentification,wherewemakeaoneofN decision
tryingtomatchaspeaker’svoiceagainstadatabaseofmanyspeakers. Thesetasks
language are related to language identification, in which we are given a wavefile and must
identification
identifywhichlanguageisbeingspoken;thisisusefulforexampleforautomatically
directingcallerstohumanoperatorsthatspeakappropriatelanguages.
16.8 Summary
Thischapterintroducedthefundamentalalgorithmsofautomaticspeechrecognition
(ASR)andtext-to-speech(TTS).
• Thetaskofspeechrecognition(orspeech-to-text)istomapacousticwave-
formstosequencesofgraphemes.
• Theinputtoaspeechrecognizerisaseriesofacousticwaves. thataresam-
pled,quantized,andconvertedtoaspectralrepresentationlikethelogmel
spectrum.
• Twocommonparadigmsforspeechrecognitionaretheencoder-decoderwith
attention model, and models based on the CTC loss function. Attention-
basedmodelshavehigheraccuracies,butmodelsbasedonCTCmoreeasily
adapttostreaming: outputtinggraphemesonlineinsteadofwaitinguntilthe
acousticinputiscomplete.
• ASR is evaluated using the Word Error Rate; the edit distance between the
hypothesisandthegoldtranscription.
• TTS systems are also based on the encoder-decoder architecture. The en-
codermapsletterstoanencoding, whichisconsumedbythedecoderwhich
generatesmelspectrogramoutput. Aneuralvocoderthenreadsthespectro-
gramandgenerateswaveforms.
• TTSsystemsrequireafirstpassoftextnormalizationtodealwithnumbers
andabbreviationsandothernon-standardwords.
• TTS is evaluated by playing a sentence to human listeners and having them
giveameanopinionscore(MOS)orbydoingABtests.
Bibliographical and Historical Notes
ASR Anumberofspeechrecognitionsystemsweredevelopedbythelate1940s
and early 1950s. An early Bell Labs system could recognize any of the 10 digits
from a single speaker (Davis et al., 1952). This system had 10 speaker-dependent
storedpatterns, one foreachdigit, eachof whichroughlyrepresented thefirsttwo
vowelformantsinthedigit.Theyachieved97%–99%accuracybychoosingthepat-
tern that had the highest relative correlation coefficient with the input. Fry (1959)
and Denes (1959) built a phoneme recognizer at University College, London, that
recognizedfourvowelsandnineconsonantsbasedonasimilarpattern-recognition
principle. FryandDenes’ssystemwasthefirsttousephonemetransitionprobabili-
tiestoconstraintherecognizer.352 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH
Thelate1960sandearly1970sproducedanumberofimportantparadigmshifts.
First were a number of feature-extraction algorithms, including the efficient fast
Fouriertransform(FFT)(CooleyandTukey,1965),theapplicationofcepstralpro-
cessingtospeech(Oppenheimetal.,1968),andthedevelopmentofLPCforspeech
coding(AtalandHanauer,1971).Secondwereanumberofwaysofhandlingwarp-
warping ing; stretching or shrinking the input signal to handle differences in speaking rate
andsegmentlengthwhenmatchingagainststoredpatterns.Thenaturalalgorithmfor
solvingthisproblemwasdynamicprogramming,and,aswesawinAppendixA,the
algorithmwasreinventedmultipletimestoaddressthisproblem. Thefirstapplica-
tiontospeechprocessingwasbyVintsyuk(1968),althoughhisresultwasnotpicked
upbyotherresearchers,andwasreinventedbyVelichkoandZagoruyko(1970)and
SakoeandChiba(1971)(and1984). Soonafterward,Itakura(1975)combinedthis
dynamicprogrammingideawiththeLPCcoefficientsthathadpreviouslybeenused
onlyforspeechcoding. TheresultingsystemextractedLPCfeaturesfromincoming
wordsanduseddynamicprogrammingtomatchthemagainststoredLPCtemplates.
Thenon-probabilisticuseofdynamicprogrammingtomatchatemplateagainstin-
dynamictime comingspeechiscalleddynamictimewarping.
warping
The third innovation of this period was the rise of the HMM. Hidden Markov
modelsseemtohavebeenappliedtospeechindependentlyattwolaboratoriesaround
1972. Oneapplicationarosefromtheworkofstatisticians,inparticularBaumand
colleagues at the Institute for Defense Analyses in Princeton who applied HMMs
to various prediction problems (Baum and Petrie 1966, Baum and Eagon 1967).
James Baker learned of this work and applied the algorithm to speech processing
(Baker,1975a)duringhisgraduateworkatCMU.Independently,FrederickJelinek
and collaborators (drawing from their research in information-theoretical models
influenced by the work of Shannon (1948)) applied HMMs to speech at the IBM
ThomasJ.WatsonResearchCenter(Jelineketal.,1975). Oneearlydifferencewas
thedecodingalgorithm;Baker’sDRAGONsystemusedViterbi(dynamicprogram-
ming)decoding, whiletheIBMsystemappliedJelinek’sstackdecodingalgorithm
(Jelinek,1969). BakerthenjoinedtheIBMgroupforabrieftimebeforefounding
thespeech-recognitioncompanyDragonSystems.
TheuseoftheHMM,withGaussianMixtureModels(GMMs)asthephonetic
component, slowly spread throughthe speechcommunity, becomingthe dominant
paradigm by the 1990s. One cause was encouragement by ARPA, the Advanced
Research Projects Agency of the U.S. Department of Defense. ARPA started a
five-year program in 1971 to build 1000-word, constrained grammar, few speaker
speech understanding (Klatt, 1977), and funded four competing systems of which
Carnegie-MellonUniversity’sHarpysystem(Lowerre,1968),whichusedasimpli-
fiedversionofBaker’sHMM-basedDRAGONsystemwasthebestofthetestedsys-
tems. ARPA(andthenDARPA)fundedanumberofnewspeechresearchprograms,
beginning with 1000-word speaker-independent read-speech tasks like “Resource
Management”(Priceetal.,1988),recognitionofsentencesreadfromtheWallStreet
Journal(WSJ),BroadcastNewsdomain(LDC1998, Graff1997)(transcriptionof
actualnewsbroadcasts,includingquitedifficultpassagessuchason-the-streetinter-
views) and the Switchboard, CallHome, CallFriend, and Fisher domains (Godfrey
et al. 1992, Cieri et al. 2004) (natural telephone conversations between friends or
bakeoff strangers). Each of the ARPA tasks involved an approximately annual bakeoff at
whichsystemswereevaluatedagainsteachother. TheARPAcompetitionsresulted
in wide-scale borrowing of techniques among labs since it was easy to see which
ideas reduced errors the previous year, and the competitions were probably an im-BIBLIOGRAPHICALANDHISTORICALNOTES 353
portantfactorintheeventualspreadoftheHMMparadigm.
By around 1990 neural alternatives to the HMM/GMM architecture for ASR
arose,basedonanumberofearlierexperimentswithneuralnetworksforphoneme
recognition and other speech tasks. Architectures included the time-delay neural
network (TDNN)—the first use of convolutional networks for speech— (Waibel
hybrid etal.1989,Langetal.1990),RNNs(RobinsonandFallside,1991),andthehybrid
HMM/MLParchitectureinwhichafeedforwardneuralnetworkistrainedasapho-
neticclassifierwhoseoutputsareusedasprobabilityestimatesforanHMM-based
architecture(MorganandBourlard1990, BourlardandMorgan1994, Morganand
Bourlard1995).
WhilethehybridsystemsshowedperformanceclosetothestandardHMM/GMM
models, theproblemwasspeed: largehybridmodelsweretooslowtotrainonthe
CPUs of that era. For example, the largest hybrid system, a feedforward network,
waslimitedtoahiddenlayerof4000units,producingprobabilitiesoveronlyafew
dozenmonophones. Yettrainingthismodelstillrequiredtheresearchgrouptode-
signspecialhardwareboardstodovectorprocessing(MorganandBourlard,1995).
A later analytic study showed the performance of such simple feedforward MLPs
for ASR increases sharply with more than 1 hidden layer, even controlling for the
totalnumberofparameters(Maasetal.,2017). Butthecomputationalresourcesof
thetimewereinsufficientformorelayers.
OverthenexttwodecadesacombinationofMoore’slawandtheriseofGPUs
alloweddeepneuralnetworkswithmanylayers. Performancewasgettingcloseto
traditional systems on smaller tasks like TIMIT phone recognition by 2009 (Mo-
hamedetal.,2009),andby2012,theperformanceofhybridsystemshadsurpassed
traditional HMM/GMM systems (Jaitly et al. 2012, Dahl et al. 2012, inter alia).
Originally it seemed that unsupervised pretraining of the networks using a tech-
nique like deep belief networks was important, but by 2013, it was clear that for
hybridHMM/GMMfeedforwardnetworks,allthatmatteredwastousealotofdata
andenoughlayers,althoughafewothercomponentsdidimproveperformance: us-
ing log mel features instead of MFCCs, using dropout, and using rectified linear
units(Dengetal.2013,Maasetal.2013,Dahletal.2013).
Meanwhile early work had proposed the CTC loss function by 2006 (Graves
et al., 2006), and by 2012 the RNN-Transducer was defined and applied to phone
recognition(Graves2012,Gravesetal.2013),andthentoend-to-endspeechrecog-
nitionrescoring(GravesandJaitly,2014),andthenrecognition(Maasetal.,2015),
with advances such as specialized beam search (Hannun et al., 2014). (Our de-
scriptionofCTCinthechapterdrawsonHannun(2017), whichweencouragethe
interestedreadertofollow).
Theencoder-decoderarchitecturewasappliedtospeechataboutthesametime
bytwodifferentgroups,intheListenAttendandSpellsystemofChanetal.(2016)
and the attention-based encoder decoder architecture of Chorowski et al. (2014)
andBahdanauetal.(2016). By2018Transformerswereincludedinthisencoder-
decoder architecture. Karita et al. (2019) is a nice comparison of RNNs vs Trans-
formersinencoder-architecturesforASR,TTS,andspeech-to-speechtranslation.
Kaldi Popular toolkits for speech processing include Kaldi (Povey et al., 2011) and
ESPnet ESPnet(Watanabeetal.2018,Hayashietal.2020).
TTS As we noted at the beginning of the chapter, speech synthesis is one of the
earliest fields of speech and language processing. The 18th century saw a number
of physical models of the articulation process, including the von Kempelen model
mentionedabove,aswellasthe1773vowelmodelofKratzensteininCopenhagen354 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH
usingorganpipes.
The early 1950s saw the development of three early paradigms of waveform
synthesis: formantsynthesis,articulatorysynthesis,andconcatenativesynthesis.
Modernencoder-decodersystemsaredistantdescendantsofformantsynthesiz-
ers. Formant synthesizers originally were inspired by attempts to mimic human
speech by generating artificial spectrograms. The Haskins Laboratories Pattern
Playback Machine generated a sound wave by painting spectrogram patterns on a
moving transparent belt and using reflectance to filter the harmonics of a wave-
form (Cooper et al., 1951); other very early formant synthesizers include those of
Lawrence (1953) and Fant (1951). Perhaps the most well-known of the formant
synthesizersweretheKlattformantsynthesizeranditssuccessorsystems,includ-
ingtheMITalksystem(Allenetal.,1987)andtheKlattalksoftwareusedinDigital
EquipmentCorporation’sDECtalk(Klatt,1982). SeeKlatt(1975)fordetails.
Asecondearlyparadigm,concatenativesynthesis,seemstohavebeenfirstpro-
posed by Harris (1953) at Bell Laboratories; he literally spliced together pieces of
magnetictapecorrespondingtophones.Soonafterwards,Petersonetal.(1958)pro-
posed a theoretical model based on diphones, including a database with multiple
copiesofeachdiphonewithdifferingprosody, eachlabeledwithprosodicfeatures
includingF0,stress,andduration,andtheuseofjoincostsbasedonF0andformant
distancebetweenneighboringunits. Butsuchdiphonesynthesismodelswerenot
actuallyimplementeduntildecadeslater(DixonandMaxey1968,Olive1977). The
1980sand1990ssawtheinventionofunitselectionsynthesis,basedonlargerunits
of non-uniform length and the use of a target cost, (Sagisaka 1988, Sagisaka et al.
1992,HuntandBlack1996,BlackandTaylor1994,Syrdaletal.2000).
A third paradigm, articulatory synthesizers attempt to synthesize speech by
modeling the physics of the vocal tract as an open tube. Representative models
include Stevens et al. (1953), Flanagan et al. (1975), and Fant (1986). See Klatt
(1975)andFlanagan(1972)formoredetails.
MostearlyTTSsystemsusedphonemesasinput;developmentofthetextanal-
ysis components of TTS came somewhat later, drawing on NLP. Indeed the first
truetext-to-speechsystemseemstohavebeenthesystemofUmedaandTeranishi
(Umeda et al. 1968, Teranishi and Umeda 1968, Umeda 1976), which included a
parserthatassignedprosodicboundaries,aswellasaccentandstress.
Exercises
16.1 Analyzeeachoftheerrorsintheincorrectlyrecognizedtranscriptionof“um
thephoneisIleftthe...” onpage343. Foreachone,giveyourbestguessas
towhetheryouthinkitiscausedbyaprobleminsignalprocessing,pronun-
ciation modeling, lexicon size, language model, or pruning in the decoding
search.Part III
ANNOTATING LINGUISTIC
STRUCTURE
Inthefinalpartofthebookwediscussthetaskofdetectinglinguisticstructure.
IntheearlyhistoryofNLPthesestructureswereanintermediatesteptowarddeeper
languageprocessing. InmodernNLP,wedon’tgenerallymakeexplicituseofparse
or other structures inside the neural language models we introduced in Part I, or
directlyinapplicationslikethosewediscussedinPartII.
Instead linguistic structure plays a number of new roles. One of the most im-
portant roles is to provide a useful interpretive lens on neural networks. Knowing
thataparticularlayerorneuronmaybecomputingsomethingrelatedtoaparticular
kind of structure can help us break open the ‘black box’ and understand what the
componentsofourlanguagemodelsaredoing. Asecondimportantroleforlinguis-
tic structure is as as a practical tool for social scientific studies of text: knowing
which adjective modifies which noun, or whether a particular implicit metaphor is
beingused,canbeimportantformeasuringattitudestowardgroupsorindividuals.
Detailedsemanticstructurecanbehelpful,forexampleinfindingparticularclauses
thathaveparticularmeaningsinlegalcontracts.Wordsenselabelscanhelpkeepany
corpusstudyfrommeasuringfactsaboutthewrongwordsense. Relationstructures
can be used to help build knowledge bases from text. Finally, linguistic structure
can be important to answer questions about language itself. To answer linguistic
questionsabouthowlanguagechangesovertimeoracrossindividualswe’llneedto
beable,forexample,toparseentiredocumentsfromdifferenttimeperiods.
In our study of linguistic structure, we begin with one of the oldest tasks in
computationallinguistics: theextractionofsyntacticstructure,andgivetwosetsof
algorithmsforparsing: extractingsyntacticstructure,includingconstituencypars-
inganddependencyparsing. Wethenintroducemodel-theoreticsemanticsandgive
algorithmsforsemanticparsing. Wethenintroduceavarietyofstructuresrelated
tomeaning,includingsemanticroles,wordsenses,entityrelations,andevents. We
concludewithlinguisticstructuresthattendtoberelatedtodiscourseandmeaning
overlargertexts,includingcoreference,anddiscoursecoherence. Ineachcasewe’ll
givealgorithmsforautomaticallyannotatingtherelevantstructure.CHAPTER
17 Context-Free Grammars and
Constituency Parsing
BecausetheNightbyBruceSpringsteenandPattySmith
TheFireNextTimebyJamesBaldwin
Ifonawinter’snightatravelerbyItaloCalvino
LoveActuallybyRichardCurtis
SuddenlyLastSummerbyTennesseeWilliams
AScannerDarklybyPhilipK.Dick
Sixtitlesthatarenotconstituents,fromGeoffreyK.Pullumon
LanguageLog(whowaspointingouttheirincrediblerarity).
One morning I shot an elephant in my pajamas.
HowhegotintomypajamasIdon’tknow.
GrouchoMarx,AnimalCrackers,1930
The study of grammar has an ancient pedigree. The grammar of Sanskrit was
describedbytheIndiangrammarianPa¯n.inisometimebetweenthe7thand4thcen-
syntax turiesBCE,inhisfamoustreatisetheAs.t.a¯dhya¯y¯ı(‘8books’). Andourwordsyntax
comesfromtheGreeksy´ntaxis,meaning“settingouttogetherorarrangement”,and
refers to the way words are arranged together. We have seen syntactic notions in
previouschaptersliketheuseofpart-of-speechcategories(Chapter8). Inthischap-
ter and the next two we introduce formal models for capturing more sophisticated
notionsofgrammaticalstructure,andalgorithmsforparsingthesestructures.
Our focus in this chapter is context-free grammars and the CKY algorithm
for parsing them. Context-free grammars are the backbone of many formal mod-
elsofthesyntaxofnaturallanguage(and,forthatmatter,ofcomputerlanguages).
Syntactic parsingis thetask of assigninga syntacticstructure to asentence. Parse
trees(whetherforcontext-freegrammarsorforthedependencyorCCGformalisms
we introduce in following chapters) can be used in applications such as grammar
checking: sentencethatcannotbeparsedmayhavegrammaticalerrors(oratleast
behardtoread). Parsetreescanbeanintermediatestageofrepresentationforthe
formalsemanticanalysisofChapter20.Andparsersandthegrammaticalstructure
theyassignasentenceareausefultextanalysistoolfortextdatascienceapplications
thatrequiremodelingtherelationshipofelementsinsentences.
Inthischapterweintroducecontext-freegrammars,giveasmallsamplegram-
mar of English, introduce more formal definitions of context-free grammars and
grammar normal form, and talk about treebanks: corpora that have been anno-
tated with syntactic structure. We then discuss parse ambiguity and the problems
it presents, and turn to parsing itself, giving the famous Cocke-Kasami-Younger
(CKY) algorithm (Kasami 1965, Younger 1967), the standard dynamic program-
mingapproachtosyntacticparsing. TheCKYalgorithmreturnsanefficientrepre-
sentationofthesetofparsetreesforasentence,butdoesn’ttelluswhichparsetree
is the right one. For that, we need to augment CKY with scores for each possible
constituent. We’llseehowtodothiswithneuralspan-basedparsers. Finally,we’ll
introducethestandardsetofmetricsforevaluatingparseraccuracy.358 CHAPTER17 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING
17.1 Constituency
Syntactic constituency is the idea that groups of words can behave as single units,
orconstituents. Partofdevelopingagrammarinvolvesbuildinganinventoryofthe
constituents in the language. How do words group together in English? Consider
nounphrase thenounphrase,asequenceofwordssurroundingatleastonenoun.Herearesome
examplesofnounphrases(thankstoDamonRunyon):
HarrytheHorse ahigh-classspotsuchasMindy’s
theBroadwaycoppers thereasonhecomesintotheHotBox
they threepartiesfromBrooklyn
Whatevidencedowehavethatthesewordsgrouptogether(or“formconstituents”)?
Onepieceofevidenceisthattheycanallappearinsimilarsyntacticenvironments,
forexample,beforeaverb.
threepartiesfromBrooklynarrive...
ahigh-classspotsuchasMindy’sattracts...
theBroadwaycopperslove...
theysit
Butwhilethewholenounphrasecanoccurbeforeaverb,thisisnottrueofeach
oftheindividualwordsthatmakeupanounphrase.Thefollowingarenotgrammat-
icalsentencesofEnglish(recallthatweuseanasterisk(*)tomarkfragmentsthat
arenotgrammaticalEnglishsentences):
*fromarrive... *asattracts...
*theis... *spotsat...
Thus, to correctly describe facts about the ordering of these words in English, we
mustbeabletosaythingslike“NounPhrasescanoccurbeforeverbs”. Let’snow
seehowtodothisinamoreformalway!
17.2 Context-Free Grammars
A widely used formal system for modeling constituent structure in natural lan-
CFG guageisthecontext-freegrammar,orCFG.Context-freegrammarsarealsocalled
phrase-structuregrammars,andtheformalismisequivalenttoBackus-Naurform,
orBNF.Theideaofbasingagrammaronconstituentstructuredatesbacktothepsy-
chologistWilhelmWundt(1900)butwasnotformalizeduntilChomsky(1956)and,
independently,Backus(1959).
rules Acontext-freegrammarconsistsofasetofrulesorproductions,eachofwhich
expresses the ways that symbols of the language can be grouped and ordered to-
lexicon gether,andalexiconofwordsandsymbols.Forexample,thefollowingproductions
NP express that an NP (or noun phrase) can be composed of either a ProperNoun or
adeterminer(Det)followedbyaNominal; aNominalinturncanconsistofoneor17.2 • CONTEXT-FREEGRAMMARS 359
moreNouns.1
NP DetNominal
→
NP ProperNoun
→
Nominal Noun NominalNoun
→ |
Context-freerulescanbehierarchicallyembedded,sowecancombinetheprevious
ruleswithothers,likethefollowing,thatexpressfactsaboutthelexicon:
Det a
→
Det the
→
Noun flight
→
The symbols that are used in a CFG are divided into two classes. The symbols
terminal that correspond to words in the language (“the”, “nightclub”) are called terminal
symbols; thelexiconisthesetofrulesthatintroducetheseterminalsymbols. The
non-terminal symbolsthatexpressabstractionsovertheseterminalsarecallednon-terminals. In
eachcontext-freerule,theitemtotherightofthearrow( )isanorderedlistofone
→
ormoreterminalsandnon-terminals;totheleftofthearrowisasinglenon-terminal
symbolexpressingsomeclusterorgeneralization.Thenon-terminalassociatedwith
eachwordinthelexiconisitslexicalcategory,orpartofspeech.
A CFG can be thought of in two ways: as a device for generating sentences
and as a device for assigning a structure to a given sentence. Viewing a CFG as a
generator,wecanreadthe arrowas“rewritethesymbolontheleftwiththestring
→
ofsymbolsontheright”.
Sostartingfromthesymbol: NP
wecanuseourfirstruletorewriteNPas: DetNominal
andthenrewriteNominalas: Noun
andfinallyrewritetheseparts-of-speechas: aflight
Wesaythestringaflightcanbederivedfromthenon-terminalNP.Thus,aCFG
canbeusedtogenerateasetofstrings. Thissequenceofruleexpansionsiscalleda
derivation derivationofthestringofwords. Itiscommontorepresentaderivationbyaparse
parsetree tree(commonlyshowninvertedwiththerootatthetop). Figure17.1showsthetree
representationofthisderivation.
NP
Det Nom
a Noun
flight
Figure17.1 Aparsetreefor“aflight”.
dominates In the parse tree shown in Fig. 17.1, we can say that the node NP dominates
all the nodes in the tree (Det, Nom, Noun, a, flight). We can say further that it
immediatelydominatesthenodesDetandNom.
The formal language defined by a CFG is the set of strings that are derivable
startsymbol from the designated start symbol. Each grammar must have one designated start
1 Whentalkingabouttheseruleswecanpronouncetherightarrow as“goesto”,andsowemight
→
readthefirstruleaboveas“NPgoestoDetNominal”.360 CHAPTER17 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING
symbol,whichisoftencalledS.Sincecontext-freegrammarsareoftenusedtodefine
sentences,Sisusuallyinterpretedasthe“sentence”node,andthesetofstringsthat
arederivablefromSisthesetofsentencesinsomesimplifiedversionofEnglish.
Let’saddafewadditionalrulestoourinventory. Thefollowingruleexpresses
verbphrase thefactthatasentencecanconsistofanounphrasefollowedbyaverbphrase:
S NP VP Ipreferamorningflight
→
A verb phrase in English consists of a verb followed by assorted other things;
forexample,onekindofverbphraseconsistsofaverbfollowedbyanounphrase:
VP Verb NP preferamorningflight
→
Ortheverbmaybefollowedbyanounphraseandaprepositionalphrase:
VP Verb NP PP leaveBostoninthemorning
→
Ortheverbphrasemayhaveaverbfollowedbyaprepositionalphrasealone:
VP Verb PP leavingonThursday
→
A prepositional phrase generally has a preposition followed by a noun phrase.
Forexample,acommontypeofprepositionalphraseintheATIScorpusisusedto
indicatelocationordirection:
PP Preposition NP fromLosAngeles
→
The NP inside a PP need not be a location; PPs are often used with times and
dates, and with other nouns as well; they can be arbitrarily complex. Here are ten
examplesfromtheATIScorpus:
toSeattle ontheseflights
inMinneapolis aboutthegroundtransportationinChicago
onWednesday oftheroundtripflightonUnitedAirlines
intheevening oftheAPfiftysevenflight
ontheninthofJuly withastopoverinNashville
Figure17.2givesasamplelexicon,andFig.17.3summarizesthegrammarrules
we’ve seen so far, which we’ll call L . Note that we can use the or-symbol to
0
|
indicatethatanon-terminalhasalternatepossibleexpansions.
Noun flights flight breeze trip morning
→ | | | |
Verb is prefer like need want fly do
→ | | | | | |
Adjective cheapest non-stop first latest
→ | | |
other direct
| |
Pronoun me I you it
→ | | |
Proper-Noun Alaska Baltimore LosAngeles
→ | |
Chicago United American
| | |
Determiner the a an this these that
→ | | | | |
Preposition from to on near in
→ | | | |
Conjunction and or but
→ | |
Figure17.2 ThelexiconforL 0.
We can use this grammar to generate sentences of this “ATIS-language”. We
startwithS,expandittoNPVP,thenchoosearandomexpansionofNP(let’ssay,to17.2 • CONTEXT-FREEGRAMMARS 361
GrammarRules Examples
S NPVP I+wantamorningflight
→
NP Pronoun I
→
Proper-Noun LosAngeles
|
DetNominal a+flight
|
Nominal NominalNoun morning+flight
→
Noun flights
|
VP Verb do
→
VerbNP want+aflight
|
VerbNPPP leave+Boston+inthemorning
|
VerbPP leaving+onThursday
|
PP PrepositionNP from+LosAngeles
→
Figure17.3 ThegrammarforL 0,withexamplephrasesforeachrule.
S
NP VP
Pro Verb NP
I prefer Det Nom
a Nom Noun
Noun flight
morning
Figure17.4 Theparsetreefor“Ipreferamorningflight”accordingtogrammarL 0.
I),andarandomexpansionofVP(let’ssay,toVerbNP),andsoonuntilwegenerate
thestringIpreferamorningflight. Figure17.4showsaparsetreethatrepresentsa
completederivationofIpreferamorningflight.
Wecanalsorepresentaparse treeinamorecompactformatcalledbracketed
bracketed notation;hereisthebracketedrepresentationoftheparsetreeofFig.17.4:
notation
(17.1) [S[NP[ProI]][VP[Vprefer][NP[Deta][Nom[Nmorning][Nom[Nflight]]]]]]
ACFGlikethatofL definesaformallanguage. Sentences(stringsofwords)
0
thatcanbederivedbyagrammarareintheformallanguagedefinedbythatgram-
grammatical mar,andarecalledgrammaticalsentences. Sentencesthatcannotbederivedbya
givenformalgrammararenotinthelanguagedefinedbythatgrammarandarere-
ungrammatical ferredtoasungrammatical.Thishardlinebetween“in”and“out”characterizesall
formallanguagesbutisonlyaverysimplifiedmodelofhownaturallanguagesreally
work. Thisisbecausedeterminingwhetheragivensentenceispartofagivennat-
urallanguage(say,English)oftendependsonthecontext. Inlinguistics,theuseof
generative formal languages to model natural languages is called generative grammar since
grammar
the language is defined by the set of possible sentences “generated” by the gram-
mar. (Notethatthisisadifferentsenseoftheword‘generate’thanweintheuseof362 CHAPTER17 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING
languagemodelstogeneratetext.)
17.2.1 FormalDefinitionofContext-FreeGrammar
We conclude this section with a quick, formal description of a context-free gram-
mar and the language it generates. A context-free grammar G is defined by four
parameters: N,Σ,R,S(technicallyitisa“4-tuple”).
N asetofnon-terminalsymbols(orvariables)
Σ asetofterminalsymbols(disjointfromN)
R asetofrulesorproductions,eachoftheformA β ,
→
whereAisanon-terminal,
β isastringofsymbolsfromtheinfinitesetofstrings(Σ N)
∗
∪
S adesignatedstartsymbolandamemberofN
For the remainder of the book we adhere to the following conventions when dis-
cussing the formal properties of context-free grammars (as opposed to explaining
particularfactsaboutEnglishorotherlanguages).
CapitalletterslikeA,B,andS Non-terminals
S Thestartsymbol
Lower-caseGreekletterslikeα,β,andγ Stringsdrawnfrom(Σ N)
∗
∪
Lower-caseRomanletterslikeu,v,andw Stringsofterminals
Alanguageisdefinedthroughtheconceptofderivation. Onestringderivesan-
otheroneifitcanberewrittenasthesecondonebysomeseriesofruleapplications.
Moreformally,followingHopcroftandUllman(1979),
ifA β isaproductionofRandα andγ areanystringsintheset
→
directlyderives (Σ N) ∗,thenwesaythatαAγ directlyderivesαβγ,orαAγ αβγ.
∪ ⇒
Derivationisthenageneralizationofdirectderivation:
Letα ,α ,...,α bestringsin(Σ N) ,m 1,suchthat
1 2 m ∗
∪ ≥
α α ,α α ,...,α α
1 2 2 3 m 1 m
⇒ ⇒ − ⇒
derives Wesaythatα 1derivesα m,orα 1 ∗ α m.
⇒
WecanthenformallydefinethelanguageL generatedbyagrammarGasthe
G
setofstringscomposedofterminalsymbolsthatcanbederivedfromthedesignated
startsymbolS.
L G= wwisinΣ ∗andS ∗ w
{ | ⇒ }
The problem of mapping from a string of words to its parse tree is called syn-
syntactic tacticparsing,aswe’llseeinSection17.6.
parsing
17.3 Treebanks
treebank Acorpusinwhicheverysentenceisannotatedwithaparsetreeiscalledatreebank.17.3 • TREEBANKS 363
Treebanksplayanimportantroleinparsingaswellasinlinguisticinvestigationsof
syntacticphenomena.
Treebanksaregenerallymadebyparsingeachsentencewithaparsethatisthen
hand-corrected by human linguists. Figure 17.5 shows sentences from the Penn
PennTreebank Treebank project, which includes various treebanks in English, Arabic, and Chi-
nese. ThePennTreebankpart-of-speechtagsetwasdefinedinChapter8,butwe’ll
seeminorformattingdifferencesacrosstreebanks. TheuseofLISP-styleparenthe-
sizednotationfortreesisextremelycommonandresemblesthebracketednotation
wesawearlierin(17.1). Forthosewhoarenotfamiliarwithitweshowastandard
node-and-linetreerepresentationinFig.17.6.
((S
(NP-SBJ (DT That) ((S
(JJ cold) (, ,) (NP-SBJ The/DT flight/NN )
(JJ empty) (NN sky) ) (VP should/MD
(VP (VBD was) (VP arrive/VB
(ADJP-PRD (JJ full) (PP-TMP at/IN
(PP (IN of) (NP eleven/CD a.m/RB ))
(NP (NN fire) (NP-TMP tomorrow/NN )))))
(CC and)
(NN light) ))))
(. .) ))
(a) (b)
Figure17.5 ParsesfromtheLDCTreebank3for(a)Brownand(b)ATISsentences.
S
NP-SBJ VP .
DT JJ , JJ NN VBD ADJP-PRD .
That cold , empty sky was JJ PP
full IN NP
of NN CC NN
fire and light
Figure17.6 ThetreecorrespondingtotheBrowncorpussentenceinthepreviousfigure.
Thesentencesinatreebankimplicitlyconstituteagrammarofthelanguage.For
example,fromtheparsedsentencesinFig.17.5wecanextracttheCFGrulesshown
in Fig. 17.7 (with rule suffixes (-SBJ) stripped for simplicity). The grammar used
toparsethePennTreebankisveryflat,resultinginverymanyrules. Forexample,364 CHAPTER17 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING
Grammar Lexicon
S NPVP. DT the that
→ → |
S NPVP JJ cold empty full
→ → | |
NP DTNN NN sky fire light flight tomorrow
→ → | | | |
NP NNCCNN CC and
→ →
NP DTJJ,JJNN IN of at
→ → |
NP NN CD eleven
→ →
VP MDVP RB a.m.
→ →
VP VBDADJP VB arrive
→ →
VP MDVP VBD was said
→ → |
VP VBPPNP MD should would
→ → |
ADJP JJPP
→
PP INNP
→
PP INNPRB
→
Figure17.7 CFGgrammarrulesandlexiconfromthetreebanksentencesinFig.17.5.
amongtheapproximately4,500differentrulesforexpandingVPsareseparaterules
forPPsequencesofanylengthandeverypossiblearrangementofverbarguments:
VP VBD PP
VP → VBD PP PP
VP → VBD PP PP PP
VP → VBD PP PP PP PP
VP → VB ADVP PP
VP → VB PP ADVP
VP → ADVP VB PP
→
17.4 Grammar Equivalence and Normal Form
Aformallanguageisdefinedasa(possiblyinfinite)setofstringsofwords.Thissug-
geststhatwecouldaskiftwogrammarsareequivalentbyaskingiftheygeneratethe
samesetofstrings. Infact,itispossibletohavetwodistinctcontext-freegrammars
strongly generatethesamelanguage. Wesaythattwogrammarsarestronglyequivalentif
equivalent
they generate the same set of strings and if they assign the same phrase structure
toeachsentence(allowingmerelyforrenamingofthenon-terminalsymbols). Two
weakly grammarsareweaklyequivalentiftheygeneratethesamesetofstringsbutdonot
equivalent
assignthesamephrasestructuretoeachsentence.
normalform It is sometimes useful to have a normal form for grammars, in which each of
theproductionstakesaparticularform. Forexample, acontext-freegrammarisin
Chomsky Chomsky normal form (CNF) (Chomsky, 1963) if it is (cid:15)-free and if in addition
normalform
eachproductioniseitheroftheformA B CorA a. Thatis,theright-handside
→ →
ofeachruleeitherhastwonon-terminalsymbolsoroneterminalsymbol. Chomsky
binary normalformgrammarsarebinarybranching,thatistheyhavebinarytrees(down
branching
totheprelexicalnodes). WemakeuseofthisbinarybranchingpropertyintheCKY
parsingalgorithminChapter17.
Anycontext-freegrammarcanbeconvertedintoaweaklyequivalentChomsky
normalformgrammar. Forexample,aruleoftheform
A B C D
→
canbeconvertedintothefollowingtwoCNFrules(Exercise17.1asksthereaderto17.5 • AMBIGUITY 365
Grammar Lexicon
S NPVP Det that this the a
→ → | | |
S AuxNPVP Noun book flight meal money
→ → | | |
S VP Verb book include prefer
→ → | |
NP Pronoun Pronoun I she me
→ → | |
NP Proper-Noun Proper-Noun Houston NWA
→ → |
NP DetNominal Aux does
→ →
Nominal Noun Preposition from to on near through
→ → | | | |
Nominal NominalNoun
→
Nominal NominalPP
→
VP Verb
→
VP VerbNP
→
VP VerbNPPP
→
VP VerbPP
→
VP VPPP
→
PP PrepositionNP
→
Figure17.8 TheL 1miniatureEnglishgrammarandlexicon.
formulatethecompletealgorithm):
A B X
→
X C D
→
Sometimesusingbinarybranchingcanactuallyproducesmallergrammars. For
example,thesentencesthatmightbecharacterizedas
VP -> VBD NP PP*
arerepresentedinthePennTreebankbythisseriesofrules:
VP VBD NP PP
→
VP VBD NP PP PP
→
VP VBD NP PP PP PP
→
VP VBD NP PP PP PP PP
→
...
butcouldalsobegeneratedbythefollowingtwo-rulegrammar:
VP VBD NP PP
→
VP VP PP
→
ThegenerationofasymbolAwithapotentiallyinfinitesequenceofsymbolsBwith
Chomsky- aruleoftheformA A BisknownasChomsky-adjunction.
adjunction →
17.5 Ambiguity
Ambiguityisthemostseriousproblemfacedbysyntacticparsers. Chapter8intro-
duced the notions of part-of-speech ambiguity and part-of-speech disambigua-
structural tion. Here, we introduce a new kind of ambiguity, called structural ambiguity,
ambiguity
illustrated with a new toy grammar L , shown in Figure 17.8, which adds a few
1
rulestotheL grammarfromthelastchapter.
0
Structuralambiguityoccurswhenthegrammarcanassignmorethanoneparse
to a sentence. Groucho Marx’s well-known line as Captain Spaulding in Animal366 CHAPTER17 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING
S S
NP VP NP VP
Pronoun Verb NP Pronoun VP PP
I shot Det Nominal I Verb NP inmypajamas
an Nominal PP shot Det Nominal
Noun inmypajamas an Noun
elephant elephant
Figure17.9 Twoparsetreesforanambiguoussentence. Theparseontheleftcorrespondstothehumorous
reading in which the elephant is in the pajamas, the parse on the right corresponds to the reading in which
CaptainSpauldingdidtheshootinginhispajamas.
Crackers is ambiguous because the phrase in my pajamas can be part of the NP
headed by elephant or a part of the verb phrase headed by shot. Figure 17.9 illus-
tratesthesetwoanalysesofMarx’slineusingrulesfromL .
1
Structuralambiguity,appropriatelyenough,comesinmanyforms.Twocommon
kinds of ambiguity are attachment ambiguity and coordination ambiguity. A
attachment sentencehasanattachmentambiguityifaparticularconstituentcanbeattachedto
ambiguity
the parse tree at more than one place. The Groucho Marx sentence is an example
PP-attachment ofPP-attachmentambiguity: theprepositionphrasecanbeattachedeitheraspart
ambiguity
of the NP or as part of the VP. Various kinds of adverbial phrases are also subject
tothiskindofambiguity. Forinstance,inthefollowingexamplethegerundive-VP
flyingtoPariscanbepartofagerundivesentencewhosesubjectistheEiffelTower
oritcanbeanadjunctmodifyingtheVPheadedbysaw:
(17.2) WesawtheEiffelTowerflyingtoParis.
coordination Incoordinationambiguityphrasescanbeconjoinedbyaconjunctionlikeand.
ambiguity
For example, the phrase old men and women can be bracketed as [old [men and
women]], referring to old men and old women, or as [old men] and [women], in
whichcaseitisonlythemenwhoareold. Theseambiguitiescombineincomplex
waysinrealsentences,likethefollowingnewssentencefromtheBrowncorpus:
(17.3) PresidentKennedytodaypushedasideotherWhiteHousebusinessto
devoteallhistimeandattentiontoworkingontheBerlincrisisaddresshe
willdelivertomorrownighttotheAmericanpeopleovernationwide
televisionandradio.
Thissentencehasanumberofambiguities,althoughsincetheyaresemantically
unreasonable,itrequiresacarefulreadingtoseethem.Thelastnounphrasecouldbe
parsed [nationwide [television and radio]] or [[nationwide television] and radio].
ThedirectobjectofpushedasideshouldbeotherWhiteHousebusinessbutcould
also be the bizarre phrase [other White House business to devote all his time and
attentiontoworking](i.e.,astructurelikeKennedyaffirmed[hisintentiontopropose
anewbudgettoaddressthedeficit]).ThenthephraseontheBerlincrisisaddresshe17.6 • CKYPARSING: ADYNAMICPROGRAMMINGAPPROACH 367
willdelivertomorrownighttotheAmericanpeoplecouldbeanadjunctmodifying
theverbpushed. APPlikeovernationwidetelevisionandradiocouldbeattached
toanyofthehigherVPsorNPs(e.g.,itcouldmodifypeopleornight).
The fact that there are many grammatically correct but semantically unreason-
ableparsesfornaturallyoccurringsentencesisanirksomeproblemthataffectsall
parsers. Fortunately, the CKY algorithm below is designed to efficiently handle
structural ambiguities. And as we’ll see in the following section, we can augment
CKYwithneuralmethodstochooseasinglecorrectparsebysyntacticdisambigua-
syntactic tion.
disambiguation
17.6 CKY Parsing: A Dynamic Programming Approach
Dynamic programming provides a powerful framework for addressing the prob-
lems caused by ambiguity in grammars. Recall that a dynamic programming ap-
proach systematically fills in a table of solutions to subproblems. The complete
tablehasthesolutiontoallthesubproblemsneededtosolvetheproblemasawhole.
Inthecaseofsyntacticparsing, thesesubproblemsrepresentparsetreesforallthe
constituentsdetectedintheinput.
Thedynamicprogrammingadvantagearisesfromthecontext-freenatureofour
grammarrules—onceaconstituenthasbeendiscoveredinasegmentoftheinputwe
can record its presence and make it available for use in any subsequent derivation
thatmightrequireit. Thisprovidesbothtimeandstorageefficienciessincesubtrees
canbelookedupinatable,notreanalyzed.ThissectionpresentstheCocke-Kasami-
Younger(CKY)algorithm,themostwidelyuseddynamic-programmingbasedap-
proachtoparsing. Chartparsing(Kaplan1973, Kay1982)isarelatedapproach,
chartparsing anddynamicprogrammingmethodsareoftenreferredtoaschartparsingmethods.
17.6.1 ConversiontoChomskyNormalForm
TheCKYalgorithmrequiresgrammarstofirstbeinChomskyNormalForm(CNF).
Recall from Section 17.4 that grammars in CNF are restricted to rules of the form
A BCorA w. Thatis,theright-handsideofeachrulemustexpandeitherto
→ →
twonon-terminalsortoasingleterminal. RestrictingagrammartoCNFdoesnot
leadtoanylossinexpressiveness,sinceanycontext-freegrammarcanbeconverted
into a corresponding CNF grammar that accepts exactly the same set of strings as
theoriginalgrammar.
Let’sstartwiththeprocessofconvertingagenericCFGintoonerepresentedin
CNF.Assumingwe’redealingwithan(cid:15)-freegrammar,therearethreesituationswe
needtoaddressinanygenericgrammar:rulesthatmixterminalswithnon-terminals
ontheright-handside,rulesthathaveasinglenon-terminalontheright-handside,
andrulesinwhichthelengthoftheright-handsideisgreaterthan2.
Theremedyforrulesthatmixterminalsandnon-terminalsistosimplyintroduce
a new dummy non-terminal that covers only the original terminal. For example, a
ruleforaninfinitiveverbphrasesuchasINF-VP toVPwouldbereplacedbythe
→
tworulesINF-VP TOVPandTO to.
→ →
Unit Ruleswithasinglenon-terminalonthe rightarecalledunitproductions. We
productions
caneliminateunitproductionsbyrewritingtheright-handsideoftheoriginalrules
withtheright-handsideofallthenon-unitproductionrulesthattheyultimatelylead
to. Moreformally,ifA ∗ BbyachainofoneormoreunitproductionsandB γ
⇒ →368 CHAPTER17 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING
is a non-unit production in our grammar, then we add A γ for each such rule in
→
the grammar and discard all the intervening unit productions. As we demonstrate
withourtoygrammar,thiscanleadtoasubstantialflatteningofthegrammaranda
consequentpromotionofterminalstofairlyhighlevelsintheresultingtrees.
Ruleswithright-handsideslongerthan2arenormalizedthroughtheintroduc-
tionofnewnon-terminalsthatspreadthelongersequencesoverseveralnewrules.
Formally,ifwehavearulelike
A BCγ
→
wereplacetheleftmostpairofnon-terminalswithanewnon-terminalandintroduce
anewproduction,resultinginthefollowingnewrules:
A X1γ
→
X1 BC
→
In the case of longer right-hand sides, we simply iterate this process until the of-
fending rule has been replaced by rules of length 2. The choice of replacing the
leftmostpairofnon-terminalsispurelyarbitrary;anysystematicschemethatresults
inbinaryruleswouldsuffice.
Inourcurrentgrammar,theruleS AuxNPVPwouldbereplacedbythetwo
→
rulesS X1VPandX1 AuxNP.
→ →
Theentireconversionprocesscanbesummarizedasfollows:
1. Copyallconformingrulestothenewgrammarunchanged.
2. Convertterminalswithinrulestodummynon-terminals.
3. Convertunitproductions.
4. Makeallrulesbinaryandaddthemtonewgrammar.
Figure 17.10 shows the results of applying this entire conversion procedure to
theL grammarintroducedearlieronpage365. Notethatthisfiguredoesn’tshow
1
theoriginallexicalrules;sincetheseoriginallexicalrulesarealreadyinCNF,they
all carry over unchanged to the new grammar. Figure 17.10 does, however, show
thevariousplaceswheretheprocessofeliminatingunitproductionshas, ineffect,
creatednewlexicalrules. Forexample,alltheoriginalverbshavebeenpromotedto
bothVPsandtoSsintheconvertedgrammar.
17.6.2 CKYRecognition
With our grammar now in CNF, each non-terminal node above the part-of-speech
levelinaparsetreewillhaveexactlytwodaughters. Atwo-dimensionalmatrixcan
beusedtoencodethestructureofanentiretree. Forasentenceoflengthn,wewill
workwiththeupper-triangularportionofan(n+1) (n+1)matrix. Eachcell[i,j]
×
inthismatrixcontainsthesetofnon-terminalsthatrepresentalltheconstituentsthat
spanpositionsithrough joftheinput.Sinceourindexingschemebeginswith0,it’s
naturaltothinkoftheindexesaspointingatthegapsbetweentheinputwords(asin
fenceposts 0Book 1that 2flight 3). Thesegapsareoftencalledfenceposts,onthemetaphorof
thepostsbetweensegmentsoffencing. Itfollowsthenthatthecellthatrepresents
theentireinputresidesinposition[0,n]inthematrix.
Sinceeachnon-terminalentryinourtablehastwodaughtersintheparse,itfol-
lowsthatforeachconstituentrepresentedbyanentry[i,j],theremustbeaposition
intheinput, k,whereitcanbesplitintotwopartssuchthati<k< j. Givensuch17.6 • CKYPARSING: ADYNAMICPROGRAMMINGAPPROACH 369
L L
1Grammar 1inCNF
S NPVP S NPVP
→ →
S AuxNPVP S X1VP
→ →
X1 AuxNP
→
S VP S book include prefer
→ → | |
S VerbNP
→
S X2PP
→
S VerbPP
→
S VPPP
→
NP Pronoun NP I she me
→ → | |
NP Proper-Noun NP TWA Houston
→ → |
NP DetNominal NP DetNominal
→ →
Nominal Noun Nominal book flight meal money
→ → | | |
Nominal NominalNoun Nominal NominalNoun
→ →
Nominal NominalPP Nominal NominalPP
→ →
VP Verb VP book include prefer
→ → | |
VP VerbNP VP VerbNP
→ →
VP VerbNPPP VP X2PP
→ →
X2 VerbNP
→
VP VerbPP VP VerbPP
→ →
VP VPPP VP VPPP
→ →
PP PrepositionNP PP PrepositionNP
→ →
Figure17.10 L 1GrammaranditsconversiontoCNF.Notethatalthoughtheyaren’tshown
here,alltheoriginallexicalentriesfromL carryoverunchangedaswell.
1
a position k, the first constituent [i,k] must lie to the left of entry [i,j] somewhere
alongrowi,andthesecondentry[k,j]mustliebeneathit,alongcolumn j.
Tomakethismoreconcrete,considerthefollowingexamplewithitscompleted
parsematrix,showninFig.17.11.
(17.4) BooktheflightthroughHouston.
The superdiagonal row in the matrix contains the parts of speech for each word in
theinput. Thesubsequentdiagonals abovethatsuperdiagonal containconstituents
thatcoverallthespansofincreasinglengthintheinput.
Giventhissetup,CKYrecognitionconsistsoffillingtheparsetableintheright
way. Todothis,we’llproceedinabottom-upfashionsothatatthepointwherewe
are filling any cell [i,j], the cells containing the parts that could contribute to this
entry (i.e., the cells to the left and the cells below) have already been filled. The
algorithm given in Fig. 17.12 fills the upper-triangular matrix a column at a time
workingfromlefttoright,witheachcolumnfilledfrombottomtotop,astheright
sideofFig.17.11illustrates. Thisschemeguaranteesthatateachpointintimewe
havealltheinformationweneed(totheleft, sinceallthecolumnstothelefthave
alreadybeenfilled,andbelowsincewe’refillingbottomtotop). Italsomirrorson-
lineprocessing,sincefillingthecolumnsfromlefttorightcorrespondstoprocessing
eachwordoneatatime.
TheoutermostloopofthealgorithmgiveninFig.17.12iteratesoverthecolumns,
andthesecondloopiteratesovertherows,fromthebottomup. Thepurposeofthe
innermost loop is to range over all the places where a substring spanning i to j in
theinputmightbesplitintwo. Askrangesovertheplaceswherethestringcanbe
split, the pairs of cells we consider move, in lockstep, to the right along row i and
downalongcolumn j. Figure 17.13illustratesthegeneralcaseoffillingcell [i,j].370 CHAPTER17 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING
Book the flight through Houston
S, VP, Verb, S,VP,X2 S,VP,X2
Nominal,
Noun
[0,1] [0,2] [0,3] [0,4] [0,5]
Det NP NP
[1,2] [1,3] [1,4] [1,5]
Nominal, Nominal
Noun
[2,3] [2,4] [2,5]
Prep PP
[3,4] [3,5]
NP,
Proper-
Noun
[4,5]
Figure17.11 CompletedparsetableforBooktheflightthroughHouston.
functionCKY-PARSE(words,grammar)returnstable
forj from1toLENGTH(words)do
←
forall A A words[j] grammar
{ | → ∈ }
table[j 1,j] table[j 1,j] A
− ← − ∪
fori from j 2downto0do
← −
fork i+1to j 1do
← −
forall A A BC grammarandB table[i,k]andC table[k,j]
{ | → ∈ ∈ ∈ }
table[i,j] table[i,j] A
← ∪
Figure17.12 TheCKYalgorithm.
Ateachsuchsplit,thealgorithmconsiderswhetherthecontentsofthetwocellscan
be combined in a way that is sanctioned by a rule in the grammar. If such a rule
exists,thenon-terminalonitsleft-handsideisenteredintothetable.
Figure17.14showshowthefivecellsofcolumn5ofthetablearefilledafterthe
wordHoustonisread. Thearrowspointoutthetwospansthatarebeingusedtoadd
anentrytothetable. Notethattheactionincell[0,5]indicatesthepresenceofthree
alternative parses for this input, one where the PP modifies the flight, one where
it modifies the booking, and one that captures the second argument in the original
VP VerbNPPPrule,nowcapturedindirectlywiththeVP X2PPrule.
→ →
17.6.3 CKYParsing
The algorithm given in Fig. 17.12 is a recognizer, not a parser. That is, it can tell
uswhetheravalidparseexistsforagivensentencebasedonwhetherornotiffinds
an S in cell [0,n], but it can’t provide the derivation, which is the actual job for a
parser. To turn it into a parser capable of returning all possible parses for a given
input, we can make two simple changes to the algorithm: the first change is to
augmenttheentriesinthetablesothateachnon-terminalispairedwithpointersto
thetableentriesfromwhichitwasderived(moreorlessasshowninFig.17.14),the
secondchangeistopermitmultipleversionsofthesamenon-terminaltobeentered
into the table (again as shown in Fig. 17.14). With these changes, the completed
tablecontainsallthepossibleparsesforagiveninput. Returninganarbitrarysingle17.6 • CKYPARSING: ADYNAMICPROGRAMMINGAPPROACH 371
[0,1] [0,n]
...
[i,j]
...
[i,i+1] [i,i+2] [i,j-2] [i,j-1]
[i+1,j]
[i+2,j]
[j-2,j]
[j-1,j]
...
[n-1, n]
Figure17.13 Allthewaystofillthe[i, j]thcellintheCKYtable.
parse consists of choosing an S from cell [0,n] and then recursively retrieving its
componentconstituentsfromthetable. Ofcourse,insteadofreturningeveryparse
for a sentence, weusually want just the bestparse; we’ll see how to dothat in the
nextsection.
17.6.4 CKYinPractice
Finally, we should note that while the restriction to CNF does not pose a problem
theoretically,itdoesposesomenon-trivialproblemsinpractice. ThereturnedCNF
treesmaynotbeconsistantwiththeoriginalgrammarbuiltbythegrammardevel-
opers,andwillcomplicateanysyntax-drivenapproachtosemanticanalysis.
One approach to getting around these problems is to keep enough information
aroundtotransformourtreesbacktotheoriginalgrammarasapost-processingstep
oftheparse.Thisistrivialinthecaseofthetransformationusedforruleswithlength
greaterthan2. Simplydeletingthenewdummynon-terminalsandpromotingtheir
daughtersrestorestheoriginaltree.
Inthecaseofunitproductions,itturnsouttobemoreconvenienttoaltertheba-
sicCKYalgorithmtohandlethemdirectlythanitistostoretheinformationneeded
torecoverthecorrecttrees. Exercise17.3asksyoutomakethischange. Manyof
theprobabilisticparserspresentedinAppendixCusetheCKYalgorithmalteredin372 CHAPTER17 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING
Book the flight through Houston Book the flight through Houston
S, VP, Verb, S,VP,X2 S, VP, Verb, S,VP,X2
Nominal, Nominal,
Noun Noun
[0,1] [0,2] [0,3] [0,4] [0,5] [0,1] [0,2] [0,3] [0,4] [0,5]
Det NP Det NP NP
[1,2] [1,3] [1,4] [1,5] [1,2] [1,3] [1,4] [1,5]
Nominal, Nominal Nominal,
Noun Noun
[2,3] [2,4] [2,5] [2,3] [2,4] [2,5]
Prep Prep PP
[3,4] [3,5] [3,4] [3,5]
NP, NP,
Proper- Proper-
Noun Noun
[4,5] [4,5]
Book the flight through Houston Book the flight through Houston
S, VP, Verb, S,VP,X2 S, VP, Verb, S,VP,X2
Nominal, Nominal,
Noun Noun
[0,1] [0,2] [0,3] [0,4] [0,5] [0,1] [0,2] [0,3] [0,4] [0,5]
Det NP NP Det NP NP
[1,2] [1,3] [1,4] [1,5] [1,2] [1,3] [1,4] [1,5]
Nominal, Nominal Nominal, Nominal
Noun Noun
[2,3] [2,4] [2,5] [2,3] [2,4] [2,5]
Prep PP Prep PP
[3,4] [3,5] [3,4] [3,5]
NP, NP,
Proper- Proper-
Noun Noun
[4,5] [4,5]
Book the flight through Houston
S N,
o
V mP in, aV le
,
rb,
S,
S1,VP, X2
Noun VP, S2, VP
[0,1] [0,2]
[X 02
,3] [0,4]
S3
Det NP NP
[1,2] [1,3] [1,4] [1,5]
Nominal, Nominal
Noun
[2,3] [2,4] [2,5]
Prep PP
[3,4] [3,5]
NP,
Proper-
Noun
[4,5]
Figure17.14 Fillingthecellsofcolumn5afterreadingthewordHouston.17.7 • SPAN-BASEDNEURALCONSTITUENCYPARSING 373
justthismanner.
17.7 Span-Based Neural Constituency Parsing
While the CKY parsing algorithm we’ve seen so far does great at enumerating all
thepossibleparsetreesforasentence,ithasalargeproblem:itdoesn’ttelluswhich
parseisthecorrectone!Thatis,itdoesn’tdisambiguateamongthepossibleparses.
To solve the disambiguation problem we’ll use a simple neural extension of the
CKYalgorithm. Theintuitionofsuchparsingalgorithms(oftencalledspan-based
constituency parsing, or neural CKY), is to train a neural classifier to assign a
scoretoeachconstituent,andthenuseamodifiedversionofCKYtocombinethese
constituentscorestofindthebest-scoringparsetree.
Here we’ll describe a version of the algorithm from Kitaev et al. (2019). This
parserlearnstomapaspanofwordstoaconstituent,and,likeCKY,hierarchically
combineslargerandlargerspanstobuildtheparse-treebottom-up. Butunlikeclas-
sic CKY, this parser doesn’t use the hand-written grammar to constrain what con-
stituentscanbecombined,insteadjustrelyingonthelearnedneuralrepresentations
ofspanstoencodelikelycombinations.
17.7.1 ComputingScoresforaSpan
span Let’sbeginbyconsideringjusttheconstituent(we’llcallitaspan)thatliesbetween
fencepostpositionsiand j withnon-terminalsymbollabell. We’llbuildasystem
toassignascores(i,j,l)tothisconstituentspan.
NP
CKY for computing best parse
Compute score for span MLP
Represent span h-h
j i
i=1 j=3
0 1 2 3 4 5
postprocessing layers
map back to words
ENCODER
map to subwords
[START] Book the flight throughHouston[END]
Figure17.15 Asimplifiedoutlineofcomputingthespanscoreforthespantheflightwith
thelabelNP.
Fig. 17.15 sketches the architecture. The input word tokens are embedded by374 CHAPTER17 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING
passingthemthroughapretrainedlanguagemodellikeBERT.BecauseBERToper-
atesonthelevelofsubword(wordpiece)tokensratherthanwords,we’llfirstneedto
converttheBERToutputstowordrepresentations. Onestandardwayofdoingthis
istosimplyusethefirstsubwordunitastherepresentationfortheentireword;us-
ingthelastsubwordunit,orthesumofallthesubwordunitsarealsocommon. The
embeddings can then be passed through some postprocessing layers; Kitaev et al.
(2019),forexample,use8Transformerlayers.
The resulting word encoder outputs y are then used to compute a span score.
t
First,wemustmapthewordencodings(indexedbywordpositions)tospanencod-
ings (indexed by fenceposts). We do this by representing each fencepost with two
separatevalues;theintuitionisthataspanendpointtotherightofawordrepresents
different information than a span endpoint to the left of a word. We convert each
word output y into a (leftward-pointing) value for spans ending at this fencepost,
t
←−y t, and a (rightward-pointing) value →−y
t
for spans beginning at this fencepost, by
splittingy intotwohalves. Eachspanthenstretchesfromonedouble-vectorfence-
t
posttoanother,asinthefollowingrepresentationoftheflight,whichisspan(1,3):
START0 Book the flight through
y
0
→−y
0
←y−1 y
1
→−y
1
←y−2 y
2
→−y
2
←y−3 y
3
→−y
3
←y−4 y
4
→−y
4
←y−5 ...
0 1 2 3 4
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
span(1,3)
Atraditionalwaytorepresentaspan, developedoriginallyforRNN-basedmodels
(Wang and Chang, 2016), but extended also to Transformers, is to take the differ-
ence between the embeddings of its start and end, i.e., representing span (i,j) by
subtractingtheembeddingofifromtheembeddingof j. Herewerepresentaspan
byconcatenatingthedifferenceofeachofitsfencepostcomponents:
v(i,j)=[→−y j →−y i ; ←y j−+−1 ←y i−+−1] (17.5)
− −
ThespanvectorvisthenpassedthroughanMLPspanclassifier,withtwofully-
connectedlayersandoneReLUactivationfunction,whoseoutputdimensionalityis
thenumberofpossiblenon-terminallabels:
s(i,j, )=W 2ReLU(LayerNorm(W 1v(i,j))) (17.6)
·
TheMLPthenoutputsascoreforeachpossiblenon-terminal.
17.7.2 IntegratingSpanScoresintoaParse
Nowwehaveascoreforeachlabeledconstituentspans(i,j,l). Butweneedascore
foranentireparsetree. FormallyatreeT isrepresentedasasetof T suchlabeled
| |
spans,withthetthspanstartingatpositioni andendingatposition j,withlabell:
t t t
T = (i t,j t,l t):t=1,..., T (17.7)
{ | |}
Thus once we have a score for each span, the parser can compute a score for the
wholetrees(T)simplybysummingoverthescoresofitsconstituentspans:
s(T)= s(i,j,l) (17.8)
(i,(cid:88)j,l) ∈T17.8 • EVALUATINGPARSERS 375
Andwecanchoosethefinalparsetreeasthetreewiththemaximumscore:
Tˆ =argmaxs(T) (17.9)
T
Thesimplestmethodtoproducethemostlikelyparseistogreedilychoosethe
highestscoringlabelforeachspan.Thisgreedymethodisnotguaranteedtoproduce
atree,sincethebestlabelforaspanmightnotfitintoacompletetree. Inpractice,
however, the greedy method tends to find trees; in their experiments Gaddy et al.
(2018)findsthat95%ofpredictedbracketingsformvalidtrees.
NonethelessitismorecommontouseavariantoftheCKYalgorithmtofindthe
fullparse. ThevariantdefinedinGaddyetal.(2018)worksasfollows. Let’sdefine
s (i,j)asthescoreofthebestsubtreespanning(i,j). Forspansoflengthone,we
best
choosethebestlabel:
s best(i,i+1)=maxs(i,i+1,l) (17.10)
l
Forotherspans(i,j),therecursionis:
s (i,j) = maxs(i,j,l)
best
l
+ max[s best(i,k)+s best(k,j)] (17.11)
k
Notethattheparserisusingthemaxlabelforspan(i,j)+themaxlabelsforspans
(i,k)and(k,j)withoutworryingaboutwhetherthosedecisionsmakesensegivena
grammar. Theroleofthegrammarinclassicalparsingistohelpconstrainpossible
combinations of constituents (NPs like to be followed by VPs). By contrast, the
neuralmodelseemstolearnthesekindsofcontextualconstraintsduringitsmapping
fromspanstonon-terminals.
Formoredetailsonspan-basedparsing,includingthemargin-basedtrainingal-
gorithm,seeSternetal.(2017),Gaddyetal.(2018),KitaevandKlein(2018),and
Kitaevetal.(2019).
17.8 Evaluating Parsers
Thestandardtoolforevaluatingparsersthatassignasingleparsetreetoasentence
PARSEVAL isthePARSEVALmetrics(Blacketal.,1991). ThePARSEVALmetricmeasures
howmuchtheconstituentsinthehypothesisparsetreelookliketheconstituentsina
hand-labeled,referenceparse.PARSEVALthusrequiresahuman-labeledreference
(or“goldstandard”)parsetreeforeachsentenceinthetestset; wegenerallydraw
thesereferenceparsesfromatreebanklikethePennTreebank.
AconstituentinahypothesisparseC ofasentencesislabeledcorrectifthere
h
isaconstituentinthereferenceparseC withthesamestartingpoint,endingpoint,
r
andnon-terminalsymbol. Wecanthenmeasuretheprecisionandrecalljustasfor
taskswe’veseenalreadylikenamedentitytagging:
#ofcorrectconstituentsinhypothesisparseofs
labeledrecall:=
#oftotalconstituentsinreferenceparseofs
#ofcorrectconstituentsinhypothesisparseofs
labeledprecision:=
#oftotalconstituentsinhypothesisparseofs376 CHAPTER17 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING
S(dumped)
NP(workers) VP(dumped)
NNS(workers) VBD(dumped) NP(sacks) PP(into)
workers dumped NNS(sacks) P NP(bin)
sacks into DT(a) NN(bin)
a bin
Figure17.16 AlexicalizedtreefromCollins(1999).
Asusual,weoftenreportacombinationofthetwo,F :
1
2PR
F = (17.12)
1
P+R
Weadditionallyuseanewmetric,crossingbrackets,foreachsentences:
cross-brackets: the number of constituents for which the reference parse has a
bracketing such as ((A B) C) but the hypothesis parse has a bracketing such
as(A(BC)).
For comparing parsers that use different grammars, the PARSEVAL metric in-
cludesacanonicalizationalgorithmforremovinginformationlikelytobegrammar-
specific(auxiliaries,pre-infinitival“to”,etc.) andforcomputingasimplifiedscore
(Black et al., 1991). The canonical implementation of the PARSEVAL metrics is
evalb calledevalb(SekineandCollins,1997).
17.8.1 HeadsandHead-Finding
Syntacticconstituentscanbeassociatedwithalexicalhead;NistheheadofanNP,
VistheheadofaVP.ThisideaofaheadforeachconstituentdatesbacktoBloom-
field1914,andiscentraltothedependencygrammarsanddependencyparsingwe’ll
introduceinChapter18. Indeed,headscanbeusedasawaytomapbetweencon-
stituency and dependency parses. Heads are also important in probabilistic pars-
ing (Appendix C) and in constituent-based grammar formalisms like Head-Driven
PhraseStructureGrammar(PollardandSag,1994)..
In one simple model of lexical heads, each context-free rule is associated with
a head (Charniak 1997, Collins 1999). The head is the word in the phrase that is
grammatically the most important. Heads are passed up the parse tree; thus, each
non-terminalinaparsetreeisannotatedwithasingleword,whichisitslexicalhead.
Figure 17.16 shows an example of such a tree from Collins (1999), in which each
non-terminalisannotatedwithitshead.
Forthegenerationofsuchatree,eachCFGrulemustbeaugmentedtoidentify
oneright-sideconstituenttobetheheadchild.Theheadwordforanodeisthensetto
theheadwordofitsheadchild. Choosingtheseheadchildrenissimplefortextbook
examples (NN is the head of NP) but is complicated and indeed controversial for
mostphrases. (Shouldthecomplementizertoortheverbbetheheadofaninfinite17.9 • SUMMARY 377
verb phrase?) Modern linguistic theories of syntax generally include a component
thatdefinesheads(see,e.g.,(PollardandSag,1994)).
Analternativeapproachtofindingaheadisusedinmostpracticalcomputational
systems. Insteadofspecifyingheadrulesinthegrammaritself,headsareidentified
dynamically in the context of trees for specific sentences. In other words, once
a sentence is parsed, the resulting tree is walked to decorate each node with the
appropriate head. Most current systems rely on a simple set of handwritten rules,
such as a practical one for Penn Treebank grammars given in Collins (1999) but
developed originally by Magerman (1995). For example, the rule for finding the
headofanNPisasfollows(Collins,1999,p. 238):
• IfthelastwordistaggedPOS,returnlast-word.
• ElsesearchfromrighttoleftforthefirstchildwhichisanNN,NNP,NNPS,NX,POS,
orJJR.
• ElsesearchfromlefttorightforthefirstchildwhichisanNP.
• Elsesearchfromrighttoleftforthefirstchildwhichisa$,ADJP,orPRN.
• ElsesearchfromrighttoleftforthefirstchildwhichisaCD.
• ElsesearchfromrighttoleftforthefirstchildwhichisaJJ,JJS,RBorQP.
• Elsereturnthelastword
SelectedotherrulesfromthissetareshowninFig.17.17. Forexample,forVP
rulesoftheformVP Y Y ,thealgorithmwouldstartfromtheleftofY
1 n 1
→ ··· ···
Y lookingforthefirstY oftypeTO;ifnoTOsarefound, itwouldsearchforthe
n i
firstY oftypeVBD;ifnoVBDsarefound,itwouldsearchforaVBN,andsoon.
i
SeeCollins(1999)formoredetails.
Parent Direction PriorityList
ADJP Left NNSQPNN$ADVPJJVBNVBGADJPJJRNPJJSDTFWRBRRBS
SBARRB
ADVP Right RBRBRRBSFWADVPTOCDJJRJJINNPJJSNN
PRN Left
PRT Right RP
QP Left $INNNSNNJJRBDTCDNCDQPJJRJJS
S Left TOINVPSSBARADJPUCPNP
SBAR Left WHNPWHPPWHADVPWHADJPINDTSSQSINVSBARFRAG
VP Left TOVBDVBNMDVBZVBVBGVBPVPADJPNNNNSNP
Figure17.17 SomeheadrulesfromCollins(1999).Theheadrulesarealsocalledaheadpercolationtable.
17.9 Summary
Thischapterintroducedconstituencyparsing. Here’sasummaryofthemainpoints:
• In many languages, groups of consecutive words act as a group or a con-
stituent, which can be modeled by context-free grammars (which are also
knownasphrase-structuregrammars).
• Acontext-freegrammarconsistsofasetofrulesorproductions,expressed
overasetofnon-terminalsymbolsandasetofterminalsymbols. Formally,
a particular context-free language is the set of strings that can be derived
fromaparticularcontext-freegrammar.
• Structuralambiguityisasignificantproblemforparsers. Commonsources
ofstructuralambiguityincludePP-attachmentandcoordinationambiguity.378 CHAPTER17 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING
• Dynamic programming parsing algorithms, such as CKY, use a table of
partialparsestoefficientlyparseambiguoussentences.
• CKYrestrictstheformofthegrammartoChomskynormalform(CNF).
• ThebasicCKYalgorithmcompactlyrepresentsallpossibleparsesofthesen-
tencebutdoesn’tchooseasinglebestparse.
• Choosing a single parse from all possible parses (disambiguation) can be
donebyneuralconstituencyparsers.
• Span-basedneuralconstituencyparsestrainaneuralclassifiertoassignascore
toeachconstituent,andthenuseamodifiedversionofCKYtocombinethese
constituentscorestofindthebest-scoringparsetree.
• Parsers are evaluated with three metrics: labeled recall, labeled precision,
andcross-brackets.
• Partial parsing and chunking are methods for identifying shallow syntac-
tic constituents in a text. They are solved by sequence models trained on
syntactically-annotateddata.
Bibliographical and Historical Notes
AccordingtoPercival(1976),theideaofbreakingupasentenceintoahierarchyof
constituentsappearedintheVo¨lkerpsychologieofthegroundbreakingpsychologist
WilhelmWundt(Wundt,1900):
...densprachlichenAusdruckfu¨rdiewillku¨rlicheGliederungeinerGe-
sammtvorstellung in ihre in logische Beziehung zueinander gesetzten
Bestandteile
[the linguistic expression for the arbitrary division of a total idea
intoitsconstituentpartsplacedinlogicalrelationstooneanother]
Wundt’sideaofconstituencywastakenupintolinguisticsbyLeonardBloom-
fieldinhisearlybookAnIntroductiontotheStudyofLanguage(Bloomfield,1914).
By the time of his later book, Language (Bloomfield, 1933), what was then called
“immediate-constituentanalysis”wasawell-establishedmethodofsyntacticstudy
in the United States. By contrast, traditional European grammar, dating from the
Classicalperiod,definedrelationsbetweenwordsratherthanconstituents,andEu-
ropeansyntacticiansretainedthisemphasisonsuchdependencygrammars,thesub-
jectofChapter18. (Andindeed,bothdependencyandconstituencygrammarshave
beeninvogueincomputationallinguisticsatdifferenttimes).
American Structuralism saw a number of specific definitions of the immediate
constituent,couchedintermsoftheirsearchfora“discoveryprocedure”:amethod-
ologicalalgorithmfordescribingthesyntaxofalanguage. Ingeneral,theseattempt
to capture the intuition that “The primary criterion of the immediate constituent
isthedegreeinwhichcombinationsbehaveassimpleunits”(Bazell,1952/1966,p.
284).ThemostwellknownofthespecificdefinitionsisHarris’ideaofdistributional
similarity toindividual units, withthe substitutability test. Essentially, themethod
proceededbybreakingupaconstructionintoconstituentsbyattemptingtosubstitute
simplestructuresforpossibleconstituents—ifasubstitutionofasimpleform, say,
man,wassubstitutableinaconstructionforamorecomplexset(likeintenseyoung
man),thentheformintenseyoungmanwasprobablyaconstituent.Harris’stestwas
thebeginningoftheintuitionthataconstituentisakindofequivalenceclass.EXERCISES 379
Thefirstformalizationofthisideaofhierarchicalconstituencywasthephrase-
structure grammar defined in Chomsky (1956) and further expanded upon (and
arguedagainst)inChomsky(1957)andChomsky(1956/1975).ShortlyafterChom-
sky’s initial work, the context-free grammar was reinvented by Backus (1959) and
independently by Naur et al. (1960) in their descriptions of the ALGOL program-
ming language; Backus (1996) noted that he was influenced by the productions of
Emil Post and that Naur’s work was independent of his (Backus’) own. After this
earlywork,agreatnumberofcomputationalmodelsofnaturallanguageprocessing
werebasedoncontext-freegrammarsbecauseoftheearlydevelopmentofefficient
parsingalgorithms.
Dynamic programming parsing has a history of independent discovery. Ac-
cordingtothelateMartinKay(personalcommunication),adynamicprogramming
parser containing the roots of the CKY algorithm was first implemented by John
Cockein1960. Laterworkextendedandformalizedthealgorithm,aswellasprov-
ingitstimecomplexity(Kay1967,Younger1967,Kasami1965). Therelatedwell-
WFST formed substring table (WFST) seems to have been independently proposed by
Kuno(1965)asadatastructurethatstorestheresultsofallpreviouscomputations
in the course of the parse. Based on a generalization of Cocke’s work, a similar
datastructurehadbeenindependentlydescribedinKay(1967)(andKay1973).The
top-downapplicationofdynamicprogrammingtoparsingwasdescribedinEarley’s
Ph.D.dissertation(Earley1968,Earley1970). Sheil(1976)showedtheequivalence
oftheWFSTandtheEarleyalgorithm. Norvig(1991)showsthattheefficiencyof-
feredbydynamicprogrammingcanbecapturedinanylanguagewithamemoization
function(suchasinLISP)simplybywrappingthememoizationoperationarounda
simpletop-downparser.
Theearliestdisambiguationalgorithmsforparsingwerebasedonprobabilistic
probabilistic
context-free context-freegrammars,firstworkedoutbyBooth(1969)andSalomaa(1969);see
grammars
AppendixCformorehistory.Neuralmethodswerefirstappliedtoparsingataround
thesametimeasstatisticalparsingmethodsweredeveloped(Henderson,1994). In
theearliestworkneuralnetworkswereusedtoestimatesomeoftheprobabilitiesfor
statisticalconstituencyparsers(Henderson,2003,2004;EmamiandJelinek,2005)
. The next decades saw a wide variety of neural parsing algorithms, including re-
cursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models
(Vinyalsetal.,2015;ChoeandCharniak,2016),andtheideaoffocusingonspans
(Cross and Huang, 2016). For more on the span-based self-attention approach we
describeinthischapterseeSternetal.(2017),Gaddyetal.(2018),KitaevandKlein
(2018), and Kitaev et al. (2019). See Chapter 18 for the parallel history of neural
dependencyparsing.
TheclassicreferenceforparsingalgorithmsisAhoandUllman(1972);although
thefocusofthatbookisoncomputerlanguages,mostofthealgorithmshavebeen
appliedtonaturallanguage.
Exercises
17.1 Implementthealgorithmtoconvertarbitrarycontext-freegrammarstoCNF.
ApplyyourprogramtotheL grammar.
1
17.2 ImplementtheCKYalgorithmandtestitwithyourconvertedL 1grammar.380 CHAPTER17 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING
17.3 Rewrite the CKY algorithm given in Fig. 17.12 on page 370 so that it can
acceptgrammarsthatcontainunitproductions.
17.4 Discusshowtoaugmentaparsertodealwithinputthatmaybeincorrect,for
example,containingspellingerrorsormistakesarisingfromautomaticspeech
recognition.
17.5 Implement the PARSEVAL metrics described in Section 17.8. Next, use a
parserandatreebank, compareyourmetricsagainstastandardimplementa-
tion. Analyzetheerrorsinyourapproach.CHAPTER
18 Dependency Parsing
The focus of the last chapter was on context-free grammars and constituent-based
representations. Herewepresentanotherimportantfamilyofgrammarformalisms
dependency calleddependencygrammars.Independencyformalisms,phrasalconstituentsand
grammars
phrase-structurerulesdonotplayadirectrole. Instead,thesyntacticstructureofa
sentence is described solely in terms of directed binary grammatical relations be-
tweenthewords,asinthefollowingdependencyparse:
root
obj
det nmod
(18.1)
nsubj nmod case
I prefer the morning flight through Denver
Relationsamongthewordsareillustratedabovethesentencewithdirected,labeled
typed arcsfromheadstodependents.Wecallthisatypeddependencystructurebecause
dependency
the labels are drawn from a fixed inventory of grammatical relations. A root node
explicitlymarkstherootofthetree,theheadoftheentirestructure.
Figure 18.1 shows the same dependency analysis as a tree alongside its corre-
sponding phrase-structure analysis of the kind given in the prior chapter. Note the
absence of nodes corresponding to phrasal constituents or lexical categories in the
dependencyparse; theinternalstructureofthedependencyparseconsistssolelyof
directed relations between words. These head-dependent relationships directly en-
codeimportantinformationthatisoftenburiedinthemorecomplexphrase-structure
parses. Forexample,theargumentstotheverbpreferaredirectlylinkedtoitinthe
dependencystructure,whiletheirconnectiontothemainverbismoredistantinthe
phrase-structuretree. Similarly,morningandDenver,modifiersofflight,arelinked
to it directly in the dependency structure. This fact that the head-dependent rela-
tions are a good proxy for the semantic relationship between predicates and their
arguments is an important reason why dependency grammars are currently more
commonthanconstituencygrammarsinnaturallanguageprocessing.
Another major advantage of dependency grammars is their ability to deal with
freewordorder languagesthathavearelativelyfreewordorder.Forexample,wordorderinCzech
canbemuchmoreflexiblethaninEnglish;agrammaticalobjectmightoccurbefore
or after a location adverbial. A phrase-structure grammar would need a separate
ruleforeachpossibleplaceintheparsetreewheresuchanadverbialphrasecould
occur. Adependency-basedapproachcanhavejustonelinktyperepresentingthis
particularadverbialrelation;dependencygrammarapproachscanthusabstractaway
abitmorefromwordorderinformation.382 CHAPTER18 • DEPENDENCYPARSING
prefer S
I flight NP VP
the morning Denver Pro Verb NP
I prefer Det Nom
through the Nom PP
Nom Noun P NP
Noun flight through Pro
morning Denver
Figure18.1 DependencyandconstituentanalysesforIpreferthemorningflightthroughDenver.
Inthefollowingsections,we’llgiveaninventoryofrelationsusedindependency
parsing, discuss two families of parsing algorithms (transition-based, and graph-
based),anddiscussevaluation.
18.1 Dependency Relations
grammatical Thetraditionallinguisticnotionofgrammaticalrelationprovidesthebasisforthe
relation
binaryrelationsthatcomprisethesedependencystructures. Theargumentstothese
head relationsconsistofaheadandadependent. Theheadplaystheroleofthecentral
dependent organizingword,andthedependentasakindofmodifier. Thehead-dependentrela-
tionshipismadeexplicitbydirectlylinkingheadstothewordsthatareimmediately
dependentonthem.
Inadditiontospecifyingthehead-dependentpairs,dependencygrammarsallow
grammatical ustoclassifythekindsofgrammaticalrelations,orgrammaticalfunctionthatthe
function
dependent plays with respect to its head. These include familiar notions such as
subject, direct object and indirect object. In English these notions strongly corre-
late with, but by no means determine, both position in a sentence and constituent
type and are therefore somewhat redundant with the kind of information found in
phrase-structure trees. However, in languages with more flexible word order, the
informationencodeddirectlyinthesegrammaticalrelationsiscriticalsincephrase-
basedconstituentsyntaxprovideslittlehelp.
Linguistshavedevelopedtaxonomiesofrelationsthatgowellbeyondthefamil-
iarnotionsofsubjectandobject. Whilethereisconsiderablevariationfromtheory
to theory, there is enough commonality that cross-linguistic standards have been
Universal developed. The Universal Dependencies (UD) project (de Marneffe et al., 2021),
Dependencies
an open community effort to annotate dependencies and other aspects of grammar
acrossmorethan100languages,providesaninventoryof37dependencyrelations.18.1 • DEPENDENCYRELATIONS 383
ClausalArgumentRelations Description
NSUBJ Nominalsubject
OBJ Directobject
IOBJ Indirectobject
CCOMP Clausalcomplement
NominalModifierRelations Description
NMOD Nominalmodifier
AMOD Adjectivalmodifier
NUMMOD Numericmodifier
APPOS Appositionalmodifier
DET Determiner
CASE Prepositions,postpositionsandothercasemarkers
OtherNotableRelations Description
CONJ Conjunct
CC Coordinatingconjunction
Figure18.2 SomeoftheUniversalDependencyrelations(deMarneffeetal.,2021).
Relation Exampleswithheadanddependent
NSUBJ Unitedcanceledtheflight.
OBJ UniteddivertedtheflighttoReno.
WebookedherthefirstflighttoMiami.
IOBJ WebookedhertheflighttoMiami.
NMOD Wetookthemorningflight.
AMOD Bookthecheapestflight.
NUMMOD BeforethestormJetBluecanceled1000flights.
APPOS United,aunitofUAL,matchedthefares.
DET Theflightwascanceled.
Whichflightwasdelayed?
CONJ WeflewtoDenveranddrovetoSteamboat.
CC WeflewtoDenveranddrovetoSteamboat.
CASE BooktheflightthroughHouston.
Figure18.3 ExamplesofsomeUniversalDependencyrelations.
Fig.18.2showsasubsetoftheUDrelationsandFig.18.3providessomeexamples.
The motivation for all of the relations in the Universal Dependency scheme is
beyondthescopeofthischapter,butthecoresetoffrequentlyusedrelationscanbe
brokenintotwosets: clausalrelationsthatdescribesyntacticroleswithrespecttoa
predicate(oftenaverb), andmodifierrelationsthatcategorizethewaysthatwords
canmodifytheirheads.
Consider,forexample,thefollowingsentence:
root
obj
det nmod
(18.2)
nsubj nmod case
United canceled the morning flights to Houston
HeretheclausalrelationsNSUBJandDOBJidentifythesubjectanddirectobjectof
thepredicatecancel,whiletheNMOD,DET,andCASErelationsdenotemodifiersof
thenounsflightsandHouston.384 CHAPTER18 • DEPENDENCYPARSING
18.1.1 DependencyFormalisms
AdependencystructurecanberepresentedasadirectedgraphG=(V,A),consisting
ofasetofverticesV,andasetoforderedpairsofverticesA,whichwe’llcallarcs.
Forthemostpartwewillassumethatthesetofvertices,V,correspondsexactly
to the set of words in a given sentence. However, they might also correspond to
punctuation, or when dealing with morphologically complex languages the set of
vertices might consist of stems and affixes. The set of arcs, A, captures the head-
dependentandgrammaticalfunctionrelationshipsbetweentheelementsinV.
Different grammatical theories or formalisms may place further constraints on
thesedependencystructures.Amongthemorefrequentrestrictionsarethatthestruc-
turesmustbeconnected,haveadesignatedrootnode,andbeacyclicorplanar. Of
most relevanceto theparsing approaches discussedin thischapter isthe common,
dependency computationally-motivated, restriction to rooted trees. That is, a dependency tree
tree
isadirectedgraphthatsatisfiesthefollowingconstraints:
1. Thereisasingledesignatedrootnodethathasnoincomingarcs.
2. Withtheexceptionoftherootnode,eachvertexhasexactlyoneincomingarc.
3. ThereisauniquepathfromtherootnodetoeachvertexinV.
Takentogether,theseconstraintsensurethateachwordhasasinglehead,thatthe
dependencystructureisconnected, andthatthereisasinglerootnodefromwhich
onecanfollowauniquedirectedpathtoeachofthewordsinthesentence.
18.1.2 Projectivity
Thenotionofprojectivityimposesanadditionalconstraintthatisderivedfromthe
order of the words in the input. An arc from a head to a dependent is said to be
projective projectiveifthereisapathfromtheheadtoeverywordthatliesbetweenthehead
andthedependentinthesentence. Adependencytreeisthensaidtobeprojectiveif
allthearcsthatmakeitupareprojective. Allthedependencytreeswe’veseenthus
farhavebeenprojective. Thereare,however,manyvalidconstructionswhichlead
tonon-projectivetrees,particularlyinlanguageswithrelativelyflexiblewordorder.
Considerthefollowingexample.
root mod
nmod
obj mod
(18.3)
nsubj det det case adv
JetBlue canceled our flight this morning which was already late
Inthisexample,thearcfromflighttoitsmodifierwasisnon-projectivesincethere
isnopathfromflighttotheinterveningwordsthisandmorning.Aswecanseefrom
this diagram, projectivity (and non-projectivity) can be detected in the way we’ve
been drawing our trees. A dependency tree is projective if it can be drawn with
nocrossingedges. Herethereisnowaytolinkflighttoitsdependentwaswithout
crossingthearcthatlinksmorningtoitshead.
Our concern with projectivity arises from two related issues. First, the most
widelyusedEnglishdependencytreebankswereautomaticallyderivedfromphrase-
structuretreebanksthroughtheuseofhead-findingrules.Thetreesgeneratedinsuch
afashionwillalwaysbeprojective,andhencewillbeincorrectwhennon-projective
exampleslikethisoneareencountered.18.1 • DEPENDENCYRELATIONS 385
Second,therearecomputationallimitationstothemostwidelyusedfamiliesof
parsingalgorithms. Thetransition-basedapproachesdiscussedinSection18.2can
only produce projective trees, hence any sentences with non-projective structures
will necessarily contain some errors. This limitation is one of the motivations for
themoreflexiblegraph-basedparsingapproachdescribedinSection18.3.
18.1.3 DependencyTreebanks
Treebanks play a critical role in the development and evaluation of dependency
parsers. Theyareusedfortrainingparsers,theyactasthegoldlabelsforevaluating
parsers,andtheyalsoprovideusefulinformationforcorpuslinguisticsstudies.
Dependencytreebanksarecreatedbyhavinghumanannotatorsdirectlygenerate
dependency structures for a given corpus, or by hand-correcting the output of an
automatic parser. A few early treebanks were also based on using a deterministic
processtotranslateexistingconstituent-basedtreebanksintodependencytrees.
ThelargestopencommunityprojectforbuildingdependencytreesistheUniver-
salDependenciesprojectathttps://universaldependencies.org/introduced
above,whichcurrentlyhasalmost200dependencytreebanksinmorethan100lan-
guages(deMarneffeetal.,2021).HereareafewUDexamplesshowingdependency
treesforsentencesinSpanish,Basque,andChinese:
punct
obl:tmod
obl
case case
det det
VERB ADP DET NOUN ADP DET NUM PUNCT
Subiremos a el tren a las cinco .
we-will-board on the train at the five .
Subiremosaltrenalascinco. “Wewillbeboardingthetrainatfive.” (18.4)
nsubj punct
obj aux
NOUN NOUN VERB AUX PUNCT
Ekaitzak itsasontzia hondoratu du .
storm(Erg.) ship(Abs.) sunk has .
Ekaitzakitsasontziahondoratudu. “Thestormhassunktheship.” (18.5)
adv
nsubj
obj:tmod obj
advmod compound:vv
ADV PRON NOUN ADV VERB VERB NOUN
但 我 昨天 才 收 到 信
but I yesterday only-then receive arrive letter .
但我昨天才收到信 “ButIdidn’treceivetheletteruntilyesterday” (18.6)386 CHAPTER18 • DEPENDENCYPARSING
18.2 Transition-Based Dependency Parsing
transition-based Our first approach to dependency parsing is called transition-based parsing. This
architecture draws on shift-reduce parsing, a paradigm originally developed for
analyzing programming languages (Aho and Ullman, 1972). In transition-based
parsing we’ll have a stack on which we build the parse, a buffer of tokens to be
parsed,andaparserwhichtakesactionsontheparseviaapredictorcalledanoracle,
asillustratedinFig.18.4.
Input buffer
w1 w2 wn
s1 Dependency
s2 Parser Action LEFTARC Relations
Stack ... Oracle RIGHTARC
w3 w2
SHIFT
sn
Figure18.4 Basictransition-basedparser.Theparserexaminesthetoptwoelementsofthe
stackandselectsanactionbyconsultinganoraclethatexaminesthecurrentconfiguration.
The parser walks through the sentenceleft-to-right, successively shifting items
fromthebufferontothestack. Ateachtimepointweexaminethetoptwoelements
onthestack,andtheoraclemakesadecisionaboutwhattransitiontoapplytobuild
theparse. Thepossibletransitionscorrespondtotheintuitiveactionsonemighttake
increatingadependencytreebyexaminingthewordsinasinglepassovertheinput
fromlefttoright(Covington,2001):
• Assignthecurrentwordastheheadofsomepreviouslyseenword,
• Assignsomepreviouslyseenwordastheheadofthecurrentword,
• Postponedealingwiththecurrentword,storingitforlaterprocessing.
We’ll formalize this intuition with the following three transition operators that
willoperateonthetoptwoelementsofthestack:
• LEFTARC: Assert a head-dependent relation between the word at the top of
thestackandthesecondword;removethesecondwordfromthestack.
• RIGHTARC: Assert a head-dependent relation between the second word on
thestackandthewordatthetop;removethetopwordfromthestack;
• SHIFT: Removethewordfromthefrontoftheinputbufferandpushitonto
thestack.
We’llsometimescalloperationslike LEFTARC and RIGHTARC reduceoperations,
based on a metaphor from shift-reduce parsing, in which reducing means combin-
ing elements on the stack. There are some preconditions for using operators. The
LEFTARCoperatorcannotbeappliedwhenROOTisthesecondelementofthestack
(since bydefinition the ROOT node cannothave any incomingarcs). And both the
LEFTARC and RIGHTARC operators require two elements to be on the stack to be
applied.
arcstandard Thisparticularsetofoperatorsimplementswhatisknownasthearcstandard
approachtotransition-basedparsing(Covington2001,Nivre2003). Inarcstandard18.2 • TRANSITION-BASEDDEPENDENCYPARSING 387
parsingthetransitionoperatorsonlyassertrelationsbetweenelementsatthetopof
the stack, and once an element has been assigned its head it is removed from the
stack and is not available for further processing. As we’ll see, there are alterna-
tive transition systems which demonstrate different parsing behaviors, but the arc
standardapproachisquiteeffectiveandissimpletoimplement.
The specification of a transition-based parser is quite simple, based on repre-
configuration sentingthecurrentstateoftheparseasaconfiguration: thestack, aninputbuffer
of words or tokens, and a set of relations representing a dependency tree. Parsing
means making a sequence of transitions through the space of possible configura-
tions. We start with an initial configuration in which the stack contains the ROOT
node,thebufferhasthetokensinthesentence,andanemptysetofrelationsrepre-
sentstheparse. Inthefinalgoalstate,thestackandthewordlistshouldbeempty,
andthesetofrelationswillrepresentthefinalparse. Fig.18.5givesthealgorithm.
functionDEPENDENCYPARSE(words)returnsdependencytree
state [root],[words],[] ;initialconfiguration
←{ }
whilestatenotfinal
t ORACLE(state) ;chooseatransitionoperatortoapply
←
state APPLY(t,state) ;applyit,creatinganewstate
←
returnstate
Figure18.5 Agenerictransition-baseddependencyparser
Ateachstep,theparserconsultsanoracle(we’llcomebacktothisshortly)that
providesthecorrecttransitionoperatortousegiventhecurrentconfiguration.Itthen
applies that operator to the current configuration, producing a new configuration.
The process ends when all the words in the sentence have been consumed and the
ROOTnodeistheonlyelementremainingonthestack.
Theefficiencyoftransition-basedparsersshouldbeapparentfromthealgorithm.
The complexity is linear in the length of the sentence since it is based on a single
lefttorightpassthroughthewordsinthesentence. (Eachwordmustfirstbeshifted
ontothestackandthenlaterreduced.)
Note that unlike the dynamic programming and search-based approaches dis-
cussedinChapter17,thisapproachisastraightforwardgreedyalgorithm—theor-
acleprovidesasinglechoiceateachstepandtheparserproceedswiththatchoice,
no other options are explored, no backtracking is employed, and a single parse is
returnedintheend.
Figure18.6illustratestheoperationoftheparserwiththesequenceoftransitions
leadingtoaparseforthefollowingexample.
root
obj
det
(18.7)
iobj nmod
Book me the morning flight
Let’sconsiderthestateoftheconfigurationatStep2,afterthewordmehasbeen
pushedontothestack.388 CHAPTER18 • DEPENDENCYPARSING
Stack WordList Relations
[root,book,me] [the,morning,flight]
ThecorrectoperatortoapplyhereisRIGHTARCwhichassignsbookastheheadof
meandpopsmefromthestackresultinginthefollowingconfiguration.
Stack WordList Relations
[root,book] [the,morning,flight] (book me)
→
AfterseveralsubsequentapplicationsoftheSHIFTandLEFTARCoperators,thecon-
figurationinStep6lookslikethefollowing:
Stack WordList Relations
[root,book,the,morning,flight] [] (book me)
→
Here, all the remaining words have been passed onto the stack and all that is left
todoistoapplytheappropriatereduceoperators. Inthecurrentconfiguration,we
employtheLEFTARCoperatorresultinginthefollowingstate.
Stack WordList Relations
[root,book,the,flight] [] (book me)
→
(morning flight)
←
Atthispoint,theparseforthissentenceconsistsofthefollowingstructure.
iobj nmod
(18.8)
Book me the morning flight
There are several important things to note when examining sequences such as
theoneinFigure18.6. First,thesequencegivenisnottheonlyonethatmightlead
toareasonableparse. Ingeneral,theremaybemorethanonepaththatleadstothe
sameresult,andduetoambiguity,theremaybeothertransitionsequencesthatlead
todifferentequallyvalidparses.
Second, we are assuming that the oracle always provides the correct operator
at each point in the parse—an assumption that is unlikely to be true in practice.
Asaresult,giventhegreedynatureofthisalgorithm,incorrectchoiceswillleadto
incorrectparsessincetheparserhasnoopportunitytogobackandpursuealternative
choices. Section18.2.4willintroduceseveraltechniquesthatallowtransition-based
approachestoexplorethesearchspacemorefully.
Finally, for simplicity, we have illustrated this example without the labels on
thedependencyrelations. Toproducelabeledtrees,wecanparameterizetheLEFT-
ARC and RIGHTARC operatorswithdependencylabels,asin LEFTARC(NSUBJ) or
RIGHTARC(OBJ).Thisisequivalenttoexpandingthesetoftransitionoperatorsfrom
ouroriginalsetofthreetoasetthatincludesLEFTARCandRIGHTARCoperatorsfor
each relation in the set of dependency relations being used, plus an additional one
for the SHIFT operator. This, of course, makes the job of the oracle more difficult
sinceitnowhasamuchlargersetofoperatorsfromwhichtochoose.
18.2.1 CreatinganOracle
Theoracleforgreedilyselectingtheappropriatetransitionistrainedbysupervised
machinelearning. Aswithallsupervisedmachinelearningmethods, wewillneed18.2 • TRANSITION-BASEDDEPENDENCYPARSING 389
Step Stack WordList Action RelationAdded
0 [root] [book,me,the,morning,flight] SHIFT
1 [root,book] [me,the,morning,flight] SHIFT
2 [root,book,me] [the,morning,flight] RIGHTARC (book me)
→
3 [root,book] [the,morning,flight] SHIFT
4 [root,book,the] [morning,flight] SHIFT
5 [root,book,the,morning] [flight] SHIFT
6 [root,book,the,morning,flight] [] LEFTARC (morning flight)
←
7 [root,book,the,flight] [] LEFTARC (the flight)
←
8 [root,book,flight] [] RIGHTARC (book flight)
→
9 [root,book] [] RIGHTARC (root book)
→
10 [root] [] Done
Figure18.6 Traceofatransition-basedparse.
training data: configurations annotated with the correct transition to take. We can
draw these from dependency trees. And we need to extract features of the con-
figuration. We’ll introduce neural classifiers that represent the configuration via
embeddings,aswellasclassicsystemsthatusehand-designedfeatures.
GeneratingTrainingData
TheoraclefromthealgorithminFig.18.5takesasinputaconfigurationandreturnsa
transitionoperator.Therefore,totrainaclassifier,wewillneedconfigurationspaired
with transition operators (i.e., LEFTARC, RIGHTARC, or SHIFT). Unfortunately,
treebanks pair entire sentences with their corresponding trees, not configurations
withtransitions.
Togeneratetherequiredtrainingdata,weemploytheoracle-basedparsingalgo-
rithminacleverway. Wesupplyouroraclewiththetrainingsentencestobeparsed
alongwiththeircorrespondingreferenceparsesfromthetreebank.Toproducetrain-
inginstances,wethensimulatetheoperationoftheparserbyrunningthealgorithm
trainingoracle andrelyingonanewtrainingoracletogiveuscorrecttransitionoperatorsforeach
successiveconfiguration.
Toseehowthisworks,let’sfirstreviewtheoperationofourparser.Itbeginswith
adefaultinitialconfigurationwherethestackcontainstheROOT,theinputlistisjust
the list of words, and the set of relations is empty. The LEFTARC and RIGHTARC
operatorseachaddrelationsbetweenthewordsatthetopofthestacktothesetof
relations being accumulated for a given sentence. Since we have a gold-standard
referenceparseforeachtrainingsentence,weknowwhichdependencyrelationsare
valid for a given sentence. Therefore, we can use the reference parse to guide the
selectionofoperatorsastheparserstepsthroughasequenceofconfigurations.
To be more precise, given a reference parse and a configuration, the training
oracleproceedsasfollows:
• Choose LEFTARC if it produces a correct head-dependent relation given the
referenceparseandthecurrentconfiguration,
• Otherwise,chooseRIGHTARCif(1)itproducesacorrecthead-dependentre-
lation given the reference parse and (2) all of the dependents of the word at
thetopofthestackhavealreadybeenassigned,
• Otherwise,chooseSHIFT.
The restriction on selecting the RIGHTARC operator is needed to ensure that a
wordisnotpoppedfromthestack,andthuslosttofurtherprocessing,beforeallits
dependentshavebeenassignedtoit.390 CHAPTER18 • DEPENDENCYPARSING
Step Stack WordList PredictedAction
0 [root] [book,the,flight,through,houston] SHIFT
1 [root,book] [the,flight,through,houston] SHIFT
2 [root,book,the] [flight,through,houston] SHIFT
3 [root,book,the,flight] [through,houston] LEFTARC
4 [root,book,flight] [through,houston] SHIFT
5 [root,book,flight,through] [houston] SHIFT
6 [root,book,flight,through,houston] [] LEFTARC
7 [root,book,flight,houston] [] RIGHTARC
8 [root,book,flight] [] RIGHTARC
9 [root,book] [] RIGHTARC
10 [root] [] Done
Figure18.7 Generatingtrainingitemsconsistingofconfiguration/predictedactionpairsbysimulatingaparse
withagivenreferenceparse.
Moreformally,duringtrainingtheoraclehasaccesstothefollowing:
• AcurrentconfigurationwithastackSandasetofdependencyrelationsR
c
• A reference parse consisting of a set of verticesV and a set of dependency
relationsR
p
Giventhisinformation,theoraclechoosestransitionsasfollows:
LEFTARC(r): if(S 1rS 2) R
p
∈
RIGHTARC(r): if(S 2rS 1) R pand r (cid:48),ws.t.(S 1r (cid:48)w) R pthen(S 1r (cid:48)w) R c
∈ ∀ ∈ ∈
SHIFT: otherwise
Let’swalkthroughtheprocessingofthefollowingexampleasshowninFig.18.7.
root
obj nmod
(18.9)
det case
Book the flight through Houston
At Step 1, LEFTARC is not applicable in the initial configuration since it asserts
a relation, (root book), not in the reference answer; RIGHTARC does assert a
←
relation contained in the final answer (root book), however book has not been
→
attachedtoanyofitsdependentsyet,sowehavetodefer,leavingSHIFTastheonly
possibleaction.Thesameconditionsholdinthenexttwosteps.Instep3,LEFTARC
isselectedtolinkthetoitshead.
NowconsiderthesituationinStep4.
Stack Wordbuffer Relations
[root,book,flight] [through,Houston] (the flight)
←
Here, we might be tempted to add a dependency relation between book and flight,
which is present in the reference parse. But doing so now would prevent the later
attachment of Houston since flight would have been removed from the stack. For-
tunately, the precondition on choosing RIGHTARC prevents this choice and we’re
againleftwithSHIFTastheonlyviableoption. Theremainingchoicescompletethe
setofoperatorsneededforthisexample.18.2 • TRANSITION-BASEDDEPENDENCYPARSING 391
To recap, we derive appropriate training instances consisting of configuration-
transitionpairsfromatreebankbysimulatingtheoperationofaparserinthecon-
textofareferencedependencytree. Wecandeterministicallyrecordcorrectparser
actionsateachstepasweprogressthrougheachtrainingexample,therebycreating
thetrainingsetwerequire.
18.2.2 Afeature-basedclassifier
We’llnowintroducetwoclassifiersforchoosingtransitions,hereaclassicfeature-
basedalgorithmandinthenextsectionaneuralclassifierusingembeddingfeatures.
Featured-basedclassifiersgenerallyusethesamefeatureswe’veseenwithpart-
of-speech tagging and partial parsing: Word forms, lemmas, parts of speech, the
head, and the dependency relation to the head. Other features may be relevant for
somelanguages,forexamplemorphosyntacticfeatureslikecasemarkingonsubjects
orobjects.Thefeaturesareextractedfromthetrainingconfigurations,whichconsist
of the stack, the buffer and the current set of relations. Most useful are features
referencingthetoplevelsofthestack,thewordsnearthefrontofthebuffer,andthe
dependencyrelationsalreadyassociatedwithanyofthoseelements.
feature Wecanusethefeaturetemplatethatweintroducedforsentimentanalysisand
template
part-of-speech tagging. Feature templates allow us to automatically generate large
numbersofspecificfeaturesfromatrainingset.Forexample,considerthefollowing
featuretemplatesthatarebasedonsinglepositionsinaconfiguration.
s .w,op , s .w,op s .t,op , s .t,op
1 2 1 2
(cid:104) (cid:105) (cid:104) (cid:105)(cid:104) (cid:105) (cid:104) (cid:105)
b 1.w,op , b 1.t,op s 1.wt,op (18.10)
(cid:104) (cid:105) (cid:104) (cid:105)(cid:104) (cid:105)
Here features are denoted as location.property, where s = stack, b = the word
buffer,w=wordforms,t =part-of-speech,andop=operator. Thusthefeaturefor
the word form at the top of the stack would be s .w, the part of speech tag at the
1
frontofthebufferb .t,andtheconcatenatedfeatures .wt representsthewordform
1 1
concatenated with the part of speech of the word at the top of the stack. Consider
applyingthesetemplatestothefollowingintermediateconfigurationderivedfroma
trainingoracleforExercise18.18.2.
Stack Wordbuffer Relations
[root,canceled,flights] [toHouston] (canceled United)
→
(flights morning)
→
(flights the)
→
ThecorrecttransitionhereisSHIFT(youshouldconvinceyourselfofthisbefore
proceeding). The application of our set of feature templates to this configuration
wouldresultinthefollowingsetofinstantiatedfeatures.
s 1.w=flights,op=shift (18.11)
(cid:104) (cid:105)
s .w=canceled,op=shift
2
(cid:104) (cid:105)
s .t=NNS,op=shift
1
(cid:104) (cid:105)
s .t=VBD,op=shift
2
(cid:104) (cid:105)
b .w=to,op=shift
1
(cid:104) (cid:105)
b .t=TO,op=shift
1
(cid:104) (cid:105)
s .wt=flightsNNS,op=shift
1
(cid:104) (cid:105)392 CHAPTER18 • DEPENDENCYPARSING
Given that the left and right arc transitions operate on the top two elements of the
stack, features that combine properties from these positions are even more useful.
Forexample,afeaturelikes .t s .t concatenatesthepartofspeechtagoftheword
1 2
◦
atthetopofthestackwiththetagofthewordbeneathit.
s 1.t s 2.t=NNSVBD,op=shift (18.12)
(cid:104) ◦ (cid:105)
Giventhetrainingdataandfeatures,anyclassifier,likemultinomiallogisticre-
gressionorsupportvectormachines,canbeused.
18.2.3 Aneuralclassifier
Theoraclecanalsobeimplementedbyaneuralclassifier. Astandardarchitecture
issimplytopassthesentencethroughanencoder,thentakethepresentationofthe
top 2 words on the stack and the first word of the buffer, concatenate them, and
present to a feedforward network that predicts the transition to take (Kiperwasser
andGoldberg,2016;Kulmizevetal.,2019).Fig.18.8sketchesthismodel.Learning
canbedonewithcross-entropyloss.
Input buffer
Parser Oracle
w …
w e(w) Dependency
Action
Relations
s1 s1 e(s1) FFN LEFTARC
Stack s2 s2 e(s2) SR HIG IFH TTARC w3 w2
...
ENCODER
w1 w2 w3 w4 w5 w6
Figure18.8 Neuralclassifierfortheoracleforthetransition-basedparser.Theparsertakes
thetop2wordsonthestackandthefirstwordofthebuffer,representsthembytheirencodings
(from running the whole sentence through the encoder), concatenates the embeddings and
passesthroughasoftmaxtochooseaparseraction(transition).
18.2.4 AdvancedMethodsinTransition-BasedParsing
The basic transition-based approach can be elaborated in a number of ways to im-
proveperformancebyaddressingsomeofthemostobviousflawsintheapproach.
AlternativeTransitionSystems
Thearc-standardtransitionsystemdescribedaboveisonlyoneofmanypossiblesys-
arceager tems. Afrequentlyusedalternativeisthearceagertransitionsystem. Thearceager
approach gets its name from its ability to assert rightward relations much sooner
thaninthearcstandardapproach. Toseethis,let’srevisitthearcstandardtraceof
Example18.9,repeatedhere.
Softmax18.2 • TRANSITION-BASEDDEPENDENCYPARSING 393
Step Stack WordList Action RelationAdded
0 [root] [book,the,flight,through,houston] RIGHTARC (root book)
→
1 [root,book] [the,flight,through,houston] SHIFT
2 [root,book,the] [flight,through,houston] LEFTARC (the flight)
←
3 [root,book] [flight,through,houston] RIGHTARC (book flight)
→
4 [root,book,flight] [through,houston] SHIFT
5 [root,book,flight,through] [houston] LEFTARC (through houston)
←
6 [root,book,flight] [houston] RIGHTARC (flight houston)
→
7 [root,book,flight,houston] [] REDUCE
8 [root,book,flight] [] REDUCE
9 [root,book] [] REDUCE
10 [root] [] Done
Figure18.9 AprocessingtraceofBooktheflightthroughHoustonusingthearc-eagertransitionoperators.
root
obj nmod
det case
Book the flight through Houston
Consider the dependency relation between book and flight in this analysis. As
isshowninFig.18.7,anarc-standardapproachwouldassertthisrelationatStep8,
despitethefactthatbookandflightfirstcometogetheronthestackmuchearlierat
Step4. Thereasonthisrelationcan’tbecapturedatthispointisduetothepresence
ofthepostnominalmodifierthroughHouston. Inanarc-standardapproach,depen-
dentsareremovedfromthestackassoonastheyareassignedtheirheads. Ifflight
hadbeenassignedbookasitsheadinStep4,itwouldnolongerbeavailabletoserve
astheheadofHouston.
Whilethisdelaydoesn’tcauseanyissuesinthisexample,ingeneralthelonger
a word has to wait to get assigned its head the more opportunities there are for
somethingtogoawry. Thearc-eagersystemaddressesthisissuebyallowingwords
to be attached to their heads as early as possible, before all the subsequent words
dependentonthemhavebeenseen. Thisisaccomplishedthroughminorchangesto
theLEFTARCandRIGHTARCoperatorsandtheadditionofanewREDUCEoperator.
• LEFTARC: Assertahead-dependentrelationbetweenthewordatthefrontof
theinputbufferandthewordatthetopofthestack;popthestack.
• RIGHTARC: Assertahead-dependentrelationbetweenthewordonthetopof
the stack and the word at the front of the input buffer; shift the word at the
frontoftheinputbuffertothestack.
• SHIFT: Removethewordfromthefrontoftheinputbufferandpushitonto
thestack.
• REDUCE: Popthestack.
The LEFTARC and RIGHTARC operatorsareappliedtothetopofthestackand
the front of the input buffer, instead of the top two elements of the stack as in the
arc-standard approach. The RIGHTARC operator now moves the dependent to the
stackfromthebufferratherthanremovingit,thusmakingitavailabletoserveasthe
headoffollowingwords. ThenewREDUCEoperatorremovesthetopelementfrom
thestack. Togetherthesechangespermitawordtobeeagerlyassigneditsheadand
stillallowittoserveastheheadforlaterdependents. ThetraceshowninFig.18.9
illustratesthenewdecisionsequenceforthisexample.
Inadditiontodemonstratingthearc-eagertransitionsystem,thisexampledemon-394 CHAPTER18 • DEPENDENCYPARSING
strates the power and flexibility of the overall transition-based approach. We were
abletoswapinanewtransitionsystemwithouthavingtomakeanychangestothe
underlying parsing algorithm. This flexibility has led to the development of a di-
versesetoftransitionsystemsthataddressdifferentaspectsofsyntaxandsemantics
including: assigning part of speech tags (Choi and Palmer, 2011a), allowing the
generationofnon-projectivedependencystructures(Nivre,2009),assigningseman-
ticroles(ChoiandPalmer,2011b),andparsingtextscontainingmultiplelanguages
(Bhatetal.,2017).
BeamSearch
Thecomputationalefficiencyofthetransition-basedapproachdiscussedearlierde-
rivesfromthefactthatitmakesasinglepassthroughthesentence,greedilymaking
decisionswithoutconsideringalternatives. Ofcourse,thisisalsoaweakness–once
a decision has been made it can not be undone, even in the face of overwhelming
beamsearch evidencearrivinglaterinasentence. Wecanusebeamsearchtoexplorealternative
decision sequences. Recall from Chapter 10 that beam search uses a breadth-first
searchstrategywithaheuristicfilterthatprunesthesearchfrontiertostaywithina
beamwidth fixed-sizebeamwidth.
In applying beam search to transition-based parsing, we’ll elaborate on the al-
gorithm given in Fig. 18.5. Instead of choosing the single best transition operator
ateachiteration,we’llapplyallapplicableoperatorstoeachstateonanagendaand
then score the resulting configurations. We then add each of these new configura-
tions to the frontier, subject to the constraint that there has to be room within the
beam. Aslongasthesizeoftheagendaiswithinthespecifiedbeamwidth,wecan
addnewconfigurationstotheagenda. Oncetheagendareachesthelimit, weonly
add new configurations that are better than the worst configuration on the agenda
(removingtheworstelementsothatwestaywithinthelimit). Finally,toinsurethat
weretrievethebestpossiblestateontheagenda,thewhileloopcontinuesaslongas
therearenon-finalstatesontheagenda.
Thebeamsearchapproachrequiresamoreelaboratenotionofscoringthanwe
used with the greedy algorithm. There, we assumed that the oracle would be a
supervisedclassifierthatchosethebesttransitionoperatorbasedonfeaturesofthe
current configuration. This choice can be viewed as assigning a score to all the
possibletransitionsandpickingthebestone.
Tˆ(c)=argmaxScore(t,c)
With beam search we are now searching through the space of decision sequences,
so it makes sense to base the score for a configuration on its entire history. So we
candefinethescoreforanewconfigurationasthescoreofitspredecessorplusthe
scoreoftheoperatorusedtoproduceit.
ConfigScore(c ) = 0.0
0
ConfigScore(c) = ConfigScore(c )+Score(t,c )
i i 1 i i 1
− −
Thisscoreisusedbothinfilteringtheagendaandinselectingthefinalanswer. The
newbeamsearchversionoftransition-basedparsingisgiveninFig.18.10.18.3 • GRAPH-BASEDDEPENDENCYPARSING 395
functionDEPENDENCYBEAMPARSE(words,width)returnsdependencytree
state [root],[words],[],0.0 ;initialconfiguration
←{ }
agenda state ;initialagenda
←(cid:104) (cid:105)
whileagendacontainsnon-finalstates
newagenda
←(cid:104)(cid:105)
foreachstate agendado
∈
forall t t VALIDOPERATORS(state) do
{ | ∈ }
child APPLY(t,state)
←
newagenda ADDTOBEAM(child,newagenda,width)
←
agenda newagenda
←
returnBESTOF(agenda)
functionADDTOBEAM(state,agenda,width)returnsupdatedagenda
ifLENGTH(agenda)<widththen
agenda INSERT(state,agenda)
←
elseifSCORE(state)>SCORE(WORSTOF(agenda))
agenda REMOVE(WORSTOF(agenda))
←
agenda INSERT(state,agenda)
←
returnagenda
Figure18.10 Beamsearchappliedtotransition-baseddependencyparsing.
18.3 Graph-Based Dependency Parsing
Graph-basedmethodsarethesecondimportantfamilyofdependencyparsingalgo-
rithms. Graph-basedparsersaremoreaccuratethantransition-basedparsers,espe-
ciallyonlongsentences;transition-basedmethodshavetroublewhentheheadsare
very far from the dependents (McDonald and Nivre, 2011). Graph-based methods
avoid this difficulty by scoring entire trees, rather than relying on greedy local de-
cisions. Furthermore, unlike transition-based approaches, graph-based parsers can
produce non-projective trees. Although projectivity is not a significant issue for
English,itisdefinitelyaproblemformanyoftheworld’slanguages.
Graph-baseddependencyparserssearchthroughthespaceofpossibletreesfora
givensentenceforatree(ortrees)thatmaximizesomescore.Thesemethodsencode
the search space as directed graphs and employ methods drawn from graph theory
tosearchthespaceforoptimalsolutions. Moreformally, givenasentenceS we’re
looking for the best dependency tree in G , the space of all possible trees for that
s
sentence,thatmaximizessomescore.
Tˆ(S)=argmaxScore(t,S)
t ∈G S
edge-factored We’ll make the simplifying assumption that this score can be edge-factored,
meaningthattheoverallscoreforatreeisthesumofthescoresofeachofthescores
oftheedgesthatcomprisethetree.
Score(t,S)= Score(e)
e t
(cid:88)∈
Graph-based algorithms have to solve two problems: (1) assigning a score to
eachedge,and(2)findingthebestparsetreegiventhescoresofallpotentialedges.396 CHAPTER18 • DEPENDENCYPARSING
Inthenextfewsectionswe’llintroducesolutionstothesetwoproblems,beginning
with the second problem of finding trees, and then giving a feature-based and a
neuralalgorithmforsolvingthefirstproblemofassigningscores.
18.3.1 Parsingviafindingthemaximumspanningtree
Ingraph-basedparsing,givenasentenceSwestartbycreatingagraphGwhichisa
fully-connected,weighted,directedgraphwheretheverticesaretheinputwordsand
thedirectededgesrepresentallpossiblehead-dependentassignments. We’llinclude
an additional ROOT node with outgoing edges directed at all of the other vertices.
The weights of each edge in G reflect the score for each possible head-dependent
relationassignedbysomescoringalgorithm.
ItturnsoutthatfindingthebestdependencyparseforSisequivalenttofinding
maximum the maximum spanning tree over G. A spanning tree over a graph G is a subset
spanningtree
ofGthatisatreeandcoversalltheverticesinG;aspanningtreeoverGthatstarts
fromtheROOTisavalidparseofS. Amaximumspanningtreeisthespanningtree
with the highest score. Thus a maximum spanning tree of G emanating from the
ROOTistheoptimaldependencyparseforthesentence.
AdirectedgraphfortheexampleBookthatflightisshowninFig.18.11,withthe
maximumspanningtreecorrespondingtothedesiredparseshowninblue. Forease
ofexposition,we’lldescribeherethealgorithmforunlabeleddependencyparsing.
4
4
12
5 8
root Book that flight
6 7
7
5
Figure18.11 Initialrooted,directedgraphforBookthatflight.
Before describing the algorithm it’s useful to consider two intuitions about di-
rected graphs and their spanning trees. The first intuition begins with the fact that
everyvertexinaspanningtreehasexactlyoneincomingedge. Itfollowsfromthis
that every connected component of a spanning tree (i.e., every set of vertices that
are linked to each other by paths over edges) will also have one incoming edge.
The second intuition is that the absolute values of the edge scores are not critical
todeterminingitsmaximumspanningtree. Instead,itistherelativeweightsofthe
edges entering each vertex that matters. If we were to subtract a constant amount
from each edge entering a given vertex it would have no impact on the choice of
the maximum spanning tree since every possible spanning tree would decrease by
exactlythesameamount.
The first step of the algorithm itself is quite straightforward. For each vertex
inthegraph, anincomingedge(representingapossibleheadassignment)withthe
highestscoreischosen. Iftheresultingsetofedgesproducesaspanningtreethen
we’redone. Moreformally,giventheoriginalfully-connectedgraphG=(V,E),a
subgraphT =(V,F)isaspanningtreeifithasnocyclesandeachvertex(otherthan
theroot)hasexactlyoneedgeenteringit. Ifthegreedyselectionprocessproduces18.3 • GRAPH-BASEDDEPENDENCYPARSING 397
suchatreethenitisthebestpossibleone.
Unfortunately,thisapproachdoesn’talwaysleadtoatreesincethesetofedges
selectedmaycontaincycles. Fortunately,inyetanothercaseofmultiplediscovery,
there is a straightforward way to eliminate cycles generated during the greedy se-
lection phase. Chu and Liu (1965) and Edmonds (1967) independently developed
anapproachthatbeginswithgreedyselectionandfollowswithanelegantrecursive
cleanupphasethateliminatescycles.
Thecleanupphasebeginsbyadjustingalltheweightsinthegraphbysubtracting
thescoreofthemaximumedgeenteringeachvertexfromthescoreofalltheedges
enteringthatvertex. Thisiswheretheintuitionsmentionedearliercomeintoplay.
Wehavescaledthevaluesoftheedgessothattheweightsoftheedgesinthecycle
havenobearingontheweightofanyofthepossiblespanningtrees. Subtractingthe
value of the edge with maximum weight from each edge entering a vertex results
in a weight of zero for all of the edges selected during the greedy selection phase,
includingalloftheedgesinvolvedinthecycle.
Having adjusted the weights, the algorithm creates a new graph by selecting a
cycle and collapsing it into a single new node. Edges that enter or leave the cycle
arealteredsothattheynowenterorleavethenewlycollapsednode. Edgesthatdo
nottouchthecycleareincludedandedgeswithinthecyclearedropped.
functionMAXSPANNINGTREE(G=(V,E),root,score)returnsspanningtree
F []
←
T’ []
←
score’ []
←
foreachv Vdo
∈
bestInEdge argmax score[e]
F F
bes←
tInEdge
e=(u,v) ∈E
← ∪
foreache=(u,v) Edo
∈
score’[e] score[e] score[bestInEdge]
← −
ifT=(V,F)isaspanningtreethenreturnit
else
C acycleinF
←
G’ CONTRACT(G,C)
←
T’ MAXSPANNINGTREE(G’,root,score’)
←
T EXPAND(T’,C)
←
returnT
functionCONTRACT(G,C)returnscontractedgraph
functionEXPAND(T,C)returnsexpandedgraph
Figure18.12 TheChu-LiuEdmondsalgorithmforfindingamaximumspanningtreeina
weighteddirectedgraph.
Now,ifweknewthemaximumspanningtreeofthisnewgraph,wewouldhave
what we need to eliminate the cycle. The edge of the maximum spanning tree di-
rected towards the vertex representing the collapsed cycle tells us which edge to
delete to eliminate the cycle. How do we find the maximum spanning tree of this
newgraph? Werecursivelyapplythealgorithm tothenewgraph. Thiswilleither
resultinaspanningtreeoragraphwithacycle.Therecursionscancontinueaslong
ascyclesareencountered. Wheneachrecursioncompletesweexpandthecollapsed398 CHAPTER18 • DEPENDENCYPARSING
vertex, restoringalltheverticesandedgesfromthecyclewiththeexceptionofthe
singleedgetobedeleted.
Puttingallthistogether,themaximumspanningtreealgorithmconsistsofgreedy
edgeselection,re-scoringofedgecostsandarecursivecleanupphasewhenneeded.
ThefullalgorithmisshowninFig.18.12.
-4
4
4 -3
12 0
Book 5 that 8 flight Book -2 that 0 flight
root root
12 6 7 7 8 12 -6 7 0 8
7 -1
5 -7
-4 -4
-3 -3
0 0
-2 -2
Book tf
root Book -6 tf root 0 -6 -1
-1 -1
-7 -7
Deleted from cycle
root Book that flight
Figure18.13 Chu-Liu-Edmondsgraph-basedexampleforBookthatflight
Fig. 18.13 steps through the algorithm with our Book that flight example. The
firstrowofthefigureillustratesgreedyedgeselectionwiththeedgeschosenshown
inblue(correspondingtothesetF inthealgorithm). Thisresultsinacyclebetween
thatandflight. Thescaledweightsusingthemaximumvalueenteringeachnodeare
showninthegraphtotheright.
Collapsing the cycle between that and flight to a single node (labelled tf) and
recursingwiththenewlyscaledcostsisshowninthesecondrow. Thegreedyselec-
tionstepinthisrecursionyieldsaspanningtreethatlinksroottobook,aswellasan
edgethatlinksbooktothecontractednode. Expandingthecontractednode,wecan
seethatthisedgecorrespondstotheedgefrombooktoflightintheoriginalgraph.
Thisinturntellsuswhichedgetodroptoeliminatethecycle.
On arbitrary directed graphs, this version of the CLE algorithm runs in O(mn)
time,wheremisthenumberofedgesandnisthenumberofnodes. Sincethispar-
ticularapplicationofthealgorithmbeginsbyconstructingafullyconnectedgraph
m=n2 yieldingarunningtimeofO(n3). Gabowetal.(1986)presentamoreeffi-
cientimplementationwitharunningtimeofO(m+nlogn).18.3 • GRAPH-BASEDDEPENDENCYPARSING 399
18.3.2 Afeature-basedalgorithmforassigningscores
Recallthatgivenasentence,S,andacandidatetree,T,edge-factoredparsingmodels
makethesimplificationthatthescoreforthetreeisthesumofthescoresoftheedges
thatcomprisethetree:
score(S,T) = score(S,e)
e T
(cid:88)∈
Inafeature-basedalgorithmwecomputetheedgescoreasaweightedsumoffea-
turesextractedfromit:
N
score(S,e) = w f(S,e)
i i
i=1
(cid:88)
Ormoresuccinctly.
score(S,e) = w f
·
Giventhisformulation,weneedtoidentifyrelevantfeaturesandtraintheweights.
Thefeatures(andfeaturecombinations)usedtotrainedge-factoredmodelsmir-
rorthoseusedintrainingtransition-basedparsers,suchas
• Wordforms,lemmas,andpartsofspeechoftheheadwordanditsdependent.
• Correspondingfeaturesfromthecontextsbefore,afterandbetweenthewords.
• Wordembeddings.
• Thedependencyrelationitself.
• Thedirectionoftherelation(totherightorleft).
• Thedistancefromtheheadtothedependent.
Given a set of features, our next problem is to learn a set of weights correspond-
ing to each. Unlike many of the learning problems discussed in earlier chapters,
here we are not training a model to associate training items with class labels, or
parser actions. Instead, we seek to train a model that assigns higher scores to cor-
recttreesthantoincorrectones. Aneffectiveframeworkforproblemslikethisisto
inference-based use inference-based learning combined with the perceptron learning rule. In this
learning
framework,weparseasentence(i.e,performinference)fromthetrainingsetusing
someinitiallyrandomsetofinitialweights. Iftheresultingparsematchesthecor-
responding tree in the training data, we do nothing to the weights. Otherwise, we
findthosefeaturesintheincorrectparsethatare notpresentinthereferenceparse
andwelowertheirweightsbyasmallamountbasedonthelearningrate. Wedothis
incrementallyforeachsentenceinourtrainingdatauntiltheweightsconverge.
18.3.3 Aneuralalgorithmforassigningscores
State-of-the-artgraph-basedmultilingualparsersarebasedonneuralnetworks. In-
steadofextractinghand-designedfeaturestorepresenteachedgebetweenwordsw
i
andw ,theseparsersrunthesentencethroughanencoder,andthenpasstheencoded
j
representationofthetwowordsw andw throughanetworkthatestimatesascore
i j
fortheedgei j.
→
Herewe’llsketchthebiaffinealgorithmofDozatandManning(2017)andDozat
etal.(2017)showninFig.18.14, drawingontheworkofGru¨newaldetal.(2021)
whotestedmanyversionsofthealgorithmviatheirSTEPSsystem. Thealgorithm400 CHAPTER18 • DEPENDENCYPARSING
score(h head, h dep)
1 3
∑
Biaffine
U W b
+
h 1 head h 1 dep h 2 head h 2 dep h 3 head h 3 dep
FFN FFN FFN FFN FFN FFN
head dep head dep head dep
r1 r2 r3
ENCODER
book that flight
Figure18.14 Computingscoresforasingleedge(book flight)inthebiaffineparserof
→
Dozat and Manning (2017); Dozat et al. (2017). The parser uses distinct feedforward net-
workstoturntheencoderoutputforeachwordintoaheadanddependentrepresentationfor
the word. The biaffine function turns the head embedding of the head and the dependent
embeddingofthedependentintoascoreforthedependencyedge.
first runs the sentence X =x ,...,x through an encoder to produce a contextual
1 n
embedding representation for each token R=r ,...,r . The embedding for each
1 n
token is now passed through two separate feedforward networks, one to produce a
representation of this token as a head, and one to produce a representation of this
tokenasadependent:
hh iead =FFNhead(r i) (18.13)
hd iep=FFNdep(r i) (18.14)
Nowtoassignascoretothedirectededgei j,(w istheheadandw isthedepen-
i j
→
dent),wefeedtheheadrepresentationofi,hhead,andthedependentrepresentation
i
of
j,hdep
,intoabiaffinescoringfunction:
j
Score(i j) = Biaff(hhead,hdep) (18.15)
→ i j
(cid:124)
Biaff(x,y) = x Uy+W(x y)+b (18.16)
⊕
whereU,W,andbareweightslearnedbythemodel. Theideaofusingabiaffine
functionistoallowthesystemtolearnmultiplicativeinteractionsbetweenthevec-
torsxandy.
IfwepassScore(i j)throughasoftmax,weendupwithaprobabilitydistri-
→
bution,foreachtoken j,overpotentialheadsi(allothertokensinthesentence):
p(i j)=softmax([Score(k j); k= j,1 k n]) (18.17)
→ → ∀ (cid:54) ≤ ≤
This probability can then be passed to the maximum spanning tree algorithm of
Section18.3.1tofindthebesttree.18.4 • EVALUATION 401
This p(i j)classifieristrainedbyoptimizingthecross-entropyloss.
→
Note that the algorithm as we’ve described it is unlabeled. To make this into
a labeled algorithm, the Dozat and Manning (2017) algorithm actually trains two
classifiers. Thefirstclassifier,theedge-scorer,theonewedescribedabove,assigns
aprobability p(i j)toeachwordw andw . ThentheMaximumSpanningTree
i j
→
algorithmisruntogetasinglebestdependencyparsetreeforthesecond. Wethen
applyasecondclassifier,thelabel-scorer,whosejobistofindthemaximumprob-
ability label for each edge in this parse. This second classifier has the same form
as (18.15-18.17), but instead of being trained to predict with binary softmax the
probabilityofanedgeexistingbetweentwowords,itistrainedwithasoftmaxover
dependencylabelstopredictthedependencylabelbetweenthewords.
18.4 Evaluation
As with phrase structure-based parsing, the evaluation of dependency parsers pro-
ceedsbymeasuringhowwelltheyworkonatestset. Anobviousmetricwouldbe
exactmatch(EM)—howmanysentencesareparsedcorrectly. Thismetricisquite
pessimistic,withmostsentencesbeingmarkedwrong. Suchmeasuresarenotfine-
grainedenoughtoguidethedevelopmentprocess. Ourmetricsneedtobesensitive
enoughtotellifactualimprovementsarebeingmade.
Forthesereasons,themostcommonmethodforevaluatingdependencyparsers
are labeled and unlabeled attachment accuracy. Labeled attachment refers to the
properassignmentofawordtoitsheadalongwiththecorrectdependencyrelation.
Unlabeled attachment simply looks at the correctness of the assigned head, ignor-
ingthedependencyrelation. Givenasystemoutputandacorrespondingreference
parse, accuracyissimplythepercentageofwordsinaninputthatareassignedthe
correct head with the correct relation. These metrics are usually referred to as the
labeledattachmentscore(LAS)andunlabeledattachmentscore(UAS).Finally,we
canmakeuseofalabelaccuracyscore(LS),thepercentageoftokenswithcorrect
labels,ignoringwheretherelationsarecomingfrom.
Asanexample,considerthereferenceparseandsystemparseforthefollowing
exampleshowninFig.18.15.
(18.18) BookmetheflightthroughHouston.
Thesystemcorrectlyfinds4ofthe6dependencyrelationspresentinthereference
parseandreceivesanLASof2/3. However, oneofthe2incorrectrelationsfound
bythesystemholdsbetweenbookandflight,whichareinahead-dependentrelation
inthereferenceparse;thesystemthereforeachievesaUASof5/6.
root root
iobj x-comp
nmod nsubj nmod
obj det case det case
Book me the flight through Houston Book me the flight through Houston
(a)Reference (b)System
Figure18.15 ReferenceandsystemparsesforBookmetheflightthroughHouston,resultinginanLASof
2/3andanUASof5/6.402 CHAPTER18 • DEPENDENCYPARSING
Beyond attachment scores, we may also be interested in how well a system is
performingonaparticularkindofdependencyrelation,forexampleNSUBJ,across
adevelopmentcorpus. Herewecanmakeuseofthenotionsofprecisionandrecall
introduced in Chapter 8, measuring the percentage of relations labeled NSUBJ by
thesystemthatwerecorrect(precision),andthepercentageofthe NSUBJ relations
present in the development set that were in fact discovered by the system (recall).
Wecanemployaconfusionmatrixtokeeptrackofhowofteneachdependencytype
wasconfusedforanother.
18.5 Summary
Thischapterhasintroducedtheconceptofdependencygrammarsanddependency
parsing. Here’sasummaryofthemainpointsthatwecovered:
• Independency-basedapproachestosyntax,thestructureofasentenceisde-
scribedintermsofasetofbinaryrelationsthatholdbetweenthewordsina
sentence. Larger notions of constituency are not directly encoded in depen-
dencyanalyses.
• Therelationsinadependencystructurecapturethehead-dependentrelation-
shipamongthewordsinasentence.
• Dependency-based analysis provides information directly useful in further
languageprocessingtasksincludinginformationextraction,semanticparsing
andquestionanswering.
• Transition-based parsing systems employ a greedy stack-based algorithm to
createdependencystructures.
• Graph-basedmethodsforcreatingdependencystructuresarebasedontheuse
ofmaximumspanningtreemethodsfromgraphtheory.
• Bothtransition-basedandgraph-basedapproachesaredevelopedusingsuper-
visedmachinelearningtechniques.
• Treebanksprovidethedataneededtotrainthesesystems. Dependencytree-
bankscanbecreateddirectlybyhumanannotatorsorviaautomatictransfor-
mationfromphrase-structuretreebanks.
• Evaluationofdependencyparsersisbasedonlabeledandunlabeledaccuracy
scoresasmeasuredagainstwithhelddevelopmentandtestcorpora.
Bibliographical and Historical Notes
Thedependency-basedapproachtogrammarismucholderthantherelativelyrecent
phrase-structureorconstituencygrammars,whichdateonlytothe20thcentury.De-
pendencygrammardatesbacktotheIndiangrammarianPa¯n.inisometimebetween
the 7th and 4th centuries BCE, as well as the ancient Greek linguistic traditions.
Contemporary theories of dependency grammar all draw heavily on the 20th cen-
turyworkofTesnie`re(1959).
Automaticparsingusingdependencygrammarswasfirstintroducedintocompu-
tationallinguisticsbyearlyworkonmachinetranslationattheRANDCorporation
led by David Hays. This work on dependency parsing closely paralleled work onBIBLIOGRAPHICALANDHISTORICALNOTES 403
constituentparsingandmadeexplicituseofgrammarstoguidetheparsingprocess.
Afterthisearlyperiod,computationalworkondependencyparsingremainedinter-
mittentoverthefollowingdecades.Notableimplementationsofdependencyparsers
forEnglishduringthisperiodincludeLinkGrammar(SleatorandTemperley,1993),
ConstraintGrammar(Karlssonetal.,1995),andMINIPAR(Lin,2003).
Dependencyparsingsawamajorresurgenceinthelate1990’swiththeappear-
anceoflargedependency-basedtreebanksandtheassociatedadventofdatadriven
approachesdescribedinthischapter. Eisner(1996)developedanefficientdynamic
programmingapproachtodependencyparsingbasedonbilexicalgrammarsderived
from the Penn Treebank. Covington (2001) introduced the deterministic word by
word approach underlying current transition-based approaches. Yamada and Mat-
sumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce
paradigmandtheuseofsupervisedmachinelearningintheformofsupportvector
machinestodependencyparsing.
Transition-based parsing is based on the shift-reduce parsing algorithm orig-
inally developed for analyzing programming languages (Aho and Ullman, 1972).
Shift-reduce parsing also makes use of a context-free grammar. Input tokens are
successivelyshiftedontothestackandthetoptwoelementsofthestackarematched
againsttheright-handsideoftherulesinthegrammar; whenamatchisfoundthe
matchedelementsarereplacedonthestack(reduced)bythenon-terminalfromthe
left-hand side of the rule being matched. In transition-based dependency parsing
we skip the grammar, and alter the reduce operation to add a dependency relation
betweenawordanditshead.
Nivre (2003) defined the modern, deterministic, transition-based approach to
dependencyparsing. SubsequentworkbyNivreandhiscolleaguesformalizedand
analyzed the performance of numerous transition systems, training methods, and
methods for dealing with non-projective language (Nivre and Scholz 2004, Nivre
2006, Nivre and Nilsson 2005, Nivre et al. 2007b, Nivre 2007). The neural ap-
proach was pioneered by Chen and Manning (2014) and extended by Kiperwasser
andGoldberg(2016);Kulmizevetal.(2019).
Thegraph-basedmaximumspanningtreeapproachtodependencyparsingwas
introducedbyMcDonaldetal.2005a,McDonaldetal.2005b. Theneuralclassifier
wasintroducedby(KiperwasserandGoldberg,2016).
Thelong-runningPragueDependencyTreebankproject(Hajicˇ,1998)isthemost
significantefforttodirectlyannotateacorpuswithmultiplelayersofmorphological,
syntactic and semantic information. PDT 3.0 contains over 1.5 M tokens (Bejcˇek
etal.,2013).
UniversalDependencies(UD)(deMarneffeetal.,2021)isanopencommunity
projecttocreateaframeworkfordependencytreebankannotation,withnearly200
treebanksinover100languages. TheUDannotationschemeevolvedoutofseveral
distincteffortsincludingStanforddependencies(deMarneffeetal.2006,deMarn-
effeandManning2008,deMarneffeetal.2014),Google’suniversalpart-of-speech
tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets
(Zeman,2008).
TheConferenceonNaturalLanguageLearning(CoNLL)hasconductedanin-
fluentialseriesofsharedtasksrelatedtodependencyparsingovertheyears(Buch-
holz and Marsi 2006, Nivre et al. 2007a, Surdeanu et al. 2008, Hajicˇ et al. 2009).
Morerecentevaluationshavefocusedonparserrobustnesswithrespecttomorpho-
logically rich languages (Seddah et al., 2013), and non-canonical language forms
such as social media, texts, and spoken language (Petrov and McDonald, 2012).404 CHAPTER18 • DEPENDENCYPARSING
Choietal.(2015)presentsaperformanceanalysisof10dependencyparsersacross
arangeofmetrics,aswellasDEPENDABLE,arobustparserevaluationtool.
ExercisesCHAPTER
19 Logical Representations of
Sentence Meaning
ISHMAEL: Surelyallthisisnotwithoutmeaning.
HermanMelville,MobyDick
Inthischapterweintroducetheideathatthemeaningoflinguisticexpressionscan
meaning be captured in formal structures called meaning representations. Consider tasks
representations
thatrequiresomeformofsemanticprocessing, likelearningtouseanewpieceof
software by reading the manual, deciding what to order at a restaurant by reading
a menu, or following a recipe. Accomplishing these tasks requires representations
that link the linguistic elements to the necessary non-linguistic knowledge of the
world. Reading a menu and deciding what to order, giving advice about where to
gotodinner, followingarecipe, andgeneratingnewrecipesallrequireknowledge
aboutfoodanditspreparation,whatpeopleliketoeat,andwhatrestaurantsarelike.
Learningtouseapieceofsoftwarebyreadingamanual,orgivingadviceonusing
software, requires knowledge about the software and similar apps, computers, and
usersingeneral.
Inthischapter,weassumethatlinguisticexpressionshavemeaningrepresenta-
tionsthataremadeupofthesamekindofstuffthatisusedtorepresentthiskindof
everydaycommon-senseknowledgeoftheworld. Theprocesswherebysuchrepre-
semantic sentationsarecreatedandassignedtolinguisticinputsiscalledsemanticparsingor
parsing
semantic analysis, and the entire enterprise of designing meaning representations
computational andassociatedsemanticparsersisreferredtoascomputationalsemantics.
semantics
e,yHaving(e) Haver(e,Speaker) HadThing(e,y) Car(y)
∃ ∧ ∧ ∧
h / have-01
(h / have-01 Having:
arg0 arg1 arg0: (i / i) Haver: Speaker
arg1: (c / car)) HadThing: Car
i / i c / car
Figure19.1 Alistofsymbols, twodirectedgraphs, andarecordstructure: asamplerof
meaningrepresentationsforIhaveacar.
ConsiderFig.19.1,whichshowsexamplemeaningrepresentationsforthesen-
tence I have a car using four commonly used meaning representation languages.
The top row illustrates a sentence in First-Order Logic, covered in detail in Sec-
tion 19.3; the directed graph and its corresponding textual form is an example of
an Abstract Meaning Representation (AMR) form (Banarescu et al., 2013), and
ontherightisaframe-basedorslot-fillerrepresentation,discussedinSection19.5
andagaininChapter21.406 CHAPTER19 • LOGICALREPRESENTATIONSOFSENTENCEMEANING
While there are non-trivial differences among these approaches, they all share
the notion that a meaning representation consists of structures composed from a
setofsymbols,orrepresentationalvocabulary. Whenappropriatelyarranged,these
symbolstructuresaretakentocorrespondtoobjects,propertiesofobjects,andrela-
tionsamongobjectsinsomestateofaffairsbeingrepresentedorreasonedabout. In
thiscase,allfourrepresentationsmakeuseofsymbolscorrespondingtothespeaker,
acar,andarelationdenotingthepossessionofonebytheother.
Importantly,theserepresentationscanbeviewedfromatleasttwodistinctper-
spectives in all of these approaches: as representations of the meaning of the par-
ticularlinguisticinputIhaveacar, andasrepresentationsofthestateofaffairsin
someworld. Itisthisdualperspectivethatallowstheserepresentationstobeused
tolinklinguisticinputstotheworldandtoourknowledgeofit.
In the next sections we give some background: our desiderata for a meaning
representationlanguageandsomeguaranteesthattheserepresentationswillactually
dowhatweneedthemtodo—provideacorrespondencetothestateofaffairsbeing
represented.InSection19.3weintroduceFirst-OrderLogic,historicallytheprimary
techniqueforinvestigatingnaturallanguagesemantics,andseeinSection19.4how
itcanbeusedtocapturethesemanticsofeventsandstatesinEnglish. Chapter20
thenintroducestechniquesforsemanticparsing: generatingtheseformalmeaning
representationsgivenlinguisticinputs.
19.1 Computational Desiderata for Representations
Let’sconsiderwhymeaningrepresentationsareneededandwhattheyshoulddofor
us. Tofocusthisdiscussion, let’sconsiderasystemthatgivesrestaurantadviceto
touristsbasedonaknowledgebase.
Verifiability
Considerthefollowingsimplequestion:
(19.1) DoesMaharaniservevegetarianfood?
Toanswerthisquestion,wehavetoknowwhatit’sasking,andknowwhetherwhat
verifiability it’saskingistrueofMahariniornot. Verifiabilityisasystem’sabilitytocompare
thestateofaffairsdescribedbyarepresentationtothestateofaffairsinsomeworld
asmodeledinaknowledgebase. Forexample,we’llneedsomesortofrepresenta-
tionlikeServes(Maharani,VegetarianFood),whichasystemcanmatchagainstits
knowledgebaseoffactsaboutparticularrestaurants,andifitfindsarepresentation
matchingthisproposition,itcanansweryes. Otherwise,itmusteithersayNoifits
knowledge of local restaurants is complete, or say that it doesn’t know if it knows
itsknowledgeisincomplete.
UnambiguousRepresentations
Semantics, like all the other domains we have studied, is subject to ambiguity.
Words and sentences have different meaning representations in different contexts.
Considerthefollowingexample:
(19.2) Iwannaeatsomeplacethat’sclosetoICSI.
Thissentencecaneithermeanthatthespeakerwantstoeatatsomenearbylocation,
orunderaGodzilla-as-speakerinterpretation,thespeakermaywanttodevoursome19.1 • COMPUTATIONALDESIDERATAFORREPRESENTATIONS 407
nearbylocation. Thesentenceisambiguous;asinglelinguisticexpressioncanhave
oneoftwomeanings. Butourmeaningrepresentationsitselfcannotbeambiguous.
Therepresentationofaninput’smeaningshouldbefreefromanyambiguity,sothat
thesystemcanreasonoverarepresentationthatmeanseitheronethingortheother
inordertodecidehowtoanswer.
vagueness Aconceptcloselyrelatedtoambiguityisvagueness: inwhichameaningrepre-
sentationleavessomepartsofthemeaningunderspecified. Vaguenessdoesnotgive
risetomultiplerepresentations. Considerthefollowingrequest:
(19.3) IwanttoeatItalianfood.
WhileItalianfoodmayprovideenoughinformationtoproviderecommendations,it
isneverthelessvagueastowhattheuserreallywantstoeat. Avaguerepresentation
ofthemeaningofthisphrasemaybeappropriateforsomepurposes,whileamore
specificrepresentationmaybeneededforotherpurposes.
CanonicalForm
canonicalform The doctrine of canonical form says that distinct inputs that mean the same thing
shouldhavethesamemeaningrepresentation. Thisapproachgreatlysimplifiesrea-
soning, since systems need only deal with a single meaning representation for a
potentiallywiderangeofexpressions.
Considerthefollowingalternativewaysofexpressing(19.1):
(19.4) DoesMaharanihavevegetariandishes?
(19.5) DotheyhavevegetarianfoodatMaharani?
(19.6) ArevegetariandishesservedatMaharani?
(19.7) DoesMaharaniservevegetarianfare?
Despitethefactthesealternativesusedifferentwordsandsyntax,wewantthem
to map to a single canonical meaning representations. If they were all different,
assumingthesystem’sknowledgebasecontainsonlyasinglerepresentationofthis
fact, most of the representations wouldn’t match. We could, of course, store all
possiblealternativerepresentationsofthesamefactintheknowledgebase,butdoing
sowouldleadtoenormousdifficultyinkeepingtheknowledgebaseconsistent.
Canonicalformdoescomplicatethetaskofsemanticparsing. Oursystemmust
conclude that vegetarian fare, vegetarian dishes, and vegetarian food refer to the
same thing, that having and serving are equivalent here, and that all these parse
structures still lead to the same meaning representation. Or consider this pair of
examples:
(19.8) Maharaniservesvegetariandishes.
(19.9) VegetariandishesareservedbyMaharani.
Despitethedifferentplacementoftheargumentstoserve,asystemmuststillassign
Maharani and vegetarian dishes to the same roles in the two examples by draw-
ingongrammaticalknowledge,suchastherelationshipbetweenactiveandpassive
sentenceconstructions.
InferenceandVariables
Whataboutmorecomplexrequestssuchas:
(19.10) CanvegetarianseatatMaharani?
Thisrequestresultsinthesameanswerastheothersnotbecausetheymeanthesame
thing,butbecausethereisacommon-senseconnectionbetweenwhatvegetarianseat408 CHAPTER19 • LOGICALREPRESENTATIONSOFSENTENCEMEANING
andwhatvegetarianrestaurantsserve. Thisisafactabouttheworld. We’llneedto
connectthemeaningrepresentationofthisrequestwiththisfactabouttheworldina
inference knowledgebase.Asystemmustbeabletouseinference—todrawvalidconclusions
based on the meaning representation of inputs and its background knowledge. It
mustbepossibleforthesystemtodrawconclusionsaboutthetruthofpropositions
that are not explicitly represented in the knowledge base but that are nevertheless
logicallyderivablefromthepropositionsthatarepresent.
Nowconsiderthefollowingsomewhatmorecomplexrequest:
(19.11) I’dliketofindarestaurantwhereIcangetvegetarianfood.
Thisrequestdoesnotmakereferencetoanyparticularrestaurant;theuserwantsin-
formationaboutanunknownrestaurantthatservesvegetarianfood.Sincenorestau-
rants are named, simple matching is not going to work. Answering this request
variables requirestheuseofvariables,usingsomerepresentationlikethefollowing:
Serves(x,VegetarianFood) (19.12)
Matching succeeds only if the variable x can be replaced by some object in the
knowledgebaseinsuchawaythattheentirepropositionwillthenmatch. Thecon-
ceptthatissubstitutedforthevariablecanthenbeusedtofulfilltheuser’srequest.
Itiscriticalforanymeaningrepresentationlanguagetobeabletohandlethesekinds
ofindefinitereferences.
Expressiveness
Finally, a meaning representation scheme must be expressive enough to handle a
wide range of subject matter, ideally any sensible natural language utterance. Al-
thoughthisisprobablytoomuchtoexpectfromanysinglerepresentationalsystem,
First-OrderLogic,asdescribedinSection19.3,isexpressiveenoughtohandlequite
alotofwhatneedstoberepresented.
19.2 Model-Theoretic Semantics
Whatisitaboutmeaningrepresentationlanguagesthatallowsthemtofulfillthese
desiderata,bridgingthegapfromformalrepresentationstorepresentationsthattell
ussomethingaboutsomestateofaffairsintheworld?
model Theanswerisamodel. Amodelisaformalconstructthatstandsforthepartic-
ularstateofaffairsintheworld. Expressionsinameaningrepresentationlanguage
can be mapped to elements of the model, like objects, properties of objects, and
relationsamongobjects. Ifthemodelaccuratelycapturesthefactswe’reinterested
in, then a consistent mapping between the meaning representation and the model
provides the bridge between meaning representation and world. Models provide a
surprisinglysimpleandpowerfulwaytogroundtheexpressionsinmeaningrepre-
sentationlanguages.
First,someterminology.Thevocabularyofameaningrepresentationconsistsof
twoparts: thenon-logicalvocabularyandthelogicalvocabulary. Thenon-logical
non-logical vocabularyconsistsoftheopen-endedsetofnamesfortheobjects,properties,and
vocabulary
relationsthatmakeuptheworldwe’retryingtorepresent. Theseappearinvarious
schemesaspredicates,nodes,labelsonlinks,orlabelsinslotsinframes. Thelog-
logical icalvocabularyconsistsoftheclosedsetofsymbols, operators,quantifiers, links,
vocabulary19.2 • MODEL-THEORETICSEMANTICS 409
etc., that provide the formal means for composing expressions in a given meaning
representationlanguage.
denotation Eachelementofthenon-logicalvocabularymusthaveadenotationinthemodel,
meaningthateveryelementcorrespondstoafixed,well-definedpartofthemodel.
domain Let’sstartwithobjects. Thedomainofamodelisthesetofobjectsthatarebeing
represented. Eachdistinctconcept,category,orindividualdenotesauniqueelement
inthedomain.
Werepresentpropertiesofobjectsinamodelbydenotingthedomainelements
thathavetheproperty;thatis,propertiesdenotesets. Thedenotationoftheproperty
redisthesetofthingswethinkarered. Similarly,arelationamongobjectdenotes
asetoforderedlists,ortuples,ofdomainelementsthattakepartintherelation: the
denotationoftherelationMarriedissetofpairsofdomainobjectsthataremarried.
extensional This approach to properties and relations is called extensional, because we define
conceptsbytheirextension,theirdenotations. Tosummarize:
• Objectsdenoteelementsofthedomain
• Propertiesdenotesetsofelementsofthedomain
• Relationsdenotesetsoftuplesofelementsofthedomain
We now need a mapping that gets us from our meaning representation to the
correspondingdenotations: afunctionthatmapsfromthenon-logicalvocabularyof
ourmeaningrepresentationtotheproperdenotationsinthemodel. We’llcallsuch
interpretation amappinganinterpretation.
Let’s return to our restaurant advice application, and let its domain consist of
sets of restaurants, patrons, facts about the likes and dislikes of the patrons, and
facts about the restaurants such as their cuisine, typical cost, and noise level. To
beginpopulatingourdomain, D, let’sassumethatwe’redealingwithfourpatrons
designated by the non-logical symbols Matthew, Franco, Katie, and Caroline. de-
notingfouruniquedomainelements. We’llusetheconstantsa,b,cand,d tostand
for these domain elements. We’re deliberately using meaningless, non-mnemonic
names for our domain elements to emphasize the fact that whatever it is that we
knowabouttheseentitieshastocomefromtheformalpropertiesofthemodeland
not from the names of the symbols. Continuing, let’s assume that our application
includesthreerestaurants,designatedasFrasca,Med,andRioinourmeaningrep-
resentation, that denote the domain elements e,f, and g. Finally, let’s assume that
we’redealingwiththethreecuisinesItalian,Mexican,andEclectic,denotedbyh,i,
and jinourmodel.
PropertieslikeNoisydenotethesubsetofrestaurantsfromourdomainthatare
knowntobenoisy. Two-placerelationalnotions,suchaswhichrestaurantsindivid-
ual patrons Like, denote ordered pairs, or tuples, of the objects from the domain.
And, since we decided to represent cuisines as objects in our model, we can cap-
turewhichrestaurantsServewhichcuisinesasasetoftuples. Onepossiblestateof
affairsusingthisschemeisgiveninFig.19.2.
Giventhissimplescheme,wecangroundourmeaningrepresentationsbycon-
sultingtheappropriatedenotationsinthecorrespondingmodel.Forexample,wecan
evaluatearepresentationclaimingthatMatthewlikestheRio,orthatTheMedserves
Italianbymappingtheobjectsinthemeaningrepresentationstotheircorresponding
domainelementsandmappinganylinks,predicates,orslotsinthemeaningrepre-
sentationtotheappropriaterelationsinthemodel. Moreconcretely,wecanverify
arepresentationassertingthatMatthewlikesFrascabyfirstusingourinterpretation
functiontomapthesymbolMatthewtoitsdenotationa,Frascatoe,andtheLikes
relation to the appropriate set of tuples. We then check that set of tuples for the410 CHAPTER19 • LOGICALREPRESENTATIONSOFSENTENCEMEANING
Domain D= a,b,c,d,e,f,g,h,i,j
{ }
Matthew,Franco,KatieandCaroline a,b,c,d
Frasca,Med,Rio e,f,g
Italian,Mexican,Eclectic h,i,j
Properties
Noisy Noisy= e,f,g
{ }
Frasca,Med,andRioarenoisy
Relations
Likes Likes= a,f , c,f , c,g , b,e , d,f , d,g
{(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)}
MatthewlikestheMed
KatielikestheMedandRio
FrancolikesFrasca
CarolinelikestheMedandRio
Serves Serves= f,j , g,i , e,h
{(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)}
Medserveseclectic
RioservesMexican
FrascaservesItalian
Figure19.2 Amodeloftherestaurantworld.
presenceofthetuple a,e . If,asitisinthiscase,thetupleispresentinthemodel,
(cid:104) (cid:105)
thenwecanconcludethatMatthewlikesFrascaistrue;ifitisn’tthenwecan’t.
This is all pretty straightforward—we’re using sets and operations on sets to
ground the expressions in our meaning representations. Of course, the more inter-
estingpartcomeswhenweconsidermorecomplexexamplessuchasthefollowing:
(19.13) KatielikestheRioandMatthewlikestheMed.
(19.14) KatieandCarolinelikethesamerestaurants.
(19.15) Francolikesnoisy,expensiverestaurants.
(19.16) NoteverybodylikesFrasca.
Oursimpleschemeforgroundingthemeaningofrepresentationsisnotadequate
for examples such as these. Plausible meaning representations for these examples
will not map directly to individual entities, properties, or relations. Instead, they
involvecomplicationssuchasconjunctions,equality,quantifiedvariables,andnega-
tions. Toassesswhetherthesestatementsareconsistentwithourmodel,we’llhave
to tear them apart, assess the parts, and then determine the meaning of the whole
fromthemeaningoftheparts.
Consider the first example above. A meaning representation for this example
willincludetwodistinctpropositionsexpressingtheindividualpatron’spreferences,
conjoined with some kind of implicit or explicit conjunction operator. Our model
doesn’thavearelationthatencodespairwisepreferencesforallofthepatronsand
restaurantsinourmodel,nordoesitneedto.WeknowfromourmodelthatMatthew
likes the Med and separately that Katie likes the Rio (that is, the tuples a,f and
(cid:104) (cid:105)
c,g are members of the set denoted by the Likes relation). All we really need to
(cid:104) (cid:105)
know is how to deal with the semantics of the conjunction operator. If we assume
the simplest possible semantics for the English word and, the whole statement is
trueifitisthecasethateachofthecomponentsistrueinourmodel. Inthiscase,
bothcomponentsaretruesincetheappropriatetuplesarepresentandthereforethe
sentenceasawholeistrue.
truth-
conditional What we’ve done with this example is provide a truth-conditional semantics
semantics19.3 • FIRST-ORDERLOGIC 411
Formula AtomicFormula
→
FormulaConnectiveFormula
|
QuantifierVariable,... Formula
|
Formula
| ¬
(Formula)
|
AtomicFormula Predicate(Term,...)
→
Term Function(Term,...)
→
Constant
|
Variable
|
Connective =
→ ∧| ∨| ⇒
Quantifier
→ ∀| ∃
Constant A VegetarianFood Maharani
→ | | ···
Variable x y
→ | | ···
Predicate Serves Near
→ | | ···
Function LocationOf CuisineOf
→ | | ···
Figure19.3 Acontext-freegrammarspecificationofthesyntaxofFirst-OrderLogicrep-
resentations.AdaptedfromRussellandNorvig2002.
for the assumed conjunction operator in some meaning representation. That is,
we’ve provided a method for determining the truth of a complex expression from
themeaningsoftheparts(byconsultingamodel)andthemeaningofanoperatorby
consultingatruthtable. Meaningrepresentationlanguagesaretruth-conditionalto
theextentthattheygiveaformalspecificationastohowwecandeterminethemean-
ingofcomplexsentencesfromthemeaningoftheirparts. Inparticular,weneedto
know the semantics of the entire logical vocabulary of the meaning representation
schemebeingused.
Notethatalthoughthedetailsofhowthishappensdependondetailsofthepar-
ticularmeaningrepresentationbeingused,itshouldbeclearthatassessingthetruth
conditionsofexamplesliketheseinvolvesnothingbeyondthesimplesetoperations
we’vebeendiscussing. Wereturntotheseissuesinthenextsectioninthecontextof
thesemanticsofFirst-OrderLogic.
19.3 First-Order Logic
First-OrderLogic(FOL)isaflexible,well-understood,andcomputationallytractable
meaningrepresentationlanguagethatsatisfiesmanyofthedesideratagiveninSec-
tion 19.1. It provides a sound computational basis for the verifiability, inference,
andexpressivenessrequirements,aswellasasoundmodel-theoreticsemantics.
AnadditionalattractivefeatureofFOListhatitmakesfewspecificcommitments
as to how things ought to be represented, and those it does are shared by many of
theschemesmentionedearlier: therepresentedworldconsistsofobjects,properties
ofobjects,andrelationsamongobjects.
TheremainderofthissectionintroducesthebasicsyntaxandsemanticsofFOL
andthendescribestheapplicationofFOLtotherepresentationofevents.
19.3.1 BasicElementsofFirst-OrderLogic
Let’sexplore FOL byfirstexaminingitsvariousatomicelementsandthenshowing
how they can be composed to create larger meaning representations. Figure 19.3,412 CHAPTER19 • LOGICALREPRESENTATIONSOFSENTENCEMEANING
which provides a complete context-free grammar for the particular syntax of FOL
thatwewilluse,isourroadmapforthissection.
term Let’sbeginbyexaminingthenotionofaterm,theFOLdeviceforrepresenting
objects. AscanbeseenfromFig.19.3, FOLprovidesthreewaystorepresentthese
basicbuildingblocks:constants,functions,andvariables. Eachofthesedevicescan
bethoughtofasdesignatinganobjectintheworldunderconsideration.
constant Constants in FOL refer to specific objects in the world being described. Such
constants are conventionally depicted as either single capitalized letters such as A
andBorsinglecapitalizedwordsthatareoftenreminiscentofpropernounssuchas
Maharani and Harry. Like programming language constants, FOL constants refer
to exactly one object. Objects can, however, have multiple constants that refer to
them.
function FunctionsinFOLcorrespondtoconceptsthatareoftenexpressedinEnglishas
genitivessuchasFrasca’slocation. AFOLtranslationofsuchanexpressionmight
looklikethefollowing.
LocationOf(Frasca) (19.17)
FOLfunctionsaresyntacticallythesameassingleargumentpredicates. Itisim-
portant to remember, however, that while they have the appearance of predicates,
theyareinfacttermsinthattheyrefertouniqueobjects. Functionsprovideacon-
venientwaytorefertospecificobjectswithouthavingtoassociateanamedconstant
with them. This is particularly convenient in cases in which many named objects,
likerestaurants,haveauniqueconceptsuchasalocationassociatedwiththem.
variable Variables are our final FOL mechanism for referring to objects. Variables, de-
pictedassinglelower-caseletters,letusmakeassertionsanddrawinferencesabout
objectswithouthavingtomakereferencetoanyparticularnamedobject.Thisability
to make statements about anonymous objects comes in two flavors: making state-
mentsaboutaparticularunknownobjectandmakingstatementsaboutalltheobjects
insomearbitraryworldofobjects. Wereturntothetopicofvariablesafterwehave
presentedquantifiers,theelementsofFOLthatmakevariablesuseful.
Now that we have the means to refer to objects, we can move on to the FOL
mechanismsthatareusedtostaterelationsthatholdamongobjects. Predicatesare
symbols that refer to, or name, the relations that hold among some fixed number
of objects in a given domain. Returning to the example introduced informally in
Section19.1,areasonableFOLrepresentationforMaharaniservesvegetarianfood
mightlooklikethefollowingformula:
Serves(Maharani,VegetarianFood) (19.18)
This FOL sentence asserts that Serves, a two-place predicate, holds between the
objectsdenotedbytheconstantsMaharaniandVegetarianFood.
Asomewhatdifferentuseofpredicatesisillustratedbythefollowingfairlytyp-
icalrepresentationforasentencelikeMaharaniisarestaurant:
Restaurant(Maharani) (19.19)
Thisisanexampleofaone-placepredicatethatisused,nottorelatemultipleobjects,
butrathertoassertapropertyofasingleobject. Inthiscase,itencodesthecategory
membershipofMaharani.
With the ability to refer to objects, to assert facts about objects, and to relate
objectstooneanother,wecancreaterudimentarycompositerepresentations. These
representations correspond to the atomic formula level in Fig. 19.3. This ability19.3 • FIRST-ORDERLOGIC 413
to compose complex representations is, however, not limited to the use of single
predicates. Larger composite representations can also be put together through the
logical use of logical connectives. As can be seen from Fig. 19.3, logical connectives let
connectives
us create larger representations by conjoining logical formulas using one of three
operators. Consider, for example, the following BERP sentence and one possible
representationforit:
(19.20) IonlyhavefivedollarsandIdon’thavealotoftime.
Have(Speaker,FiveDollars) Have(Speaker,LotOfTime) (19.21)
∧¬
The semantic representation for this example is built up in a straightforward way
fromthesemanticsoftheindividualclausesthroughtheuseofthe and operators.
∧ ¬
NotethattherecursivenatureofthegrammarinFig.19.3allowsaninfinitenumber
oflogicalformulastobecreatedthroughtheuseoftheseconnectives. Thus,aswith
syntax,wecanuseafinitedevicetocreateaninfinitenumberofrepresentations.
19.3.2 VariablesandQuantifiers
Wenowhaveallthemachinerynecessarytoreturntoourearlierdiscussionofvari-
ables. Asnotedabove,variablesareusedintwowaysinFOL: torefertoparticular
anonymousobjectsandtorefergenericallytoallobjectsinacollection. Thesetwo
quantifiers usesaremadepossiblethroughtheuseofoperatorsknownasquantifiers. Thetwo
operatorsthatarebasictoFOLaretheexistentialquantifier,whichisdenoted and
∃
ispronouncedas“thereexists”,andtheuniversalquantifier,whichisdenoted and
∀
ispronouncedas“forall”.
Theneedforanexistentiallyquantifiedvariableisoftensignaledbythepresence
ofanindefinitenounphraseinEnglish. Considerthefollowingexample:
(19.22) arestaurantthatservesMexicanfoodnearICSI.
Here,referenceisbeingmadetoananonymousobjectofaspecifiedcategorywith
particular properties. The following would be a reasonable representation of the
meaningofsuchaphrase:
xRestaurant(x) Serves(x,MexicanFood) (19.23)
∃ ∧
Near((LocationOf(x),LocationOf(ICSI))
∧
The existential quantifier at the head of this sentence instructs us on how to
interpret the variable x in the context of this sentence. Informally, it says that for
this sentence to be true there must be at least one object such that if we were to
substitute it for the variable x, the resulting sentence would be true. For example,
ifAyCarambaisaMexicanrestaurantnearICSI,thensubstitutingAyCarambaforx
resultsinthefollowinglogicalformula:
Restaurant(AyCaramba) Serves(AyCaramba,MexicanFood) (19.24)
∧
Near((LocationOf(AyCaramba),LocationOf(ICSI))
∧
Basedonthesemanticsofthe operator, thissentencewillbetrueifallofits
∧
three component atomic formulas are true. These in turn will be true if they are
eitherpresentinthesystem’sknowledgebaseorcanbeinferredfromotherfactsin
theknowledgebase.
The use of the universal quantifier also has an interpretation based on substi-
tution of known objects for variables. The substitution semantics for the universal414 CHAPTER19 • LOGICALREPRESENTATIONSOFSENTENCEMEANING
quantifiertakestheexpressionforallquiteliterally;the operatorstatesthatforthe
∀
logicalformulainquestiontobetrue,thesubstitutionofanyobjectintheknowledge
basefortheuniversallyquantifiedvariableshouldresultinatrueformula. Thisisin
marked contrast to the operator, which only insists on a single valid substitution
∃
forthesentencetobetrue.
Considerthefollowingexample:
(19.25) Allvegetarianrestaurantsservevegetarianfood.
Areasonablerepresentationforthissentencewouldbesomethinglikethefollowing:
xVegetarianRestaurant(x) = Serves(x,VegetarianFood) (19.26)
∀ ⇒
Forthissentencetobetrue,everysubstitutionofaknownobjectforxmustresultina
sentencethatistrue.Wecandividethesetofallpossiblesubstitutionsintothesetof
objectsconsistingofvegetarianrestaurantsandthesetconsistingofeverythingelse.
Letusfirstconsiderthecaseinwhichthesubstitutedobjectactuallyisavegetarian
restaurant;onesuchsubstitutionwouldresultinthefollowingsentence:
VegetarianRestaurant(Maharani) = Serves(Maharani,VegetarianFood)
⇒
(19.27)
Ifweassumethatweknowthattheconsequentclause
Serves(Maharani,VegetarianFood) (19.28)
is true, then this sentence as a whole must be true. Both the antecedent and the
consequent have the value True and, therefore, according to the first two rows of
Fig.19.4onpage416thesentenceitselfcanhavethevalueTrue. Thisresultwillbe
thesameforallpossiblesubstitutionsofTermsrepresentingvegetarianrestaurants
forx.
Remember, however, that for this sentence to be true, it must be true for all
possiblesubstitutions. Whathappenswhenweconsiderasubstitutionfromtheset
of objects that are not vegetarian restaurants? Consider the substitution of a non-
vegetarianrestaurantsuchasAyCarambaforthevariablex:
VegetarianRestaurant(AyCaramba) = Serves(AyCaramba,VegetarianFood)
⇒
SincetheantecedentoftheimplicationisFalse,wecandeterminefromFig.19.4
thatthesentenceisalwaysTrue,againsatisfyingthe constraint.
∀
Note that it may still be the case that AyCaramba serves vegetarian food with-
out actually being a vegetarian restaurant. Note also that, despite our choice of
examples, there are no implied categorical restrictions on the objects that can be
substitutedforxbythiskindofreasoning. Inotherwords,thereisnorestrictionof
xtorestaurantsorconceptsrelatedtothem. Considerthefollowingsubstitution:
VegetarianRestaurant(Carburetor) = Serves(Carburetor,VegetarianFood)
⇒
Heretheantecedentisstillfalsesotheruleremainstrueunderthiskindofirrelevant
substitution.
To review, variables in logical formulas must be either existentially ( ) or uni-
∃
versally ( ) quantified. To satisfy an existentially quantified variable, at least one
∀
substitutionmustresultinatruesentence. Tosatisfyauniversallyquantifiedvari-
able,allsubstitutionsmustresultintruesentences.19.3 • FIRST-ORDERLOGIC 415
19.3.3 LambdaNotation
Thefinalelementweneedtocompleteourdiscussionof FOL iscalledthelambda
lambda notation(Church,1940). Thisnotationprovidesawaytoabstractfromfullyspeci-
notation
fiedFOLformulasinawaythatwillbeparticularlyusefulforsemanticanalysis.The
lambdanotationextendsthesyntaxof FOL toincludeexpressionsofthefollowing
form:
λx.P(x) (19.29)
SuchexpressionsconsistoftheGreeksymbolλ,followedbyoneormorevariables,
followedbyaFOLformulathatmakesuseofthosevariables.
The usefulness of these λ-expressions is based on the ability to apply them to
logicaltermstoyieldnewFOLexpressionswheretheformalparametervariablesare
λ-reduction bound to the specified terms. This process is known as λ-reduction, and consists
of a simple textual replacement of the λ variables and the removal of the λ. The
followingexpressionsillustratetheapplicationofaλ-expressiontotheconstantA,
followedbytheresultofperformingaλ-reductiononthisexpression:
λx.P(x)(A) (19.30)
P(A)
An important and useful variation of this technique is the use of one λ-expression
asthebodyofanotherasinthefollowingexpression:
λx.λy.Near(x,y) (19.31)
This fairly abstract expression can be glossed as the state of something being near
somethingelse. Thefollowingexpressionsillustrateasingleλ-applicationandsub-
sequentreductionwiththiskindofembeddedλ-expression:
λx.λy.Near(x,y)(Bacaro) (19.32)
λy.Near(Bacaro,y)
Theimportantpointhereisthattheresultingexpressionisstillaλ-expression;the
first reduction bound the variable x and removed the outer λ, thus revealing the
inner expression. As might be expected, this resulting λ-expression can, in turn,
be applied to another term to arrive at a fully specified logical formula, as in the
following:
λy.Near(Bacaro,y)(Centro) (19.33)
Near(Bacaro,Centro)
currying Thisgeneraltechnique,calledcurrying1(Scho¨nfinkel,1924)isawayofconverting
apredicatewithmultipleargumentsintoasequenceofsingle-argumentpredicates.
AsweshowinChapter20,theλ-notationprovidesawaytoincrementallygather
arguments to a predicate when they do not all appear together as daughters of the
predicateinaparsetree.
19.3.4 TheSemanticsofFirst-OrderLogic
Thevariousobjects,properties,andrelationsrepresentedina FOL knowledgebase
acquiretheirmeaningsbyvirtueoftheircorrespondencetoobjects,properties,and
1 Curryingisthestandardterm,althoughHeimandKratzer(1998)presentaninterestingargumentfor
thetermScho¨nfinkelizationovercurrying,sinceCurrylaterbuiltonScho¨nfinkel’swork.416 CHAPTER19 • LOGICALREPRESENTATIONSOFSENTENCEMEANING
relationsoutintheexternalworldbeingmodeled. Wecanaccomplishthisbyem-
ploying the model-theoretic approach introduced in Section 19.2. Recall that this
approach employs simple set-theoretic notions to provide a truth-conditional map-
ping from the expressions in a meaning representation to the state of affairs being
modeled. Wecanapplythisapproachto FOL bygoingthroughalltheelementsin
Fig.19.3onpage411andspecifyinghoweachshouldbeaccountedfor.
We can start by asserting that the objects in our world, FOL terms, denote ele-
mentsinadomain,andassertingthatatomicformulasarecapturedeitherassetsof
domainelementsforproperties,orassetsoftuplesofelementsforrelations. Asan
example,considerthefollowing:
(19.34) CentroisnearBacaro.
Capturing the meaning of this example in FOL involves identifying the Terms
andPredicatesthatcorrespondtothevariousgrammaticalelementsinthesentence
and creating logical formulas that capture the relations implied by the words and
syntaxofthesentence. Forthisexample,suchaneffortmightyieldsomethinglike
thefollowing:
Near(Centro,Bacaro) (19.35)
The meaning of this logical formula is based on whether the domain elements de-
noted by the termsCentro and Bacaro are contained among the tuples denoted by
therelationdenotedbythepredicateNearinthecurrentmodel.
The interpretation of formulas involving logical connectives is based on the
meanings of the components in the formulas combined with the meanings of the
connectives they contain. Figure 19.4 gives interpretations for each of the logical
operatorsshowninFig.19.3.
P Q P P Q P Q P = Q
¬ ∧ ∨ ⇒
False False True False False True
False True True False True True
True False False False True False
True True False True True True
Figure19.4 Truthtablegivingthesemanticsofthevariouslogicalconnectives.
The semantics of the (and) and (not) operators are fairly straightforward,
∧ ¬
and are correlated with at least some of the senses of the corresponding English
terms. However, it is worth pointing out that the (or) operator is not disjunctive
∨
in the same way that the corresponding English word is, and that the = (im-
⇒
plies) operator is only loosely based on any common-sense notions of implication
orcausation.
The final bit we need to address involves variables and quantifiers. Recall that
there are no variables in our set-based models, only elements of the domain and
relationsthatholdamongthem.Wecanprovideamodel-basedaccountforformulas
withvariablesbyemployingthenotionofasubstitutionintroducedearlieronpage
413. Formulas involving are true if a substitution of terms for variables results
∃
inaformulathatistrueinthemodel. Formulasinvolving mustbetrueunderall
∀
possiblesubstitutions.
19.3.5 Inference
Ameaningrepresentationlanguagemustsupportinferencetoaddvalidnewpropo-
sitionstoaknowledgebaseortodeterminethetruthofpropositionsnotexplicitly19.3 • FIRST-ORDERLOGIC 417
contained within a knowledge base (Section 19.1). This section briefly discusses
modusponens,themostwidelyimplementedinferencemethodprovidedbyFOL.
Modusponens Modus ponens is a form of inference that corresponds to what is informally
known as if-then reasoning. We can abstractly define modus ponens as follows,
whereα andβ shouldbetakenasFOLformulas:
α
α = β
⇒ (19.36)
β
Aschemalikethisindicatesthattheformulabelowthelinecanbeinferredfromthe
formulasabovethelinebysomeformofinference. Modusponensstatesthatifthe
left-handsideofanimplicationruleistrue,thentheright-handsideoftherulecan
be inferred. In the following discussions, we will refer to the left-hand side of an
implicationastheantecedentandtheright-handsideastheconsequent.
Foratypicaluseofmodusponens,considerthefollowingexample,whichuses
arulefromthelastsection:
VegetarianRestaurant(Leaf)
xVegetarianRestaurant(x) = Serves(x,VegetarianFood)
∀ ⇒ (19.37)
Serves(Leaf,VegetarianFood)
Here,theformulaVegetarianRestaurant(Leaf)matchestheantecedentoftherule,
thusallowingustousemodusponenstoconcludeServes(Leaf,VegetarianFood).
Modusponenscanbeputtopracticaluseinoneoftwoways: forwardchaining
forward and backward chaining. In forward chaining systems, modus ponens is used in
chaining
preciselythemannerjustdescribed. Asindividualfactsareaddedtotheknowledge
base, modus ponens is used to fire all applicable implication rules. In this kind of
arrangement, as soon as a new fact is added to the knowledge base, all applicable
implicationrulesarefoundandapplied,eachresultingintheadditionofnewfactsto
theknowledgebase. Thesenewpropositionsinturncanbeusedtofireimplication
rulesapplicabletothem.Theprocesscontinuesuntilnofurtherfactscanbededuced.
The forward chaining approach has the advantage that facts will be present in
theknowledgebasewhenneeded,because,inasenseallinferenceisperformedin
advance.Thiscansubstantiallyreducethetimeneededtoanswersubsequentqueries
sincetheyshouldallamounttosimplelookups. Thedisadvantageofthisapproach
isthatfactsthatwillneverbeneededmaybeinferredandstored.
backward Inbackwardchaining,modusponensisruninreversetoprovespecificpropo-
chaining
sitionscalledqueries.Thefirststepistoseeifthequeryformulaistruebydetermin-
ingifitispresentintheknowledgebase. Ifitisnot,thenthenextstepistosearch
for applicable implication rulespresent in the knowledge base. An applicable rule
is one whereby the consequent of the rule matches the query formula. If there are
anysuchrules,thenthequerycanbeprovediftheantecedentofanyonethemcan
be shown to be true. This can be performed recursively by backward chaining on
the antecedent as a new query. The Prolog programming language is a backward
chainingsystemthatimplementsthisstrategy.
Toseehowthisworks,let’sassumethatwehavebeenaskedtoverifythetruthof
the proposition Serves(Leaf,VegetarianFood), assuming the facts given above the
linein(19.37). Sincethispropositionisnotpresentintheknowledgebase,asearch
foranapplicableruleisinitiatedresultingintherulegivenabove. Aftersubstituting
theconstantLeaf forthevariablex, ournexttaskistoprovetheantecedentofthe
rule,VegetarianRestaurant(Leaf),which,ofcourse,isoneofthefactswearegiven.418 CHAPTER19 • LOGICALREPRESENTATIONSOFSENTENCEMEANING
Note that it is critical to distinguish between reasoning by backward chaining
from queries to known facts and reasoning backwards from known consequents to
unknownantecedents. Tobespecific, byreasoningbackwardswemeanthatifthe
consequentofaruleisknowntobetrue, weassumethattheantecedentwillbeas
well. Forexample,let’sassumethatweknowthatServes(Leaf,VegetarianFood)is
true. Sincethisfactmatchestheconsequentofourrule,wemightreasonbackwards
totheconclusionthatVegetarianRestaurant(Leaf).
Whilebackwardchainingisasoundmethodofreasoning,reasoningbackwards
isaninvalid, thoughfrequentlyuseful, formofplausiblereasoning. Plausiblerea-
abduction soningfromconsequentstoantecedentsisknownasabduction,andasweshowin
Chapter 27, is often useful in accounting for many of the inferences people make
whileanalyzingextendeddiscourses.
complete While forward and backward reasoning are sound, neither is complete. This
means that there are valid inferences that cannot be found by systems using these
methodsalone. Fortunately,thereisanalternativeinferencetechniquecalledreso-
resolution lution that is sound and complete. Unfortunately, inference systems based on res-
olutionarefarmorecomputationallyexpensivethanforwardorbackwardchaining
systems. Inpractice,therefore,mostsystemsusesomeformofchainingandplace
a burden on knowledge base developers to encode the knowledge in a fashion that
permitsthenecessaryinferencestobedrawn.
19.4 Event and State Representations
Muchofthesemanticsthatwewishtocaptureconsistsofrepresentationsofstates
and events. States are conditions, or properties, that remain unchanged over an
extended period of time, and events denote changes in some state of affairs. The
representation of both states and events may involve a host of participants, props,
timesandlocations.
The representations for events and states that we have used thus far have con-
sistedofsinglepredicateswithasmanyargumentsasareneededtoincorporateall
therolesassociatedwithagivenexample. Forexample,therepresentationforLeaf
serves vegetarian fare consists of a single predicate with arguments for the entity
doingtheservingandthethingserved.
Serves(Leaf,VegetarianFare) (19.38)
Thisapproachassumesthatthepredicateusedtorepresentaneventverbhasthe
same number of arguments as are present in the verb’s syntactic subcategorization
frame. Unfortunately, this is clearly not always the case. Consider the following
examplesoftheverbeat:
(19.39) Iate.
(19.40) Iateaturkeysandwich.
(19.41) Iateaturkeysandwichatmydesk.
(19.42) Iateatmydesk.
(19.43) Iatelunch.
(19.44) Iateaturkeysandwichforlunch.
(19.45) Iateaturkeysandwichforlunchatmydesk.19.5 • DESCRIPTIONLOGICS 419
Clearly, choosing the correct number of arguments for the predicate represent-
ing themeaningof eat is atrickyproblem. Theseexamples introducefive distinct
arguments,orroles,inanarrayofdifferentsyntacticforms,locations,andcombina-
arity tions. Unfortunately,predicatesinFOLhavefixedarity–theytakeafixednumber
ofarguments.
eventvariable Toaddressthisproblem,weintroducethenotionofaneventvariabletoallow
ustomakeassertionsaboutparticularevents. Todothis,wecanrefactorourevent
predicates to have an existentially quantified variable as their first, and only, argu-
ment. Usingthiseventvariable,wecanintroduceadditionalpredicatestorepresent
theotherinformationwehaveabouttheevent. Thesepredicatestakeaneventvari-
able as their first argument and related FOL terms as their second argument. The
followingformulaillustratesthisschemewiththemeaningrepresentationof19.40
fromourearlierdiscussion.
eEating(e) Eater(e,Speaker) Eaten(e,TurkeySandwich)
∃ ∧ ∧
Here,thequantifiedvariableestandsfortheeatingeventandisusedtobindthe
event predicate with the core information provided via the named roles Eater and
Eaten. To handle the more complex examples, we simply add additional relations
tocapturetheprovidedinformation,asinthefollowingfor19.45.
eEating(e) Eater(e,Speaker) Eaten(e,TurkeySandwich) (19.46)
∃ ∧ ∧
Meal(e,Lunch) Location(e,Desk)
∧ ∧
neo- Eventrepresentationsofthissortarereferredtoasneo-Davidsonianeventrep-
Davidsonian
resentations(Davidson1967,Parsons1990)afterthephilosopherDonaldDavidson
whointroducedthenotionofaneventvariable(Davidson,1967). Tosummarize,in
theneo-Davidsonianapproachtoeventrepresentations:
• Events are captured with predicates that take a single event variable as an
argument.
• There is no need to specify a fixed number of arguments for a given FOL
predicate;rather,asmanyrolesandfillerscanbegluedonasareprovidedin
theinput.
• Nomorerolesarepostulatedthanarementionedintheinput.
• Thelogicalconnectionsamongcloselyrelatedinputsthatsharethesamepred-
icatearesatisfiedwithouttheneedforadditionalinference.
This approach still leaves us with the problem of determining the set of predi-
catesneededtorepresentrolesassociatedwithspecificeventslikeEaterandEaten,
aswellasmoregeneralconceptslikeLocationandTime. We’llreturntothisprob-
leminmoredetailinChapter22andChapter24.
19.5 Description Logics
Asnotedatthebeginningofthischapter,afairnumberofrepresentationalschemes
havebeeninventedtocapturethemeaningoflinguisticutterances. Itisnowwidely
acceptedthatmeaningsrepresentedinthesevariousapproachescan,inprinciple,be
translatedintoequivalentstatementsinFOLwithrelativeease. Thedifficultyisthat
inmanyoftheseapproachesthesemanticsofastatementaredefinedprocedurally.
Thatis,themeaningarisesfromwhateverthesystemthatinterpretsitdoeswithit.420 CHAPTER19 • LOGICALREPRESENTATIONSOFSENTENCEMEANING
Description logics are an effort to better specify the semantics of these earlier
structured network representations and to provide a conceptual framework that is
especiallywellsuitedtocertainkindsofdomainmodeling. Formally,thetermDe-
scriptionLogicsreferstoafamilyoflogicalapproachesthatcorrespondtovarying
subsetsofFOL. TherestrictionsplacedontheexpressivenessofDescriptionLogics
serve to guarantee the tractability of various critical kinds of inference. Our focus
here,however,willbeonthemodelingaspectsofDLsratherthanoncomputational
complexityissues.
When using Description Logics to model an application domain, the emphasis
is on the representation of knowledge about categories, individuals that belong to
those categories, and the relationships that can hold among these individuals. The
setofcategories,orconcepts,thatmakeupaparticularapplicationdomainiscalled
terminology itsterminology. Theportionofaknowledgebasethatcontainstheterminologyis
TBox traditionallycalledtheTBox;thisisincontrasttotheABoxthatcontainsfactsabout
ABox individuals. The terminology is typically arranged into a hierarchical organization
ontology calledanontologythatcapturesthesubset/supersetrelationsamongthecategories.
Returning to our earlier culinary domain, we represented domain concepts us-
ing unary predicates such as Restaurant(x); the DL equivalent omits the variable,
so the restaurant category is simply written as Restaurant.2 To capture the fact
thataparticulardomainelement,suchasFrasca,isarestaurant,weassertRestau-
rant(Frasca) in much the same way we would in FOL. The semantics of these
categories are specified in precisely the same way that was introduced earlier in
Section19.2:acategorylikeRestaurantsimplydenotesthesetofdomainelements
thatarerestaurants.
Oncewe’vespecifiedthecategoriesofinterestinaparticulardomain, thenext
step is to arrange them into a hierarchical structure. There are two ways to cap-
ture the hierarchical relationships present in a terminology: we can directly assert
relationsbetweencategoriesthatarerelatedhierarchically,orwecanprovidecom-
pletedefinitionsforourconceptsandthenrelyoninferencetoprovidehierarchical
relationships. Thechoicebetweenthesemethodshingesontheusetowhichthere-
sultingcategorieswillbeputandthefeasibilityofformulatingprecisedefinitionsfor
manynaturallyoccurringcategories. We’lldiscussthefirstoptionhereandreturnto
thenotionofdefinitionslaterinthissection.
subsumption Todirectlyspecifyahierarchicalstructure,wecanassertsubsumptionrelations
between the appropriate concepts in a terminology. The subsumption relation is
conventionally written as C D and is read as C is subsumed by D; that is, all
(cid:118)
membersofthecategoryCarealsomembersofthecategoryD.Notsurprisingly,the
formalsemanticsofthisrelationareprovidedbyasimplesetrelation; anydomain
elementthatisinthesetdenotedbyCisalsointhesetdenotedbyD.
AddingthefollowingstatementstotheTBoxassertsthatallrestaurantsarecom-
mercialestablishmentsand,moreover,thattherearevarioussubtypesofrestaurants.
Restaurant CommercialEstablishment (19.47)
(cid:118)
ItalianRestaurant Restaurant (19.48)
(cid:118)
ChineseRestaurant Restaurant (19.49)
(cid:118)
MexicanRestaurant Restaurant (19.50)
(cid:118)
Ontologiessuchasthisareconventionallyillustratedwithdiagramssuchastheone
2 DLstatementsareconventionallytypesetwithasansseriffont. We’llfollowthatconventionhere,
revertingtoourstandardmathematicalnotationwhengivingFOLequivalentsofDLstatements.19.5 • DESCRIPTIONLOGICS 421
showninFig.19.5, wheresubsumptionrelationsaredenotedbylinksbetweenthe
nodesrepresentingthecategories.
Commercial
Establishment
Restaurant
Italian Chinese Mexican
Restaurant Restaurant Restaurant
Figure19.5 A graphical network representation of a set of subsumption relations in the
restaurantdomain.
Note, thatitwaspreciselythevaguenatureofsemanticnetworkdiagramslike
thisthatmotivatedthedevelopmentofDescriptionLogics. Forexample,fromthis
diagram we can’t tell whether the given set of categories is exhaustive or disjoint.
Thatis,wecan’ttelliftheseareallthekindsofrestaurantsthatwe’llbedealingwith
inourdomainorwhethertheremightbeothers. Wealsocan’ttellifanindividual
restaurantmustfallintoonlyoneofthesecategories,orifitispossible,forexample,
forarestauranttobebothItalianandChinese. TheDLstatementsgivenaboveare
moretransparentintheirmeaning;theysimplyassertasetofsubsumptionrelations
betweencategoriesandmakenoclaimsaboutcoverageormutualexclusion.
If an application requires coverage and disjointness information, then such in-
formation must be made explicitly. The simplest ways to capture this kind of in-
formation is through the use of negation and disjunction operators. For example,
the following assertion would tell us that Chinese restaurants can’t also be Italian
restaurants.
ChineseRestaurant notItalianRestaurant (19.51)
(cid:118)
Specifyingthatasetofsubconceptscoversacategorycanbeachievedwithdisjunc-
tion,asinthefollowing:
Restaurant (19.52)
(cid:118)
(orItalianRestaurantChineseRestaurantMexicanRestaurant)
Having a hierarchy such as the one given in Fig. 19.5 tells us next to nothing
about the concepts in it. We certainly don’t know anything about what makes a
restaurantarestaurant,muchlessItalian,Chinese,orexpensive. Whatisneededare
additionalassertionsaboutwhatitmeanstobeamemberofanyofthesecategories.
In Description Logics such statements come in the form of relations between the
concepts being described and other concepts in the domain. In keeping with its
origins in structured network representations, relations in Description Logics are
typicallybinaryandareoftenreferredtoasroles,orrole-relations.
Toseehowsuchrelationswork,let’sconsidersomeofthefactsaboutrestaurants
discussedearlierinthechapter. We’llusethehasCuisinerelationtocaptureinfor-
mationastowhatkindsoffoodrestaurantsserveandthehasPriceRangerelation
to capture how pricey particular restaurants tend to be. We can use these relations
tosaysomethingmoreconcreteaboutourvariousclassesofrestaurants. Let’sstart422 CHAPTER19 • LOGICALREPRESENTATIONSOFSENTENCEMEANING
withourItalianRestaurantconcept. Asafirstapproximation,wemightsaysome-
thinguncontroversiallikeItalianrestaurantsserveItaliancuisine. Tocapturethese
notions, let’s first add some new concepts to our terminology to represent various
kindsofcuisine.
MexicanCuisine Cuisine ExpensiveRestaurant Restaurant
(cid:118) (cid:118)
ItalianCuisine Cuisine ModerateRestaurant Restaurant
(cid:118) (cid:118)
ChineseCuisine Cuisine CheapRestaurant Restaurant
(cid:118) (cid:118)
VegetarianCuisine Cuisine
(cid:118)
Next,let’sreviseourearlierversionofItalianRestauranttocapturecuisineinfor-
mation.
ItalianRestaurant Restaurant hasCuisine.ItalianCuisine (19.53)
(cid:118) (cid:117)∃
The correct way to read this expression is that individuals in the category Italian-
Restaurant are subsumed both by the category Restaurant and by an unnamed
classdefinedbytheexistentialclause—thesetofentitiesthatserveItaliancuisine.
AnequivalentstatementinFOLwouldbe
xItalianRestaurant(x) Restaurant(x) (19.54)
∀ →
( yServes(x,y) ItalianCuisine(y))
∧ ∃ ∧
This FOL translation should make it clear what the DL assertions given above do
anddonotentail. Inparticular,theydon’tsaythatdomainentitiesclassifiedasItal-
ianrestaurantscan’tengageinotherrelationslikebeingexpensiveorevenserving
Chinesecuisine. Andcritically,theydon’tsaymuchaboutdomainentitiesthatwe
know do serve Italian cuisine. In fact, inspection of the FOL translation makes it
clearthatwecannotinferthatanynewentitiesbelongtothiscategorybasedontheir
characteristics. The best we can do is infer new facts about restaurants that we’re
explicitlytoldaremembersofthiscategory.
Ofcourse,inferringthecategorymembershipofindividualsgivencertainchar-
acteristics is a common and critical reasoning task that we need to support. This
brings us back to the alternative approach to creating hierarchical structures in a
terminology: actuallyprovidingadefinitionofthecategorieswe’recreatinginthe
formofnecessaryandsufficientconditionsforcategorymembership. Inthiscase,
wemightexplicitlyprovideadefinitionforItalianRestaurantasbeingthoserestau-
rants that serve Italian cuisine, and ModerateRestaurant as being those whose
pricerangeismoderate.
ItalianRestaurant Restaurant hasCuisine.ItalianCuisine (19.55)
≡ (cid:117)∃
ModerateRestaurant Restaurant hasPriceRange.ModeratePrices (19.56)
≡ (cid:117)
Whileourearlierstatementsprovidednecessaryconditionsformembershipinthese
categories,thesestatementsprovidebothnecessaryandsufficientconditions.
Finally,let’snowconsiderthesuperficiallysimilarcaseofvegetarianrestaurants.
Clearly,vegetarianrestaurantsarethosethatservevegetariancuisine.Buttheydon’t
merelyservevegetarianfare,that’salltheyserve. Wecanaccommodatethiskindof
constraintbyaddinganadditionalrestrictionintheformofauniversalquantifierto19.5 • DESCRIPTIONLOGICS 423
ourearlierdescriptionofVegetarianRestaurants,asfollows:
VegetarianRestaurant Restaurant (19.57)
≡
hasCuisine.VegetarianCuisine
(cid:117)∃
hasCuisine.VegetarianCuisine
(cid:117)∀
Inference
ParallelingthefocusofDescriptionLogicsoncategories,relations,andindividuals
isaprocessingfocusonarestrictedsubsetoflogicalinference.Ratherthanemploy-
ingthefullrangeofreasoningpermittedbyFOL,DLreasoningsystemsemphasize
thecloselycoupledproblemsofsubsumptionandinstancechecking.
subsumption Subsumption, as a form of inference, is the task of determining, based on the
factsassertedinaterminology,whetherasuperset/subsetrelationshipexistsbetween
instance two concepts. Correspondingly, instance checking asks if an individual can be a
checking
memberofaparticularcategorygiventhefactsweknowaboutboththeindividual
and the terminology. The inference mechanisms underlying subsumption and in-
stancecheckinggobeyondsimplycheckingforexplicitlystatedsubsumptionrela-
tionsinaterminology. Theymustexplicitlyreasonusingtherelationalinformation
asserted about the terminology to infer appropriate subsumption and membership
relations.
Returningtoourrestaurantdomain,let’saddanewkindofrestaurantusingthe
followingstatement:
IlFornaio ModerateRestaurant hasCuisine.ItalianCuisine (19.58)
(cid:118) (cid:117)∃
Giventhisassertion,wemightaskwhethertheIlFornaiochainofrestaurantsmight
beclassifiedasanItalianrestaurantoravegetarianrestaurant. Moreprecisely, we
canposethefollowingquestionstoourreasoningsystem:
IlFornaio ItalianRestaurant (19.59)
(cid:118)
IlFornaio VegetarianRestaurant (19.60)
(cid:118)
TheanswertothefirstquestionispositivesinceIlFornaiomeetsthecriteriawe
specifiedforthecategoryItalianRestaurant: it’saRestaurantsinceweexplicitly
classified it as a ModerateRestaurant, which is a subtype of Restaurant, and it
meetsthehas.Cuisineclassrestrictionsincewe’veassertedthatdirectly.
Theanswertothesecondquestionisnegative. Recall,thatourcriteriaforveg-
etarian restaurants contains two requirements: it has to serve vegetarian fare, and
that’sallitcanserve. OurcurrentdefinitionforIlFornaiofailsonbothcountssince
we have not asserted any relations that state that IlFornaio serves vegetarian fare,
andtherelationwehaveasserted,hasCuisine.ItalianCuisine,contradictsthesec-
ondcriteria.
Arelatedreasoningtask,basedonthebasicsubsumptioninference,istoderive
implied theimpliedhierarchyforaterminologygivenfactsaboutthecategoriesintheter-
hierarchy
minology. Thistaskroughlycorrespondstoarepeatedapplicationofthesubsump-
tionoperatortopairsofconceptsintheterminology. Givenourcurrentcollectionof
statements,theexpandedhierarchyshowninFig.19.6canbeinferred. Youshould
convinceyourselfthatthisdiagramcontainsallandonlythesubsumptionlinksthat
shouldbepresentgivenourcurrentknowledge.
Instancecheckingisthetaskofdeterminingwhetheraparticularindividualcan
beclassifiedasamemberofaparticularcategory. Thisprocesstakeswhatisknown424 CHAPTER19 • LOGICALREPRESENTATIONSOFSENTENCEMEANING
Restaurant
Italian Chinese Mexican Vegetarian Cheap Moderate Expensive
Restaurant Restaurant Restaurant Restaurant Restaurant Restaurant Restaurant
Il Fornaio
Figure19.6 Agraphicalnetworkrepresentationofthecompletesetofsubsumptionrela-
tionsintherestaurantdomaingiventhecurrentsetofassertionsintheTBox.
aboutagivenindividual,intheformofrelationsandexplicitcategoricalstatements,
andthencomparesthatinformationwithwhatisknownaboutthecurrentterminol-
ogy. Itthenreturnsalistofthemostspecificcategoriestowhichtheindividualcan
belong.
Asanexampleofacategorizationproblem,consideranestablishmentthatwe’re
toldisarestaurantandservesItaliancuisine.
Restaurant(Gondolier)
hasCuisine(Gondolier,ItalianCuisine)
Here,we’rebeingtoldthattheentitydenotedbythetermGondolierisarestau-
rant and serves Italian food. Given this new information and the contents of our
currentTBox,wemightreasonablyliketoaskifthisisanItalianrestaurant,ifitis
avegetarianrestaurant,orifithasmoderateprices.
Assuming the definitional statements given earlier, we can indeed categorize
the Gondolier as an Italian restaurant. That is, the information we’ve been given
about it meets the necessary and sufficient conditions required for membership in
thiscategory. AndaswiththeIlFornaiocategory,thisindividualfailstomatchthe
stated criteria for the VegetarianRestaurant. Finally, the Gondolier might also
turn out to be a moderately priced restaurant, but we can’t tell at this point since
wedon’tknowanythingaboutitsprices. Whatthismeansisthatgivenourcurrent
knowledgetheanswertothequeryModerateRestaurant(Gondolier)wouldbefalse
sinceitlackstherequiredhasPriceRangerelation.
Theimplementationofsubsumption,instancechecking,aswellasotherkindsof
inferencesneededforpracticalapplications,variesaccordingtotheexpressivityof
theDescriptionLogicbeingused. However,foraDescriptionLogicofevenmodest
power, the primary implementation techniques are based on satisfiability methods
thatinturnrelyontheunderlyingmodel-basedsemanticsintroducedearlierinthis
chapter.
OWLandtheSemanticWeb
The highest-profile role for Description Logics, to date, has been as a part of the
developmentoftheSemanticWeb. TheSemanticWebisanongoingefforttopro-
vide a way to formally specify the semantics of the contents of the Web (Fensel
etal.,2003). Akeycomponentofthiseffortinvolvesthecreationanddeployment
ofontologiesforvariousapplicationareasofinterest. Themeaningrepresentation19.6 • SUMMARY 425
WebOntology languageusedtorepresentthisknowledgeistheWebOntologyLanguage(OWL)
Language
(McGuiness and van Harmelen, 2004). OWL embodies a Description Logic that
correspondsroughlytotheonewe’vebeendescribinghere.
19.6 Summary
Thischapterhasintroducedtherepresentationalapproachtomeaning. Thefollow-
ingaresomeofthehighlightsofthischapter:
• A major approach to meaning in computational linguistics involves the cre-
ation of formal meaning representations that capture the meaning-related
contentoflinguisticinputs. Theserepresentationsareintendedtobridgethe
gapfromlanguagetocommon-senseknowledgeoftheworld.
• The frameworks that specify the syntax and semantics of these representa-
tionsarecalledmeaningrepresentationlanguages. Awidevarietyofsuch
languagesareusedinnaturallanguageprocessingandartificialintelligence.
• Such representations need to be able to support the practical computational
requirementsofsemanticprocessing. Amongthesearetheneedtodetermine
thetruthofpropositions,tosupportunambiguousrepresentations,torep-
resentvariables,tosupportinference,andtobesufficientlyexpressive.
• Human languages have a wide variety of features that are used to convey
meaning.Amongthemostimportantoftheseistheabilitytoconveyapredicate-
argumentstructure.
• First-Order Logic is a well-understood, computationally tractable meaning
representationlanguagethatoffersmuchofwhatisneededinameaningrep-
resentationlanguage.
• Important elements of semantic representation including states and events
canbecapturedinFOL.
• SemanticnetworksandframescanbecapturedwithintheFOLframework.
• Modern Description Logics consist of useful and computationally tractable
subsets of full First-Order Logic. The most prominent use of a description
logicistheWebOntologyLanguage(OWL),usedinthespecificationofthe
SemanticWeb.
Bibliographical and Historical Notes
Theearliestcomputationaluseofdeclarativemeaningrepresentationsinnaturallan-
guage processing was in the context of question-answering systems (Green et al.
1961, Raphael 1968, Lindsey 1963). These systems employed ad hoc representa-
tionsforthefactsneededtoanswerquestions. Questionswerethentranslatedinto
aformthatcouldbematchedagainstfactsintheknowledgebase. Simmons(1965)
providesanoverviewoftheseearlyefforts.
Woods (1967) investigated the use of FOL-like representations in question an-
swering as a replacement for the ad hoc representations in use at the time. Woods
(1973) further developed and extended these ideas in the landmark Lunar system.426 CHAPTER19 • LOGICALREPRESENTATIONSOFSENTENCEMEANING
Interestingly,therepresentationsusedinLunarhadbothtruth-conditionalandpro-
ceduralsemantics. Winograd(1972)employedasimilarrepresentationbasedonthe
Micro-PlannerlanguageinhisSHRDLUsystem.
Duringthissameperiod,researchersinterestedinthecognitivemodelingoflan-
guage and memory had been working with various forms of associative network
representations. Masterman (1957) was the first to make computational use of a
semantic network-like knowledge representation, although semantic networks are
generally credited to Quillian (1968). A considerable amount of work in the se-
manticnetworkframeworkwascarriedoutduringthisera(NormanandRumelhart
1975, Schank 1972, Wilks 1975c, Wilks 1975b, Kintsch 1974). It was during this
period thata numberof researchers beganto incorporateFillmore’s notionof case
roles (Fillmore, 1968) into their representations. Simmons (1973) was the earliest
adopterofcaserolesaspartofrepresentationsfornaturallanguageprocessing.
DetailedanalysesbyWoods(1975)andBrachman(1979)aimedatfiguringout
whatsemanticnetworksactuallymeanledtothedevelopmentofanumberofmore
sophisticatednetwork-likelanguagesincludingKRL(BobrowandWinograd,1977)
and KL-ONE (BrachmanandSchmolze,1985). Astheseframeworksbecamemore
sophisticatedandwelldefined,itbecameclearthattheywererestrictedvariantsof
FOLcoupledwithspecializedindexinginferenceprocedures. Ausefulcollectionof
paperscoveringmuchofthisworkcanbefoundinBrachmanandLevesque(1985).
RussellandNorvig(2002)describeamodernperspectiveontheserepresentational
efforts.
Linguisticeffortstoassignsemanticstructurestonaturallanguagesentencesin
the generative era began with the work of Katz and Fodor (1963). The limitations
of their simple feature-based representations and the natural fit of logic to many
of the linguistic problems of the day quickly led to the adoption of a variety of
predicate-argumentstructuresaspreferredsemanticrepresentations(Lakoff1972a,
McCawley 1968). The subsequent introduction by Montague (1973) of the truth-
conditional model-theoretic framework into linguistic theory led to a much tighter
integrationbetweentheoriesofformalsyntaxandawiderangeofformalsemantic
frameworks. Good introductions to Montague semantics and its role in linguistic
theorycanbefoundinDowtyetal.(1981)andPartee(1976).
The representation of events as reified objects is due to Davidson (1967). The
approachpresentedhere,whichexplicitlyreifieseventparticipants,isduetoParsons
(1990).
A recent comprehensive treatment of logic and language can be found in van
BenthemandterMeulen(1997). AclassicsemanticstextisLyons(1977). McCaw-
ley(1993)isanindispensabletextbookcoveringawiderangeoftopicsconcerning
logic and language. Chierchia and McConnell-Ginet (1991) also broadly covers
semantic issues from a linguistic perspective. Heim and Kratzer (1998) is a more
recenttextwrittenfromtheperspectiveofcurrentgenerativetheory.
Exercises
19.1 Peruse your daily newspaper for three examples of ambiguous sentences or
headlines. Describethevarioussourcesoftheambiguities.
19.2 Consider a domain in which the word coffee can refer to the following con-
ceptsinaknowledge-basedsystem: acaffeinatedordecaffeinatedbeverage,
groundcoffeeusedtomakeeitherkindofbeverage,andthebeansthemselves.EXERCISES 427
Giveargumentsastowhichofthefollowingusesofcoffeeareambiguousand
whicharevague.
1. I’vehadmycoffeefortoday.
2. Buysomecoffeeonyourwayhome.
3. Pleasegrindsomemorecoffee.
19.3 Thefollowingrule,whichwegaveasatranslationforExample19.25,isnot
areasonabledefinitionofwhatitmeanstobeavegetarianrestaurant.
xVegetarianRestaurant(x) = Serves(x,VegetarianFood)
∀ ⇒
GiveaFOLrulethatbetterdefinesvegetarianrestaurantsintermsofwhatthey
serve.
19.4 GiveFOLtranslationsforthefollowingsentences:
1. Vegetariansdonoteatmeat.
2. Notallvegetarianseateggs.
19.5 Giveasetoffactsandinferencesnecessarytoprovethefollowingassertions:
1. McDonald’sisnotavegetarianrestaurant.
2. SomevegetarianscaneatatMcDonald’s.
Don’t just place these facts in your knowledge base. Show that they can be
inferredfromsomemoregeneralfactsaboutvegetariansandMcDonald’s.
19.6 Onpage416, wegavetherepresentationNear(Centro,Bacaro)asatransla-
tionforthesentenceCentroisnearBacaro. Inatruth-conditionalsemantics,
this formula is either true or false given some model. Critique this truth-
conditionalapproachwithrespecttothemeaningofwordslikenear.CHAPTER
20 Computational Semantics and
Semantic Parsing
Placeholder
428CHAPTER
21 Relation and Event Extraction
Imagine that you are an analyst with an investment firm that tracks airline stocks.
You’re given the task of determining the relationship (if any) between airline an-
nouncements of fare increases and the behavior of their stocks the next day. His-
torical data about stock prices is easy to come by, but what about the airline an-
nouncements? Youwillneedtoknowatleastthenameoftheairline,thenatureof
theproposedfarehike,thedatesoftheannouncement,andpossiblytheresponseof
otherairlines. Fortunately,thesecanbeallfoundinnewsarticleslikethisone:
Citinghighfuelprices,UnitedAirlinessaidFridayithasincreasedfares
by $6 per round trip on flights to some cities also served by lower-
cost carriers. American Airlines, a unit of AMR Corp., immediately
matchedthemove,spokesmanTimWagnersaid.United,aunitofUAL
Corp.,saidtheincreasetookeffectThursdayandappliestomostroutes
whereitcompetesagainstdiscountcarriers,suchasChicagotoDallas
andDenvertoSanFrancisco.
This chapter presents techniques for extracting limited kinds of semantic con-
information tentfromtext. Thisprocessofinformationextraction(IE)turnstheunstructured
extraction
information embedded in texts into structured data, for example for populating a
relationaldatabasetoenablefurtherprocessing.
relation Webeginwiththetaskofrelationextraction: findingandclassifyingsemantic
extraction
relations among entities mentioned in a text, like child-of (X is the child-of Y), or
part-whole or geospatial relations. Relation extraction has close links to populat-
knowledge ing a relational database, and knowledge graphs, datasets of structured relational
graphs
knowledge,areausefulwayforsearchenginestopresentinformationtousers.
event Next,wediscusseventextraction,thetaskoffindingeventsinwhichtheseen-
extraction
titiesparticipate,like,inoursampletext,thefareincreasesbyUnitedandAmerican
templatefilling and the reporting events said and cite. The related task of template filling is to
findrecurringstereotypicalefentsorsituationsindocumentsandfillinthetemplate
slots.Theseslot-fillersmayconsistoftextsegmentsextracteddirectlyfromthetext,
orconceptsliketimes,amounts,orontologyentitiesthathavebeeninferredthrough
additionalprocessing. Ourairlinetextpresentssuchastereotypicalsituationsince
airlinesoftenraisefaresandthenwaittoseeifcompetitorsfollowalong. Herewe
canidentifyUnitedasaleadairlinethatinitiallyraiseditsfares,$6astheamount,
Thursdayastheincreasedate,andAmericanasanairlinethatfollowedalong,lead-
ingtoafilledtemplatelikethefollowing:
FARE-RAISEATTEMPT: LEADAIRLINE: UNITEDAIRLINES
AMOUNT: $6
 
EFFECTIVEDATE: 2006-10-26
 
FOLLOWER: AMERICANAIRLINES
 
 430 CHAPTER21 • RELATIONANDEVENTEXTRACTION
PERSON- GENERAL PART-
PHYSICAL
SOCIAL AFFILIATION WHOLE
Family Lasting Near Citizen- Subsidiary
Personal Resident- Geographical
Located Ethnicity- Org-Location-
Business
Religion Origin
ORG
AFFILIATION ARTIFACT
Investor
Founder
Student-Alum
Ownership User-Owner-Inventor-
Employment Manufacturer
Membership
Sports-Affiliation
Figure21.1 The17relationsusedintheACErelationextractiontask.
Relations Types Examples
Physical-Located PER-GPE HewasinTennessee
Part-Whole-Subsidiary ORG-ORG XYZ,theparentcompanyofABC
Person-Social-Family PER-PER Yoko’shusbandJohn
Org-AFF-Founder PER-ORG SteveJobs,co-founderofApple...
Figure21.2 Semanticrelationswithexamplesandthenamedentitytypestheyinvolve.
21.1 Relation Extraction
Let’s assume that we have detected the named entities in our sample text (perhaps
usingthetechniquesofChapter8),andwouldliketodiscerntherelationshipsthat
existamongthedetectedentities:
Citing high fuel prices, [ ORG United Airlines] said [ TIME Friday] it
has increased fares by [
MONEY
$6] per round trip on flights to some
citiesalsoservedbylower-costcarriers. [
ORG
AmericanAirlines],a
unitof[ ORGAMRCorp.],immediatelymatchedthemove,spokesman
[
PER
TimWagner]said. [
ORG
United],aunitof[
ORG
UALCorp.],
said the increase took effect [ TIME Thursday] and applies to most
routeswhereitcompetesagainstdiscountcarriers,suchas[ LOCChicago]
to[ LOCDallas]and[ LOCDenver]to[ LOCSanFrancisco].
The text tells us, for example, that Tim Wagner is a spokesman for American
Airlines, that United is a unit of UAL Corp., and that American is a unit of AMR.
These binary relations are instances of more generic relations such as part-of or
employsthatarefairlyfrequentinnews-styletexts.Figure21.1liststhe17relations
used in the ACE relation extraction evaluations and Fig. 21.2 shows some sample
relations. Wemightalsoextractmoredomain-specificrelationsuchasthenotionof
anairlineroute. ForexamplefromthistextwecanconcludethatUnitedhasroutes
toChicago,Dallas,Denver,andSanFrancisco.
Theserelationscorrespondnicelytothemodel-theoreticnotionsweintroduced
inChapter19togroundthemeaningsofthelogicalforms.Thatis,arelationconsists
ofasetoforderedtuplesoverelementsofadomain. Inmoststandardinformation-
extraction applications, the domain elements correspond to the named entities that
occurinthetext,totheunderlyingentitiesthatresultfromcoreferenceresolution,or
toentitiesselectedfromadomainontology. Figure21.3showsamodel-basedview
ofthesetofentitiesandrelationsthatcanbeextractedfromourrunningexample.21.1 • RELATIONEXTRACTION 431
Domain D= a,b,c,d,e,f,g,h,i
{ }
United,UAL,AmericanAirlines,AMR a,b,c,d
TimWagner e
Chicago,Dallas,Denver,andSanFrancisco f,g,h,i
Classes
United,UAL,American,andAMRareorganizations Org= a,b,c,d
{ }
TimWagnerisaperson Pers= e
{ }
Chicago,Dallas,Denver,andSanFranciscoareplaces Loc= f,g,h,i
{ }
Relations
UnitedisaunitofUAL PartOf = a,b , c,d
{(cid:104) (cid:105) (cid:104) (cid:105)}
AmericanisaunitofAMR
TimWagnerworksforAmericanAirlines OrgAff = c,e
{(cid:104) (cid:105)}
UnitedservesChicago,Dallas,Denver,andSanFrancisco Serves= a,f , a,g , a,h , a,i
{(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)}
Figure21.3 Amodel-basedviewoftherelationsandentitiesinoursampletext.
Noticehowthismodel-theoreticviewsubsumestheNERtaskaswell;namedentity
recognitioncorrespondstotheidentificationofaclassofunaryrelations.
Setsofrelationshavebeendefinedformanyotherdomainsaswell.Forexample
UMLS, the Unified Medical Language System from the US National Library of
Medicinehasanetworkthatdefines134broadsubjectcategories,entitytypes,and
54relationsbetweentheentities,suchasthefollowing:
Entity Relation Entity
Injury disrupts PhysiologicalFunction
BodilyLocation location-of BiologicFunction
AnatomicalStructure part-of Organism
PharmacologicSubstance causes PathologicalFunction
PharmacologicSubstance treats PathologicFunction
Givenamedicalsentencelikethisone:
(21.1) Dopplerechocardiographycanbeusedtodiagnoseleftanteriordescending
arterystenosisinpatientswithtype2diabetes
WecouldthusextracttheUMLSrelation:
Echocardiography,DopplerDiagnosesAcquiredstenosis
infoboxes Wikipediaalsooffersalargesupplyofrelations, drawnfrominfoboxes, struc-
turedtablesassociatedwithcertainWikipediaarticles. Forexample,theWikipedia
infobox for Stanford includes structured facts like state = "California" or
president = "Marc Tessier-Lavigne". These facts can be turned into rela-
RDF tionslikepresident-oforlocated-in. orintorelationsinametalanguagecalledRDF
RDFtriple (Resource Description Framework). An RDF triple is a tuple of entity-relation-
entity,calledasubject-predicate-objectexpression. Here’sasampleRDFtriple:
subject predicate object
GoldenGatePark location SanFrancisco
ForexamplethecrowdsourcedDBpedia(Bizeretal.,2009)isanontologyde-
rived from Wikipedia containing over 2 billion RDF triples. Another dataset from
Freebase Wikipediainfoboxes,Freebase(Bollackeretal.,2008),nowpartofWikidata(Vrandecˇic´
andKro¨tzsch,2014),hasrelationsbetweenpeopleandtheirnationality,orlocations,
andotherlocationstheyarecontainedin.432 CHAPTER21 • RELATIONANDEVENTEXTRACTION
WordNetorotherontologiesofferusefulontologicalrelationsthatexpresshier-
is-a archicalrelationsbetweenwordsorconcepts. ForexampleWordNethastheis-aor
hypernym hypernymrelationbetweenclasses,
Giraffeis-aruminantis-aungulateis-amammalis-avertebrate...
WordNet also has Instance-of relation between individuals and classes, so that for
example San Francisco is in the Instance-of relation with city. Extracting these
relationsisanimportantstepinextendingorbuildingontologies.
Finally, there are large datasets that contain sentences hand-labeled with their
relations,designedfortrainingandtestingrelationextractors. TheTACREDdataset
(Zhang et al., 2017) contains 106,264 examples of relation triples about particular
peopleororganizations,labeledinsentencesfromnewsandwebtextdrawnfromthe
annualTACKnowledgeBasePopulation(TACKBP)challenges.TACREDcontains
41relationtypes(likeper:cityofbirth,org:subsidiaries,org:memberof,per:spouse),
plusanorelationtag;examplesareshowninFig.21.4. About80%ofallexamples
areannotatedasnorelation;havingsufficientnegativedataisimportantfortraining
supervisedclassifiers.
Example EntityTypes&Label
Carey will succeed Cathleen P. Black, who held the position for 15 PERSON/TITLE
years and will take on a new role as chairwoman of Hearst Maga- Relation: per:title
zines,thecompanysaid.
IreneMorganKirkaldy,whowasbornandrearedinBaltimore,lived PERSON/CITY
onLongIslandandranachild-carecenterinQueenswithhersecond Relation: per:city of birth
husband,StanleyKirkaldy.
Baldwindeclinedfurthercomment,andsaidJetBluechiefexecutive Types: PERSON/TITLE
DaveBargerwasunavailable. Relation: no relation
Figure21.4 ExamplesentencesandlabelsfromtheTACREDdataset(Zhangetal.,2017).
A standard dataset was also produced for the SemEval 2010 Task 8, detecting
relationsbetweennominals(Hendrickxetal.,2009). Thedatasethas10,717exam-
ples, each with a pair of nominals (untyped) hand-labeled with one of 9 directed
relationslikeproduct-producer(afactorymanufacturessuits)orcomponent-whole
(myapartmenthasalargekitchen).
21.2 Relation Extraction Algorithms
Therearefivemainclassesofalgorithmsforrelationextraction: handwrittenpat-
terns,supervisedmachinelearning,semi-supervised(viabootstrappingordis-
tant supervision), and unsupervised. We’ll introduce each of these in the next
sections.
21.2.1 UsingPatternstoExtractRelations
The earliest and still common algorithm for relation extraction is lexico-syntactic
patterns, firstdevelopedbyHearst(1992a), andthereforeoftencalledHearstpat-
Hearstpatterns terns. Considerthefollowingsentence:
Agarisasubstancepreparedfromamixtureofredalgae, suchasGe-
lidium,forlaboratoryorindustrialuse.21.2 • RELATIONEXTRACTIONALGORITHMS 433
HearstpointsoutthatmosthumanreaderswillnotknowwhatGelidiumis,butthat
theycanreadilyinferthatitisakindof(ahyponymof)redalgae,whateverthatis.
Shesuggeststhatthefollowinglexico-syntacticpattern
NP suchasNP ,NP ...,(andor)NP ,i 1 (21.2)
0 1 2 i
{ | } ≥
impliesthefollowingsemantics
NP,i 1,hyponym(NP,NP ) (21.3)
i i 0
∀ ≥
allowingustoinfer
hyponym(Gelidium,redalgae) (21.4)
NP ,NP * , (andor)otherNP H temples,treasuries,andotherimportantcivicbuildings
{ } {} |
NP H suchas NP, * (orand) NP redalgaesuchasGelidium
{ } { | }
suchNP H as NP, * (orand) NP suchauthorsasHerrick,Goldsmith,andShakespeare
{ } { | }
NP H , including NP, * (orand) NP common-lawcountries,includingCanadaandEngland
{} { } { | }
NP H , especially NP * (orand) NP Europeancountries,especiallyFrance,England,andSpain
{} { } { | }
Figure21.5 Hand-builtlexico-syntacticpatternsforfindinghypernyms,using tomarkoptionality(Hearst
{}
1992a,Hearst1998).
Figure 21.5 shows five patterns Hearst (1992a, 1998) suggested for inferring
thehyponymrelation;we’veshownNP astheparent/hyponym. Modernversions
H
of the pattern-based approach extend it by adding named entity constraints. For
exampleifourgoalistoanswerquestionsabout“Whoholdswhatofficeinwhich
organization?”,wecanusepatternslikethefollowing:
PER,POSITIONofORG:
GeorgeMarshall,SecretaryofStateoftheUnitedStates
PER(namedappointedchoseetc.) PERPrep? POSITION
| | |
TrumanappointedMarshallSecretaryofState
PER[be]? (namedappointedetc.) Prep? ORGPOSITION
| |
GeorgeMarshallwasnamedUSSecretaryofState
Hand-builtpatternshavetheadvantageofhigh-precisionandtheycanbetailored
to specific domains. On the other hand, they are often low-recall, and it’s a lot of
worktocreatethemforallpossiblepatterns.
21.2.2 RelationExtractionviaSupervisedLearning
Supervisedmachinelearningapproachestorelationextractionfollowaschemethat
shouldbefamiliarbynow. Afixedsetofrelationsandentitiesischosen,atraining
corpusishand-annotatedwiththerelationsandentities,andtheannotatedtextsare
thenusedtotrainclassifierstoannotateanunseentestset.
Themoststraightforwardapproach,illustratedinFig.21.6is: (1)Findpairsof
named entities (usually in the same sentence). (2): Apply a relation-classification
on each pair. The classifier can use any supervised technique (logistic regression,
RNN,Transformer,randomforest,etc.).
Anoptionalintermediatefilteringclassifiercanbeusedtospeeduptheprocess-
ingbymakingabinarydecisiononwhetheragivenpairofnamedentitiesarerelated
(byanyrelation). It’strainedonpositiveexamplesextracteddirectlyfromallrela-
tionsintheannotatedcorpus,andnegativeexamplesgeneratedfromwithin-sentence
entitypairsthatarenotannotatedwitharelation.434 CHAPTER21 • RELATIONANDEVENTEXTRACTION
functionFINDRELATIONS(words)returnsrelations
relations nil
←
entities FINDENTITIES(words)
←
forall entitypairs e1,e2 inentitiesdo
(cid:104) (cid:105)
ifRELATED?(e1,e2)
relations relations+CLASSIFYRELATION(e1,e2)
←
Figure21.6 Findingandclassifyingtherelationsamongentitiesinatext.
Feature-basedsupervisedrelationclassifiers. Let’sconsidersamplefeaturesfor
afeature-basedclassifier(likelogisticregressionorrandomforests),classifyingthe
relationshipbetweenAmericanAirlines(Mention1,orM1)andTimWagner(Men-
tion2,M2)fromthissentence:
(21.5) AmericanAirlines,aunitofAMR,immediatelymatchedthemove,
spokesmanTimWagnersaid
Theseincludewordfeatures(asembeddings,or1-hot,stemmedornot):
• TheheadwordsofM1andM2andtheirconcatenation
Airlines Wagner Airlines-Wagner
• Bag-of-wordsandbigramsinM1andM2
American,Airlines,Tim,Wagner,AmericanAirlines,TimWagner
• Wordsorbigramsinparticularpositions
M2: -1spokesman
M2: +1said
• BagofwordsorbigramsbetweenM1andM2:
a,AMR,of,immediately,matched,move,spokesman,the,unit
Namedentityfeatures:
• Named-entitytypesandtheirconcatenation
(M1: ORG,M2: PER,M1M2: ORG-PER)
• EntityLevelofM1andM2(fromthesetNAME,NOMINAL,PRONOUN)
M1: NAME[itorhewouldbePRONOUN]
M2: NAME[thecompanywouldbeNOMINAL]
• Numberofentitiesbetweenthearguments(inthiscase1,forAMR)
Syntactic structure is a useful signal, often represented as the dependency or
constituencysyntacticpathtraversedthroughthetreebetweentheentities.
• ConstituentpathsbetweenM1andM2
NP NP S S NP
↑ ↑ ↑ ↓
• Dependency-treepaths
Airlines matched said Wagner
subj comp subj
← ← →
Neuralsupervisedrelationclassifiers Neuralmodelsforrelationextractionsim-
ilarlytreatthetaskassupervisedclassification. Let’sconsideratypicalsystemap-
plied to the TACRED relation extraction dataset and task (Zhang et al., 2017). In
TACRED we are given a sentence and two spans within it: a subject, which is a
personororganization,andanobject,whichisanyotherentity. Thetaskistoassign
arelationfromthe42TACrelations,ornorelation.21.2 • RELATIONEXTRACTIONALGORITHMS 435
p(relation|SUBJ,OBJ)
Linear
Classifier
ENCODER
[CLS] [SUBJ_PERSON]was born in [OBJ_LOC] , Michigan
Figure21.7 Relationextractionasalinearlayerontopofanencoder(inthiscaseBERT),
withthesubjectandobjectentitiesreplacedintheinputbytheirNERtags(Zhangetal.2017,
Joshietal.2020).
A typical Transformer-encoder algorithm, shown in Fig. 21.7, simply takes a
pretrainedencoderlikeBERTandaddsalinearlayerontopofthesentencerepre-
sentation(forexampletheBERT[CLS]token),alinearlayerthatisfinetunedasa
1-of-N classifier to assign one of the 43 labels. The input to the BERT encoder is
partiallyde-lexified;thesubjectandobjectentitiesarereplacedintheinputbytheir
NERtags.Thishelpskeepthesystemfromoverfittingtotheindividuallexicalitems
(Zhangetal.,2017). WhenusingBERT-typeTransformersforrelationextraction,it
helpstouseversionsofBERTlikeRoBERTa(Liuetal.,2019)orSPANbert(Joshi
etal.,2020)thatdon’thavetwosequencesseparatedbya[SEP]token,butinstead
formtheinputfromasinglelongsequenceofsentences.
In general, if the test set is similar enough to the training set, and if there is
enough hand-labeled data, supervised relation extraction systems can get high ac-
curacies. But labeling a large training set is extremely expensive and supervised
modelsarebrittle: theydon’tgeneralizewelltodifferenttextgenres. Forthisrea-
son, much research in relation extraction has focused on the semi-supervised and
unsupervisedapproachesweturntonext.
21.2.3 SemisupervisedRelationExtractionviaBootstrapping
Supervised machine learning assumes that we have lots of labeled data. Unfortu-
nately, this is expensive. But suppose we just have a few high-precision seed pat-
seedpatterns terns, like those in Section 21.2.1, or perhaps a few seed tuples. That’s enough
seedtuples tobootstrapaclassifier! Bootstrappingproceedsbytakingtheentitiesintheseed
bootstrapping pair,andthenfindingsentences(ontheweb,orwhateverdatasetweareusing)that
containbothentities. Fromallsuchsentences,weextractandgeneralizethecontext
aroundtheentitiestolearnnewpatterns. Fig.21.8sketchesabasicalgorithm.
functionBOOTSTRAP(RelationR)returnsnewrelationtuples
tuples GatherasetofseedtuplesthathaverelationR
←
iterate
sentences findsentencesthatcontainentitiesintuples
←
patterns generalizethecontextbetweenandaroundentitiesinsentences
←
newpairs usepatternstoidentifymoretuples
←
newpairs newpairswithhighconfidence
←
tuples tuples+newpairs
←
returntuples
Figure21.8 Bootstrappingfromseedentitypairstolearnrelations.436 CHAPTER21 • RELATIONANDEVENTEXTRACTION
Suppose, forexample, thatweneedtocreatealistofairline/hubpairs, andwe
knowonlythatRyanairhasahubatCharleroi. Wecanusethisseedfacttodiscover
new patterns by finding other mentions of this relation in our corpus. We search
for the terms Ryanair, Charleroi and hub in some proximity. Perhaps we find the
followingsetofsentences:
(21.6) BudgetairlineRyanair,whichusesCharleroiasahub,scrappedall
weekendflightsoutoftheairport.
(21.7) AllflightsinandoutofRyanair’shubatCharleroiairportweregroundedon
Friday...
(21.8) AspokesmanatCharleroi,amainhubforRyanair,estimatedthat8000
passengershadalreadybeenaffected.
Fromtheseresults,wecanusethecontextofwordsbetweentheentitymentions,
the words before mention one, the word after mention two, and the named entity
types of the two mentions, and perhaps other features, to extract general patterns
suchasthefollowing:
/ [ORG], which uses [LOC] as a hub /
/ [ORG]’s hub at [LOC] /
/ [LOC], a main hub for [ORG] /
Thesenewpatternscanthenbeusedtosearchforadditionaltuples.
confidence Bootstrappingsystemsalsoassignconfidencevaluestonewtuplestoavoidse-
values
semanticdrift mantic drift. In semantic drift, an erroneous pattern leads to the introduction of
erroneoustuples,which,inturn,leadtothecreationofproblematicpatternsandthe
meaningoftheextractedrelations‘drifts’. Considerthefollowingexample:
(21.9) SydneyhasaferryhubatCircularQuay.
If accepted as a positive example, this expression could lead to the incorrect in-
troduction of the tuple Sydney,CircularQuay . Patterns based on this tuple could
(cid:104) (cid:105)
propagatefurthererrorsintothedatabase.
Confidencevaluesforpatternsarebasedonbalancingtwofactors: thepattern’s
performance with respect to the current setof tuples and the pattern’s productivity
in terms of the number of matches it produces in the document collection. More
formally,givenadocumentcollectionD,acurrentsetoftuplesT,andaproposed
pattern p,weneedtotracktwofactors:
• hits(p): thesetoftuplesinT that pmatcheswhilelookinginD
• finds(p): Thetotalsetoftuplesthat pfindsinD
Thefollowingequationbalancestheseconsiderations(RiloffandJones,1999).
hits(p)
Conf (p)= | | log(finds(p)) (21.10)
RlogF finds(p) | |
| |
Thismetricisgenerallynormalizedtoproduceaprobability.
Wecanassesstheconfidenceinaproposednewtuplebycombiningtheevidence
supportingitfromallthepatternsP thatmatchthattupleinD(AgichteinandGra-
(cid:48)
noisy-or vano,2000). Onewaytocombinesuchevidenceisthenoisy-ortechnique. Assume
that a given tuple is supported by a subset of the patterns in P, each with its own
confidence assessed as above. In the noisy-or model, we make two basic assump-
tions. First,thatforaproposedtupletobefalse,allofitssupportingpatternsmust
have been in error, and second, that the sources of their individual failures are all
independent. Ifwelooselytreatourconfidencemeasuresasprobabilities, thenthe
probabilityofanyindividualpattern pfailingis1 Conf(p);theprobabilityofall
−21.2 • RELATIONEXTRACTIONALGORITHMS 437
ofthesupportingpatternsforatuplebeingwrongistheproductoftheirindividual
failureprobabilities,leavinguswiththefollowingequationforourconfidenceina
newtuple.
Conf(t)=1 (1 Conf(p)) (21.11)
− −
p(cid:89)∈P
(cid:48)
Setting conservative confidence thresholds for the acceptance of new patterns
andtuplesduringthebootstrappingprocesshelpspreventthesystemfromdrifting
awayfromthetargetedrelation.
21.2.4 DistantSupervisionforRelationExtraction
Although hand-labeling text with relation labels is expensive to produce, there are
distant ways to find indirect sources of training data. The distant supervision method
supervision
(Mintzetal.,2009)combinestheadvantagesofbootstrappingwithsupervisedlearn-
ing. Insteadofjustahandfulofseeds, distantsupervisionusesalargedatabaseto
acquireahugenumberofseedexamples,createslotsofnoisypatternfeaturesfrom
alltheseexamplesandthencombinestheminasupervisedclassifier.
For example suppose we are trying to learn the place-of-birth relationship be-
tweenpeopleandtheirbirthcities. Intheseed-basedapproach,wemighthaveonly
5examplestostartwith. ButWikipedia-baseddatabaseslikeDBPediaorFreebase
have tens of thousands of examples of many relations; including over 100,000 ex-
amplesofplace-of-birth,(<Edwin Hubble, Marshfield>,<Albert Einstein,
Ulm>,etc.,). Thenextstepistorunnamedentitytaggersonlargeamountsoftext—
Mintzetal.(2009)used800,000articlesfromWikipedia—andextractallsentences
thathavetwonamedentitiesthatmatchthetuple,likethefollowing:
...HubblewasborninMarshfield...
...Einstein,born(1879),Ulm...
...Hubble’sbirthplaceinMarshfield...
Training instances can now be extracted from this data, one training instance
foreachidenticaltuple<relation, entity1, entity2>.Thustherewillbeone
traininginstanceforeachof:
<born-in, Edwin Hubble, Marshfield>
<born-in, Albert Einstein, Ulm>
<born-year, Albert Einstein, 1879>
andsoon.
We can then apply feature-based or neural classification. For feature-based
classification, we can use standard supervised relation extraction features like the
named entity labels of the two mentions, the words and dependency paths in be-
tween the mentions, and neighboring words. Each tuple will have features col-
lectedfrommanytraininginstances;thefeaturevectorforasingletraininginstance
like(<born-in,Albert Einstein, Ulm>willhavelexicalandsyntacticfeatures
frommanydifferentsentencesthatmentionEinsteinandUlm.
Becausedistantsupervisionhasverylargetrainingsets,itisalsoabletousevery
rich features that are conjunctions of these individual features. So we will extract
thousands of patterns that conjoin the entity types with the intervening words or
dependencypathslikethese:438 CHAPTER21 • RELATIONANDEVENTEXTRACTION
PERwasborninLOC
PER,born(XXXX),LOC
PER’sbirthplaceinLOC
Toreturntoourrunningexample,forthissentence:
(21.12) AmericanAirlines,aunitofAMR,immediatelymatchedthemove,
spokesmanTimWagnersaid
wewouldlearnrichconjunctionfeatureslikethisone:
M1=ORG&M2=PER&nextword=“said”&path=NP NP S S NP
↑ ↑ ↑ ↓
The result is a supervised classifier that has a huge rich set of features to use
in detecting relations. Since not every test sentence will have one of the training
relations,theclassifierwillalsoneedtobeabletolabelanexampleasno-relation.
This label is trained by randomly selecting entity pairs that do not appear in any
Freebase relation, extracting features for them, and building a feature vector for
eachsuchtuple. ThefinalalgorithmissketchedinFig.21.9.
functionDISTANTSUPERVISION(DatabaseD,TextT)returnsrelationclassifierC
foreachrelationR
foreachtuple(e1,e2)ofentitieswithrelationRinD
sentences SentencesinTthatcontain e1ande2
←
f Frequentfeaturesinsentences
←
observations observations+newtrainingtuple(e1,e2,f,R)
←
C Trainsupervisedclassifieronobservations
←
returnC
Figure21.9 Thedistantsupervisionalgorithmforrelationextraction. Aneuralclassifier
wouldskipthefeatureset f.
Distant supervision shares advantages with each of the methods we’ve exam-
ined. Like supervised classification, distant supervision uses a classifier with lots
offeatures,andsupervisedbydetailedhand-createdknowledge. Likepattern-based
classifiers, itcanmakeuseofhigh-precisionevidencefortherelationbetweenen-
tities. Indeed, distance supervision systems learn patterns just like the hand-built
patterns of early relation extractors. For example the is-a or hypernym extraction
system of Snow et al. (2005) used hypernym/hyponym NP pairs from WordNet as
distantsupervision,andthenlearnednewpatternsfromlargeamountsoftext. Their
systeminducedexactlytheoriginal5templatepatternsofHearst(1992a),butalso
70,000additionalpatternsincludingthesefour:
NP likeNP Manyhormoneslikeleptin...
H
NP calledNP ...usingamarkuplanguagecalledXHTML
H
NPisaNP Rubyisaprogramminglanguage...
H
NP,aNP IBM,acompanywithalong...
H
This ability to use a large number of features simultaneously means that, un-
like the iterative expansion of patterns in seed-based systems, there’s no semantic
drift. Like unsupervised classification, it doesn’t use a labeled training corpus of
texts, so it isn’t sensitive to genre issues in the training corpus, and relies on very
largeamountsofunlabeleddata. Distantsupervisionalsohastheadvantagethatit
can create training tuples to be used with neural classifiers, where features are not
required.21.2 • RELATIONEXTRACTIONALGORITHMS 439
Themainproblemwithdistantsupervisionisthatittendstoproducelow-precision
results,andsocurrentresearchfocusesonwaystoimproveprecision. Furthermore,
distant supervision can only help in extracting relations for which a large enough
database already exists. To extract new relations without datasets, or relations for
newdomains,purelyunsupervisedmethodsmustbeused.
21.2.5 UnsupervisedRelationExtraction
The goal of unsupervised relation extraction is to extract relations from the web
whenwehavenolabeledtrainingdata,andnotevenanylistofrelations. Thistask
open
information isoftencalledopeninformationextractionorOpenIE.InOpenIE,therelations
extraction
aresimplystringsofwords(usuallybeginningwithaverb).
Forexample, theReVerbsystem(Faderetal.,2011)extractsarelationfroma
sentencesin4steps:
1. Runapart-of-speechtaggerandentitychunkerovers
2. Foreachverbins,findthelongestsequenceofwordswthatstartwithaverb
andsatisfysyntacticandlexicalconstraints,mergingadjacentmatches.
3. For each phrase w, find the nearest noun phrase x to the left which is not a
relativepronoun,wh-wordorexistential“there”.Findthenearestnounphrase
ytotheright.
4. Assign confidence c to the relation r=(x,w,y) using a confidence classifier
andreturnit.
A relation is only accepted if it meets syntactic and lexical constraints. The
syntacticconstraintsensurethatitisaverb-initialsequencethatmightalsoinclude
nouns(relationsthatbeginwithlightverbslikemake,have,ordooftenexpressthe
coreoftherelationwithanoun,likehaveahubin):
V VP VW*P
| |
V=verbparticle? adv?
W=(noun adj adv pron det)
| | | |
P=(prep particle infinitive“to”)
| |
The lexical constraints are based on a dictionary D that is used to prune very rare,
longrelationstrings. Theintuitionistoeliminatecandidaterelationsthatdon’toc-
cur with sufficient number of distinct argument types and so are likely to be bad
examples. The system first runs the above relation extraction algorithm offline on
500millionwebsentencesandextractsalistofalltherelationsthatoccurafternor-
malizingthem(removinginflection,auxiliaryverbs,adjectives,andadverbs). Each
relationrisaddedtothedictionaryifitoccurswithatleast20differentarguments.
Faderetal.(2011)usedadictionaryof1.7millionnormalizedrelations.
Finally, a confidence value is computed for each relation using a logistic re-
gression classifier. The classifier istrained by taking 1000 random web sentences,
runningtheextractor,andhandlabelingeachextractedrelationascorrectorincor-
rect. Aconfidenceclassifieristhentrainedonthishand-labeleddata,usingfeatures
oftherelationandthesurroundingwords. Fig.21.10showssomesamplefeatures
usedintheclassification.
Forexamplethefollowingsentence:
(21.13) UnitedhasahubinChicago,whichistheheadquartersofUnited
ContinentalHoldings.
hastherelationphraseshasahubinandistheheadquartersof(italsohashasand
is,butlongerphrasesarepreferred). Step3findsUnitedtotheleftandChicagoto440 CHAPTER21 • RELATIONANDEVENTEXTRACTION
(x,r,y)coversallwordsins
thelastprepositioninrisfor
thelastprepositioninrison
len(s) 10
≤
thereisacoordinatingconjunctiontotheleftofrins
rmatchesaloneVinthesyntacticconstraints
thereisprepositiontotheleftofxins
thereisanNPtotherightofyins
Figure21.10 Featuresfortheclassifierthatassignsconfidencetorelationsextractedbythe
OpenInformationExtractionsystemREVERB(Faderetal.,2011).
therightofhasahubin, andskipsoverwhichtofindChicagototheleftofisthe
headquartersof. Thefinaloutputis:
r1: <United, has a hub in, Chicago>
r2: <Chicago, is the headquarters of, United Continental Holdings>
The great advantage of unsupervised relation extraction is its ability to handle
a huge number of relations without having to specify them in advance. The dis-
advantage is the need to map all the strings into some canonical form for adding
todatabasesorknowledgegraphs. Currentmethodsfocusheavilyonrelationsex-
pressedwithverbs,andsowillmissmanyrelationsthatareexpressednominally.
21.2.6 EvaluationofRelationExtraction
Supervisedrelationextractionsystemsareevaluatedbyusingtestsetswithhuman-
annotated,gold-standardrelationsandcomputingprecision,recall,andF-measure.
Labeled precision and recall require the system to classify the relation correctly,
whereasunlabeledmethodssimplymeasureasystem’sabilitytodetectentitiesthat
arerelated.
Semi-supervisedandunsupervisedmethodsaremuchmoredifficulttoevalu-
ate, since they extract totally new relations from the web or a large text. Because
thesemethodsuseverylargeamountsoftext,itisgenerallynotpossibletorunthem
solely on a small labeled test set, and as a result it’s not possible to pre-annotate a
goldsetofcorrectinstancesofrelations.
For these methods it’s possible to approximate (only) precision by drawing a
randomsampleofrelationsfromtheoutput,andhavingahumanchecktheaccuracy
ofeachoftheserelations.Usuallythisapproachfocusesonthetuplestobeextracted
from a body of text rather than on the relation mentions; systems need not detect
every mention of a relation to be scored correctly. Instead, the evaluation is based
on the set of tuples occupying the database when the system is finished. That is,
wewanttoknowifthesystemcandiscoverthatRyanairhasahubatCharleroi;we
don’treallycarehowmanytimesitdiscoversit. TheestimatedprecisionPˆisthen
#ofcorrectlyextractedrelationtuplesinthesample
Pˆ= (21.14)
total#ofextractedrelationtuplesinthesample.
Anotherapproachthatgivesusalittlebitofinformationaboutrecallistocom-
pute precision at different levels of recall. Assuming that our system is able to
ranktherelationsitproduces(byprobability,orconfidence)wecanseparatelycom-
puteprecisionforthetop1000newrelations,thetop10,000newrelations,thetop
100,000, and so on. In each case we take a random sample of that set. This will
showushowtheprecisioncurvebehavesasweextractmoreandmoretuples. But
thereisnowaytodirectlyevaluaterecall.21.3 • EXTRACTINGEVENTS 441
21.3 Extracting Events
event The task of event extraction is to identify mentions of events in texts. For the
extraction
purposesofthistask,aneventmentionisanyexpressiondenotinganeventorstate
thatcanbeassignedtoaparticularpoint,orinterval,intime. Thefollowingmarkup
ofthesampletextonpage429showsalltheeventsinthistext.
[ Citing] high fuel prices, United Airlines [ said] Fri-
EVENT EVENT
day it has [ increased] fares by $6 per round trip on flights to
EVENT
somecitiesalsoservedbylower-costcarriers.AmericanAirlines,aunit
of AMR Corp., immediately [ matched] [ the move],
EVENT EVENT
spokesman Tim Wagner [ said]. United, a unit of UAL Corp.,
EVENT
[ said][ theincrease]tookeffectThursdayand[
EVENT EVENT EVENT
applies] to most routes where it [ competes] against discount
EVENT
carriers,suchasChicagotoDallasandDenvertoSanFrancisco.
InEnglish,mosteventmentionscorrespondtoverbs,andmostverbsintroduce
events.However,aswecanseefromourexample,thisisnotalwaysthecase.Events
canbeintroducedbynounphrases,asinthemoveandtheincrease,andsomeverbs
failtointroduceevents,asinthephrasalverbtookeffect,whichreferstowhenthe
lightverbs eventbeganratherthantotheeventitself. Similarly,lightverbssuchasmake,take,
andhaveoftenfailtodenoteevents.Alightverbisaverbthathasverylittlemeaning
itself,andtheassociatedeventisinsteadexpressedbyitsdirectobjectnoun. Inlight
verbexamplesliketookaflight,it’sthewordflightthatdefinestheevent;theselight
verbsjustprovideasyntacticstructureforthenoun’sarguments.
Various versions of the event extraction task exist, depending on the goal. For
exampleintheTempEvalsharedtasks(Verhagenetal.2009)thegoalistoextract
events and aspects like their aspectual and temporal properties. Events are to be
reporting classifiedasactions,states,reportingevents(say,report,tell,explain),perception
events
events, and so on. The aspect, tense, and modality of each event also needs to be
extracted. Thus for example the various said events in the sample text would be
annotatedas(class=REPORTING,tense=PAST,aspect=PERFECTIVE).
Eventextractionisgenerallymodeledviasupervisedlearning,detectingevents
viaIOBsequencemodelsandassigningeventclassesandattributeswithmulti-class
classifiers.Theinputcanbeneuralmodelsstartingfromencoders;orclassicfeature-
basedmodelsusingfeatureslikethoseinFig.21.11.
Feature Explanation
Characteraffixes Character-levelprefixesandsuffixesoftargetword
Nominalizationsuffix Character-levelsuffixesfornominalizations(e.g.,-tion)
Partofspeech Partofspeechofthetargetword
Lightverb Binaryfeatureindicatingthatthetargetisgovernedbyalightverb
Subjectsyntacticcategory Syntacticcategoryofthesubjectofthesentence
Morphologicalstem Stemmedversionofthetargetword
Verbroot Rootformoftheverbbasisforanominalization
WordNethypernyms Hypernymsetforthetarget
Figure21.11 Featurescommonlyusedinclassicfeature-basedapproachestoeventdetection.442 CHAPTER21 • RELATIONANDEVENTEXTRACTION
21.4 Template Filling
Many texts contain reports of events, and possibly sequences of events, that often
correspond to fairly common, stereotypical situations in the world. These abstract
scripts situations or stories, related to what have been called scripts (Schank and Abel-
son, 1977), consist of prototypical sequences of sub-events, participants, and their
roles. The strong expectations provided by these scripts can facilitate the proper
classificationofentities,theassignmentofentitiesintorolesandrelations,andmost
critically,thedrawingofinferencesthatfillinthingsthathavebeenleftunsaid. In
templates theirsimplestform,suchscriptscanberepresentedastemplatesconsistingoffixed
setsofslotsthattakeasvaluesslot-fillersbelongingtoparticularclasses. Thetask
templatefilling oftemplatefillingistofinddocumentsthatinvokeparticularscriptsandthenfillthe
slotsintheassociatedtemplateswithfillersextractedfromthetext.Theseslot-fillers
mayconsistoftextsegmentsextracteddirectlyfromthetext,ortheymayconsistof
concepts that have been inferred from text elements through some additional pro-
cessing.
Afilledtemplatefromouroriginalairlinestorymightlooklikethefollowing.
FARE-RAISEATTEMPT: LEADAIRLINE: UNITEDAIRLINES
AMOUNT: $6
 
EFFECTIVEDATE: 2006-10-26
 
FOLLOWER: AMERICANAIRLINES
 
 
Thistemplatehasfourslots(LEADAIRLINE,AMOUNT,EFFECTIVEDATE,FOL-
LOWER).Thenextsectiondescribesastandardsequence-labelingapproachtofilling
slots. Section21.4.2thendescribesanoldersystembasedontheuseofcascadesof
finite-statetransducersanddesignedtoaddressamorecomplextemplate-fillingtask
thatcurrentlearning-basedsystemsdon’tyetaddress.
21.4.1 MachineLearningApproachestoTemplateFilling
Inthestandardparadigmfortemplatefilling,wearegiventrainingdocumentswith
textspansannotatedwithpredefinedtemplatesandtheirslotfillers. Ourgoalisto
createonetemplateforeacheventintheinput,fillingintheslotswithtextspans.
Thetaskisgenerallymodeledbytrainingtwoseparatesupervisedsystems. The
first system decides whether the template is present in a particular sentence. This
template task is called template recognition or sometimes, in a perhaps confusing bit of
recognition
terminology,eventrecognition. Templaterecognitioncanbetreatedasatextclassi-
ficationtask,withfeaturesextractedfromeverysequenceofwordsthatwaslabeled
intrainingdocumentsasfillinganyslotfromthetemplatebeingdetected. Theusual
setoffeaturescanbeused: tokens, embeddings, wordshapes, part-of-speechtags,
syntacticchunktags,andnamedentitytags.
role-filler Thesecondsystemhasthejobofrole-fillerextraction. Aseparateclassifieris
extraction
trained to detect each role (LEAD-AIRLINE, AMOUNT, and so on). This can be a
binaryclassifierthatisrunoneverynoun-phraseintheparsedinputsentence,ora
sequencemodelrunoversequencesofwords. Eachroleclassifieristrainedonthe
labeleddatainthetrainingset. Again,theusualsetoffeaturescanbeused,butnow
trainedonlyonanindividualnounphraseorthefillersofasingleslot.
Multiple non-identical text segments might be labeled with the same slot la-
bel. Forexampleinoursampletext,thestringsUnitedorUnitedAirlinesmightbe21.4 • TEMPLATEFILLING 443
labeledastheLEAD AIRLINE. Thesearenotincompatiblechoicesandthecorefer-
enceresolutiontechniquesintroducedinChapter26canprovideapathtoasolution.
A variety of annotated collections have been used to evaluate this style of ap-
proachtotemplatefilling,includingsetsofjobannouncements,conferencecallsfor
papers, restaurant guides, and biological texts. A key open question is extracting
templatesincaseswherethereisnotrainingdataorevenpredefinedtemplates, by
inducingtemplatesassetsoflinkedevents(ChambersandJurafsky,2011).
21.4.2 EarlierFinite-StateTemplate-FillingSystems
The templates above are relatively simple. But consider the task of producing a
template that contained all the information in a text like this one (Grishman and
Sundheim,1995):
Bridgestone Sports Co. said Friday it has set up a joint venture in Taiwan
withalocalconcernandaJapanesetradinghousetoproducegolfclubstobe
shippedtoJapan. Thejointventure,BridgestoneSportsTaiwanCo.,capital-
izedat20millionnew Taiwandollars, will startproductioninJanuary1990
withproductionof20,000ironand“metalwood”clubsamonth.
TheMUC-5‘jointventure’task(theMessageUnderstandingConferenceswere
a series of U.S. government-organized information-extraction evaluations) was to
produce hierarchically linked templates describing joint ventures. Figure 21.12
showsastructureproducedbythe FASTUS system(Hobbsetal.,1997). Notehow
thefilleroftheACTIVITYslotoftheTIE-UPtemplateisitselfatemplatewithslots.
Tie-up-1 Activity-1:
RELATIONSHIP tie-up COMPANY BridgestoneSportsTaiwanCo.
ENTITIES BridgestoneSportsCo. PRODUCT ironand“metalwood”clubs
alocalconcern STARTDATE DURING:January1990
aJapanesetradinghouse
JOINTVENTURE BridgestoneSportsTaiwanCo.
ACTIVITY Activity-1
AMOUNT NT$20000000
Figure21.12 ThetemplatesproducedbyFASTUSgiventheinputtextonpage443.
Earlysystemsfordealingwiththesecomplextemplateswerebasedoncascades
oftransducersbasedonhandwrittenrules,assketchedinFig.21.13.
No. Step Description
1 Tokens Tokenizeinputstreamofcharacters
2 ComplexWords Multiwordphrases,numbers,andpropernames.
3 Basicphrases Segmentsentencesintonounandverbgroups
4 Complexphrases Identifycomplexnoungroupsandverbgroups
5 SemanticPatterns Identifyentitiesandevents,insertintotemplates.
6 Merging Mergereferencestothesameentityorevent
Figure21.13 Levelsofprocessingin FASTUS (Hobbsetal.,1997). Eachlevelextractsa
specifictypeofinformationwhichisthenpassedontothenexthigherlevel.
The first four stages use handwritten regular expression and grammar rules to
do basic tokenization, chunking, and parsing. Stage 5 then recognizes entities and
eventswitharecognizerbasedonfinite-statetransducers(FSTs),andinsertstherec-
ognizedobjectsintotheappropriateslotsintemplates.ThisFSTrecognizerisbased444 CHAPTER21 • RELATIONANDEVENTEXTRACTION
onhand-builtregularexpressionslikethefollowing(NGindicatesNoun-Groupand
VGVerb-Group),whichmatchesthefirstsentenceofthenewsstoryabove.
NG(Company/ies) VG(Set-up) NG(Joint-Venture) with NG(Company/ies)
VG(Produce) NG(Product)
Theresultofprocessingthesetwosentencesisthefivedrafttemplates(Fig.21.14)
thatmustthenbemergedintothesinglehierarchicalstructureshowninFig.21.12.
Themergingalgorithm,afterperformingcoreferenceresolution,mergestwoactivi-
tiesthatarelikelytobedescribingthesameevents.
# Template/Slot Value
1 RELATIONSHIP: TIE-UP
ENTITIES: BridgestoneCo.,alocalconcern,aJapanesetradinghouse
2 ACTIVITY: PRODUCTION
PRODUCT: “golfclubs”
3 RELATIONSHIP: TIE-UP
JOINTVENTURE: “BridgestoneSportsTaiwanCo.”
AMOUNT: NT$20000000
4 ACTIVITY: PRODUCTION
COMPANY: “BridgestoneSportsTaiwanCo.”
STARTDATE: DURING: January1990
5 ACTIVITY: PRODUCTION
PRODUCT: “ironand“metalwood”clubs”
Figure21.14 Thefivepartialtemplatesproducedbystage5ofFASTUS. Thesetemplates
aremergedinstage6toproducethefinaltemplateshowninFig.21.12onpage443.
21.5 Summary
Thischapterhasexploredtechniquesforextractinglimitedformsofsemanticcon-
tentfromtexts.
• Relationsamongentitiescanbeextractedbypattern-basedapproaches, su-
pervised learning methods when annotated training data is available, lightly
supervised bootstrapping methods when small numbers of seed tuples or
seedpatternsareavailable,distantsupervisionwhenadatabaseofrelations
isavailable,andunsupervisedorOpenIEmethods.
• Template-filling applications can recognize stereotypical situations in texts
andassignelementsfromthetexttorolesrepresentedasfixedsetsofslots.
Bibliographical and Historical Notes
Theearliestworkoninformationextractionaddressedthetemplate-fillingtaskinthe
contextoftheFrumpsystem(DeJong,1982).LaterworkwasstimulatedbytheU.S.
government-sponsoredMUCconferences(Sundheim1991,Sundheim1992,Sund-
heim 1993, Sundheim 1995). Early MUC systems like CIRCUS system (LehnertEXERCISES 445
etal.,1991)andSCISOR(JacobsandRau,1990)werequiteinfluentialandinspired
latersystemslikeFASTUS(Hobbsetal.,1997). Chinchoretal.(1993)describethe
MUCevaluationtechniques.
Due to the difficulty of porting systems from one domain to another, attention
shifted to machine learning approaches. Early supervised learning approaches to
IE (Cardie 1993, Cardie 1994, Riloff 1993, Soderland et al. 1995, Huffman 1996)
focused on automating the knowledge acquisition process, mainly for finite-state
rule-based systems. Their success, and the earlier success of HMM-based speech
recognition,ledtotheuseofsequencelabeling(HMMs:Bikeletal.1997;MEMMs
McCallumetal.2000; CRFs: Laffertyetal.2001), andawideexplorationoffea-
tures(Zhouetal.,2005). Neuralapproachesfollowedfromthepioneeringresultsof
Collobertetal.(2011),whoappliedaCRFontopofaconvolutionalnet.
Progressinthisareacontinuestobestimulatedbyformalevaluationswithshared
benchmarkdatasets,includingtheAutomaticContentExtraction(ACE)evaluations
of 2000-2007 on named entity recognition, relation extraction, and temporal ex-
KBP
pressions1,theKBP(KnowledgeBasePopulation)evaluations(Jietal.2010,Sur-
slotfilling deanu2013)ofrelationextractiontaskslikeslotfilling(extractingattributes(‘slots’)
like age, birthplace, and spouse for a given entity) and a series of SemEval work-
shops(Hendrickxetal.,2009).
Semisupervised relation extraction was first proposed by Hearst (1992b), and
extendedbysystemslikeAutoSlog-TS(Riloff,1996),DIPRE(Brin,1998),SNOW-
BALL (Agichtein and Gravano, 2000), and Jones et al. (1999). The distant super-
vision algorithm we describe was drawn from Mintz et al. (2009), who first used
the term ‘distant supervision’ (which was suggested to them by Chris Manning)
but similar ideas had occurred in earlier systems like Craven and Kumlien (1999)
and Morgan et al. (2004) under the name weakly labeled data, as well as in Snow
et al. (2005) and Wu and Weld (2007). Among the many extensions are Wu and
Weld(2010),Riedeletal.(2010),andRitteretal.(2013). OpenIEsystemsinclude
KNOWITALL Etzionietal.(2005),TextRunner(Bankoetal.,2007),and REVERB
(Faderetal.,2011). SeeRiedeletal.(2013)forauniversalschemathatcombines
theadvantagesofdistantsupervisionandOpenIE.
Exercises
21.1 Acronymexpansion,theprocessofassociatingaphrasewithanacronym,can
be accomplished by a simple form of relational analysis. Develop a system
basedontherelationanalysisapproachesdescribedinthischaptertopopulate
a database of acronym expansions. If you focus on English Three Letter
Acronyms(TLAs)youcanevaluateyoursystem’sperformancebycomparing
ittoWikipedia’sTLApage.
21.2 Acquire the CMU seminar corpus and develop a template-filling system by
using any of the techniques mentioned in Section 21.4. Analyze how well
yoursystemperformsascomparedwithstate-of-the-artresultsonthiscorpus.
1 www.nist.gov/speech/tests/ace/446 CHAPTER22 • TIMEANDTEMPORALREASONING
CHAPTER
22 Time and Temporal Reasoning
Timewillexplain.
JaneAusten,Persuasion
When we discussed events back in Chapter 19 and Chapter 21, we temporally
delayeddiscussingtime. Butastheysay,timewaitsfornoone,andthere’snotime
like the present! Let’s take the time now to talk about representing and extracting
time. Eventsaresituatedintime,occurringataparticulardateortime,andevents
can be related temporally, happening before or after or simultaneously with each
other. Consider what we would need to understand such temporal issues in this
shortenedversionofthenewsstorywesawinChapter21:
UnitedAirlinessaidFridayithasincreasedfaresby$6perroundtripon
flightstosomecitiesalsoservedbylower-costcarriers. AmericanAir-
lines,aunitofAMRCorp.,immediatelymatchedthemove,spokesman
TimWagnersaid. United,aunitofUALCorp.,saidtheincreasetook
effectThursday.
We’ll need to recognize temporal expressions like days of the week (Friday
and Thursday) or two days from now and times such as 3:30 P.M., and normal-
izethemontospecificcalendardatesortimes. We’llneedtolinkFridaytothetime
ofUnited’sannouncement, Thursdaytothepreviousday’sfareincrease, andwe’ll
needtoproduceatimelineinwhichUnited’sannouncementfollowsthefareincrease
andAmerican’sannouncementfollowsbothofthoseevents. Andwe’llneedtobase
all this on theories of how time and aspect work, how to represent how events are
locatedintimeandtheirtemporalrelationtoeachother. Andwe’llneedtobeable
to practically extract all these things from text. Not all of these tasks are solvable
in current systems, but we’ll explore what is currently doable and what questions
remainopen.
22.1 Representing Time
temporallogic Let’sbeginbyintroducingthebasicsoftemporallogicandhowhumanlanguages
conveytemporalinformation. Themoststraightforwardtheoryoftimeholdsthatit
flowsinexorablyforwardandthateventsareassociatedwitheitherpointsorinter-
valsintime,asonatimeline. Wecanorderdistincteventsbysituatingthemonthe
timeline; one event precedes another if the flow of time leads from the first event
tothesecond. Accompanyingthesenotionsinmosttheoriesistheideaofthecur-
rent moment in time. Combining this notion with the idea of a temporal ordering
relationshipyieldsthefamiliarnotionsofpast,present,andfuture.22.1 • REPRESENTINGTIME 447
Many schemes can represent this kind of temporal information. The one pre-
sented here is a fairly simple one that stays within the FOL framework of reified
eventsthatwepursuedinChapter19. Considerthefollowingexamples:
(22.1) IarrivedinNewYork.
(22.2) IamarrivinginNewYork.
(22.3) IwillarriveinNewYork.
These sentences all refer to the same kind of event and differ solely in the verb
tense. In the Davidsonian scheme for representing events (Chapter 19), all three
wouldsharethefollowingrepresentation,whichlacksanytemporalinformation:
eArriving(e) Arriver(e,Speaker) Destination(e,NewYork) (22.4)
∃ ∧ ∧
The temporal information provided by the tense of the verbs can be exploited by
predicatingadditionalinformationabouttheeventvariablee. Variouskindsoftem-
poral logics can be used to talk about temporal ordering relationship. One of the
intervalalgebra most commonly used in computational modeling is the interval algebra of Allen
(1984). Allen models all events and time expressions as intervals there is no rep-
resentation for points (although intervals can be very short). In order to deal with
intervals without points, he identifies 13 primitive relations that can hold between
Allenrelations thesetemporalintervals. Fig.22.1showsthese13Allenrelations.
A A
B
A before B A overlaps B B
B after A B overlaps' A
A
A
A equals B
A meets B B (B equals A)
B
B meets' A
A A
A starts B A finishes B
B starts' A B finishes' A
B B
A during B A
B during' A
B
Time
Figure22.1 The13temporalrelationsfromAllen(1984).
To include the interval algebra in our model, we add a temporal variable that
representstheintervalcorrespondingtotheevent,another(verysmall)intervalcor-
responding to the current time Now, and temporal predicates relating the event to448 CHAPTER22 • TIMEANDTEMPORALREASONING
thecurrenttimeasindicatedbythetenseoftheverb. Thisintervalalgebraapproach
yieldsthefollowingrepresentationsforourarrivingexamples:
e,iArriving(e) Arriver(e,Speaker) Destination(e,NewYork)
∃ ∧ ∧
IntervalOf(e,i) Before(i,Now)
∧ ∧
e,iArriving(e) Arriver(e,Speaker) Destination(e,NewYork)
∃ ∧ ∧
IntervalOf(e,i) During(i,Now)
∧ ∧
e,iArriving(e) Arriver(e,Speaker) Destination(e,NewYork)
∃ ∧ ∧
IntervalOf(e,i) After(i,Now)
∧ ∧
Inadditiontothevariableithatstandsfortheintervaloftimeassociatedwiththe
event,weseethetwo-placepredicateBeforethatrepresentsthenotionthatthefirst
interval argument precedes the second in time and the constant Now that refers to
theintervalcorrespondingtothecurrenttime. Forpastevents,theintervalmustend
beforethecurrenttime.Similarly,forfutureeventsthecurrenttimemustprecedethe
endoftheevent. Foreventshappeninginthepresent,thecurrenttimeiscontained
withintheeventinterval,usingthepredicateDuring. We’llseelaterinthischapter
howthesepredicatescanbeusedtodetectandlinkthetemporalrelationsbetween
eventsinatexttogiveusacompletetimeline.
22.1.1 Reichenbach’sreferencepoint
Therelationbetweensimpleverbtensesandpointsintimeisbynomeansstraight-
forward. Thepresenttensecanbeusedtorefertoafutureevent,asinthisexample:
(22.5) Ok,weflyfromSanFranciscotoBostonat10.
Orconsiderthefollowingexamples:
(22.6) Flight1902arrivedlate.
(22.7) Flight1902hadarrivedlate.
Althoughbothrefertoeventsinthepast,representingtheminthesamewayseems
wrong. The second example seems to have another unnamed event lurking in the
background (e.g., Flight 1902 had already arrived late when something else hap-
pened).
To account for this phenomena, Reichenbach (1947) introduced the notion of
referencepoint a reference point. In our simple temporal scheme, the current moment in time is
equated with the time of the utterance and is used as a reference point for when
the event occurred (before, at, or after). In Reichenbach’s approach, the notion of
the reference point is separated from the utterance time and the event time. The
followingexamplesillustratethebasicsofthisapproach:
(22.8) WhenMary’sflightdeparted,Iatelunch.
(22.9) WhenMary’sflightdeparted,Ihadeatenlunch.
Inbothoftheseexamples,theeatingeventhashappenedinthepast,thatis,prior
totheutterance.However,theverbtenseinthefirstexampleindicatesthattheeating
event began when the flight departed, while the second example indicates that the
eatingwasaccomplishedpriortotheflight’sdeparture. Therefore,inReichenbach’s
termsthedepartureeventspecifiesthereferencepoint. Thesefactscanbeaccom-
modated by additional constraints relating the eating and departure events. In the
firstexample,thereferencepointprecedestheeatingevent,andinthesecondexam-
ple, the eating precedes the reference point. Figure 22.2 illustrates Reichenbach’s22.2 • REPRESENTINGASPECT 449
approachwiththeprimaryEnglishtenses. Exercise22.2asksyoutorepresentthese
examplesinFOL.
Past Perfect Simple Past Present Perfect
E R U R,E U E R,U
Present Simple Future Future Perfect
U,R,E U,R E U E R
Figure22.2 Reichenbach’sapproachappliedtovariousEnglishtenses. Inthesediagrams,
timeflowsfromlefttoright,Edenotesthetimeoftheevent,Rdenotesthereferencetime,
andUdenotesthetimeoftheutterance.
Languageshavemanyotherwaystoconveytemporalinformationbesidestense.
Most useful for our purposes will be temporal expressions like in the morning or
6:45orafterwards.
(22.10) I’dliketogoat6:45inthemorning.
(22.11) Somewherearoundnoon,please.
(22.12) Iwanttotakethetrainbackafterwards.
Incidentally,temporalexpressionsdisplayafascinatingmetaphoricalconceptual
organization. Temporal expressions in English are frequently expressed in spatial
terms, as is illustrated by the various uses of at, in, somewhere, and near in these
examples(LakoffandJohnson1980,Jackendoff1983). Metaphoricalorganizations
suchasthese,inwhichonedomainissystematicallyexpressedintermsofanother,
areverycommoninlanguagesoftheworld.
22.2 Representing Aspect
aspect A related notion to time is aspect, which is what we call the way events can be
categorized by their internal temporal structure or temporal contour. By this we
meanquestionslikewhethereventsareongoingorhaveended,orwhethertheyare
conceptualizedashappeningatapointintimeoroversomeinterval. Suchnotions
of temporal contour have been used to divide event expressions into classes since
Aristotle, although the set of four classes we’ll introduce here is due to Vendler
aktionsart (1967)(youmayalsoseetheGermantermaktionsartusedtorefertotheseclasses).
events Themostbasicaspectualdistinctionisbetweenevents(whichinvolvechange)
states andstates(whichdonotinvolvechange). Stativeexpressionsrepresentthenotion
stative of an event participant being in a state, or having a particular property, at a given
point in time. Stative expressions capture aspects of the world at a single point in450 CHAPTER22 • TIMEANDTEMPORALREASONING
time,andconceptualizetheparticipantasunchangingandcontinuous. Considerthe
followingATISexamples.
(22.13) Ilikeexpresstrains.
(22.14) Ineedthecheapestfare.
(22.15) Iwanttogofirstclass.
In examples like these, the event participant denoted by the subject can be seen as
experiencing something at a specific point in time, and don’t involve any kind of
internalchangeovertime(thelikingorneedingisconceptualizedascontinuousand
unchanging).
Non-states (which we’ll refer to as events) are divided into subclasses; we’ll
activity introducethreehere. Activityexpressionsdescribeeventsundertakenbyapartic-
ipant that occur over a span of time (rather than being conceptualized as a single
pointintimelikestativeexpressions), andhavenoparticularendpoint. Ofcourse
in practice all things end, but the meaning of the expression doesn’t represent this
fact. Considerthefollowingexamples:
(22.16) ShedroveaMazda.
(22.17) IliveinBrooklyn.
These examples both specify that the subject is engaged in, or has engaged in, the
activityspecifiedbytheverbforsomeperiodoftime,butdoesn’tspecifywhenthe
drivingorlivingmighthavestopped.
Two more classes of expressions, achievement expressions and accomplish-
mentexpressions,describeeventsthattakeplaceovertime,butalsoconceptualize
the event as having a particular kind of endpoint or goal. The Greek word telos
means‘end’or’goal’andsotheeventsdescribedbythesekindsofexpressionsare
telic oftencalledtelicevents.
accomplishment Accomplishmentexpressionsdescribeeventsthathaveanaturalendpointand
expressions
resultinaparticularstate. Considerthefollowingexamples:
(22.18) Hebookedmeareservation.
(22.19) The7:00traingotmetoNewYorkCity.
Intheseexamples,aneventisseenasoccurringoversomeperiodoftimethatends
whentheintendedstateisaccomplished(i.e., thestateofmehavingareservation,
ormebeinginNewYorkCity).
achievement Thefinalaspectualclass,achievementexpressions,isonlysubtlydifferentthan
expressions
accomplishments. Considerthefollowing:
(22.20) Shefoundhergate.
(22.21) IreachedNewYork.
Like accomplishment expressions, achievement expressions result in a state. But
unlikeaccomplishments,achievementeventsare‘punctual’: theyarethoughtofas
happening in an instant and the verb doesn’t conceptualize the process or activ-
ity leading up the state. Thus the events in these examples may in fact have been
precededbyextendedsearchingortravelingevents,buttheverbdoesn’tconceptu-
alizetheseprecedingprocesses,butratherconceptualizestheeventscorresponding
tofindingandreachingaspoints,notintervals.
Insummary,astandardwayofcategorizingeventexpressionsbytheirtemporal
contoursisviathesefourgeneralclasses:
Stative: Iknowmydeparturegate.
Activity: Johnisflying.22.3 • TEMPORALLYANNOTATEDDATASETS: TIMEBANK 451
Accomplishment: Sallybookedherflight.
Achievement: Shefoundhergate.
Before moving on, note that event expressions can easily be shifted from one
classtoanother. Considerthefollowingexamples:
(22.22) Iflew.
(22.23) IflewtoNewYork.
The first example is a simple activity; it has no natural end point. The second ex-
ampleisclearlyanaccomplishmenteventsinceithasanendpoint,andresultsina
particularstate. Clearly,theclassificationofaneventisnotsolelygovernedbythe
verb,butbythesemanticsoftheentireexpressionincontext.
22.3 Temporally Annotated Datasets: TimeBank
TimeBank The TimeBank corpus consistsof AmericanEnglish text annotatedwith temporal
information (Pustejovsky et al., 2003). The annotations use TimeML (Saur´ı et al.,
2006),amarkuplanguagefortimebasedonAllen’sintervalalgebradiscussedabove
(Allen,1984).TherearethreetypesofTimeMLobjects:anEVENTrepresentevents
and states, a TIME represents time expressions like dates, and a LINK represents
variousrelationshipsbetweeneventsandtimes(event-event, event-time, andtime-
time). Thelinksincludetemporallinks(TLINK)forthe13Allenrelations, aspec-
tual links (ALINK) for aspectual relationships between events and subevents, and
SLINKSwhichmarkfactuality.
Considerthefollowingsamplesentenceanditscorrespondingmarkupshownin
Fig.22.3,selectedfromoneoftheTimeBankdocuments.
(22.24) DeltaAirLinesearningssoared33%toarecordinthefiscalfirstquarter,
buckingtheindustrytrendtowarddecliningprofits.
<TIMEX3 tid="t57" type="DATE" value="1989-10-26" functionInDocument="CREATION_TIME">
10/26/89 </TIMEX3>
Delta Air Lines earnings <EVENT eid="e1" class="OCCURRENCE"> soared </EVENT> 33% to a
record in <TIMEX3 tid="t58" type="DATE" value="1989-Q1" anchorTimeID="t57"> the
fiscal first quarter </TIMEX3>, <EVENT eid="e3" class="OCCURRENCE">bucking</EVENT>
the industry trend toward <EVENT eid="e4" class="OCCURRENCE">declining</EVENT>
profits.
Figure22.3 ExamplefromtheTimeBankcorpus.
Thistexthasthreeeventsandtwotemporalexpressions(includingthecreation
timeofthearticle,whichservesasthedocumenttime),andfourtemporallinksthat
capturetheusingtheAllenrelations:
• Soaring e1isincludedinthefiscalfirstquarter
t58
• Soaring e1isbefore1989-10-26
t57
• Soaring e1issimultaneouswiththebucking
e3
• Declining e4includessoaring
e1
Wecanalsovisualizethelinksasagraph. TheTimeBanksnippetinEq.22.25
wouldberepresentedwithagraphlikeFig.22.4.452 CHAPTER22 • TIMEANDTEMPORALREASONING
(22.25) [DCT:11/02/891]1: PacificFirstFinancialCorp. said2shareholders
approved3itsacquisition4byRoyalTrustcoLtd. ofTorontofor$27ashare,
or$212million. Thethriftholdingcompanysaid5itexpects6toobtain7
regulatoryapproval8andcomplete9thetransaction10byyear-end11.
BEFORE BEFORE AFTER
1 2 3 4
SIMULTANEOUS
EVIDENTIAL MODAL
EVIDENTIAL MODAL FACTIVE
5 6 7 8
MODAL
BEFORE ENDS
11 9 10
CULMINATES
Figure22.4 AgraphofthetextinEq.22.25,adaptedfrom(Ocaletal.,2022).TLINKSare
showninblue,ALINKSinred,andSLINKSingreen.
22.4 Automatic Temporal Analysis
Hereweintroducethethreecommonstepsusedinanalyzingtimeintext:
1. Extractingtemporalexpressions
2. Normalizingtheseexpressions,byconvertingthemtoastandardformat.
3. Linkingeventstotimesandextractingtimegraphsandtimelines
22.4.1 ExtractingTemporalExpressions
Temporalexpressionsarephrasesthatrefertoabsolutepointsintime,relativetimes,
absolute durations, and sets of these. Absolute temporal expressions are those that can be
relative mappeddirectlytocalendardates,timesofday,orboth. Relativetemporalexpres-
sionsmaptoparticulartimesthroughsomeotherreferencepoint(asinaweekfrom
duration lastTuesday). Finally,durationsdenotespansoftimeatvaryinglevelsofgranular-
ity (seconds, minutes, days, weeks, centuries, etc.). Figure 22.5 lists some sample
temporalexpressionsineachofthesecategories.
Absolute Relative Durations
April24,1916 yesterday fourhours
Thesummerof’77 nextsemester threeweeks
10:15AM twoweeksfromyesterday sixdays
The3rdquarterof2006 lastquarter thelastthreequarters
Figure22.5 Examplesofabsolute,relationalanddurationaltemporalexpressions.
Temporal expressions are grammatical constructions that often have temporal
lexicaltriggers lexical triggers as their heads, making them easy to find. Lexical triggers might
be nouns, proper nouns, adjectives, and adverbs; full temporal expressions consist
oftheirphrasalprojections: nounphrases,adjectivephrases,andadverbialphrases
(Figure22.6).
The task is to detect temporal expressions in running text, like this examples,
shownwithTIMEX3tags(Pustejovskyetal.2005,Ferroetal.2005).22.4 • AUTOMATICTEMPORALANALYSIS 453
Category Examples
Noun morning,noon,night,winter,dusk,dawn
ProperNoun January,Monday,Ides,Easter,RoshHashana,Ramadan,Tet
Adjective recent,past,annual,former
Adverb hourly,daily,monthly,yearly
Figure22.6 Examplesoftemporallexicaltriggers.
A fare increase initiated <TIMEX3>last week</TIMEX3> by UAL
Corp’sUnitedAirlineswasmatchedbycompetitorsover<TIMEX3>the
weekend</TIMEX3>, markingthesecondsuccessfulfareincreasein
<TIMEX3>twoweeks</TIMEX3>.
Rule-basedapproachesusecascadesofregularexpressionstorecognizelarger
andlargerchunksfrompreviousstages,basedonpatternscontainingpartsofspeech,
triggerwords(e.g.,February)orclasses(e.g.,MONTH)(ChangandManning,2012;
Stro¨tgenandGertz,2013;Chambers,2013).Here’sarulefromSUTime(Changand
Manning,2012)fordetectingexpressionslike3yearsold:
/(\d+)[-\s]($TEUnits)(s)?([-\s]old)?/
Sequence-labeling approaches use the standard IOB scheme, marking words
thatareeither(I)nside,(O)utsideoratthe(B)eginningofatemporalexpression:
A fareincreaseinitiatedlastweekbyUALCorp’s...
OO O O B I O O O
A statistical sequence labeler is trained, using either embeddings or a fine-tuned
encoder, or classic features extracted from the token and context including words,
lexicaltriggers,andPOS.
Temporal expression recognizers are evaluated with the usual recall, precision,
and F-measures. A major difficulty for all of these very lexicalized approaches is
avoidingexpressionsthattriggerfalsepositives:
(22.26) 1984tellsthestoryofWinstonSmith...
(22.27) ...U2’sclassicSundayBloodySunday
22.4.2 TemporalNormalization
temporal Temporal normalization is the task of mapping a temporal expression to a point
normalization
in time or to a duration. Points in time correspond to calendar dates, to times of
day, or both. Durations primarily consist of lengths of time. Normalized times
arerepresentedviatheISO8601standardforencodingtemporalvalues(ISO8601,
2004). Fig.22.7reproducesourearlierexamplewiththesevalueattributes.
<TIMEX3 id=”t1’’ type=”DATE” value=”2007−07−02” functionInDocument=”CREATIONTIME”>
July 2, 2007 </TIMEX3> A fare increase initiated <TIMEX3 id=”t2” type=”DATE”
value=”2007−W26” anchorTimeID=”t1”>last week</TIMEX3> by United Airlines was
matched by competitors over <TIMEX3 id=”t3” type=”DURATION” value=”P1WE”
anchorTimeID=”t1”> the weekend </TIMEX3>, marking the second successful fare
increase in <TIMEX3 id=”t4” type=”DURATION” value=”P2W” anchorTimeID=”t1”> two
weeks </TIMEX3>.
Figure22.7 TimeMLmarkupincludingnormalizedvaluesfortemporalexpressions.
The dateline, or document date, for this text was July 2, 2007. The ISO repre-
sentationforthiskindofexpressionisYYYY-MM-DD,orinthiscase,2007-07-02.
Theencodingsforthetemporalexpressionsinoursampletextallfollowfromthis
date,andareshownhereasvaluesfortheVALUEattribute.454 CHAPTER22 • TIMEANDTEMPORALREASONING
Thefirsttemporalexpressioninthetextproperreferstoaparticularweekofthe
year. In the ISO standard, weeks are numbered from 01 to 53, with the first week
of the year being the one that has the first Thursday of the year. These weeks are
representedwiththetemplateYYYY-Wnn. TheISOweekforourdocumentdateis
week27;thusthevalueforlastweekisrepresentedas“2007-W26”.
The next temporal expression is the weekend. ISO weeks begin on Monday;
thus, weekends occur at the end of a week and are fully contained within a single
week. Weekends are treated as durations, so the value of the VALUE attribute has
to be a length. Durations are represented according to the pattern Pnx, where n is
an integer denoting the length and x represents the unit, as in P3Y for three years
or P2D for two days. In this example, one weekend is captured as P1WE. In this
case,thereisalsosufficientinformationtoanchorthisparticularweekendaspartof
a particular week. Such information is encoded in the ANCHORTIMEID attribute.
Finally,thephrasetwoweeksalsodenotesadurationcapturedasP2W.Figure22.8
givesomemoreexamples,butthereisalotmoretothevarioustemporalannotation
standards;consultISO8601(2004),Ferroetal.(2005),andPustejovskyetal.(2005)
formoredetails.
Unit Pattern SampleValue
Fullyspecifieddates YYYY-MM-DD 1991-09-28
Weeks YYYY-Wnn 2007-W27
Weekends PnWE P1WE
24-hourclocktimes HH:MM:SS 11:13:45
Datesandtimes YYYY-MM-DDTHH:MM:SS 1991-09-28T11:00:00
Financialquarters Qn 1999-Q3
Figure22.8 SampleISOpatternsforrepresentingvarioustimesanddurations.
Most current approaches to temporal normalization are rule-based (Chang and
Manning2012,Stro¨tgenandGertz2013). Patternsthatmatchtemporalexpressions
areassociatedwithsemanticanalysisprocedures.Forexample,thepatternabovefor
recognizingphraseslike3yearsoldcanbeassociatedwiththepredicateDuration
thattakestwoarguments,thelengthandtheunitoftime:
pattern: /(\d+)[-\s]($TEUnits)(s)?([-\s]old)?/
result: Duration($1, $2)
Thetaskisdifficultbecausefullyqualifiedtemporalexpressionsarefairlyrare
inrealtexts.Mosttemporalexpressionsinnewsarticlesareincompleteandareonly
implicitlyanchored,oftenwithrespecttothedatelineofthearticle,whichwerefer
temporal to as the document’s temporal anchor. The values of temporal expressions such
anchor
astoday,yesterday,ortomorrowcanallbecomputedwithrespecttothistemporal
anchor. Thesemanticprocedurefortodaysimplyassignstheanchor,andtheattach-
ments for tomorrow and yesterday add a day and subtract a day from the anchor,
respectively. Of course, given the cyclic nature of our representations for months,
weeks,days,andtimesofday,ourtemporalarithmeticproceduresmustusemodulo
arithmeticappropriatetothetimeunitbeingused.
Unfortunately, even simple expressions such as the weekend or Wednesday in-
troduce a fair amount of complexity. In our current example, the weekend clearly
referstotheweekendoftheweekthatimmediatelyprecedesthedocumentdate.But
thiswon’talwaysbethecase,asisillustratedinthefollowingexample.
(22.28) RandomsecuritychecksthatbeganyesterdayatSkyHarborwillcontinue
atleastthroughtheweekend.22.4 • AUTOMATICTEMPORALANALYSIS 455
Inthiscase,theexpressiontheweekendreferstotheweekendoftheweekthatthe
anchoring date is part of (i.e., the coming weekend). The information that signals
thismeaningcomesfromthetenseofcontinue,theverbgoverningtheweekend.
Relative temporal expressions are handled with temporal arithmetic similar to
that used for today and yesterday. The document date indicates that our example
articleisISOweek27, sotheexpressionlastweeknormalizestothecurrentweek
minus1. Toresolveambiguousnextandlastexpressionsweconsiderthedistance
from the anchoring date to the nearest unit. Next Friday can refer either to the
immediatelynextFridayortotheFridayfollowingthat,butthecloserthedocument
dateistoaFriday,themorelikelyitisthatthephrasewillskipthenearestone.Such
ambiguities are handled by encoding language and domain-specific heuristics into
thetemporalattachments.
22.4.3 TemporalOrderingofEvents
Thegoaloftemporalanalysis,istolinktimestoeventsandthenfitalltheseevents
intoacompletetimeline. Thisambitioustaskisthesubjectofconsiderablecurrent
research but solving it with a high level of accuracy is beyond the capabilities of
currentsystems. Asomewhatsimpler,butstilluseful,taskistoimposeapartialor-
deringontheeventsandtemporalexpressionsmentionedinatext.Suchanordering
canprovidemanyofthesamebenefitsasatruetimeline. Anexampleofsuchapar-
tialorderingisthedeterminationthatthefareincreasebyAmericanAirlinescame
afterthefareincreasebyUnitedinoursampletext. Determiningsuchanordering
canbeviewedasabinaryrelationdetectionandclassificationtasksimilartothose
describedinChapter21.
Eventhispartialorderingtaskassumesthatinadditiontothedetectingandnor-
malizing time expressions steps described above, we have already detected all the
events in the text using the methods we saw in Chapter 21. Indeed, many tempo-
ralexpressionsareanchoredtoeventsmentionedinatextandnotdirectlytoother
temporalexpressions. Considerthefollowingexample:
(22.29) Oneweekafterthestorm,JetBlueissueditscustomerbillofrights.
TodeterminewhenJetBlueissueditscustomerbillofrightsweneedtodetermine
thetimeofthestormevent, andthenweneedtomodifythattimebythetemporal
expressiononeweekafter.
Thusoncetheeventsandtimeshavebeendetected,ourgoalnextistoassertlinks
between all the times and events: i.e. creating event-event, event-time, time-time,
DCT-event, and DCT-time TimeML TLINKS. This can be done by training time
relation classifiers to predict the correct TLINK between each pair of times/events,
supervisedbythegoldlabelsintheTimeBankcorpuswithfeatureslikewords/em-
beddings,parsepaths,tenseandaspectThesieve-basedarchitectureusingprecision-
ranked sets of classifiers, which we’ll introduce in Chapter 26, is also commonly
used.
Systems that perform all 4 tasks (time extraction creation and normalization,
event extraction, and time/event linking) include TARSQI (Verhagen et al., 2005)
CLEARTK (Bethard, 2013), CAEVO (Chambers et al., 2014), and CATENA (Mirza
andTonelli,2016).456 CHAPTER22 • TIMEANDTEMPORALREASONING
22.5 Summary
This chapter has introduced ways of representing, extracting, and reasoning about
time. Thefollowingaresomeofthehighlightsofthischapter:
• Reasoning about time can be facilitated by detection and normalization of
temporalexpressions.
• Eventscanbeorderedintimeusingsequencemodelsandclassifierstrained
ontemporally-andevent-labeleddataliketheTimeBankcorpus.
Bibliographical and Historical Notes
Exercises
22.1 Ausefulfunctionalityinneweremailandcalendarapplicationsistheability
to associate temporal expressions connected with events in email (doctor’s
appointments,meetingplanning,partyinvitations,etc.)withspecificcalendar
entries. Collectacorpusofemailcontainingtemporalexpressionsrelatedto
eventplanning.Howdotheseexpressionscomparetothekindsofexpressions
commonlyfoundinnewstextthatwe’vebeendiscussinginthischapter?
22.2 For the following sentences, give FOL translations that capture the temporal
relationshipsbetweentheevents.
1. WhenMary’sflightdeparted,Iatelunch.
2. WhenMary’sflightdeparted,Ihadeatenlunch.CHAPTER
23 Word Senses and WordNet
LadyBracknell. Areyourparentsliving?
Jack. Ihavelostbothmyparents.
Lady Bracknell. To lose one parent, Mr. Worthing, may be regarded as a
misfortune;tolosebothlookslikecarelessness.
OscarWilde,TheImportanceofBeingEarnest
ambiguous Wordsareambiguous: thesamewordcanbeusedtomeandifferentthings. In
Chapter 6 we saw that the word “mouse” has (at least) two meanings: (1) a small
rodent, or (2) a hand-operated device to control a cursor. The word “bank” can
mean: (1) a financial institution or (2) a sloping mound. In the quote above from
hisplayTheImportanceofBeingEarnest,OscarWildeplayswithtwomeaningsof
“lose”(tomisplaceanobject,andtosufferthedeathofacloseperson).
Wesaythatthewords‘mouse’or‘bank’arepolysemous(fromGreek‘having
wordsense many senses’, poly- ‘many’ + sema, ‘sign, mark’).1 A sense (or word sense) is
a discrete representation of one aspect of the meaning of a word. In this chapter
WordNet wediscusswordsensesinmoredetailandintroduceWordNet,alargeonlinethe-
saurus—adatabasethatrepresentswordsenses—withversionsinmanylanguages.
WordNet also represents relations between senses. For example, there is an IS-A
relation between dog and mammal (a dog is a kind of mammal) and a part-whole
relationbetweenengineandcar(anengineisapartofacar).
Knowing the relation between two senses can play an important role in tasks
involving meaning. Consider the antonymy relation. Two words are antonyms if
theyhaveoppositemeanings, likelongandshort, orupanddown. Distinguishing
these is quite important; if a user asks a dialogue agent to turn up the music, it
wouldbeunfortunatetoinsteadturnitdown. Butinfactinembeddingmodelslike
word2vec, antonymsareeasilyconfusedwitheachother, becauseoftenoneofthe
closest words in embedding space to a word (e.g., up) is its antonym (e.g., down).
Thesaurusesthatrepresentthisrelationshipcanhelp!
wordsense Wealsointroducewordsensedisambiguation(WSD),thetaskofdetermining
disambiguation
which sense of a word is being used in a particular context. We’ll give supervised
andunsupervisedalgorithmsfordecidingwhichsensewasintendedinaparticular
context. Thistaskhasaverylonghistoryincomputationallinguisticsandmanyap-
plications. Inquestionanswering,wecanbemorehelpfultoauserwhoasksabout
“batcare”ifweknowwhichsenseofbatisrelevant. (Istheuserisavampire? or
just wants to play baseball.) And the different senses of a word often have differ-
enttranslations;inSpanishtheanimalbatisamurcie´lagowhilethebaseballbatis
a bate, and indeed word sense algorithms may help improve MT (Pu et al., 2018).
Finally,WSDhaslongbeenusedasatoolforevaluatinglanguageprocessingmod-
els,andunderstandinghowmodelsrepresentdifferentwordsensesisanimportant
1 Thewordpolysemyitselfisambiguous;youmayseeitusedinadifferentway,toreferonlytocases
whereaword’ssensesarerelatedinsomestructuredway,reservingthewordhomonymytomeansense
ambiguitieswithnorelationbetweenthesenses(HaberandPoesio,2020).Herewewilluse‘polysemy’
tomeananykindofsenseambiguity,and‘structuredpolysemy’forpolysemywithsenserelations.458 CHAPTER23 • WORDSENSESANDWORDNET
analyticdirection.
23.1 Word Senses
wordsense Asense(orwordsense)isadiscreterepresentationofoneaspectofthemeaningof
aword. Looselyfollowinglexicographictradition, werepresenteachsensewitha
superscript:bank1andbank2,mouse1andmouse2. Incontext,it’seasytoseethe
differentmeanings:
mouse1: .... amousecontrollingacomputersystemin1968.
mouse2: .... aquietanimallikeamouse
bank1: ...abankcanholdtheinvestmentsinacustodialaccount...
bank2: ...asagricultureburgeonsontheeastbank,theriver...
23.1.1 DefiningWordSenses
Howcanwedefinethemeaningofawordsense? WeintroducedinChapter6the
standardcomputationalapproachofrepresentingawordasanembedding,apoint
in semantic space. The intuition of embedding models like word2vec or GloVe is
thatthemeaningofawordcanbedefinedbyitsco-occurrences,thecountsofwords
thatoftenoccurnearby. Butthatdoesn’ttellushowtodefinethemeaningofaword
sense. As we saw in Chapter 11, contextual embeddings like BERT go further by
offeringanembeddingthatrepresentsthemeaningofawordinitstextualcontext,
andwe’llseethatcontextualembeddingslieattheheartofmodernalgorithmsfor
wordsensedisambiguation.
But first, we need to consider the alternative ways that dictionaries and the-
saurusesofferfordefiningsenses. Oneisbasedonthefactthatdictionariesorthe-
gloss saurusesgivetextualdefinitionsforeachsensecalledglosses. Herearetheglosses
fortwosensesofbank:
1. financial institution that accepts deposits and channels
the money into lending activities
2. sloping land (especially the slope beside a body of water)
Glossesarenotaformalmeaningrepresentation;theyarejustwrittenforpeople.
Considerthefollowingfragmentsfromthedefinitionsofright,left,red,andblood
fromtheAmericanHeritageDictionary(Morris,1985).
right adj. locatednearertherighthandesp. beingontherightwhen
facingthesamedirectionastheobserver.
left adj. locatednearertothissideofthebodythantheright.
red n. thecolorofbloodoraruby.
blood n. theredliquidthatcirculatesintheheart,arteriesandveinsof
animals.
Notethecircularityinthesedefinitions. Thedefinitionofrightmakestwodirect
references to itself, and the entry for left contains an implicit self-reference in the
phrasethissideofthebody,whichpresumablymeanstheleftside. Theentriesfor
redandbloodreferenceeachotherintheirdefinitions. Forhumans,suchentriesare
usefulsincetheuserofthedictionaryhassufficientgraspoftheseotherterms.23.1 • WORDSENSES 459
Yet despite their circularity and lack of formal representation, glosses can still
beusefulforcomputationalmodelingofsenses.Thisisbecauseaglossisjustasen-
tence,andfromsentenceswecancomputesentenceembeddingsthattellussome-
thing about the meaning of the sense. Dictionaries often give example sentences
alongwithglosses,andthesecanagainbeusedtohelpbuildasenserepresentation.
Thesecondwaythatthesaurusesofferfordefiningasenseis—likethedictionary
definitions—definingasensethroughitsrelationshipwithothersenses. Forexam-
ple,theabovedefinitionsmakeitclearthatrightandleftaresimilarkindsoflemmas
thatstandinsomekindofalternation, oropposition, tooneanother. Similarly, we
cangleanthatredisacolorandthatbloodisaliquid. Senserelationsofthissort
(IS-A,orantonymy)areexplicitlylistedinon-linedatabaseslikeWordNet. Given
a sufficiently large database of such relations, many applications are quite capable
of performing sophisticated semantic tasks about word senses (even if they do not
reallyknowtheirrightfromtheirleft).
23.1.2 Howmanysensesdowordshave?
Dictionariesandthesaurusesgivediscretelistsofsenses. Bycontrast,embeddings
(whetherstaticorcontextual)offeracontinuoushigh-dimensionalmodelofmeaning
thatdoesn’tdivideupintodiscretesenses.
Thereforecreatingathesaurusdependsoncriteriafordecidingwhenthediffer-
ing uses of a word should be represented with discrete senses. We might consider
twosensesdiscreteiftheyhaveindependenttruthconditions,differentsyntacticbe-
havior,andindependentsenserelations,oriftheyexhibitantagonisticmeanings.
ConsiderthefollowingusesoftheverbservefromtheWSJcorpus:
(23.1) Theyrarelyserveredmeat,preferringtoprepareseafood.
(23.2) HeservedasU.S.ambassadortoNorwayin1976and1977.
(23.3) Hemighthaveservedhistime,comeoutandledanupstandinglife.
The serve of serving red meat and that of serving time clearly have different truth
conditions and presuppositions; the serve of serve as ambassador has the distinct
subcategorizationstructureserveasNP.Theseheuristicssuggestthattheseareprob-
ably three distinct senses of serve. One practical technique for determining if two
senses are distinct is to conjoin two uses of a word in a single sentence; this kind
zeugma of conjunction of antagonistic readings is called zeugma. Consider the following
examples:
(23.4) Whichofthoseflightsservebreakfast?
(23.5) DoesAirFranceservePhiladelphia?
(23.6) ?DoesAirFranceservebreakfastandPhiladelphia?
Weuse(?) tomarkthoseexamplesthataresemanticallyill-formed. Theoddnessof
theinventedthirdexample(acaseofzeugma)indicatesthereisnosensiblewayto
makeasinglesenseofserveworkforbothbreakfastandPhiladelphia. Wecanuse
thisasevidencethatservehastwodifferentsensesinthiscase.
Dictionariestendtousemanyfine-grainedsensessoastocapturesubtlemeaning
differences, a reasonable approach given that the traditional role of dictionaries is
aiding word learners. For computational purposes, we often don’t need these fine
distinctions,soweoftengrouporclusterthesenses; wehavealreadydonethisfor
some of the examples in this chapter. Indeed, clustering examples into senses, or
sensesintobroader-grainedcategories,isanimportantcomputationaltaskthatwe’ll
discussinSection23.7.460 CHAPTER23 • WORDSENSESANDWORDNET
23.2 Relations Between Senses
Thissectionexplorestherelationsbetweenwordsenses,especiallythosethathave
receivedsignificantcomputationalinvestigationlikesynonymy,antonymy,andhy-
pernymy.
Synonymy
We introduced in Chapter 6 the idea that when two senses of two different words
synonym (lemmas) are identical, or nearly identical, we say the two senses are synonyms.
Synonymsincludesuchpairsas
couch/sofa vomit/throwup filbert/hazelnut car/automobile
And we mentioned that in practice, the word synonym is commonly used to
describe a relationship of approximate or rough synonymy. But furthermore, syn-
onymyisactuallyarelationshipbetweensensesratherthanwords. Consideringthe
words big and large. These mayseemto besynonyms inthefollowing sentences,
sincewecouldswapbigandlargeineithersentenceandretainthesamemeaning:
(23.7) Howbigisthatplane?
(23.8) WouldIbeflyingonalargeorsmallplane?
Butnotethefollowingsentenceinwhichwecannotsubstitutelargeforbig:
(23.9) MissNelson,forinstance,becameakindofbigsistertoBenjamin.
(23.10) ?MissNelson,forinstance,becameakindoflargesistertoBenjamin.
Thisisbecausethewordbighasasensethatmeansbeingolderorgrownup,while
largelacksthissense. Thus, wesaythatsomesensesofbigandlargeare(nearly)
synonymouswhileotheronesarenot.
Antonymy
antonym Whereas synonyms are words with identical or similar meanings, antonyms are
wordswithanoppositemeaning,like:
long/short big/little fast/slow cold/hot dark/light
rise/fall up/down in/out
Twosensescanbeantonymsiftheydefineabinaryoppositionorareatopposite
endsofsomescale. Thisisthecaseforlong/short,fast/slow,orbig/little,whichare
reversives atoppositeendsofthelengthorsizescale. Anothergroupofantonyms,reversives,
describechangeormovementinoppositedirections,suchasrise/fallorup/down.
Antonymsthusdiffercompletelywithrespecttooneaspectoftheirmeaning—
their position on a scale or their direction—but are otherwise very similar, sharing
almostallotheraspectsof meaning. Thus, automaticallydistinguishingsynonyms
fromantonymscanbedifficult.
TaxonomicRelations
Another way word senses can be related is taxonomically. A word (or sense) is a
hyponym hyponymofanotherwordorsenseifthefirstismorespecific,denotingasubclass
oftheother. Forexample,carisahyponymofvehicle,dogisahyponymofanimal,
hypernym andmangoisahyponymoffruit. Conversely,wesaythatvehicleisahypernymof
car,andanimalisahypernymofdog.Itisunfortunatethatthetwowords(hypernym23.2 • RELATIONSBETWEENSENSES 461
andhyponym)areverysimilarandhenceeasilyconfused;forthisreason,theword
superordinate superordinateisoftenusedinsteadofhypernym.
Superordinate vehicle fruit furniture mammal
Subordinate car mango chair dog
Wecandefinehypernymymoreformallybysayingthattheclassdenotedbythe
superordinate extensionally includes the class denoted by the hyponym. Thus, the
classofanimalsincludesasmembersalldogs, andtheclassofmovingactionsin-
cludesallwalkingactions. Hypernymycanalsobedefinedintermsofentailment.
Underthisdefinition,asenseAisahyponymofasenseBifeverythingthatisAis
alsoB,andhencebeinganAentailsbeingaB,or x A(x) B(x). Hyponymy/hy-
∀ ⇒
pernymyisusuallyatransitiverelation;ifAisahyponymofBandBisahyponym
ofC,thenAisahyponymofC.Anothernameforthehypernym/hyponymstructure
IS-A istheIS-Ahierarchy,inwhichwesayAIS-AB,orBsubsumesA.
Hypernymy is useful for tasks like textual entailment or question answering;
knowingthatleukemiaisatypeofcancer,forexample,wouldcertainlybeusefulin
answeringquestionsaboutleukemia.
Meronymy
part-whole Anothercommonrelationismeronymy,thepart-wholerelation. Alegispartofa
chair;awheelispartofacar. Wesaythatwheelisameronymofcar,andcarisa
holonymofwheel.
StructuredPolysemy
The senses of a word can also be related semantically, in which case we call the
structured relationshipbetweenthemstructuredpolysemy. Considerthissensebank:
polysemy
(23.11) ThebankisonthecornerofNassauandWitherspoon.
This sense, perhaps bank4, means something like “the building belonging to
a financial institution”. These two kinds of senses (an organization and the build-
ing associated with an organization ) occur together for many other words as well
(school,university,hospital,etc.). Thus,thereisasystematicrelationshipbetween
sensesthatwemightrepresentas
BUILDING ORGANIZATION
↔
metonymy Thisparticularsubtypeofpolysemyrelationiscalledmetonymy. Metonymyis
theuseofoneaspectofaconceptorentitytorefertootheraspectsoftheentityor
totheentityitself. WeareperformingmetonymywhenweusethephrasetheWhite
House to refer to the administration whose office is in the White House. Other
commonexamplesofmetonymyincludetherelationbetweenthefollowingpairings
ofsenses:
AUTHOR WORKSOFAUTHOR
↔
(JaneAustenwroteEmma) (IreallyloveJaneAusten)
FRUITTREE FRUIT
↔
(Plumshavebeautifulblossoms) (Iateapreservedplumyesterday)462 CHAPTER23 • WORDSENSESANDWORDNET
23.3 WordNet: A Database of Lexical Relations
The most commonly used resource for sense relations in English and many other
WordNet languages is the WordNet lexical database (Fellbaum, 1998). English WordNet
consists of three separate databases, one each for nouns and verbs and a third for
adjectivesandadverbs;closedclasswordsarenotincluded. Eachdatabasecontains
asetoflemmas,eachoneannotatedwithasetofsenses. TheWordNet3.0release
has117,798nouns, 11,529verbs, 22,479adjectives, and4,481adverbs. Theaver-
age noun has 1.23 senses, and the average verb has 2.16 senses. WordNet can be
accessedontheWebordownloadedlocally. Figure23.1showsthelemmaentryfor
thenounandadjectivebass.
Thenoun“bass”has8sensesinWordNet.
1.bass1-(thelowestpartofthemusicalrange)
2.bass2,basspart1-(thelowestpartinpolyphonicmusic)
3.bass3,basso1-(anadultmalesingerwiththelowestvoice)
4.seabass1,bass4-(theleanfleshofasaltwaterfishofthefamilySerranidae)
5.freshwaterbass1,bass5-(anyofvariousNorthAmericanfreshwaterfishwith
leanflesh(especiallyofthegenusMicropterus))
6.bass6,bassvoice1,basso2-(thelowestadultmalesingingvoice)
7.bass7-(thememberwiththelowestrangeofafamilyofmusicalinstruments)
8.bass8-(nontechnicalnameforanyofnumerousediblemarineand
freshwaterspiny-finnedfishes)
Figure23.1 AportionoftheWordNet3.0entryforthenounbass.
Note that there are eight senses for the noun and one for the adjective, each of
gloss whichhasagloss(adictionary-styledefinition),alistofsynonymsforthesense,and
sometimes also usage examples (shown for the adjective sense). WordNet doesn’t
representpronunciation,sodoesn’tdistinguishthepronunciation[baes]inbass4,
bass5,andbass8fromtheothersensespronounced[beys].
synset Thesetofnear-synonymsforaWordNetsenseiscalledasynset(forsynonym
set); synsets are an important primitive in WordNet. The entry for bass includes
synsets like bass1, deep6 , or bass6, bass voice1, basso2 . We can think of a
{ } { }
synset as representing a concept of the type we discussed in Chapter 19. Thus,
insteadofrepresentingconceptsinlogicalterms,WordNetrepresentsthemaslists
of the word senses that can be used to express the concept. Here’s another synset
example:
chump1, fool2, gull1, mark9, patsy1, fall guy1,
{
sucker1, soft touch1, mug2
}
Theglossofthissynsetdescribesitas:
Gloss: apersonwhoisgullibleandeasytotakeadvantageof.
Glossesarepropertiesofasynset,sothateachsenseincludedinthesynsethasthe
same gloss and can express this concept. Because they share glosses, synsets like
this one are the fundamental unit associated with WordNet entries, and hence it is
synsets,notwordforms,lemmas,orindividualsenses,thatparticipateinmostofthe
lexicalsenserelationsinWordNet.
WordNet also labels each synset with a lexicographic category drawn from a
semantic field for examplethe 26categories fornouns shownin Fig.23.2, aswell23.3 • WORDNET: ADATABASEOFLEXICALRELATIONS 463
as15forverbs(plus2foradjectivesand1foradverbs). Thesecategoriesareoften
supersense calledsupersenses,becausetheyactascoarsesemanticcategoriesorgroupingsof
senses which can be useful when word senses are too fine-grained (Ciaramita and
Johnson2003,CiaramitaandAltun2006). Supersenseshavealsobeendefinedfor
adjectives(Tsvetkovetal.,2014)andprepositions(Schneideretal.,2018).
Category Example Category Example Category Example
ACT service GROUP place PLANT tree
ANIMAL dog LOCATION area POSSESSION price
ARTIFACT car MOTIVE reason PROCESS process
ATTRIBUTE quality NATURALEVENT experience QUANTITY amount
BODY hair NATURALOBJECT flower RELATION portion
COGNITION way OTHER stuff SHAPE square
COMMUNICATION review PERSON people STATE pain
FEELING discomfort PHENOMENON result SUBSTANCE oil
FOOD food TIME day
Figure23.2 Supersenses:26lexicographiccategoriesfornounsinWordNet.
23.3.1 SenseRelationsinWordNet
WordNetrepresentsallthekindsofsenserelationsdiscussedintheprevioussection,
asillustratedinFig.23.3andFig.23.4.
Relation AlsoCalled Definition Example
Hypernym Superordinate Fromconceptstosuperordinates breakfast1 meal1
→
Hyponym Subordinate Fromconceptstosubtypes meal1 lunch1
→
InstanceHypernym Instance Frominstancestotheirconcepts Austen1 author1
→
InstanceHyponym Has-Instance Fromconceptstotheirinstances composer1 Bach1
→
PartMeronym Has-Part Fromwholestoparts table2 leg3
→
PartHolonym Part-Of Frompartstowholes course7 meal1
→
Antonym Semanticoppositionbetweenlemmas leader1 follower1
⇐⇒
Derivation Lemmasw/samemorphologicalroot destruction1 destroy1
⇐⇒
Figure23.3 SomeofthenounrelationsinWordNet.
Relation Definition Example
Hypernym Fromeventstosuperordinateevents fly9 travel5
→
Troponym Fromeventstosubordinateevent walk1 stroll1
Entails Fromverbs(events)totheverbs(events)theyentail snore1→ sleep1
→
Antonym Semanticoppositionbetweenlemmas increase1 decrease1
⇐⇒
Figure23.4 SomeverbrelationsinWordNet.
ForexampleWordNetrepresentshyponymy(page460)byrelatingeachsynset
toitsimmediatelymoregeneralandmorespecificsynsetsthroughdirecthypernym
and hyponym relations. These relations can be followed to produce longer chains
of more general or more specific synsets. Figure 23.5 shows hypernym chains for
bass3andbass7;moregeneralsynsetsareshownonsuccessivelyindentedlines.
WordNethastwokindsoftaxonomicentities:classesandinstances.Aninstance
isanindividual,apropernounthatisauniqueentity. SanFranciscoisaninstance
ofcity, forexample. Butcityisaclass, ahyponymofmunicipalityandeventually464 CHAPTER23 • WORDSENSESANDWORDNET
bass3, basso (an adult male singer with the lowest voice)
=> singer, vocalist, vocalizer, vocaliser
=> musician, instrumentalist, player
=> performer, performing artist
=> entertainer
=> person, individual, someone...
=> organism, being
=> living thing, animate thing,
=> whole, unit
=> object, physical object
=> physical entity
=> entity
bass7 (member with the lowest range of a family of instruments)
=> musical instrument, instrument
=> device
=> instrumentality, instrumentation
=> artifact, artefact
=> whole, unit
=> object, physical object
=> physical entity
=> entity
Figure23.5 Hyponymychainsfortwoseparatesensesofthelemmabass. Notethatthe
chainsarecompletelydistinct,onlyconvergingattheveryabstractlevelwhole,unit.
of location. Fig. 23.6 shows a subgraph of WordNet demonstrating many of the
relations.
Figure23.6 WordNetviewedasagraph.FigurefromNavigli(2016).23.4 • WORDSENSEDISAMBIGUATION 465
23.4 Word Sense Disambiguation
Thetaskofselectingthecorrectsenseforawordiscalledwordsensedisambigua-
wordsense tion,orWSD.WSDalgorithmstakeasinputawordincontextandafixedinventory
disambiguation
WSD ofpotentialwordsensesandoutputsthecorrectwordsenseincontext.
23.4.1 WSD:TheTaskandDatasets
In this section we introduce the task setup for WSD, and then turn to algorithms.
The inventory of sense tags depends on the task. For sense tagging in the context
oftranslationfromEnglishtoSpanish,thesensetaginventoryforanEnglishword
might be the set of different Spanish translations. For automatic indexing of med-
ical articles, the sense-tag inventory might be the set of MeSH (Medical Subject
Headings) thesaurus entries. Or we can use the set of senses from a resource like
WordNet,orsupersensesifwewantacoarser-grainset. Figure23.4.1showssome
suchexamplesforthewordbass.
WordNet Spanish WordNet
Sense Translation Supersense TargetWordinContext
bass4 lubina FOOD ...fishasPacificsalmonandstripedbassand...
bass7 bajo ARTIFACT ...playbassbecausehedoesn’thavetosolo...
Figure23.7 Somepossibilesensetaginventoriesforbass.
In some situations, we just need to disambiguate a small number of words. In
lexicalsample suchlexicalsampletasks, wehaveasmallpre-selectedsetoftargetwordsandan
inventoryofsensesforeachwordfromsomelexicon. Sincethesetofwordsandthe
setofsensesaresmall,simplesupervisedclassificationapproachesworkverywell.
Morecommonly, however, wehaveaharderprobleminwhichwehavetodis-
all-words ambiguateallthewordsinsometext. Inthisall-wordstask,thesystemisgivenan
entiretextsandalexiconwithaninventoryofsensesforeachentryandwehaveto
disambiguate every word in the text (or sometimes just every content word). The
all-wordstaskissimilartopart-of-speechtagging,exceptwithamuchlargersetof
tags since each lemma has its own set. A consequence of this larger set of tags is
datasparseness.
Supervisedall-worddisambiguationtasksaregenerallytrainedfromasemantic
semantic concordance, a corpus in which each open-class word in each sentence is labeled
concordance
with its word sense from a specific dictionary or thesaurus, most often WordNet.
The SemCor corpus is a subset of the Brown Corpus consisting of over 226,036
wordsthatweremanuallytaggedwithWordNetsenses(Milleretal.1993, Landes
etal.1998). Othersense-taggedcorporahavebeenbuiltfortheSENSEVALandSe-
mEvalWSDtasks,suchastheSENSEVAL-3Task1Englishall-wordstestdatawith
2282 annotations (Snyder and Palmer, 2004) or the SemEval-13 Task 12 datasets.
LargesemanticconcordancesarealsoavailableinotherlanguagesincludingDutch
(Vossenetal.,2011)andGerman(Henrichetal.,2012).
Here’s an example from the SemCor corpus showing the WordNet sense num-
bersofthetaggedwords;we’veusedthestandardWSDnotationinwhichasubscript
marksthepartofspeech(Navigli,2009):
(23.12) Youwillfind9thatavocado1is1unlike1other1fruit1youhaveever1tasted2
v n v j j n r v
Given each noun, verb, adjective, or adverb word in the hand-labeled test set (say
fruit),theSemCor-basedWSDtaskistochoosethecorrectsensefromthepossible466 CHAPTER23 • WORDSENSESANDWORDNET
sensesinWordNet. Forfruitthiswouldmeanchoosingbetweenthecorrectanswer
fruit1(theripenedreproductivebodyofaseedplant),andtheothertwosensesfruit2
n n
(yield;anamountofaproduct)andfruit3(theconsequenceofsomeeffortoraction).
n
Fig.23.8sketchesthetask.
y y
5 6
y
3
stand1: side1:
y y
1 bass1: 4 upright relative
low range … region
electric1: … player1: stand5: …
using bass4: in game bear side3:
electricity sea fish player2: … of body
electric2: y … musician stand10: …
tense 2 bass7: player3: put side11:
electric3: instrument actor upright slope
thrilling guitar1 … … … …
x x x x x x
1 2 3 4 5 6
an electric guitar and bass player stand off to one side
Figure23.8 Theall-wordsWSDtask, mappingfrominputwords(x)toWordNetsenses
(y). Onlynouns,verbs,adjectives,andadverbsaremapped,andnotethatsomewords(like
guitar in the example) only have one sense in WordNet. Figure inspired by Chaplot and
Salakhutdinov(2018).
WSD systems are typically evaluated intrinsically, by computing F1 against
hand-labeled sense tags in a held-out set, such as the SemCor corpus or SemEval
corporadiscussedabove.
mostfrequent Asurprisinglystrongbaselineissimplytochoosethemostfrequentsensefor
sense
eachwordfromthesensesinalabeledcorpus(Galeetal.,1992a).ForWordNet,this
corresponds to the first sense, since senses in WordNet are generally ordered from
most frequent to least frequent based on their counts in the SemCor sense-tagged
corpus. The most frequent sense baseline can be quite accurate, and is therefore
often used as a default, to supply a word sense when a supervised algorithm has
insufficienttrainingdata.
onesenseper A second heuristic, called one sense per discourse is based on the work of
discourse
Gale et al. (1992b), who noticed that a word appearing multiple times in a text or
discourseoftenappearswiththesamesense. Thisheuristicseemstoholdbetterfor
coarse-grainedsensesandparticularlywhenaword’ssensesareunrelated,soisn’t
generally used as a baseline. Nonetheless various kinds of disambiguation tasks
oftenincludesomesuchbiastowardresolvinganambiguitythesamewayinsidea
discoursesegment.
23.4.2 TheWSDAlgorithm: ContextualEmbeddings
ThebestperformingWSDalgorithmisasimple1-nearest-neighboralgorithmusing
contextualwordembeddings,duetoMelamudetal.(2016)andPetersetal.(2018).
AttrainingtimewepasseachsentenceintheSemCorelabeleddatasetthroughany
contextual embedding (e.g., BERT) resulting in a contextual embedding for each
labeledtokeninSemCore. (Therearevariouswaystocomputethiscontextualem-
beddingv foratokeni;forBERTitiscommontopoolmultiplelayersbysumming
i
thevectorrepresentationsofifromthelastfourBERTlayers). Thenforeachsense23.4 • WORDSENSEDISAMBIGUATION 467
sofanywordinthecorpus,foreachofthentokensofthatsense,weaveragetheir
ncontextualrepresentationsv itoproduceacontextualsenseembeddingv sfors:
1
v s= v i v i tokens(s) (23.13)
n ∀ ∈
i
(cid:88)
Attesttime,givenatokenofatargetwordt incontext,wecomputeitscontextual
embedding t and choose its nearest neighbor sense from the training set, i.e., the
sensewhosesenseembeddinghasthehighestcosinewitht:
sense(t)= argmax cosine(t,v s) (23.14)
s senses(t)
∈
Fig.23.9illustratesthemodel.
5
4 find v
find
v 1
find
v
9
find
v
c c c c c
I found the jar empty
ENCODER
I found the jar empty
Figure23.9 Thenearest-neighboralgorithmforWSD.Ingreenarethecontextualembed-
dingsprecomputedforeachsenseofeachword; herewejustshowafewofthesensesfor
find. A contextual embedding is computed for the target word found, and then the nearest
neighborsense(inthiscasefind9)ischosen.FigureinspiredbyLoureiroandJorge(2019).
v
What do we do for words we haven’t seen in the sense-labeled training data?
Afterall,thenumberofsensesthatappearinSemCorisonlyasmallfractionofthe
wordsinWordNet.ThesimplestalgorithmistofallbacktotheMostFrequentSense
baseline,i.e. takingthefirstsenseinWordNet. Butthat’snotverysatisfactory.
A more powerful approach, due to Loureiro and Jorge (2019), is to impute the
missingsenseembeddings,bottom-up,byusingtheWordNettaxonomyandsuper-
senses. Wegetasenseembeddingforanyhigher-levelnodeintheWordNettaxon-
omybyaveragingtheembeddingsofitschildren,thuscomputingtheembeddingfor
eachsynsetastheaverageofitssenseembeddings,theembeddingforahypernym
astheaverageofitssynsetembeddings,andthelexicographiccategory(supersense)
embeddingastheaverageofthelargesetofsynsetembeddingswiththatcategory.
Moreformally,foreachmissingsenseinWordNetsˆ W,letthesenseembeddings
∈
fortheothermembersofitssynsetbeS ,thehypernym-specificsynsetembeddings
sˆ
beH ,andthelexicographic(supersense-specific)synsetembeddingsbeL . Wecan
sˆ sˆ
thencomputethesenseembeddingforsˆasfollows:
1
if S sˆ >0, v sˆ= v s, v s S sˆ (23.15)
| | S ∀ ∈
sˆ
| |
1 (cid:88)
elseif H sˆ >0, v sˆ= v syn, v syn H sˆ (23.16)
| | H ∀ ∈
sˆ
| |
1 (cid:88)
elseif L sˆ >0, v sˆ= v syn, v syn L sˆ (23.17)
| | L ∀ ∈
sˆ
| |
(cid:88)468 CHAPTER23 • WORDSENSESANDWORDNET
Since all of the supersenses have some labeled data in SemCor, the algorithm is
guaranteed to have some representation for all possible senses by the time the al-
gorithmbacksofftothemostgeneral(supersense)information,althoughofcourse
withaverycoarsemodel.
23.5 Alternate WSD algorithms and Tasks
23.5.1 Feature-BasedWSD
Feature-based algorithms for WSD are extremely simple and function almost as
wellascontextuallanguagemodelalgorithms. ThebestperformingIMSalgorithm
(ZhongandNg,2010),augmentedbyembeddings(Iacobaccietal.2016,Raganato
etal.2017b), usesanSVMclassifiertochoosethesenseforeachinputwordwith
thefollowingsimplefeaturesofthesurroundingwords:
• part-of-speech tags (for a window of 3 words on each side, stopping at sen-
tenceboundaries)
collocation • collocation features of words or n-grams of lengths 1, 2, 3 at a particular
location in a window of 3 words on each side (i.e., exactly one word to the
right,orthetwowordsstarting3wordstotheleft,andsoon).
• weighted average of embeddings (of all words in a window of 10 words on
eachside,weightedexponentiallybydistance)
ConsidertheambiguouswordbassinthefollowingWSJsentence:
(23.18) Anelectricguitarandbassplayerstandofftooneside,
Ifweusedasmall2-wordwindow,astandardfeaturevectormightincludeparts-of-
speech,unigramandbigramcollocationfeatures,andaweightedsumgofembed-
dings,thatis:
[w
i
2,POS
i
2,w
i
1,POS
i
1,w i+1,POS i+1,w i+2,POS i+2,wi i−1 2,
− − − − −
wi i+ +2 1,g(E(w i 2),E(w i 1),E(w i+1),E(w i+2)] (23.19)
− −
wouldyieldthefollowingvector:
[guitar, NN, and, CC, player, NN, stand, VB, guitar and,
player stand, g(E(guitar),E(and),E(player),E(stand))]
23.5.2 TheLeskAlgorithmasWSDBaseline
GeneratingsenselabeledcorporalikeSemCorisquitedifficultandexpensive. An
knowledge- alternative class of WSD algorithms, knowledge-based algorithms, rely solely on
based
WordNetorothersuchresourcesanddon’trequirelabeleddata. Whilesupervised
algorithms generally work better, knowledge-based methods can be used in lan-
guagesordomainswherethesaurusesordictionariesbutnotsenselabeledcorpora
areavailable.
Leskalgorithm The Lesk algorithm is the oldest and most powerful knowledge-based WSD
method, andisausefulbaseline. Leskisreallyafamilyofalgorithmsthatchoose
thesensewhosedictionaryglossordefinitionsharesthemostwordswiththetarget
word’s neighborhood. Figure 23.10 shows the simplest version of the algorithm,
SimplifiedLesk oftencalledtheSimplifiedLeskalgorithm(KilgarriffandRosenzweig,2000).23.5 • ALTERNATEWSDALGORITHMSANDTASKS 469
functionSIMPLIFIEDLESK(word,sentence)returnsbestsenseofword
best-sense mostfrequentsenseforword
←
max-overlap 0
←
context setofwordsinsentence
←
foreachsenseinsensesofworddo
signature setofwordsintheglossandexamplesofsense
←
overlap COMPUTEOVERLAP(signature,context)
←
ifoverlap>max-overlapthen
max-overlap overlap
←
best-sense sense
←
end
return(best-sense)
Figure23.10 The Simplified Lesk algorithm. The COMPUTEOVERLAP function returns
thenumberofwordsincommonbetweentwosets,ignoringfunctionwordsorotherwords
onastoplist.TheoriginalLeskalgorithmdefinesthecontextinamorecomplexway.
AsanexampleoftheLeskalgorithmatwork,considerdisambiguatingtheword
bankinthefollowingcontext:
(23.20) Thebankcanguaranteedepositswilleventuallycoverfuturetuitioncosts
becauseitinvestsinadjustable-ratemortgagesecurities.
giventhefollowingtwoWordNetsenses:
bank1 Gloss: a financial institution that accepts deposits and channels the
moneyintolendingactivities
Examples: “hecashedacheckatthebank”,“thatbankholdsthemortgage
onmyhome”
bank2 Gloss: slopingland(especiallytheslopebesideabodyofwater)
Examples: “theypulledthecanoeuponthebank”,“hesatonthebankof
theriverandwatchedthecurrents”
Sense bank1 has two non-stopwords overlapping with the context in (23.20):
depositsandmortgage,whilesensebank2haszerowords,sosensebank1ischosen.
There are many obvious extensions to Simplified Lesk, such as weighing the
overlappingwordsbyIDF(inversedocumentfrequency)Chapter6todownweight
frequentwordslikefunctionwords; bestperformingistousewordembeddingco-
sineinsteadofwordoverlaptocomputethesimilaritybetweenthedefinitionandthe
context(Basileetal.,2014). ModernneuralextensionsofLeskusethedefinitions
tocomputesenseembeddingsthatcanbedirectlyusedinsteadofSemCor-training
embeddings(Kumaretal.2019,Luoetal.2018a,Luoetal.2018b).
23.5.3 Word-in-ContextEvaluation
WordSenseDisambiguationisamuchmorefine-grainedevaluationofwordmean-
ingthanthecontext-freewordsimilaritytaskswedescribedinChapter6.Recallthat
tasks like LexSim-999 require systems to match human judgments on the context-
free similarity between two words (how similar is cup to mug?). We can think of
WSDasakindofcontextualizedsimilaritytask,sinceourgoalistobeabletodistin-
guishthemeaningofawordlikebassinonecontext(playingmusic)fromanother
context(fishing).
word-in-context
Somewhereinbetweenliestheword-in-contexttask. Herethesystemisgiven470 CHAPTER23 • WORDSENSESANDWORDNET
twosentences,eachwiththesametargetwordbutinadifferentsententialcontext.
Thesystemmustdecidewhetherthetargetwordsareusedinthesamesenseinthe
WiC twosentencesorinadifferentsense. Fig.23.11showssamplepairsfromtheWiC
datasetofPilehvarandCamacho-Collados(2019).
F There’salotoftrashonthebedoftheriver—
IkeepaglassofwaternexttomybedwhenIsleep
F Justifythemargins—Theendjustifiesthemeans
T Airpollution—Openawindowandletinsomeair
T Theexpandedwindowwillgiveustimetocatchthethieves—
Youhaveatwo-hourwindowofclearweathertofinishworkingonthelawn
Figure23.11 Positive (T) and negative (F) pairs from the WiC dataset (Pilehvar and
Camacho-Collados,2019).
The WiC sentences are mainly taken from the example usages for senses in
WordNet. But WordNet senses are very fine-grained. For this reason tasks like
word-in-context first cluster the word senses into coarser clusters, so that the two
sentential contexts forthe target word are marked asT if the two sensesare in the
samecluster. WiCclustersallpairsofsensesiftheyarefirstdegreeconnectionsin
theWordNetsemanticgraph, includingsistersenses, oriftheybelongtothesame
supersense;wepointtoothersenseclusteringalgorithmsattheendofthechapter.
The baseline algorithm to solve the WiC task uses contextual embeddings like
BERT with a simple thresholded cosine. We first compute the contextual embed-
dingsforthetargetwordineachofthetwosentences,andthencomputethecosine
betweenthem. Ifit’saboveathresholdtunedonadevsetwerespondtrue(thetwo
sensesarethesame)elsewerespondfalse.
23.5.4 Wikipediaasasourceoftrainingdata
DatasetsotherthanSemCorhavebeenusedforall-wordsWSD.Oneimportantdi-
rection is to use Wikipedia as a source of sense-labeled data. When a concept is
mentionedinaWikipediaarticle,thearticletextmaycontainanexplicitlinktothe
concept’sWikipediapage, whichisnamedbyauniqueidentifier. Thislinkcanbe
used as a sense annotation. For example, the ambiguous word bar is linked to a
differentWikipediaarticledependingonitsmeaningincontext,includingthepage
BAR (LAW), the page BAR (MUSIC), and so on, as in the following Wikipedia
examples(Mihalcea,2007).
In 1834, Sumner was admitted to the [[bar (law) bar]] at the age of
|
twenty-three,andenteredprivatepracticeinBoston.
It is danced in 3/4 time (like most waltzes), with the couple turning
approx. 180degreesevery[[bar(music)bar]].
|
Jengaisapopularbeerinthe[[bar(establishment)bar]]sofThailand.
|
Thesesentencescanthenbeaddedtothetrainingdataforasupervisedsystem.
In order to use Wikipedia in this way, however, it is necessary to map from Wiki-
pediaconceptstowhateverinventoryofsensesisrelevantfortheWSDapplication.
Automatic algorithms that map from Wikipedia to WordNet, for example, involve
finding the WordNet sense that has the greatest lexical overlap with the Wikipedia
sense, bycomparingthevectorofwordsintheWordNetsynset, gloss, andrelated
senseswiththevectorofwordsintheWikipediapagetitle,outgoinglinks,andpage23.6 • USINGTHESAURUSESTOIMPROVEEMBEDDINGS 471
category (Ponzetto and Navigli, 2010). The resulting mapping has been used to
createBabelNet,alargesense-annotatedresource(NavigliandPonzetto,2012).
23.6 Using Thesauruses to Improve Embeddings
Thesauruses have also been used to improve both static and contextual word em-
beddings. For example, static word embeddings have a problem with antonyms.
A word like expensive is often very similar in embedding cosine to its antonym
like cheap. Antonymy information from thesauruses can help solve this problem;
Fig.23.12showsnearestneighborstosometargetwordsinGloVe,andtheimprove-
mentafteronesuchmethod.
Beforecounterfitting Aftercounterfitting
east west north south eastward eastern easterly
expensive pricey cheaper costly costly pricy overpriced
British American Australian Britain Brits London BBC
Figure23.12 The nearest neighbors in GloVe to east, expensive, and British include
antonyms like west. The right side showing the improvement in GloVe nearest neighbors
afterthecounterfittingmethod(Mrksˇic´etal.,2016).
Therearetwofamiliesofsolutions. Thefirstrequiresretraining: wemodifythe
embedding training to incorporate thesaurus relations like synonymy, antonym, or
supersenses. Thiscanbedonebymodifyingthestaticembeddinglossfunctionfor
word2vec (Yu and Dredze 2014, Nguyen et al. 2016) or by modifying contextual
embeddingtraining(Levineetal.2020,Lauscheretal.2019).
The second, for static embeddings, is more light-weight; after the embeddings
have been trained we learn a second mapping based on a thesaurus that shifts the
embeddingsofwordsinsuchawaythatsynonyms(accordingtothethesaurus)are
retrofitting pushed closer and antonyms further apart. Such methods are called retrofitting
(Faruquietal.2015,Lengerichetal.2018)orcounterfitting(Mrksˇic´ etal.,2016).
23.7 Word Sense Induction
Itisexpensiveanddifficulttobuildlargecorporainwhicheachwordislabeledfor
itswordsense. Forthisreason,anunsupervisedapproachtosensedisambiguation,
wordsense often called word sense induction or WSI, is an important direction. In unsu-
induction
pervised approaches, we don’t use human-defined word senses. Instead, the set of
“senses” of each word is created automatically from the instances of each word in
thetrainingset.
Most algorithms for word sense induction follow the early work of Schu¨tze
(Schu¨tze1992b, Schu¨tze1998)inusingsomesortofclusteringoverwordembed-
dings. Intraining,weusethreesteps:
1. Foreachtokenw ofwordwinacorpus,computeacontextvectorc.
i
2. Useaclusteringalgorithmtoclustertheseword-tokencontextvectorscinto
apredefinednumberofgroupsorclusters. Eachclusterdefinesasenseofw.
3. Compute the vector centroid of each cluster. Each vector centroid s j is a
sensevectorrepresentingthatsenseofw.472 CHAPTER23 • WORDSENSESANDWORDNET
Sincethisisanunsupervisedalgorithm,wedon’thavenamesforeachofthese
“senses”ofw;wejustrefertothe jthsenseofw.
Todisambiguateaparticulartokent ofwweagainhavethreesteps:
1. Computeacontextvectorcfort.
2. Retrieveallsensevectorss forw.
j
3. Assignt tothesenserepresentedbythesensevectors thatisclosesttot.
j
All we need is a clustering algorithm and a distance metric between vectors.
Clusteringisawell-studiedproblemwithawidenumberofstandardalgorithmsthat
canbeappliedtoinputsstructuredasvectorsofnumericalvalues(DudaandHart,
1973). Afrequentlyusedtechniqueinlanguageapplicationsisknownasagglom-
agglomerative erative clustering. In this technique, each of the N training instances is initially
clustering
assignedtoitsowncluster. Newclustersarethenformedinabottom-upfashionby
thesuccessivemergingofthetwoclustersthataremostsimilar. Thisprocesscon-
tinuesuntileitheraspecifiednumberofclustersisreached,orsomeglobalgoodness
measure among the clusters is achieved. In cases in which the number of training
instances makes this method too expensive, random sampling can be used on the
originaltrainingsettoachievesimilarresults.
Howcanweevaluateunsupervisedsensedisambiguationapproaches? Asusual,
thebestwayistodoextrinsicevaluationembeddedinsomeend-to-endsystem;one
example used in a SemEval bakeoff is to improve search result clustering and di-
versification (Navigli and Vannella, 2013). Intrinsic evaluation requires a way to
maptheautomaticallyderivedsenseclassesintoahand-labeledgold-standardsetso
thatwecancompareahand-labeledtestsetwithasetlabeledbyourunsupervised
classifier. Varioussuchmetricshavebeentested,forexampleintheSemEvaltasks
(Manandhar et al. 2010, Navigli and Vannella 2013, Jurgens and Klapaftis 2013),
includingclusteroverlapmetrics, ormethodsthatmapeachsenseclustertoapre-
definedsensebychoosingthesensethat(insometrainingset)hasthemostoverlap
withthecluster. Howeveritisfairtosaythatnoevaluationmetricforthistaskhas
yetbecomestandard.
23.8 Summary
Thischapterhascoveredawiderangeofissuesconcerningthemeaningsassociated
withlexicalitems. Thefollowingareamongthehighlights:
• Awordsenseisthelocusofwordmeaning;definitionsandmeaningrelations
aredefinedatthelevelofthewordsenseratherthanwordforms.
• Manywordsarepolysemous,havingmanysenses.
• Relations between senses include synonymy, antonymy, meronymy, and
taxonomicrelationshyponymyandhypernymy.
• WordNet is a large database of lexical relations for English, and WordNets
existforavarietyoflanguages.
• Word-sense disambiguation (WSD) is the task of determining the correct
sense of a word in context. Supervised approaches make use of a corpus
of sentences in which individual words (lexical sample task) or all words
(all-wordstask)arehand-labeledwithsensesfromaresourcelikeWordNet.
SemCoristhelargestcorpuswithWordNet-labeledsenses.BIBLIOGRAPHICALANDHISTORICALNOTES 473
• ThestandardsupervisedalgorithmforWSDisnearestneighborswithcontex-
tualembeddings.
• Feature-based algorithms using parts of speech and embeddings of words in
thecontextofthetargetwordalsoworkwell.
• An important baseline for WSD is the most frequent sense, equivalent, in
WordNet,totakethefirstsense.
• Another baseline is a knowledge-based WSD algorithm called the Lesk al-
gorithmwhichchoosesthesensewhosedictionarydefinitionsharesthemost
wordswiththetargetword’sneighborhood.
• Wordsenseinductionisthetaskoflearningwordsensesunsupervised.
Bibliographical and Historical Notes
Word sense disambiguation traces its roots to some of the earliest applications of
digitalcomputers. Theinsightthatunderliesmodernalgorithmsforwordsensedis-
ambiguationwasfirstarticulatedbyWeaver(1949/1955)inthecontextofmachine
translation:
Ifoneexaminesthewordsinabook,oneatatimeasthroughanopaque
mask with a hole in it one word wide, then it is obviously impossible
to determine, one at a time, the meaning of the words. [...] But if
one lengthens the slit in the opaque mask, until one can see not only
the central word in question but also say N words on either side, then
ifNislargeenoughonecanunambiguouslydecidethemeaningofthe
centralword. [...] Thepracticalquestionis:“Whatminimumvalueof
Nwill,atleastinatolerablefractionofcases,leadtothecorrectchoice
ofmeaningforthecentralword?”
Othernotionsfirstproposedinthisearlyperiodincludetheuseofathesaurusfordis-
ambiguation(Masterman,1957),supervisedtrainingofBayesianmodelsfordisam-
biguation(MadhuandLytel,1965),andtheuseofclusteringinwordsenseanalysis
(SparckJones,1986).
MuchdisambiguationworkwasconductedwithinthecontextofearlyAI-oriented
naturallanguageprocessingsystems. Quillian(1968)andQuillian(1969)proposed
a graph-based approach to language processing, in which the definition of a word
was represented by a network of word nodes connected by syntactic and semantic
relations, and sense disambiguation by finding the shortest path between senses in
thegraph. Simmons(1973)isanotherinfluentialearlysemanticnetworkapproach.
Wilksproposedoneoftheearliestnon-discretemodelswithhisPreferenceSeman-
tics (Wilks 1975c, Wilks 1975b, Wilks 1975a), and Small and Rieger (1982) and
Riesbeck(1975)proposedunderstandingsystemsbasedonmodelingrichprocedu-
ral information for each word. Hirst’s ABSITY system (Hirst and Charniak 1982,
Hirst1987,Hirst1988),whichusedatechniquecalledmarkerpassingbasedonse-
mantic networks, represents the most advanced system of this type. As with these
largelysymbolicapproaches, earlyneuralnetwork(atthetimecalled‘connection-
ist’) approaches to word sense disambiguation relied on small lexicons with hand-
codedrepresentations(Cottrell1985,Kawamoto1988).
Theearliestimplementationofarobustempiricalapproachtosensedisambigua-
tion is due to Kelly and Stone (1975), who directed a team that hand-crafted a set474 CHAPTER23 • WORDSENSESANDWORDNET
of disambiguation rules for 1790 ambiguous English words. Lesk (1986) was the
firsttouseamachine-readabledictionaryforwordsensedisambiguation. Fellbaum
(1998) collects early work on WordNet. Early work using dictionaries as lexical
resourcesincludeAmsler’s1981useoftheMerriamWebsterdictionaryandLong-
man’sDictionaryofContemporaryEnglish(BoguraevandBriscoe,1989).
Supervised approaches to disambiguation began with the use of decision trees
byBlack(1988). InadditiontotheIMSandcontextual-embeddingbasedmethods
forsupervisedWSD,recentsupervisedalgorithmsincludesencoder-decodermodels
(Raganatoetal.,2017a).
The need for large amounts of annotated text in supervised methods led early
ontoinvestigationsintotheuseofbootstrappingmethods(Hearst1991,Yarowsky
1995). For example the semi-supervised algorithm of Diab and Resnik (2002) is
based on aligned parallel corpora in two languages. For example, the fact that the
French word catastrophe might be translated as English disaster in one instance
and tragedy in another instance can be used to disambiguate the senses of the two
Englishwords(i.e.,tochoosesensesofdisasterandtragedythataresimilar).
The earliest use of clustering in the study of word senses was by Sparck Jones
(1986); Pedersen and Bruce (1997), Schu¨tze (1997b), and Schu¨tze (1998) applied
coarsesenses distributional methods. Clustering word senses into coarse senses has also been
usedtoaddresstheproblemofdictionarysensesbeingtoofine-grained(Section23.5.3)
(Dolan 1994, Chen and Chang 1998, Mihalcea and Moldovan 2001, Agirre and
deLacalle2003,Palmeretal.2004,Navigli2006,Snowetal.2007,Pilehvaretal.
2013). Corporawithclusteredwordsensesfortrainingsupervisedclusteringalgo-
OntoNotes rithmsincludePalmeretal.(2006)andOntoNotes(Hovyetal.,2006).
SeePustejovsky(1995), PustejovskyandBoguraev(1996),Martin(1986), and
CopestakeandBriscoe(1995), interalia, forcomputationalapproachestotherep-
generative resentation of polysemy. Pustejovsky’s theory of the generative lexicon, and in
lexicon
qualia particularhistheoryofthequaliastructureofwords,isawayofaccountingforthe
structure
dynamicsystematicpolysemyofwordsincontext.
HistoricaloverviewsofWSDincludeAgirreandEdmonds(2006)andNavigli
(2009).
Exercises
23.1 Collect a small corpus of example sentences of varying lengths from any
newspaper or magazine. Using WordNet or any standard dictionary, deter-
minehowmanysensesthereareforeachoftheopen-classwordsineachsen-
tence. Howmanydistinctcombinationsofsensesarethereforeachsentence?
Howdoesthisnumberseemtovarywithsentencelength?
23.2 UsingWordNetorastandardreferencedictionary, tageachopen-classword
inyourcorpuswithitscorrecttag. Waschoosingthecorrectsensealwaysa
straightforwardtask? Reportonanydifficultiesyouencountered.
23.3 Using your favorite dictionary, simulate the original Lesk word overlap dis-
ambiguationalgorithmdescribedonpage469onthephraseTimeflieslikean
arrow. Assume that the words are to be disambiguated one at a time, from
left to right, and that the results from earlier decisions are used later in the
process.EXERCISES 475
23.4 Build an implementation of your solution to the previous exercise. Using
WordNet, implement the original Lesk word overlap disambiguation algo-
rithmdescribedonpage469onthephraseTimeflieslikeanarrow.476 CHAPTER24 • SEMANTICROLELABELING
CHAPTER
24 Semantic Role Labeling
“Who,What,Where,When,Withwhat,Why,How”
Thesevencircumstances,associatedwithHermagorasandAristotle(Sloan,2010)
Sometimebetweenthe7thand4thcenturiesBCE,theIndiangrammarianPa¯n.ini1
wroteafamoustreatiseonSanskritgrammar,theAs.t.a¯dhya¯y¯ı(‘8books’),atreatise
thathasbeencalled“oneofthegreatestmonumentsofhu-
man intelligence” (Bloomfield, 1933, 11). The work de-
scribesthelinguisticsoftheSanskritlanguageintheform
of 3959 sutras, each very efficiently (since it had to be
memorized!) expressingpartofaformalrulesystemthat
brilliantly prefigured modern mechanisms of formal lan-
guagetheory(PennandKiparsky,2012). Onesetofrules
describes the ka¯rakas, semantic relationships between a
verbandnounarguments,roleslikeagent,instrument,or
destination. Pa¯n.ini’s work was the earliest we know of
thatmodeledthelinguisticrealizationofeventsandtheir
participants. This task of understanding how participants relate to events—being
abletoanswerthequestion“Whodidwhattowhom”(andperhapsalso“whenand
where”)—isacentralquestionofnaturallanguageprocessing.
Let’smoveforward2.5millenniatothepresentandconsidertheverymundane
goal of understanding text about a purchase of stock by XYZ Corporation. This
purchasingeventanditsparticipantscanbedescribedbyawidevarietyofsurface
forms. The event can be described by a verb (sold, bought) or a noun (purchase),
andXYZCorpcanbethesyntacticsubject(ofbought),theindirectobject(ofsold),
orinagenitiveornouncompoundrelation(withthenounpurchase)despitehaving
notionallythesameroleinallofthem:
• XYZcorporationboughtthestock.
• TheysoldthestocktoXYZcorporation.
• ThestockwasboughtbyXYZcorporation.
• ThepurchaseofthestockbyXYZcorporation...
• ThestockpurchasebyXYZcorporation...
Inthischapterweintroducealevelofrepresentationthatcapturesthecommon-
ality between these sentences: there was a purchase event, the participants were
XYZCorpandsomestock,andXYZCorpwasthebuyer. Theseshallowsemantic
representations,semanticroles,expresstherolethatargumentsofapredicatetake
in the event, codified in databases like PropBank and FrameNet. We’ll introduce
semanticrolelabeling,thetaskofassigningrolestospansinsentences,andselec-
tional restrictions, the preferences that predicates express about their arguments,
suchasthefactthatthethemeofeatisgenerallysomethingedible.
1 FigureshowsabirchbarkmanuscriptfromKashmiroftheRupavatra,agrammaticaltextbookbased
ontheSanskritgrammarofPanini.ImagefromtheWellcomeCollection.24.1 • SEMANTICROLES 477
24.1 Semantic Roles
ConsiderhowinChapter19werepresentedthemeaningofargumentsforsentences
likethese:
(24.1) Sashabrokethewindow.
(24.2) Patopenedthedoor.
Aneo-Davidsonianeventrepresentationofthesetwosentenceswouldbe
e,x,yBreaking(e) Breaker(e,Sasha)
∃ ∧
BrokenThing(e,y) Window(y)
∧ ∧
e,x,yOpening(e) Opener(e,Pat)
∃ ∧
OpenedThing(e,y) Door(y)
∧ ∧
In this representation, the roles of the subjects of the verbs break and open are
deeproles BreakerandOpenerrespectively.Thesedeeprolesarespecifictoeachevent;Break-
ingeventshaveBreakers,OpeningeventshaveOpeners,andsoon.
If we are going to be able to answer questions, perform inferences, or do any
furtherkindsofsemanticprocessingoftheseevents,we’llneedtoknowalittlemore
about the semantics of these arguments. Breakers and Openers have something in
common. Theyarebothvolitionalactors,oftenanimate,andtheyhavedirectcausal
responsibilityfortheirevents.
thematicroles ThematicrolesareawaytocapturethissemanticcommonalitybetweenBreak-
agents ers and Openers. We say that the subjects of both these verbs are agents. Thus,
AGENTisthethematicrolethatrepresentsanabstractideasuchasvolitionalcausa-
tion.Similarly,thedirectobjectsofboththeseverbs,theBrokenThingandOpenedThing,
arebothprototypicallyinanimateobjectsthatareaffectedinsomewaybytheaction.
theme Thesemanticrolefortheseparticipantsistheme.
ThematicRole Definition
AGENT Thevolitionalcauserofanevent
EXPERIENCER Theexperiencerofanevent
FORCE Thenon-volitionalcauseroftheevent
THEME Theparticipantmostdirectlyaffectedbyanevent
RESULT Theendproductofanevent
CONTENT Thepropositionorcontentofapropositionalevent
INSTRUMENT Aninstrumentusedinanevent
BENEFICIARY Thebeneficiaryofanevent
SOURCE Theoriginoftheobjectofatransferevent
GOAL Thedestinationofanobjectofatransferevent
Figure24.1 Somecommonlyusedthematicroleswiththeirdefinitions.
Althoughthematicrolesareoneoftheoldestlinguisticmodels,aswesawabove,
their modern formulation is due to Fillmore (1968) and Gruber (1965). Although
there is no universally agreed-upon set of roles, Figs. 24.1 and 24.2 list some the-
maticrolesthathavebeenusedinvariouscomputationalpapers,togetherwithrough
definitionsandexamples.Mostthematicrolesetshaveaboutadozenroles,butwe’ll
seesetswithsmallernumbersofroleswithevenmoreabstractmeanings, andsets
withverylargenumbersofrolesthatarespecifictosituations. We’llusethegeneral
semanticroles termsemanticrolesforallsetsofroles,whethersmallorlarge.478 CHAPTER24 • SEMANTICROLELABELING
ThematicRole Example
AGENT Thewaiterspilledthesoup.
EXPERIENCER Johnhasaheadache.
FORCE Thewindblowsdebrisfromthemallintoouryards.
THEME OnlyafterBenjaminFranklinbroketheice...
RESULT Thecitybuiltaregulation-sizebaseballdiamond...
CONTENT Monaasked“YoumetMaryAnnatasupermarket?”
INSTRUMENT Hepoachedcatfish,stunningthemwithashockingdevice...
BENEFICIARY WheneverAnnCallahanmakeshotelreservationsforherboss...
SOURCE IflewinfromBoston.
GOAL IdrovetoPortland.
Figure24.2 Someprototypicalexamplesofvariousthematicroles.
24.2 Diathesis Alternations
The main reason computational systems use semantic roles is to act as a shallow
meaning representation that can let us make simple inferences that aren’t possible
from the pure surface string of words, or even from the parse tree. To extend the
earlier examples, if a document says that Company A acquired Company B, we’d
liketoknowthatthisanswersthequeryWasCompanyBacquired? despitethefact
that the two sentences have very different surface syntax. Similarly, this shallow
semanticsmightactasausefulintermediatelanguageinmachinetranslation.
Semantic roles thus help generalize over different surface realizations of pred-
icate arguments. For example, while the AGENT is often realized as the subject of
thesentence,inothercasesthe THEME canbethesubject. Considerthesepossible
realizationsofthethematicargumentsoftheverbbreak:
(24.3) John brokethewindow.
AGENT THEME
(24.4) John brokethewindowwitharock.
AGENT THEME INSTRUMENT
(24.5) Therock brokethewindow.
INSTRUMENT THEME
(24.6) Thewindowbroke.
THEME
(24.7) ThewindowwasbrokenbyJohn.
THEME AGENT
Theseexamplessuggestthatbreakhas(atleast)thepossibleargumentsAGENT,
THEME, and INSTRUMENT. The set of thematic role arguments taken by a verb is
thematicgrid often called the thematic grid, θ-grid, or case frame. We can see that there are
caseframe (amongothers)thefollowingpossibilitiesfortherealizationoftheseargumentsof
break:
AGENT/Subject, THEME/Object
AGENT/Subject, THEME/Object, INSTRUMENT/PP
with
INSTRUMENT/Subject, THEME/Object
THEME/Subject
Itturnsoutthatmanyverbsallowtheirthematicrolestoberealizedinvarious
syntacticpositions. Forexample,verbslikegivecanrealizethe THEME and GOAL
argumentsintwodifferentways:24.3 • SEMANTICROLES: PROBLEMSWITHTHEMATICROLES 479
(24.8) a. Doris gavethebooktoCary.
AGENT THEME GOAL
b. Doris gaveCary thebook.
AGENT GOALTHEME
Thesemultipleargumentstructurerealizations(thefactthatbreakcantakeAGENT,
INSTRUMENT, or THEME as subject, and give can realize its THEME and GOAL in
verb eitherorder)arecalledverbalternationsordiathesisalternations. Thealternation
alternation
dative weshowedaboveforgive,thedativealternation,seemstooccurwithparticularse-
alternation
manticclassesofverbs,including“verbsoffuturehaving”(advance,allocate,offer,
owe), “send verbs” (forward, hand, mail), “verbs of throwing” (kick, pass, throw),
andsoon. Levin(1993)listsfor3100Englishverbsthesemanticclassestowhich
they belong (47 high-level classes, divided into 193 more specific classes) and the
variousalternationsinwhichtheyparticipate. Theselistsofverbclasseshavebeen
incorporatedintotheonlineresourceVerbNet(Kipperetal.,2000),whichlinkseach
verbtobothWordNetandFrameNetentries.
24.3 Semantic Roles: Problems with Thematic Roles
Representing meaning at the thematic role level seems like it should be useful in
dealingwithcomplicationslikediathesisalternations. Yetithasprovedquitediffi-
culttocomeupwithastandardsetofroles,andequallydifficulttoproduceaformal
definitionofroleslikeAGENT,THEME,orINSTRUMENT.
For example, researchers attempting to define role sets often find they need to
fragmentarolelike AGENT or THEME intomanyspecificroles. LevinandRappa-
portHovav(2005)summarizeanumberofsuchcases, suchasthefactthereseem
tobeatleasttwokindsofINSTRUMENTS,intermediaryinstrumentsthatcanappear
assubjectsandenablinginstrumentsthatcannot:
(24.9) a. Thecookopenedthejarwiththenewgadget.
b. Thenewgadgetopenedthejar.
(24.10) a. Shellyatetheslicedbananawithafork.
b. *Theforkatetheslicedbanana.
Inadditiontothefragmentationproblem, therearecasesinwhichwe’dliketo
reasonaboutandgeneralizeacrosssemanticroles,butthefinitediscretelistsofroles
don’tletusdothis.
Finally,ithasproveddifficulttoformallydefinethethematicroles.Considerthe
AGENTrole;mostcasesofAGENTSareanimate,volitional,sentient,causal,butany
individualnounphrasemightnotexhibitalloftheseproperties.
semanticrole These problems have led to alternative semantic role models that use either
manyfewerormanymoreroles.
The first of these options is to define generalized semantic roles that abstract
proto-agent overthespecificthematicroles. Forexample,PROTO-AGENTandPROTO-PATIENT
proto-patient aregeneralizedrolesthatexpressroughlyagent-likeandroughlypatient-likemean-
ings. Theserolesaredefined,notbynecessaryandsufficientconditions,butrather
by a set of heuristic features that accompany more agent-like or more patient-like
meanings. Thus, the more an argument displays agent-like properties (being voli-
tionallyinvolvedintheevent,causinganeventorachangeofstateinanotherpar-
ticipant,beingsentientorintentionallyinvolved,moving)thegreaterthelikelihood480 CHAPTER24 • SEMANTICROLELABELING
thattheargumentcanbelabeledaPROTO-AGENT.Themorepatient-liketheproper-
ties(undergoingchangeofstate,causallyaffectedbyanotherparticipant,stationary
relativetootherparticipants, etc.), thegreaterthelikelihoodthattheargumentcan
belabeledaPROTO-PATIENT.
The second direction is instead to define semantic roles that are specific to a
particularverboraparticulargroupofsemanticallyrelatedverbsornouns.
Inthenexttwosectionswedescribetwocommonlyusedlexicalresourcesthat
makeuseofthesealternativeversionsofsemanticroles.PropBankusesbothproto-
rolesandverb-specificsemanticroles. FrameNetusessemanticrolesthatarespe-
cifictoageneralsemanticideacalledaframe.
24.4 The Proposition Bank
PropBank The Proposition Bank, generally referred to as PropBank, is a resource of sen-
tencesannotatedwithsemanticroles.TheEnglishPropBanklabelsallthesentences
inthePennTreeBank;theChinesePropBanklabelssentencesinthePennChinese
TreeBank. Because of the difficulty of defining a universal set of thematic roles,
thesemanticrolesinPropBankaredefinedwithrespecttoanindividualverbsense.
Eachsenseofeachverbthushasaspecificsetofroles,whicharegivenonlynumbers
rather than names: Arg0, Arg1, Arg2, and so on. In general, Arg0 represents the
PROTO-AGENT, and Arg1, the PROTO-PATIENT. The semantics of the other roles
arelessconsistent,oftenbeingdefinedspecificallyforeachverb. Nonethelessthere
aresomegeneralization;theArg2isoftenthebenefactive,instrument,attribute,or
endstate,theArg3thestartpoint,benefactive,instrument,orattribute,andtheArg4
theendpoint.
Here are some slightly simplified PropBank entries for one sense each of the
verbs agree and fall. Such PropBank entries are called frame files; note that the
definitionsintheframefileforeachrole(“Otherentityagreeing”,“Extent,amount
fallen”)areinformalglossesintendedtobereadbyhumans,ratherthanbeingformal
definitions.
(24.11) agree.01
Arg0: Agreer
Arg1: Proposition
Arg2: Otherentityagreeing
Ex1: [ Thegroup]agreed[ itwouldn’tmakeanoffer].
Arg0 Arg1
Ex2: [ Usually][ John]agrees[ withMary]
ArgM-TMP Arg0 Arg2
[ oneverything].
Arg1
(24.12) fall.01
Arg1: Logicalsubject,patient,thingfalling
Arg2: Extent,amountfallen
Arg3: startpoint
Arg4: endpoint,endstateofarg1
Ex1: [ Sales]fell[ to$25million][ from$27million].
Arg1 Arg4 Arg3
Ex2: [ Theaveragejunkbond]fell[ by4.2%].
Arg1 Arg2
Note that there is no Arg0 role for fall, because the normal subject of fall is a
PROTO-PATIENT.24.5 • FRAMENET 481
The PropBank semantic roles can beuseful in recovering shallowsemantic in-
formationaboutverbalarguments. Considertheverbincrease:
(24.13) increase.01“goupincrementally”
Arg0: causerofincrease
Arg1: thingincreasing
Arg2: amountincreasedby,EXT,orMNR
Arg3: startpoint
Arg4: endpoint
APropBanksemanticrolelabelingwouldallowustoinferthecommonalityin
the event structures of the following three examples, that is, that in each case Big
FruitCo. istheAGENTandthepriceofbananasistheTHEME,despitethediffering
surfaceforms.
(24.14) [ BigFruitCo. ] increased[ thepriceofbananas].
Arg0 Arg1
(24.15) [ Thepriceofbananas]wasincreasedagain[ byBigFruitCo. ]
Arg1 Arg0
(24.16) [ Thepriceofbananas]increased[ 5%].
Arg1 Arg2
PropBankalsohasanumberofnon-numberedargumentscalledArgMs,(ArgM-
TMP, ArgM-LOC, etc.) which represent modification or adjunct meanings. These
are relatively stable across predicates, so aren’t listed with each frame file. Data
labeled with these modifiers can be helpful in training systems to detect temporal,
location,ordirectionalmodificationacrosspredicates. SomeoftheArgM’sinclude:
TMP when? yesterdayevening,now
LOC where? atthemuseum,inSanFrancisco
DIR whereto/from? down,toBangkok
MNR how? clearly,withmuchenthusiasm
PRP/CAU why? because... ,inresponsetotheruling
REC themselves,eachother
ADV miscellaneous
PRD secondarypredication ...atethemeatraw
NomBank WhilePropBankfocusesonverbs, arelatedproject, NomBank(Meyersetal.,
2004) adds annotations to noun predicates. For example the noun agreement in
Apple’sagreementwithIBMwouldbelabeledwithAppleastheArg0andIBMas
the Arg2. This allows semantic role labelers to assign labels to arguments of both
verbalandnominalpredicates.
24.5 FrameNet
While making inferences about the semantic commonalities across different sen-
tenceswithincreaseisuseful,itwouldbeevenmoreusefulifwecouldmakesuch
inferencesinmanymoresituations, acrossdifferentverbs, andalsobetweenverbs
andnouns. Forexample,we’dliketoextractthesimilarityamongthesethreesen-
tences:
(24.17) [ Thepriceofbananas]increased[ 5%].
Arg1 Arg2
(24.18) [ Thepriceofbananas]rose[ 5%].
Arg1 Arg2
(24.19) Therehasbeena [ 5%]rise[ inthepriceofbananas].
Arg2 Arg1
Notethatthesecondexampleusesthedifferentverbrise,andthethirdexample
uses the noun rather than the verb rise. We’d like a system to recognize that the482 CHAPTER24 • SEMANTICROLELABELING
priceofbananasiswhatwentup,andthat5%istheamountitwentup,nomatter
whetherthe5%appearsastheobjectoftheverbincreasedorasanominalmodifier
ofthenounrise.
FrameNet The FrameNet project is another semantic-role-labeling project that attempts
to address just these kinds of problems (Baker et al. 1998, Fillmore et al. 2003,
FillmoreandBaker2009,Ruppenhoferetal.2016). WhereasrolesinthePropBank
projectarespecifictoanindividualverb,rolesintheFrameNetprojectarespecific
toaframe.
Whatisaframe? Considerthefollowingsetofwords:
reservation,flight,travel,buy,price,cost,fare,rates,meal,plane
Therearemanyindividuallexicalrelationsofhyponymy,synonymy,andsoon
between many of the words in this list. The resulting set of relations does not,
however, add up to a complete account of how these words are related. They are
clearly all defined with respect to a coherent chunk of common-sense background
informationconcerningairtravel.
frame Wecalltheholisticbackgroundknowledgethatunitesthesewordsaframe(Fill-
more,1985). Theideathatgroupsofwordsaredefinedwithrespecttosomeback-
ground information is widespread in artificial intelligence and cognitive science,
model wherebesidesframeweseerelatedworkslikeamodel(Johnson-Laird,1983), or
script evenscript(SchankandAbelson,1977).
AframeinFrameNetisabackgroundknowledgestructurethatdefinesasetof
frameelements frame-specific semantic roles, called frame elements, and includes a set of predi-
catesthatusetheseroles. Eachwordevokesaframeandprofilessomeaspectofthe
frame and its elements. The FrameNet dataset includes a set of frames and frame
elements, the lexical units associated with each frame, and a set of labeled exam-
ple sentences. For example, the change position on a scale frame is defined as
follows:
ThisframeconsistsofwordsthatindicatethechangeofanItem’sposi-
tiononascale(theAttribute)fromastartingpoint(Initial value)toan
endpoint(Final value).
Some of the semantic roles (frame elements) in the frame are defined as in
coreroles Fig.24.3.Notethattheseareseparatedintocoreroles,whichareframespecific,and
non-coreroles non-coreroles,whicharemoreliketheArg-MargumentsinPropBank,expressing
moregeneralpropertiesoftime,location,andsoon.
Herearesomeexamplesentences:
(24.20) [ Oil]rose[ inprice][ by2%].
ITEM ATTRIBUTE DIFFERENCE
(24.21) [ It]hasincreased[ tohavingthem1dayamonth].
ITEM FINAL STATE
(24.22) [ Microsoftshares]fell[ to75/8].
ITEM FINAL VALUE
(24.23) [ Coloncancerincidence]fell[ by50%][ among
ITEM DIFFERENCE GROUP
men].
(24.24) asteadyincrease [ from9.5][ to14.3][
INITIAL VALUE FINAL VALUE ITEM
individends]
(24.25) a[ 5%][ dividend]increase...
DIFFERENCE ITEM
Notefromtheseexamplesentencesthattheframeincludestargetwordslikerise,
fall,andincrease. Infact,thecompleteframeconsistsofthefollowingwords:24.6 • SEMANTICROLELABELING 483
CoreRoles
ATTRIBUTE TheATTRIBUTEisascalarpropertythattheITEMpossesses.
DIFFERENCE ThedistancebywhichanITEMchangesitspositiononthescale.
FINAL STATE AdescriptionthatpresentstheITEM’sstateafterthechangeintheATTRIBUTE’s
valueasanindependentpredication.
FINAL VALUE ThepositiononthescalewheretheITEMendsup.
INITIAL STATE A description that presents the ITEM’s state before the change in the AT-
TRIBUTE’svalueasanindependentpredication.
INITIAL VALUE TheinitialpositiononthescalefromwhichtheITEMmovesaway.
ITEM Theentitythathasapositiononthescale.
VALUE RANGE A portion of the scale, typically identified by its end points, along which the
valuesoftheATTRIBUTEfluctuate.
SomeNon-CoreRoles
DURATION Thelengthoftimeoverwhichthechangetakesplace.
SPEED TherateofchangeoftheVALUE.
GROUP TheGROUPinwhichanITEMchangesthevalueofan
ATTRIBUTEinaspecifiedway.
Figure24.3 The frame elements in the change position on a scale frame from the FrameNet Labelers
Guide(Ruppenhoferetal.,2016).
VERBS: dwindle move soar escalation shift
advance edge mushroom swell explosion tumble
climb explode plummet swing fall
decline fall reach triple fluctuation ADVERBS:
decrease fluctuate rise tumble gain increasingly
diminish gain rocket growth
dip grow shift NOUNS: hike
double increase skyrocket decline increase
drop jump slide decrease rise
FrameNet also codes relationships between frames, allowing frames to inherit
fromeachother, orrepresentingrelationsbetweenframeslikecausation(andgen-
eralizationsamongframeelementsindifferentframescanberepresentedbyinheri-
tanceaswell). Thus,thereisaCause change of position on a scaleframethatis
linked to the Change of position on a scale frame by the cause relation, but that
addsanAGENTroleandisusedforcausativeexamplessuchasthefollowing:
(24.26) [ They]raised[ thepriceoftheirsoda][ by2%].
AGENT ITEM DIFFERENCE
Together,thesetwoframeswouldallowanunderstandingsystemtoextractthe
commoneventsemanticsofalltheverbalandnominalcausativeandnon-causative
usages.
FrameNetshavealsobeendevelopedformanyotherlanguagesincludingSpan-
ish,German,Japanese,Portuguese,Italian,andChinese.
24.6 Semantic Role Labeling
semanticrole Semanticrolelabeling(sometimesshortenedasSRL)isthetaskofautomatically
labeling
findingthesemanticrolesofeachargumentofeachpredicateinasentence. Cur-
rentapproachestosemanticrolelabelingarebasedonsupervisedmachinelearning,
oftenusingtheFrameNetandPropBankresourcestospecifywhatcountsasapred-
icate,definethesetofrolesusedinthetask,andprovidetrainingandtestsets.484 CHAPTER24 • SEMANTICROLELABELING
Recall that the difference between these two models of semantic roles is that
FrameNet(24.27)employsmanyframe-specificframeelementsasroles,whileProp-
Bank(24.28)usesasmallernumberofnumberedargumentlabelsthatcanbeinter-
preted as verb-specific labels, along with the more general ARGM labels. Some
examples:
[You] can’t [blame] [theprogram] [forbeingunabletoidentifyit]
(24.27)
COGNIZER TARGET EVALUEE REASON
[TheSanFranciscoExaminer] issued [aspecialedition] [yesterday]
(24.28)
ARG0 TARGET ARG1 ARGM-TMP
24.6.1 AFeature-basedAlgorithmforSemanticRoleLabeling
Asimplifiedfeature-basedsemanticrolelabelingalgorithmissketchedinFig.24.4.
Feature-basedalgorithms—fromtheveryearliestsystemslike(Simmons,1973)—
beginbyparsing,usingbroad-coverageparserstoassignaparsetotheinputstring.
Figure24.5showsaparseof(24.28)above. Theparseisthentraversedtofindall
wordsthatarepredicates.
For each of these predicates, the algorithm examines each node in the parse
tree and uses supervised classification to decide the semantic role (if any) it plays
for this predicate. Given a labeled training set such as PropBank or FrameNet, a
feature vector is extracted for each node, using feature templates described in the
next subsection. A 1-of-N classifier is then trained to predict a semantic role for
eachconstituentgiventhesefeatures, whereNisthenumberofpotentialsemantic
rolesplusanextraNONErolefornon-roleconstituents.Anystandardclassification
algorithmscanbeused. Finally,foreachtestsentencetobelabeled,theclassifieris
runoneachrelevantconstituent.
functionSEMANTICROLELABEL(words)returnslabeledtree
parse PARSE(words)
←
foreachpredicateinparsedo
foreachnodeinparsedo
featurevector EXTRACTFEATURES(node,predicate,parse)
←
CLASSIFYNODE(node,featurevector,parse)
Figure24.4 Agenericsemantic-role-labelingalgorithm.CLASSIFYNODEisa1-of-Nclas-
sifierthatassignsasemanticrole(orNONEfornon-roleconstituents),trainedonlabeleddata
suchasFrameNetorPropBank.
Insteadoftrainingasingle-stageclassifierasinFig.24.5,thenode-levelclassi-
ficationtaskcanbebrokendownintomultiplesteps:
1. Pruning: Since only a small number of the constituents in a sentence are
argumentsofanygivenpredicate,manysystemsusesimpleheuristicstoprune
unlikelyconstituents.
2. Identification: abinary classification ofeachnodeas anargumenttobe la-
beledoraNONE.
3. Classification:a1-of-Nclassificationofalltheconstituentsthatwerelabeled
asargumentsbythepreviousstage
Theseparationofidentificationandclassificationmayleadtobetteruseoffea-
tures (different features may be useful for the two tasks) or to computational effi-
ciency.24.6 • SEMANTICROLELABELING 485
S
NP-SBJ=ARG0 VP
DT NNP NNP NNP
The San Francisco Examiner
VBD=TARGET NP=ARG1 PP-TMP=ARGM-TMP
issued DT JJ NN IN NP
a special edition around NN NP-TMP
noon yesterday
Figure24.5 Parse tree for a PropBank sentence, showing the PropBank argument labels. The dotted line
showsthepathfeatureNP S VP VBDforARG0,theNP-SBJconstituentTheSanFranciscoExaminer.
↑ ↓ ↓
GlobalOptimization
The classification algorithm of Fig. 24.5 classifies each argument separately (‘lo-
cally’),makingthesimplifyingassumptionthateachargumentofapredicatecanbe
labeledindependently.Thisassumptionisfalse;thereareinteractionsbetweenargu-
mentsthatrequireamore‘global’assignmentoflabelstoconstituents.Forexample,
constituentsinFrameNetandPropBankarerequiredtobenon-overlapping. More
significantly, the semantic roles of constituents are not independent. For example
PropBankdoesnotallowmultipleidenticalarguments;twoconstituentsofthesame
verbcannotbothbelabeledARG0.
Rolelabelingsystemsthusoftenaddafourthsteptodealwithglobalconsistency
acrossthelabelsinasentence. Forexample,thelocalclassifierscanreturnalistof
possiblelabelsassociatedwithprobabilitiesforeachconstituent,andasecond-pass
Viterbi decoding or re-ranking approach can be used to choose the best consensus
label.Integerlinearprogramming(ILP)isanothercommonwaytochooseasolution
thatconformsbesttomultipleconstraints.
FeaturesforSemanticRoleLabeling
Most systems use some generalization of the core set of features introduced by
Gildea and Jurafsky (2000). Common basic features templates (demonstrated on
theNP-SBJconstituentTheSanFranciscoExaminerinFig.24.5)include:
• Thegoverningpredicate,inthiscasetheverbissued. Thepredicateisacru-
cialfeaturesincelabelsaredefinedonlywithrespecttoaparticularpredicate.
• The phrase type of the constituent, in this case, NP (or NP-SBJ). Some se-
manticrolestendtoappearasNPs,othersasSorPP,andsoon.
• Theheadwordoftheconstituent, Examiner. Theheadwordofaconstituent
canbecomputedwithstandardheadrules,suchasthosegiveninAppendixD
inFig.17.17. Certainheadwords(e.g.,pronouns)placestrongconstraintson
thepossiblesemanticrolestheyarelikelytofill.
• Theheadwordpartofspeechoftheconstituent,NNP.
• Thepathintheparsetreefromtheconstituenttothepredicate. Thispathis
markedbythedottedlineinFig.24.5. FollowingGildeaandJurafsky(2000),
wecanuseasimplelinearrepresentationofthepath,NP S VP VBD. and
↑ ↓ ↓ ↑
represent upward and downward movement in the tree, respectively. The
↓486 CHAPTER24 • SEMANTICROLELABELING
pathisveryusefulasacompactrepresentationofmanykindsofgrammatical
functionrelationshipsbetweentheconstituentandthepredicate.
• Thevoiceoftheclauseinwhichtheconstituentappears, inthiscase, active
(ascontrastedwithpassive). Passivesentencestendtohavestronglydifferent
linkingsofsemanticrolestosurfaceformthandoactiveones.
• The binary linear position of the constituent with respect to the predicate,
eitherbeforeorafter.
• The subcategorization of the predicate, the set of expected arguments that
appearintheverbphrase.Wecanextractthisinformationbyusingthephrase-
structurerulethatexpandstheimmediateparentofthepredicate;VP VBD
→
NPPPforthepredicateinFig.24.5.
• Thenamedentitytypeoftheconstituent.
• Thefirstwordsandthelastwordoftheconstituent.
ThefollowingfeaturevectorthusrepresentsthefirstNPinourexample(recall
thatmostobservationswillhavethevalue NONE ratherthan,forexample, ARG0,
sincemostconstituentsintheparsetreewillnotbearasemanticrole):
ARG0: [issued, NP,Examiner, NNP,NP S VP VBD,active, before, VP NPPP,
↑ ↓ ↓ →
ORG,The,Examiner]
Other features are often used in addition, such as sets of n-grams inside the
constituent,ormorecomplexversionsofthepathfeatures(theupwardordownward
halves,orwhetherparticularnodesoccurinthepath).
It’salsopossibletousedependencyparsesinsteadofconstituencyparsesasthe
basisoffeatures,forexampleusingdependencyparsepathsinsteadofconstituency
paths.
24.6.2 ANeuralAlgorithmforSemanticRoleLabeling
AsimpleneuralapproachtoSRListotreatitasasequencelabelingtasklikenamed-
entity recognition, using the BIO approach. Let’s assume that we are given the
predicate and the task is just detecting and labeling spans. Recall that with BIO
tagging, wehaveabeginandendtagforeachpossiblerole(B-ARG0, I-ARG0; B-
ARG1,I-ARG1,andsoon),plusanoutsidetagO.
B-ARG0 I-ARG0 B-PRED B-ARG1
Softmax
FFN FFN FFN FFN FFN
concatenate
with predicate
ENCODER
[CLS] the cats love hats [SEP] love [SEP]
Figure24.6 A simple neural approach to semantic role labeling. The input sentence is
followedby[SEP]andanextrainputforthepredicate,inthiscaselove.Theencoderoutputs
areconcatenatedtoanindicatorvariablewhichis1forthepredicateand0forallotherwords
AfterHeetal.(2017)andShiandLin(2019).24.7 • SELECTIONALRESTRICTIONS 487
As with all the taggers, the goal is to compute the highest probability tag se-
quenceyˆ,giventheinputsequenceofwordsw:
yˆ = argmaxP(yw)
|
y T
∈
Fig.24.6showsasketchofastandardalgorithmfromHeetal.(2017). Hereeach
inputwordismappedtopretrainedembeddings,andtheneachtokenisconcatenated
withthepredicateembeddingandthenpassedthroughafeedforwardnetworkwith
a softmax which outputs a distribution over each SRL label. For decoding, a CRF
layercanbeusedinsteadoftheMLPlayerontopofthebiLSTMoutputtodoglobal
inference,butinpracticethisdoesn’tseemtoprovidemuchbenefit.
24.6.3 EvaluationofSemanticRoleLabeling
Thestandardevaluationforsemanticrolelabelingistorequirethateachargument
labelmustbeassignedtotheexactlycorrectwordsequenceorparseconstituent,and
thencomputeprecision,recall,andF-measure. Identificationandclassificationcan
alsobeevaluatedseparately. TwocommondatasetsusedforevaluationareCoNLL-
2005(CarrerasandMa`rquez,2005)andCoNLL-2012(Pradhanetal.,2013).
24.7 Selectional Restrictions
We turn in this section to another way to represent facts about the relationship be-
selectional tweenpredicatesandarguments. Aselectionalrestrictionisasemantictypecon-
restriction
straintthataverbimposesonthekindofconceptsthatareallowedtofillitsargument
roles. Considerthetwomeaningsassociatedwiththefollowingexample:
(24.29) Iwanttoeatsomeplacenearby.
There are two possible parses and semantic interpretations for this sentence. In
the sensible interpretation, eat is intransitive and the phrase someplace nearby is
anadjunctthatgivesthelocationoftheeatingevent. Inthenonsensicalspeaker-as-
Godzillainterpretation,eatistransitiveandthephrasesomeplacenearbyisthedirect
object and the THEME of the eating, like the NP Malaysian food in the following
sentences:
(24.30) IwanttoeatMalaysianfood.
Howdoweknowthatsomeplacenearbyisn’tthedirectobjectinthissentence?
One useful cue is the semantic fact that the THEME of EATING events tends to be
something that is edible. This restriction placed by the verb eat on the filler of its
THEMEargumentisaselectionalrestriction.
Selectional restrictions are associated with senses, not entire lexemes. We can
seethisinthefollowingexamplesofthelexemeserve:
(24.31) Therestaurantservesgreen-lippedmussels.
(24.32) WhichairlinesserveDenver?
Example (24.31) illustrates the offering-food sense of serve, which ordinarily re-
strictsitsTHEMEtobesomekindoffoodExample(24.32)illustratestheprovidesa
commercialservicetosenseofserve,whichconstrainsits THEME tobesometype
ofappropriatelocation.488 CHAPTER24 • SEMANTICROLELABELING
Selectional restrictions vary widely in their specificity. The verb imagine, for
example, imposes strict requirements on its AGENT role (restricting it to humans
andotheranimateentities)butplacesveryfewsemanticrequirementsonitsTHEME
role. A verb like diagonalize, on the other hand, places a very specific constraint
on the filler of its THEME role: it has to be a matrix, while the arguments of the
adjectiveodorlessarerestrictedtoconceptsthatcouldpossessanodor:
(24.33) Inrehearsal,Ioftenaskthemusicianstoimagineatennisgame.
(24.34) Radonisanodorlessgasthatcan’tbedetectedbyhumansenses.
(24.35) Todiagonalizeamatrixistofinditseigenvalues.
Theseexamplesillustratethatthesetofconceptsweneedtorepresentselectional
restrictions(beingamatrix,beingabletopossessanodor,etc)isquiteopenended.
Thisdistinguishesselectionalrestrictionsfromotherfeaturesforrepresentinglexical
knowledge,likeparts-of-speech,whicharequitelimitedinnumber.
24.7.1 RepresentingSelectionalRestrictions
Onewaytocapturethesemanticsofselectionalrestrictionsistouseandextendthe
eventrepresentationofChapter19. Recallthattheneo-Davidsonianrepresentation
ofaneventconsistsofasinglevariablethatstandsfortheevent,apredicatedenoting
thekindofevent,andvariablesandrelationsfortheeventroles.Ignoringtheissueof
theλ-structuresandusingthematicrolesratherthandeepeventroles,thesemantic
contributionofaverblikeeatmightlooklikethefollowing:
e,x,yEating(e) Agent(e,x) Theme(e,y)
∃ ∧ ∧
With this representation, all we know about y, the filler of the THEME role, is that
it is associated with an Eating event through the Theme relation. To stipulate the
selectionalrestrictionthatymustbesomethingedible,wesimplyaddanewtermto
thateffect:
e,x,yEating(e) Agent(e,x) Theme(e,y) EdibleThing(y)
∃ ∧ ∧ ∧
Whenaphraselikeateahamburgerisencountered, asemanticanalyzercanform
thefollowingkindofrepresentation:
e,x,yEating(e) Eater(e,x) Theme(e,y) EdibleThing(y) Hamburger(y)
∃ ∧ ∧ ∧ ∧
Thisrepresentationisperfectlyreasonablesincethemembershipofyinthecategory
HamburgerisconsistentwithitsmembershipinthecategoryEdibleThing,assuming
areasonablesetoffactsintheknowledgebase. Correspondingly,therepresentation
for a phrase such as ate a takeoff would be ill-formed because membership in an
event-like category such as Takeoff would be inconsistent with membership in the
categoryEdibleThing.
Whilethisapproachadequatelycapturesthesemanticsofselectionalrestrictions,
there are two problems with its direct use. First, using FOL to perform the simple
taskofenforcingselectionalrestrictionsisoverkill. Other, farsimpler, formalisms
can do the job with far less computational cost. The second problem is that this
approach presupposes a large, logical knowledge base of facts about the concepts
that make up selectional restrictions. Unfortunately, although such common-sense
knowledge bases are being developed, none currently have the kind of coverage
necessarytothetask.24.7 • SELECTIONALRESTRICTIONS 489
Sense 1
hamburger, beefburger --
(a fried cake of minced beef served on a bun)
=> sandwich
=> snack food
=> dish
=> nutriment, nourishment, nutrition...
=> food, nutrient
=> substance
=> matter
=> physical entity
=> entity
Figure24.7 EvidencefromWordNetthathamburgersareedible.
AmorepracticalapproachistostateselectionalrestrictionsintermsofWordNet
synsetsratherthanaslogicalconcepts. EachpredicatesimplyspecifiesaWordNet
synsetastheselectionalrestrictiononeachofitsarguments. Ameaningrepresenta-
tioniswell-formediftherolefillerwordisahyponym(subordinate)ofthissynset.
For our ate a hamburger example, for instance, we could set the selectional
restrictionontheTHEMEroleoftheverbeattothesynset food,nutrient ,glossed
{ }
as any substance that can be metabolized by an animal to give energy and build
tissue. Luckily, the chain of hypernyms for hamburger shown in Fig. 24.7 reveals
that hamburgers are indeed food. Again, the filler of a role need not match the
restrictionsynsetexactly;itjustneedstohavethesynsetasoneofitssuperordinates.
WecanapplythisapproachtotheTHEMErolesoftheverbsimagine,lift,anddi-
agonalize,discussedearlier. Letusrestrictimagine’sTHEMEtothesynset entity ,
{ }
lift’s THEME to physical entity , and diagonalize to matrix . This arrangement
{ } { }
correctly permits imagine a hamburger and lift a hamburger, while also correctly
rulingoutdiagonalizeahamburger.
24.7.2 SelectionalPreferences
In the earliest implementations, selectional restrictions were considered strict con-
straints on the kind of arguments a predicate could take (Katz and Fodor 1963,
Hirst 1987). For example, the verb eat might require that its THEME argument be
[+FOOD].Earlywordsensedisambiguationsystemsusedthisideatoruleoutsenses
thatviolatedtheselectionalrestrictionsoftheirgoverningpredicates.
Very quickly, however, it became clear that these selectional restrictions were
betterrepresentedaspreferencesratherthanstrictconstraints(Wilks1975c, Wilks
1975b). For example, selectional restriction violations (like inedible arguments of
eat) often occur in well-formed sentences, for example because they are negated
(24.36),orbecauseselectionalrestrictionsareoverstated(24.37):
(24.36) Butitfellapartin1931,perhapsbecausepeoplerealizedyoucan’teat
goldforlunchifyou’rehungry.
(24.37) Inhistwochampionshiptrials,Mr. Kulkarniateglassonanempty
stomach,accompaniedonlybywaterandtea.
Modern systems for selectional preferences therefore specify the relation be-
tweenapredicateanditspossibleargumentswithsoftconstraintsofsomekind.490 CHAPTER24 • SEMANTICROLELABELING
SelectionalAssociation
One of the most influential has been the selectional association model of Resnik
selectional
preference (1993). Resnik defines the idea of selectional preference strength as the general
strength
amountofinformationthatapredicatetellsusaboutthesemanticclassofitsargu-
ments. Forexample,theverbeattellsusalotaboutthesemanticclassofitsdirect
objects, since they tend to be edible. The verb be, by contrast, tells us less about
its direct objects. The selectional preference strength can be defined by the differ-
enceininformationbetweentwodistributions:thedistributionofexpectedsemantic
classes P(c) (how likely is it that a direct object will fall into class c) and the dis-
tribution of expected semantic classes for the particular verb P(cv) (how likely is
|
it that the direct object of the specific verb v will fall into semantic class c). The
greater the difference between these distributions, the more information the verb
isgivingusaboutpossibleobjects. Thedifferencebetweenthesetwodistributions
relativeentropy canbequantifiedbyrelativeentropy,ortheKullback-Leiblerdivergence(Kullback
KLdivergence andLeibler,1951). TheKullback-LeiblerorKLdivergenceD(P Q)expressesthe
||
differencebetweentwoprobabilitydistributionsPandQ
P(x)
D(P Q) = P(x)log (24.38)
|| Q(x)
x
(cid:88)
The selectional preference S (v) uses the KL divergence to express how much in-
R
formation,inbits,theverbvexpressesaboutthepossiblesemanticclassofitsargu-
ment.
S (v) = D(P(cv) P(c))
R
| ||
P(cv)
= P(cv)log | (24.39)
| P(c)
c
(cid:88)
selectional Resnikthendefinestheselectionalassociationofaparticularclassandverbasthe
association
relativecontributionofthatclasstothegeneralselectionalpreferenceoftheverb:
1 P(cv)
A R(v,c) = P(cv)log | (24.40)
S (v) | P(c)
R
The selectional association is thus a probabilistic measure of the strength of asso-
ciation between a predicate and a class dominating the argument to the predicate.
Resnikestimatestheprobabilitiesfortheseassociationsbyparsingacorpus,count-
ing all the times each predicate occurs with each argument word, and assuming
that each word is a partial observation of all the WordNet concepts containing the
word. The following table from Resnik (1996) shows some sample high and low
selectionalassociationsforverbsandsomeWordNetsemanticclassesoftheirdirect
objects.
DirectObject DirectObject
Verb SemanticClass Assoc SemanticClass Assoc
read WRITING 6.80 ACTIVITY -.20
write WRITING 7.26 COMMERCE 0
see ENTITY 5.79 METHOD -0.01
SelectionalPreferenceviaConditionalProbability
AnalternativetousingselectionalassociationbetweenaverbandtheWordNetclass
of its arguments is to use the conditional probability of an argument word given a24.8 • PRIMITIVEDECOMPOSITIONOFPREDICATES 491
predicateverb,directlymodelingthestrengthofassociationofoneverb(predicate)
withonenoun(argument).
Theconditionalprobabilitymodelcanbecomputedbyparsingaverylargecor-
pus (billions of words), and computing co-occurrence counts: how often a given
verboccurswithagivennouninagivenrelation. Theconditionalprobabilityofan
argumentnoungivenaverbforaparticularrelationP(nv,r)canthenbeusedasa
|
selectional preference metric for that pair of words (Brockmann and Lapata 2003,
KellerandLapata2003):
C(n,v,r) ifC(n,v,r)>0
P(nv,r)= C(v,r)
| (cid:40) 0 otherwise
TheinverseprobabilityP(vn,r)wasfoundtohavebetterperformanceinsomecases
|
(BrockmannandLapata,2003):
C(n,v,r) ifC(n,v,r)>0
P(vn,r)= C(n,r)
| (cid:40) 0 otherwise
An even simpler approach is to use the simple log co-occurrence frequency of
thepredicatewiththeargumentlogcount(v,n,r)insteadofconditionalprobability;
this seems to do better for extracting preferences for syntactic subjects rather than
objects(BrockmannandLapata,2003).
EvaluatingSelectionalPreferences
pseudowords Onewaytoevaluatemodelsofselectionalpreferencesistousepseudowords(Gale
etal.1992c,Schu¨tze1992a). Apseudowordisanartificialwordcreatedbyconcate-
natingatestwordinsomecontext(saybanana)withaconfounderword(saydoor)
tocreatebanana-door). Thetaskofthesystemistoidentifywhichofthetwowords
istheoriginalword. Toevaluateaselectionalpreferencemodel(forexampleonthe
relationshipbetweenaverbandadirectobject)wetakeatestcorpusandselectall
verbtokens. Foreachverbtoken(saydrive)weselectthedirectobject(e.g., car),
concatenatedwithaconfounderwordthatisitsnearestneighbor,thenounwiththe
frequencyclosesttotheoriginal(sayhouse), tomakecar/house). Wethenusethe
selectionalpreferencemodeltochoosewhichofcarandhousearemorepreferred
objectsofdrive,andcomputehowoftenthemodelchoosesthecorrectoriginalob-
ject(e.g.,car)(ChambersandJurafsky,2010).
Another evaluation metric is to get human preferences for a test set of verb-
argumentpairs,andhavethemratetheirdegreeofplausibility. Thisisusuallydone
byusingmagnitudeestimation,atechniquefrompsychophysics,inwhichsubjects
rate the plausibility of an argument proportional to a modulus item. A selectional
preference model can then be evaluated by its correlation with the human prefer-
ences(KellerandLapata,2003).
24.8 Primitive Decomposition of Predicates
Onewayofthinkingaboutthesemanticroleswehavediscussedthroughthechapter
is that they help us define the roles that arguments play in a decompositional way,
basedonfinitelistsofthematicroles(agent,patient,instrument,proto-agent,proto-
patient, etc.). This idea of decomposing meaning into sets of primitive semantic492 CHAPTER24 • SEMANTICROLELABELING
componential elements or features, called primitive decomposition or componential analysis,
analysis
hasbeentakenevenfurther,andfocusedparticularlyonpredicates.
Considertheseexamplesoftheverbkill:
(24.41) Jimkilledhisphilodendron.
(24.42) Jimdidsomethingtocausehisphilodendrontobecomenotalive.
Thereisatruth-conditional(‘propositionalsemantics’)perspectivefromwhichthese
twosentenceshavethesamemeaning. Assumingthisequivalence,wecouldrepre-
sentthemeaningofkillas:
(24.43) KILL(x,y) CAUSE(x,BECOME(NOT(ALIVE(y))))
⇔
thususingsemanticprimitiveslikedo,cause,becomenot,andalive.
Indeed, one such set of potential semantic primitives has been used to account
forsomeoftheverbalalternationsdiscussedinSection24.2(Lakoff1965, Dowty
1979). Considerthefollowingexamples.
(24.44) Johnopenedthedoor. CAUSE(John,BECOME(OPEN(door)))
⇒
(24.45) Thedooropened. BECOME(OPEN(door))
⇒
(24.46) Thedoorisopen. OPEN(door)
⇒
The decompositional approach asserts that a single state-like predicate associ-
atedwithopenunderliesalloftheseexamples.Thedifferencesamongthemeanings
oftheseexamplesarisesfromthecombinationofthissinglepredicatewiththeprim-
itivesCAUSEandBECOME.
While this approach to primitive decomposition can explain the similarity be-
tweenstatesandactionsorcausativeandnon-causativepredicates, itstillrelieson
havingalargenumberofpredicateslikeopen. Moreradicalapproacheschooseto
breakdownthesepredicatesaswell. Onesuchapproachtoverbalpredicatedecom-
positionthatplayedaroleinearlynaturallanguagesystemsisconceptualdepen-
conceptual dency(CD),asetoftenprimitivepredicates,showninFig.24.8.
dependency
Primitive Definition
ATRANS Theabstracttransferofpossessionorcontrolfromoneentityto
another
PTRANS Thephysicaltransferofanobjectfromonelocationtoanother
MTRANS The transfer of mental concepts between entities or within an
entity
MBUILD Thecreationofnewinformationwithinanentity
PROPEL Theapplicationofphysicalforcetomoveanobject
MOVE Theintegralmovementofabodypartbyananimal
INGEST Thetakinginofasubstancebyananimal
EXPEL Theexpulsionofsomethingfromananimal
SPEAK Theactionofproducingasound
ATTEND Theactionoffocusingasenseorgan
Figure24.8 Asetofconceptualdependencyprimitives.
BelowisanexamplesentencealongwithitsCDrepresentation.Theverbbrought
istranslatedintothetwoprimitivesATRANSandPTRANStoindicatethatthewaiter
both physically conveyed the check to Mary and passed control of it to her. Note
thatCDalsoassociatesafixedsetofthematicroleswitheachprimitivetorepresent
thevariousparticipantsintheaction.
(24.47) ThewaiterbroughtMarythecheck.24.9 • SUMMARY 493
x,yAtrans(x) Actor(x,Waiter) Object(x,Check) To(x,Mary)
∃ ∧ ∧ ∧
Ptrans(y) Actor(y,Waiter) Object(y,Check) To(y,Mary)
∧ ∧ ∧ ∧
24.9 Summary
• Semanticrolesareabstractmodelsoftheroleanargumentplaysintheevent
describedbythepredicate.
• Thematicrolesareamodelofsemanticrolesbasedonasinglefinitelistof
roles. Other semantic role models include per-verb semantic role lists and
proto-agent/proto-patient, both of which are implemented in PropBank,
andper-framerolelists,implementedinFrameNet.
• Semantic role labeling is the task of assigning semantic role labels to the
constituents of a sentence. The task is generally treated as a supervised ma-
chine learning task, with models trained on PropBank or FrameNet. Algo-
rithms generally start by parsing a sentence and then automatically tag each
parsetreenodewithasemanticrole. Neuralmodelsmapstraightfromwords
end-to-end.
• Semanticselectionalrestrictionsallowwords(particularlypredicates)topost
constraints on the semantic properties of their argument words. Selectional
preferencemodels(likeselectionalassociationorsimpleconditionalproba-
bility)allowaweightorprobabilitytobeassignedtotheassociationbetween
apredicateandanargumentwordorclass.
Bibliographical and Historical Notes
Although the idea of semantic roles dates back to Pa¯n.ini, they were re-introduced
intomodernlinguisticsbyGruber(1965),Fillmore(1966)andFillmore(1968).Fill-
more had become interested in argument structure by studying Lucien Tesnie`re’s
groundbreakingE´le´mentsdeSyntaxeStructurale(Tesnie`re,1959)inwhichtheterm
‘dependency’ was introduced and the foundations were laid for dependency gram-
mar. FollowingTesnie`re’sterminology,Fillmorefirstreferredtoargumentrolesas
actants(Fillmore,1966)butquicklyswitchedtothetermcase,(seeFillmore(2003))
andproposedauniversallistofsemanticrolesorcases(Agent,Patient,Instrument,
etc.),thatcouldbetakenonbytheargumentsofpredicates.Verbswouldbelistedin
thelexiconwiththeircaseframe,thelistofobligatory(oroptional)casearguments.
The idea that semantic roles could provide an intermediate level of semantic
representation that could help map from syntactic parse structures to deeper, more
fully-specifiedrepresentationsofmeaningwasquicklyadoptedinnaturallanguage
processing,andsystemsforextractingcaseframeswerecreatedformachinetransla-
tion(Wilks,1973),question-answering(Hendrixetal.,1973),spoken-languagepro-
cessing(Nash-Webber,1975),anddialoguesystems(Bobrowetal.,1977).General-
purposesemanticrolelabelersweredeveloped. Theearliestones(Simmons,1973)
firstparsedasentencebymeansofanATN(AugmentedTransitionNetwork)parser.494 CHAPTER24 • SEMANTICROLELABELING
Eachverbthenhadasetofrulesspecifyinghowtheparseshouldbemappedtose-
manticroles. Theserulesmainlymadereferencetogrammaticalfunctions(subject,
object, complement of specific prepositions) but also checked constituent internal
featuressuchastheanimacyofheadnouns. Latersystemsassignedrolesfrompre-
built parse trees, again by using dictionaries with verb-specific case frames (Levin
1977,Marcus1980).
By1977caserepresentationwaswidelyusedandtaughtinAIandNLPcourses,
andwasdescribedasastandardofnaturallanguageprocessinginthefirsteditionof
Winston’s1977textbookArtificialIntelligence.
In the 1980s Fillmore proposed his model of frame semantics, later describing
theintuitionasfollows:
“The idea behind frame semantics is that speakers are aware of possi-
blyquitecomplexsituationtypes,packagesofconnectedexpectations,
thatgobyvariousnames—frames,schemas,scenarios,scripts,cultural
narratives,memes—andthewordsinourlanguageareunderstoodwith
suchframesastheirpresupposedbackground.”(Fillmore,2012,p.712)
The word frame seemed to be in the air for a suite of related notions proposed at
about the same time by Minsky (1974), Hymes (1974), and Goffman (1974), as
well as related notions with other names like scripts (Schank and Abelson, 1975)
and schemata (Bobrow and Norman, 1975) (see Tannen (1979) for a comparison).
FillmorewasalsoinfluencedbythesemanticfieldtheoristsandbyavisittotheYale
AIlabwherehetooknoticeofthelistsofslotsandfillersusedbyearlyinformation
extractionsystemslikeDeJong(1982)andSchankandAbelson(1977).Inthe1990s
FillmoredrewontheseinsightstobegintheFrameNetcorpusannotationproject.
Atthesametime,BethLevindrewonherearlycaseframedictionaries(Levin,
1977)todevelopherbookwhichsummarizedsetsofverbclassesdefinedbyshared
argumentrealizations(Levin,1993).TheVerbNetprojectbuiltonthiswork(Kipper
etal.,2000),leadingsoonafterwardstothePropBanksemantic-role-labeledcorpus
createdbyMarthaPalmerandcolleagues(Palmeretal.,2005).
The combination of rich linguistic annotation and corpus-based approach in-
stantiated in FrameNet and PropBank led to a revival of automatic approaches to
semantic role labeling, first on FrameNet (Gildea and Jurafsky, 2000) and then on
PropBankdata(GildeaandPalmer,2002,interalia). Theproblemfirstaddressedin
the1970sbyhandwrittenruleswasthusnowgenerallyrecastasoneofsupervised
machinelearningenabledbylargeandconsistentdatabases. Manypopularfeatures
used for role labeling are defined in Gildea and Jurafsky (2002), Surdeanu et al.
(2003),XueandPalmer(2004),Pradhanetal.(2005),Cheetal.(2009),andZhao
etal.(2009). Theuseofdependencyratherthanconstituencyparseswasintroduced
in the CoNLL-2008 shared task (Surdeanu et al., 2008). For surveys see Palmer
etal.(2010)andMa`rquezetal.(2008).
The use of neural approaches to semantic role labeling was pioneered by Col-
lobert et al. (2011), who applied a CRF on top of a convolutional net. Early work
likeFoland,Jr.andMartin(2015)focusedonusingdependencyfeatures.Laterwork
eschewedsyntacticfeaturesaltogether;ZhouandXu(2015b)introducedtheuseof
a stacked (6-8 layer) biLSTM architecture, and (He et al., 2017) showed how to
augmentthebiLSTMarchitecturewithhighwaynetworksandalsoreplacetheCRF
withA*decodingthatmakeitpossibletoapplyawidevarietyofglobalconstraints
inSRLdecoding.
Most semantic role labeling schemes only work within a single sentence, fo-
cusingontheobjectoftheverbal(ornominal,inthecaseofNomBank)predicate.EXERCISES 495
However,inmanycases,averbalornominalpredicatemayhaveanimplicitargu-
implicit ment: onethatappearsonlyinacontextualsentence,orperhapsnotatallandmust
argument
beinferred.InthetwosentencesThishousehasanewowner.Thesalewasfinalized
10daysago. thesaleinthesecondsentencehasnoARG1,butareasonablereader
wouldinferthattheArg1shouldbethehousementionedinthepriorsentence.Find-
iSRL ingthesearguments,implicitargumentdetection(sometimesshortenedasiSRL)
wasintroducedbyGerberandChai(2010)andRuppenhoferetal.(2010). SeeDo
etal.(2017)formorerecentneuralmodels.
To avoid the need for huge labeled training sets, unsupervised approaches for
semanticrolelabelingattempttoinducethesetofsemanticrolesbyclusteringover
arguments. ThetaskwaspioneeredbyRiloffandSchmelzenbach(1998)andSwier
and Stevenson (2004); see Grenager and Manning (2006), Titov and Klementiev
(2012),LangandLapata(2014),WoodsendandLapata(2015),andTitovandKhod-
dam(2014).
Recentinnovationsinframelabelingincludeconnotationframes,whichmark
richerinformationabouttheargumentofpredicates. Connotationframesmarkthe
sentimentofthewriterorreadertowardthearguments(forexampleusingtheverb
surviveinhesurvivedabombingexpressesthewriter’ssympathytowardthesubject
heandnegativesentimenttowardthebombing. SeeChapter25formoredetails.
Selectional preference has been widely studied beyond the selectional associa-
tionmodelsofResnik(1993)andResnik(1996). Methodshaveincludedclustering
(Roothetal.,1999),discriminativelearning(Bergsmaetal.,2008a),andtopicmod-
els(Se´aghdha2010,Ritteretal.2010b),andconstraintscanbeexpressedatthelevel
ofwordsorclasses(AgirreandMartinez,2001). Selectionalpreferenceshavealso
been successfully integrated into semantic role labeling (Erk 2007, Zapirain et al.
2013,Doetal.2017).
Exercises496 CHAPTER25 • LEXICONSFORSENTIMENT,AFFECT,ANDCONNOTATION
CHAPTER
25 Lexicons for Sentiment, Affect,
and Connotation
Somedaywe’llbeabletomeasurethepowerofwords
MayaAngelou
affective Inthischapterweturntotoolsforinterpretingaffectivemeaning,extendingour
studyofsentimentanalysisinChapter4. Weusetheword‘affective’,followingthe
tradition in affective computing (Picard, 1995) to mean emotion, sentiment, per-
subjectivity sonality, mood, andattitudes. Affectivemeaningiscloselyrelatedtosubjectivity,
thestudyofaspeakerorwriter’sevaluations,opinions,emotions,andspeculations
(Wiebeetal.,1999).
How should affective meaning be defined? One influential typology of affec-
tivestatescomesfromScherer(2000),whodefineseachclassofaffectivestatesby
factorslikeitscognitiverealizationandtimecourse(Fig.25.1).
Emotion: Relativelybriefepisodeofresponsetotheevaluationofanexternal
orinternaleventasbeingofmajorsignificance.
(angry,sad,joyful,fearful,ashamed,proud,elated,desperate)
Mood: Diffuseaffectstate,mostpronouncedaschangeinsubjectivefeeling,of
lowintensitybutrelativelylongduration,oftenwithoutapparentcause.
(cheerful,gloomy,irritable,listless,depressed,buoyant)
Interpersonalstance: Affective stance taken toward another person in a spe-
cificinteraction,coloringtheinterpersonalexchangeinthatsituation.
(distant,cold,warm,supportive,contemptuous,friendly)
Attitude: Relativelyenduring,affectivelycoloredbeliefs,preferences,andpre-
dispositionstowardsobjectsorpersons.
(liking,loving,hating,valuing,desiring)
Personalitytraits: Emotionally laden, stable personality dispositions and be-
haviortendencies,typicalforaperson.
(nervous,anxious,reckless,morose,hostile,jealous)
Figure25.1 TheScherertypologyofaffectivestates(Scherer,2000).
We can design extractors for each of these kinds of affective states. Chapter 4
alreadyintroducedsentimentanalysis,thetaskofextractingthepositiveornegative
orientationthatawriterexpressesinatext. ThiscorrespondsinScherer’stypology
totheextractionofattitudes: figuringoutwhatpeoplelikeordislike,fromaffect-
richtextslikeconsumerreviewsofbooksormovies,newspapereditorials,orpublic
sentimentinblogsortweets.
Detectingemotionandmoodsisusefulfordetectingwhetherastudentiscon-
fused,engaged,orcertainwheninteractingwithatutorialsystem,whetheracaller
toahelplineisfrustrated,whethersomeone’sblogpostsortweetsindicateddepres-
sion. Detectingemotionslikefearinnovels,forexample,couldhelpustracewhat
groupsorsituationsarefearedandhowthatchangesovertime.25.1 • DEFININGEMOTION 497
Detectingdifferentinterpersonalstancescanbeusefulwhenextractinginfor-
mation from human-human conversations. The goal here is to detect stances like
friendlinessorawkwardnessininterviewsorfriendlyconversations,forexamplefor
summarizingmeetingsorfindingpartsofaconversationwherepeopleareespecially
excited or engaged, conversational hot spots that can help in meeting summariza-
tion. Detectingthepersonalityofauser—suchaswhethertheuserisanextrovert
or the extent to which they are open to experience— can help improve conversa-
tional agents, which seem to work better if they match users’ personality expecta-
tions (Mairesse and Walker, 2008). And affect is important for generation as well
asrecognition;synthesizingaffectisimportantforconversationalagentsinvarious
domains,includingliteracytutorssuchaschildren’sstorybooks,orcomputergames.
In Chapter 4 we introduced the use of naive Bayes classification to classify a
document’ssentiment.Variousclassifiershavebeensuccessfullyappliedtomanyof
thesetasks,usingallthewordsinthetrainingsetasinputtoaclassifierwhichthen
determinestheaffectstatusofthetext.
Inthischapterwefocusonanalternativemodel,inwhichinsteadofusingevery
wordasafeature,wefocusonlyoncertainwords,onesthatcarryparticularlystrong
cuestoaffectorsentiment. Wecalltheselistsofwordsaffectivelexiconsorsenti-
mentlexicons. Theselexiconspresupposeafactaboutsemantics: thatwordshave
connotations affectivemeaningsorconnotations. Thewordconnotationhasdifferentmeanings
indifferentfields, buthereweuseittomeantheaspectsofaword’smeaningthat
arerelatedtoawriterorreader’semotions, sentiment, opinions, orevaluations. In
additiontotheirabilitytohelpdeterminetheaffectivestatusofatext, connotation
lexiconscanbeusefulfeaturesforotherkindsofaffectivetasks, andforcomputa-
tionalsocialscienceanalysis.
Inthenextsectionsweintroducebasictheoriesofemotion,showhowsentiment
lexiconsareaspecialcaseofemotionlexicons,andmentionsomeusefullexicons.
Wethensurveythreewaysforbuildinglexicons: humanlabeling,semi-supervised,
andsupervised.Finally,wetalkabouthowtodetectaffecttowardaparticularentity,
andintroduceconnotationframes.
25.1 Defining Emotion
emotion Oneofthemostimportantaffectiveclassesisemotion,whichScherer(2000)defines
asa“relativelybriefepisodeofresponsetotheevaluationofanexternalorinternal
eventasbeingofmajorsignificance”.
Detectingemotionhasthepotentialtoimproveanumberoflanguageprocessing
tasks. Emotionrecognitioncouldhelpdialoguesystemsliketutoringsystemsdetect
that a student was unhappy, bored, hesitant, confident, and so on. Automatically
detecting emotions in reviews or customer responses (anger, dissatisfaction, trust)
couldhelpbusinessesrecognizespecificproblemareasoronesthataregoingwell.
EmotioncanplayaroleinmedicalNLPtaskslikehelpingdiagnosedepressionor
suicidal intent. Detecting emotions expressed toward characters in novels might
playaroleinunderstandinghowdifferentsocialgroupswereviewedbysocietyat
differenttimes.
ComputationalmodelsofemotioninNLPhavemainlybeenbasedontwofami-
liesoftheoriesofemotion(outofthemanystudiedinthefieldofaffectivescience).
Inoneofthesefamilies,emotionsareviewedasfixedatomicunits,limitedinnum-
basicemotions ber, and from which others are generated, often called basic emotions (Tomkins498 CHAPTER25 • LEXICONSFORSENTIMENT,AFFECT,ANDCONNOTATION
1962,Plutchik1962),amodeldatingbacktoDarwin. Perhapsthemostwell-known
ofthisfamilyoftheoriesarethe6emotionsproposedbyEkman(e.g.,Ekman1999)
to be universally present in all cultures: surprise, happiness, anger, fear, disgust,
sadness. AnotheratomictheoryisthePlutchik(1980)wheelofemotion,consisting
of8basicemotionsinfouropposingpairs: joy–sadness,anger–fear,trust–disgust,
andanticipation–surprise,togetherwiththeemotionsderivedfromthem,shownin
Fig.25.2.
Figure25.2 Plutchikwheelofemotion.
The second class of emotion theories widely used in NLP views emotion as a
spacein2or3dimensions(Russell,1980).Mostmodelsincludethetwodimensions
valenceandarousal,andmanyaddathird,dominance. Thesecanbedefinedas:
valence: thepleasantnessofthestimulus
arousal: thelevelofalertness,activeness,orenergyprovokedbythestimulus
dominance: the degree of control or dominance exerted by the stimulus or the
emotion
Sentimentcanbeviewedasaspecialcaseofthissecondviewofemotionsaspoints
inspace.Inparticular,thevalencedimension,measuringhowpleasantorunpleasant
awordis,isoftenuseddirectlyasameasureofsentiment.
Intheselexicon-basedmodelsofaffect,theaffectivemeaningofawordisgen-
erally fixed, irrespective of the linguistic context in which a word is used, or the
dialectorcultureofthespeaker.Bycontrast,othermodelsinaffectivesciencerepre-
sentemotionsasmuchricherprocessesinvolvingcognition(Barrettetal.,2007). In
appraisaltheory,forexample,emotionsarecomplexprocesses,inwhichaperson
considershowaneventiscongruentwiththeirgoals,takingintoaccountvariables
like the agency, certainty, urgency, novelty and control associated with the event
(Moorsetal.,2013). ComputationalmodelsinNLPtakingintoaccountthesericher
theoriesofemotionwilllikelyplayanimportantroleinfuturework.25.2 • AVAILABLESENTIMENTANDAFFECTLEXICONS 499
25.2 Available Sentiment and Affect Lexicons
A wide variety of affect lexicons have been created and released. The most basic
lexicons label words along one dimension of semantic variability, generally called
“sentiment”or“valence”.
Inthesimplestlexiconsthisdimensionisrepresentedinabinaryfashion, with
a wordlist for positive words and a wordlist for negative words. The oldest is the
General GeneralInquirer(Stoneetal.,1966),whichdrewoncontentanalysisandonearly
Inquirer
workinthecognitivepsychologyofwordmeaning(Osgoodetal.,1957). TheGen-
eral Inquirer has a lexicon of 1915 positive words and a lexicon of 2291 negative
words(aswellasotherlexiconsdiscussedbelow). TheMPQASubjectivitylexicon
(Wilson et al., 2005) has 2718 positive and 4912 negative words drawn from prior
lexiconsplusabootstrappedlistofsubjectivewordsandphrases(RiloffandWiebe,
2003). Eachentryinthelexiconishand-labeledforsentimentandalsolabeledfor
reliability (strongly subjective or weakly subjective). The polarity lexicon of Hu
andLiu(2004b)gives2006positiveand4783negativewords,drawnfromproduct
reviews,labeledusingabootstrappingmethodfromWordNet.
Positive admire, amazing, assure, celebration, charm, eager, enthusiastic, excellent, fancy, fan-
tastic, frolic, graceful, happy, joy, luck, majesty, mercy, nice, patience, perfect, proud,
rejoice,relief,respect,satisfactorily,sensational,super,terrific,thank,vivid,wise,won-
derful,zest
Negative abominable, anger, anxious, bad, catastrophe, cheap, complaint, condescending, deceit,
defective,disappointment,embarrass,fake,fear,filthy,fool,guilt,hate,idiot,inflict,lazy,
miserable,mourn,nervous,objection,pest,plot,reject,scream,silly,terrible,unfriendly,
vile,wicked
Figure25.3 Some words with consistent sentiment across the General Inquirer (Stone et al., 1966), the
MPQASubjectivitylexicon(Wilsonetal.,2005),andthepolaritylexiconofHuandLiu(2004b).
Slightlymoregeneralthanthesesentimentlexiconsarelexiconsthatassigneach
word a value on all three affective dimensions. The NRC Valence, Arousal, and
Dominance(VAD)lexicon(Mohammad,2018a)assignsvalence,arousal,anddom-
inancescoresto20,000words. SomeexamplesareshowninFig.25.4.
Valence Arousal Dominance
vacation .840 enraged .962 powerful .991
delightful .918 party .840 authority .935
whistle .653 organized .337 saxophone .482
consolation .408 effortless .120 discouraged .0090
torture .115 napping .046 weak .045
Figure25.4 ValuesofsamplewordsontheemotionaldimensionsofMohammad(2018a).
EmoLex The NRC Word-Emotion Association Lexicon, also called EmoLex (Moham-
mad and Turney, 2013), uses the Plutchik (1980) 8 basic emotions defined above.
The lexicon includes around 14,000 words including words from prior lexicons as
well as frequent nouns, verbs, adverbs and adjectives. Values from the lexicon for
somesamplewords:500 CHAPTER25 • LEXICONSFORSENTIMENT,AFFECT,ANDCONNOTATION
Word
reward 0 1 0 0 1 0 1 1 1 0
worry 0 1 0 1 0 1 0 0 0 1
tenderness 0 0 0 0 1 0 0 0 1 0
sweetheart 0 1 0 0 1 1 0 1 1 0
suddenly 0 0 0 0 0 0 1 0 0 0
thirst 0 1 0 0 0 1 1 0 0 0
garbage 0 0 1 0 0 0 0 0 0 1
For a smaller set of 5,814 words, the NRC Emotion/Affect Intensity Lexicon
(Mohammad,2018b)containsreal-valuedscoresofassociationforanger,fear,joy,
andsadness;Fig.25.5showsexamples.
Anger Fear Joy Sadness
outraged 0.964 horror 0.923 superb 0.864 sad 0.844
violence 0.742 anguish 0.703 cheered 0.773 guilt 0.750
coup 0.578 pestilence 0.625 rainbow 0.531 unkind 0.547
oust 0.484 stressed 0.531 gesture 0.387 difficulties 0.421
suspicious 0.484 failing 0.531 warms 0.391 beggar 0.422
nurture 0.059 confident 0.094 hardship .031 sing 0.017
Figure25.5 Sampleemotionalintensitiesforwordsforanger,fear,joy,andsadnessfrom
Mohammad(2018b).
LIWC LIWC, Linguistic Inquiry and Word Count, is a widely used set of 73 lex-
icons containing over 2300 words (Pennebaker et al., 2007), designed to capture
aspects of lexical meaning relevant for social psychological tasks. In addition to
sentiment-related lexicons like ones for negative emotion (bad, weird, hate, prob-
lem, tough) and positive emotion (love, nice, sweet), LIWC includes lexicons for
categorieslikeanger,sadness,cognitivemechanisms,perception,tentative,andin-
hibition,showninFig.25.6.
There are various other hand-built affective lexicons. The General Inquirer in-
cludes additional lexicons for dimensions like strong vs. weak, active vs. passive,
overstated vs. understated, as well as lexicons for categories like pleasure, pain,
virtue,vice,motivation,andcognitiveorientation.
concrete Another useful feature for various tasks is the distinction between concrete
abstract words like banana or bathrobe and abstract words like belief and although. The
lexiconinBrysbaertetal.(2014)usedcrowdsourcingtoassignaratingfrom1to5
oftheconcretenessof40,000words,thusassigningbanana,bathrobe,andbagel5,
belief1.19,although1.07,andinbetweenwordslikebriska2.5.
25.3 Creating Affect Lexicons by Human Labeling
Theearliestmethodusedtobuildaffectlexicons,andstillincommonuse,istohave
crowdsourcing humans label each word. This is now most commonly done via crowdsourcing:
breakingthetaskintosmallpiecesanddistributingthemtoalargenumberofanno-
regna
noitapicitna
tsugsid
raef
yoj
ssendas esirprus
tsurt
evitisop evitagen25.3 • CREATINGAFFECTLEXICONSBYHUMANLABELING 501
Positive Negative
Emotion Emotion Insight Inhibition Family Negate
appreciat* anger* aware* avoid* brother* aren’t
comfort* bore* believe careful* cousin* cannot
great cry decid* hesitat* daughter* didn’t
happy despair* feel limit* family neither
interest fail* figur* oppos* father* never
joy* fear know prevent* grandf* no
perfect* griev* knew reluctan* grandm* nobod*
please* hate* means safe* husband none
safe* panic* notice* stop mom nor
terrific suffers recogni* stubborn* mother nothing
value terrify sense wait niece* nowhere
wow* violent* think wary wife without
Figure25.6 Samplesfrom5ofthe73lexicalcategoriesinLIWC(Pennebakeretal.,2007).
The*meansthepreviouslettersareawordprefixandallwordswiththatprefixareincluded
inthecategory.
tators.Let’stakealookatsomeofthemethodologicalchoicesfortwocrowdsourced
emotionlexicons.
TheNRCEmotionLexicon(EmoLex)(MohammadandTurney,2013),labeled
emotionsintwosteps. Toensurethattheannotatorswerejudgingthecorrectsense
of the word, they first answered a multiple-choice synonym question that primed
thecorrectsenseoftheword(withoutrequiringtheannotatortoreadapotentially
confusingsensedefinition). Thesewerecreatedautomaticallyusingtheheadwords
associated with the thesaurus category of the sense in question in the Macquarie
dictionaryandtheheadwordsof3randomdistractorcategories. Anexample:
Which word is closest in meaning (most related) to startle?
• automobile
• shake
• honesty
• entertain
Foreachword(e.g. startle),theannotatorwasthenaskedtoratehowassociated
that word is with each of the 8 emotions (joy, fear, anger, etc.). The associations
were rated on a scale of not, weakly, moderately, and strongly associated. Outlier
ratingswereremoved,andtheneachtermwasassignedtheclasschosenbythema-
jorityoftheannotators,withtiesbrokenbychoosingthestrongerintensity,andthen
the4levelsweremappedintoabinarylabelforeachword(noandweakmappedto
0,moderateandstrongmappedto1).
TheNRCVADLexicon(Mohammad,2018a)wasbuiltbyselectingwordsand
emoticonsfrompriorlexiconsandannotatingthemwithcrowd-sourcingusingbest-
best-worst worst scaling (Louviere et al. 2015, Kiritchenko and Mohammad 2017). In best-
scaling
worstscaling,annotatorsaregivenNitems(usually4)andareaskedwhichitemis
the best (highest) and which is the worst (lowest) in terms of some property. The
setofwordsusedtodescribetheendsofthescalesaretakenfrompriorliterature.
Forvalence,forexample,theraterswereasked:
Q1.WhichofthefourwordsbelowisassociatedwiththeMOSThappi-
ness/pleasure/positiveness/satisfaction/contentedness/hopefulness
OR LEAST unhappiness / annoyance / negativeness / dissatisfaction /502 CHAPTER25 • LEXICONSFORSENTIMENT,AFFECT,ANDCONNOTATION
melancholy/despair? (Fourwordslistedasoptions.)
Q2. WhichofthefourwordsbelowisassociatedwiththeLEASThap-
piness/pleasure/positiveness/satisfaction/contentedness/hopeful-
nessORMOSTunhappiness/annoyance/negativeness/dissatisfaction
/melancholy/despair? (Fourwordslistedasoptions.)
Thescoreforeachwordinthelexiconistheproportionoftimestheitemwaschosen
asthebest(highestV/A/D)minustheproportionoftimestheitemwaschosenasthe
worst (lowest V/A/D). The agreement between annotations are evaluated by split-
split-half half reliability: split the corpus in half and compute the correlations between the
reliability
annotationsinthetwohalves.
25.4 Semi-supervised Induction of Affect Lexicons
Anothercommonwaytolearnsentimentlexiconsistostartfromasetofseedwords
thatdefinetwopolesofasemanticaxis(wordslikegoodorbad),andthenfindways
tolabeleachwordwbyitssimilaritytothetwoseedsets. Herewesummarizetwo
familiesofseed-basedsemi-supervisedlexiconinductionalgorithms,axis-basedand
graph-based.
25.4.1 SemanticAxisMethods
One of the most well-known lexicon induction methods, the Turney and Littman
(2003)algorithm,isgivenseedwordslikegoodorbad,andthenforeachwordwto
belabeled,measuresbothhowsimilaritistogoodandhowdifferentitisfrombad.
HerewedescribeaslightextensionofthealgorithmduetoAnetal.(2018),which
isbasedoncomputingasemanticaxis.
In the first step, we choose seed words by hand. There are two methods for
dealing with the fact that the affect of a word is different in different contexts: (1)
startwithasinglelargeseedlexiconandrelyontheinductionalgorithmtofine-tune
it to the domain, or (2) choose different seed words for different genres. Hellrich
etal.(2019)suggeststhatformodelingaffectacrossdifferenthistoricaltimeperiods,
startingwithalargemodernaffectdictionaryisbetterthansmallseedsetstunedtobe
stableacrosstime. Asanexampleofthesecondapproach,Hamiltonetal.(2016a)
defineonesetofseedwordsforgeneralsentimentanalysis,adifferentsetforTwitter,
andyetanothersetforsentimentinfinancialtext:
Domain Positiveseeds Negativeseeds
General good,lovely,excellent,fortunate,pleas- bad, horrible, poor, unfortunate, un-
ant, delightful, perfect, loved, love, pleasant, disgusting, evil, hated, hate,
happy unhappy
Twitter love, loved, loves, awesome, nice, hate,hated,hates,terrible,nasty,awful,
amazing,best,fantastic,correct,happy worst,horrible,wrong,sad
Finance successful, excellent, profit, beneficial, negligent, loss, volatile, wrong, losses,
improving, improved, success, gains, damages, bad, litigation, failure, down,
positive negative
Inthesecondstep, wecomputeembeddingsforeachofthepolewords. These
embeddingscanbeoff-the-shelfword2vecembeddings,orcanbecomputeddirectly25.4 • SEMI-SUPERVISEDINDUCTIONOFAFFECTLEXICONS 503
onaspecificcorpus(forexampleusingafinancialcorpusifafinancelexiconisthe
goal),orwecanfine-tuneoff-the-shelfembeddingstoacorpus. Fine-tuningisespe-
ciallyimportantifwehaveaveryspecificgenreoftextbutdon’thaveenoughdata
to train good embeddings. In fine-tuning, we begin with off-the-shelf embeddings
likeword2vec,andcontinuetrainingthemonthesmalltargetcorpus.
Once we have embeddings for each pole word, we create an embedding that
represents each pole by taking the centroid of the embeddings of each of the seed
words; recallthatthecentroidisthemultidimensionalversionofthemean. Given
asetofembeddingsforthepositiveseedwordsS+= E(w+),E(w+),...,E(w+) ,
{ 1 2 n }
andembeddingsforthenegativeseedwordsS −= {E(w−1),E(w−2),...,E(w −m) },the
polecentroidsare:
n
1
V+= E(w+)
n i
1
(cid:88)
m
1
V −=
m
E(w−i ) (25.1)
1
(cid:88)
Thesemanticaxisdefinedbythepolesiscomputedjustbysubtractingthetwovec-
tors:
V axis=V+ V − (25.2)
−
V , the semantic axis, is a vector in the direction of positive sentiment. Finally,
axis
wecompute(viacosinesimilarity)theanglebetweenthevectorinthedirectionof
positivesentimentandthedirectionofw’sembedding. Ahighercosinemeansthat
wismorealignedwithS+thanS .
−
score(w) = cos E(w),V
axis
E(w) V
(cid:0) axis (cid:1)
= · (25.3)
E(w) V
axis
(cid:107) (cid:107)(cid:107) (cid:107)
Ifadictionaryofwordswithsentimentscoresissufficient,we’redone! Orifwe
needtogroupwordsintoapositiveandanegativelexicon, wecanuseathreshold
orothermethodtogiveusdiscretelexicons.
25.4.2 LabelPropagation
An alternative family of methods defines lexicons by propagating sentiment labels
on graphs, an idea suggested in early work by Hatzivassiloglou and McKeown
(1997). We’ll describe the simple SentProp (Sentiment Propagation) algorithm of
Hamiltonetal.(2016a),whichhasfoursteps:
1. Defineagraph: Givenwordembeddings,buildaweightedlexicalgraphby
connectingeachwordwithitsknearestneighbors(accordingtocosinesimi-
larity). Theweightsoftheedgebetweenwordsw andw aresetas:
i j
w w
E i,j=arccos
−
wi(cid:62) wj . (25.4)
(cid:18) (cid:107) i (cid:107)(cid:107) j (cid:107)(cid:19)
2. Defineaseedset: Choosepositiveandnegativeseedwords.
3. Propagatepolaritiesfromtheseedset: Nowweperformarandomwalkon
thisgraph, startingattheseedset. Inarandomwalk, westartatanodeand504 CHAPTER25 • LEXICONSFORSENTIMENT,AFFECT,ANDCONNOTATION
thenchooseanodetomovetowithprobabilityproportionaltotheedgeprob-
ability. Aword’spolarityscoreforaseedsetisproportionaltotheprobability
ofarandomwalkfromtheseedsetlandingonthatword(Fig.25.7).
4. Create word scores: We walk from both positive and negative seed sets,
resultinginpositive(rawscore+(w))andnegative(rawscore (w))rawlabel
i − i
scores. Wethencombinethesevaluesintoapositive-polarityscoreas:
rawscore+(w)
score+(w i)= rawscore+(w)+rawsi
core (w)
(25.5)
i − i
It’softenhelpfultostandardizethescorestohavezeromeanandunitvariance
withinacorpus.
5. Assignconfidencetoeachscore:Becausesentimentscoresareinfluencedby
theseedset,we’dliketoknowhowmuchthescoreofawordwouldchangeif
adifferentseedsetisused. Wecanusebootstrapsamplingtogetconfidence
regions, by computing the propagation B times over random subsets of the
positiveandnegativeseedsets(forexampleusingB=50andchoosing7of
the10seedwordseachtime).Thestandarddeviationofthebootstrapsampled
polarityscoresgivesaconfidencemeasure.
loathe loathe
like like
abhor abhor
love idolize find hate love idolize find hate
dislike dislike
see uncover see uncover
adore despise adore despise
disapprove disapprove
notice notice
appreciate appreciate
(a) (b)
Figure25.7 IntuitionoftheSENTPROPalgorithm. (a)Runrandomwalksfromtheseedwords. (b)Assign
polarityscores(shownhereascolorsgreenorred)basedonthefrequencyofrandomwalkvisits.
25.4.3 OtherMethods
The core of semisupervised algorithms is the metric for measuring similarity with
the seed words. The Turney and Littman (2003) and Hamilton et al. (2016a) ap-
proachesaboveusedembeddingcosineasthedistancemetric: wordswerelabeled
as positive basically if their embeddings had high cosines with positive seeds and
lowcosineswithnegativeseeds. Othermethodshavechosenotherkindsofdistance
metricsbesidesembeddingcosine.
ForexampletheHatzivassiloglouandMcKeown(1997)algorithmusessyntactic
cues;twoadjectivesareconsideredsimilariftheywerefrequentlyconjoinedbyand
andrarelyconjoinedbybut. Thisisbasedontheintuitionthatadjectivesconjoined
by the words and tend to have the same polarity; positive adjectives are generally
coordinatedwithpositive,negativewithnegative:
fairandlegitimate,corruptandbrutal
butlessoftenpositiveadjectivescoordinatedwithnegative:
*fairandbrutal,*corruptandlegitimate
Bycontrast,adjectivesconjoinedbybutarelikelytobeofoppositepolarity:25.5 • SUPERVISEDLEARNINGOFWORDSENTIMENT 505
fairbutbrutal
Anothercuetooppositepolaritycomesfrommorphologicalnegation(un-,im-,
-less). Adjectiveswiththesamerootbutdifferinginamorphologicalnegative(ad-
equate/inadequate,thoughtful/thoughtless)tendtobeofoppositepolarity.
Yetanothermethodforfindingwordsthathaveasimilarpolaritytoseedwordsis
tomakeuseofathesauruslikeWordNet(KimandHovy2004,HuandLiu2004b).
Aword’ssynonymspresumablyshareitspolaritywhileaword’santonymsprobably
havetheoppositepolarity. Afteraseedlexiconisbuilt, eachlexiconisupdatedas
follows,possiblyiterated.
Lex+: Add synonyms of positive words (well) and antonyms (like fine) of negative
words
Lex : Addsynonymsofnegativewords(awful)andantonyms(likeevil)ofpositive
−
words
An extension of this algorithm assigns polarity to WordNet senses, called Senti-
SentiWordNet WordNet(Baccianellaetal.,2010). Fig.25.8showssomeexamples.
Synset Pos Neg Obj
good#6 ‘agreeableorpleasing’ 1 0 0
respectable#2honorable#4good#4estimable#2 ‘deservingofesteem’ 0.75 0 0.25
estimable#3computable#1 ‘maybecomputedorestimated’ 0 0 1
sting#1burn#4bite#2 ‘causeasharporstingingpain’ 0 0.875 .125
acute#6 ‘ofcriticalimportanceandconsequence’ 0.625 0.125 .250
acute#4 ‘ofanangle;lessthan90degrees’ 0 0 1
acute#1 ‘havingorexperiencingarapidonsetandshortbutseverecourse’ 0 0.5 0.5
Figure25.8 ExamplesfromSentiWordNet3.0(Baccianellaetal.,2010).Notethedifferencesbetweensenses
ofhomonymouswords: estimable#3ispurelyobjective, whileestimable#2ispositive; acutecanbepositive
(acute#6),negative(acute#1),orneutral(acute#4).
In this algorithm, polarity is assigned to entire synsets rather than words. A
positivelexiconisbuiltfromallthesynsetsassociatedwith7positivewords,anda
negativelexiconfromsynsetsassociatedwith7negativewords. Aclassifieristhen
trainedfromthisdatatotakeaWordNetglossanddecideifthesensebeingdefined
ispositive,negativeorneutral. Afurtherstep(involvingarandom-walkalgorithm)
assigns a score to each WordNet synset for its degree of positivity, negativity, and
neutrality.
Insummary,semisupervisedalgorithmsuseahuman-definedsetofseedwords
forthetwopolesofadimension,andusesimilaritymetricslikeembeddingcosine,
coordination,morphology,orthesaurusstructuretoscorewordsbyhowsimilarthey
aretothepositiveseedsandhowdissimilartothenegativeseeds.
25.5 Supervised Learning of Word Sentiment
Semi-supervisedmethodsrequireonlyminimalhumansupervision(intheformof
seedsets). Butsometimesasupervisionsignalexistsintheworldandcanbemade
useof. Onesuchsignalisthescoresassociatedwithonlinereviews.
Thewebcontainsanenormousnumberofonlinereviewsforrestaurants,movies,
books, or other products, each of which have the text of the review along with an506 CHAPTER25 • LEXICONSFORSENTIMENT,AFFECT,ANDCONNOTATION
Moviereviewexcerpts(IMDb)
10 Agreatmovie. Thisfilmisjustawonderfulexperience. It’ssurreal,zany,wittyandslapstick
allatthesametime. Andterrificperformancestoo.
1 ThiswasprobablytheworstmovieIhaveeverseen. Thestorywentnowhereeventhoughthey
couldhavedonesomeinterestingstuffwithit.
Restaurantreviewexcerpts(Yelp)
5 Theservicewasimpeccable. Thefoodwascookedandseasonedperfectly... Thewatermelon
wasperfectlysquare... Thegrilledoctopuswas... mouthwatering...
2 ...ittookawhiletogetourwaters,wegotourentreebeforeourstarter,andweneverreceived
silverwareornapkinsuntilwerequestedthem...
Bookreviewexcerpts(GoodReads)
1 Iamgoingtotryandstopbeingdeceivedbyeye-catchingtitles. Isowantedtolikethisbook
andwassodisappointedbyit.
5 This book is hilarious. I would recommend it to anyone looking for a satirical read with a
romantictwistandanarratorthatkeepsbuttingin
Productreviewexcerpts(Amazon)
5 ThelidonthisblenderthoughisprobablywhatIlikethebestaboutit... enablesyoutopour
intosomethingwithouteventakingthelidoff! ... theperfectpitcher! ... worksfantastic.
1 Ihatethisblender... Itisnearlyimpossibletogetfrozenfruitandicetoturnintoasmoothie...
YouhavetoaddaTONofliquid. Ialsowishithadaspout...
Figure25.9 Excerptsfromsomereviewsfromvariousreviewwebsites,allonascaleof1to5starsexcept
IMDb,whichisonascaleof1to10stars.
associatedreviewscore: avaluethatmayrangefrom1starto5stars,orscoring1
to10. Fig.25.9showssamplesextractedfromrestaurant,book,andmoviereviews.
We can use this review score as supervision: positive words are more likely to
appear in 5-star reviews; negative words in 1-star reviews. And instead of just a
binarypolarity,thiskindofsupervisionallowsustoassignawordamorecomplex
representationofitspolarity: itsdistributionoverstars(orotherscores).
Thus in a ten-star system we could represent the sentiment of each word as a
10-tuple,eachnumberascorerepresentingtheword’sassociationwiththatpolarity
level. This association can be a raw count, or a likelihood P(wc), or some other
|
functionofthecount,foreachclasscfrom1to10.
For example, we could compute the IMDb likelihood of a word like disap-
point(ed/ing) occurring in a 1 star review by dividing the number of times disap-
point(ed/ing)occursin1-starreviewsintheIMDbdataset(8,557)bythetotalnum-
ber of words occurring in 1-star reviews (25,395,214), so the IMDb estimate of
P(disappointing1)is.0003.
|
Aslightmodificationofthisweighting,thenormalizedlikelihood,canbeused
asanilluminatingvisualization(Potts,2011)1
count(w,c)
P(wc) =
| count(w,c)
w C
∈
P(wc)
PottsScore(w) = (cid:80) | (25.6)
P(wc)
c |
Dividing the IMDb estimate P(disappoint(cid:80)ing1) of .0003 by the sum of the likeli-
|
hoodP(wc)overallcategoriesgivesaPottsscoreof0.10. Theworddisappointing
|
thusisassociatedwiththevector[.10,.12,.14,.14,.13,.11,.08,.06,.06,.05]. The
1 Each element of the Potts score of a word w and category c can be shown to be a variant of the
pointwisemutualinformationpmi(w,c)withoutthelogterm;seeExercise25.1.Overview Data Methods Categorization Scale induction Looking ahead
Example: attenuators
somewhat/r
IMDB – 53,775 tokens OpenTable – 3,890 tokens Goodreads – 3,424 tokens Amazon/Tripadvisor – 2,060 tokens
Cat = 0.33 (p = 0.004) Cat = 0.11 (p = 0.707) Cat = -0.55 (p = 0.128) Cat = 0.42 (p = 0.207)
Cat^2 = -4.02 (p < 0.001) Cat^2 = -6.2 (p = 0.014) Cat^2 = -5.04 (p = 0.016) Cat^2 = -2.74 (p = 0.05)
0.38 0.36
0.28
25.5 • SUPERVISEDLEARNINGOFWORDSENTIMENT 507 0.15 0.19
0.09 0.12
0.05 0.08 0.08
Pottsdiagram Pottsdiagram(Potts,2011)isavisualizationofthesewordscores,representingthe
priorsentimentofawordasadistributionovertheratingcategories. Category Category Category Category
Fig.25.10showsthePottsdiagramsfor3positiveand3negativescalaradjec-
tives. Note that the curve for strongly positive scalars have the shape of the letter
J, while strongly negative scalars look like a reverse J. By contrast, weakly posi-
tive and negative scalars have a hump-shape, with the maximum either below the fairly/r
mean (weakly negative wo“rdPs oliketdtissa&ppdoiinatingg)roraambovesth”e mean (weakly po Ps-
otts,&Christopher.&2011.&NSF&wIMoDBr k– s33h,5o15 pto&keonns& OpenTable – 2,829 tokens Goodreads – 1,806 tokens Amazon/Tripadvisor – 2,158 tokens
itive words like good). These shapes offer an illuminating typology of affective
restructuring&adjectives. Cat = -0.13 (p = 0.284) Cat = 0.2 (p = 0.265) Cat = -0.87 (p = 0.016) Cat = 0.54 (p = 0.183)
meaning. Cat^2 = -5.37 (p < 0.001) Cat^2 = -4.16 (p = 0.007) Cat^2 = -5.74 (p = 0.004) Cat^2 = -3.32 (p = 0.045)
0.35
0.31 0.29
Emphatics 0.17 Attenuators
Positive scalars Negative scalars 0.18
good disappointing
totally 00 .. 00 49 somewhat 0.08 00 .. 01 52 0.11
Category Category Category Category
1 2 3 4 5 ra t6 in g7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 ra t6 in g7 8 9 10 1 2 3 4 5 ra t6 in g7 8 9 10
rating
absolutely fairly
great bad pretty/r
IMDB – 176,264 tokens OpenTable – 8,982 tokens Goodreads – 11,895 tokens Amazon/Tripadvisor – 5,980 tokens
1 2 3 4 5 6 7 8 9 10 1 2 3 4Ca t5 = -06.4 37 ( p 8< 0 9.0 0 11)0 Cat = -0.64 (p = 0.035) Cat = -0.71 (p = 0.072) Cat = 0.26 (p = 0.496)
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
rating Cat^2r a=t -i3n.6g (p < 0.001) Cat^2 = -4.47 (p = 0.007) Cat^2 = -4.59 (p = 0.018) Cat^2 = -2.23 (p = 0.131)
rating rating 0.32 0.34
utterly pretty 0.28
excellent terrible 0.13 00 .. 11 49 0.15 0.15
0.09
0.05 0.08 0.07
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
rating rating
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 Category Category Category Category
rating rating
Figure25.10 Pottsdiagrams(Potts,2011)forpositiveandnegativescalaradjectives,show-
ing the J-shape and reverse J-shape for strongly positive and negative adjectives, and the
hump-shapeformoreweaklypolarizedadjectives.
Fig. 25.11 shows the Potts diagrams for emphasizing and attenuating adverbs.
Note that emphatics tend to have a J-shape (most likely to occur in the most posi-
tive reviews) or a U-shape (most likely to occur in the strongly positive and nega-
tive). Attenuatorsallhavethehump-shape,emphasizingthemiddleofthescaleand
downplayingbothextremes. Thediagramscanbeusedbothasatypologyoflexical
sentiment,andalsoplayaroleinmodelingsentimentcompositionality.
InadditiontofunctionslikeposteriorP(cw),likelihoodP(wc),ornormalized
| |
likelihood(Eq.25.6)manyotherfunctionsofthecountofawordoccurringwitha
sentimentlabelhavebeenused. We’llintroducesomeoftheseonpage511,includ-
ingideaslikenormalizingthecountsperwriterinEq.25.14.
25.5.1 LogOddsRatioInformativeDirichletPrior
One thing we often want to do with word polarity is to distinguish between words
thataremorelikelytobeusedinonecategoryoftextsthaninanother. Wemay,for
example,wanttoknowthewordsmostassociatedwith1starreviewsversusthose
associated with 5 star reviews. These differences may not be just related to senti-
ment.WemightwanttofindwordsusedmoreoftenbyDemocraticthanRepublican
membersofCongress,orwordsusedmoreofteninmenusofexpensiverestaurants
05.0-
05.0-
05.0-
93.0-
93.0-
93.0-
82.0-
82.0-
82.0-
71.0-
71.0-
71.0-
60.0-
60.0-
60.0-
60.0
60.0
60.0
71.0
71.0
71.0
82.0
82.0
82.0
93.0
93.0
93.0
05.0
05.0
05.0
05.0-
05.0-
05.0-
52.0-
52.0-
52.0-
00.0
00.0
00.0
52.0
52.0
52.0
05.0
05.0
05.0
05.0-
05.0-
05.0-
52.0-
52.0-
52.0-
00.0
00.0
00.0
52.0
52.0
52.0
05.0
05.0
05.0
05.0-
05.0-
05.0-
52.0-
52.0-
52.0-
00.0
00.0
00.0
52.0
52.0
52.0
05.0
05.0
05.0Overview Data Methods Categorization Scale induction Looking ahead
Example: attenuators
somewhat/r
IMDB – 53,775 tokens OpenTable – 3,890 tokens Goodreads – 3,424 tokens Amazon/Tripadvisor – 2,060 tokens
Cat = 0.33 (p = 0.004) Cat = 0.11 (p = 0.707) Cat = -0.55 (p = 0.128) Cat = 0.42 (p = 0.207)
Cat^2 = -4.02 (p < 0.001) Cat^2 = -6.2 (p = 0.014) Cat^2 = -5.04 (p = 0.016) Cat^2 = -2.74 (p = 0.05)
0.38 0.36
0.28
0.15 0.19
0.09 0.12
0.05 0.08 0.08
Category Category Category Category
fairly/r
“Potts&diagrams” Potts,&Christopher.&2011.&NSF&wIMoDBr k– s33h,5o15 pto&keonns& OpenTable – 2,829 tokens Goodreads – 1,806 tokens Amazon/Tripadvisor – 2,158 tokens
508 CHAPTER25 • LEXICOrNeSstFrOucRtuSrEinNgT&IaMdEjeNcTt,ivAeFsF.ECT,ANDCat C= -0O.1N3 (pN =O 0.T28A4)TION Cat = 0.2 (p = 0.265) Cat = -0.87 (p = 0.016) Cat = 0.54 (p = 0.183)
Cat^2 = -5.37 (p < 0.001) Cat^2 = -4.16 (p = 0.007) Cat^2 = -5.74 (p = 0.004) Cat^2 = -3.32 (p = 0.045)
0.35
0.31 0.29
Emphatics 0.17 Attenuators
Positive scalars Negative scalars 0.18
good disappointing
totally 00 .. 00 49 somewhat 0.08 00 .. 01 52 0.11
Category Category Category Category
1 2 3 4 5 ra t6 in g7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 ra t6 in g7 8 9 10 1 2 3 4 5 ra t6 in g7 8 9 10
rating
absolutely fairly
great bad pretty/r
IMDB – 176,264 tokens OpenTable – 8,982 tokens Goodreads – 11,895 tokens Amazon/Tripadvisor – 5,980 tokens
1 2 3 4 5 6 7 8 9 10 1 2 3 4Ca t5 = -06.4 37 ( p 8< 0 9.0 0 11)0 Cat = -0.64 (p = 0.035) Cat = -0.71 (p = 0.072) Cat = 0.26 (p = 0.496)
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
rating Cat^2r a=t -i3n.6g (p < 0.001) Cat^2 = -4.47 (p = 0.007) Cat^2 = -4.59 (p = 0.018) Cat^2 = -2.23 (p = 0.131)
rating rating 0.32 0.34
utterly pretty 0.28
excellent terrible 0.13 00 .. 11 49 0.15 0.15
0.09
0.05 0.08 0.07
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
rating rating
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 Category Category Category Category
rating rating Figure25.11 Pottsdiagrams(Potts,2011)foremphaticandattenuatingadverbs.
thancheaprestaurants.
Given two classes of documents, to find words more associated with one cate-
gorythananother,wecouldmeasurethedifferenceinfrequencies(isawordwmore
frequentinclassAorclassB?). Orinsteadofthedifferenceinfrequencieswecould
computetheratiooffrequencies,orcomputethelogoddsratio(thelogoftheratio
betweentheoddsofthetwowords). Wecouldthensortwordsbywhicheverassoci-
ationmeasurewepick,rangingfromwordsoverrepresentedincategoryAtowords
overrepresentedincategoryB.
Theproblemwithsimplelog-likelihoodorlogoddsmethodsisthattheyoverem-
phasizedifferencesinveryrarewords,andoftenalsoinveryfrequentwords. Very
rare words will seem to occur very differently in the two corpora since with tiny
counts there may be statistical fluctations, or even zero occurrences in one corpus
comparedtonon-zerooccurrencesintheother. Veryfrequentwordswillalsoseem
differentsinceallcountsarelarge.
Inthissectionwewalkthroughthedetailsofonesolutiontothisproblem: the
“logoddsratioinformativeDirichletprior”methodofMonroeetal.(2008)thatisa
particularlyusefulmethodforfindingwordsthatarestatisticallyoverrepresentedin
oneparticularcategoryoftextscomparedtoanother. It’sbasedontheideaofusing
anotherlargecorpustogetapriorestimateofwhatweexpectthefrequencyofeach
wordtobe.
Let’s start with the goal: assume we want to know whether the word horrible
loglikelihood occurs more in corpus i or corpus j. We could compute the log likelihood ratio,
ratio
using fi(w) to mean the frequency of word w in corpus i, and ni to mean the total
numberofwordsincorpusi:
Pi(horrible)
llr(horrible) = log
Pj(horrible)
= logPi(horrible) logPj(horrible)
−
fi(horrible) fj(horrible)
= log log (25.7)
ni − nj
logoddsratio Instead,let’scomputethelogoddsratio: doeshorriblehavehigheroddsiniorin
05.0-
05.0-
05.0-
93.0-
93.0-
93.0-
82.0-
82.0-
82.0-
71.0-
71.0-
71.0-
60.0-
60.0-
60.0-
60.0
60.0
60.0
71.0
71.0
71.0
82.0
82.0
82.0
93.0
93.0
93.0
05.0
05.0
05.0
05.0-
05.0-
05.0-
52.0-
52.0-
52.0-
00.0
00.0
00.0
52.0
52.0
52.0
05.0
05.0
05.0
05.0-
05.0-
05.0-
52.0-
52.0-
52.0-
00.0
00.0
00.0
52.0
52.0
52.0
05.0
05.0
05.0
05.0-
05.0-
05.0-
52.0-
52.0-
52.0-
00.0
00.0
00.0
52.0
52.0
52.0
05.0
05.0
05.025.5 • SUPERVISEDLEARNINGOFWORDSENTIMENT 509
j:
Pi(horrible) Pj(horrible)
lor(horrible) = log log
1 Pi(horrible) − 1 Pj(horrible)
(cid:18) − (cid:19) (cid:18) − (cid:19)
fi(horrible) fj(horrible)
ni nj
= log log
 fi(horrible)
−
 fj(horrible)
1 1
 − ni   − nj 
 fi(horrible)   fj(horrible) 
= log log (25.8)
ni fi(horrible) − nj fj(horrible)
(cid:18) − (cid:19) (cid:18) − (cid:19)
TheDirichletintuitionistousealargebackgroundcorpustogetapriorestimateof
what we expect the frequency of each word w to be. We’ll do this very simply by
addingthecountsfromthatcorpustothenumeratoranddenominator,sothatwe’re
essentiallyshrinkingthecountstowardthatprior. It’slikeaskinghowlargearethe
differencesbetweeniand j givenwhatwewouldexpectgiventheirfrequenciesin
awell-estimatedlargebackgroundcorpus.
The method estimates the difference between the frequency of word w in two
(i j)
corporaiand jviatheprior-modifiedlogoddsratioforw,δ w− ,whichisestimated
as:
fi +α fj+α
δ w(i −j) =log
(cid:18)ni+α
0w
−(f
wiw
+α w)
(cid:19)−log
(cid:32)nj+α
0w
−(f
ww
j+α w)(cid:33)
(25.9)
(whereni isthesizeofcorpusi, nj isthesizeofcorpus j, fi isthecountofword
w
w in corpus i, fj is the count of word w in corpus j, α is the scaled size of the
w 0
backgroundcorpus,andα isthescaledcountofwordwinthebackgroundcorpus.)
w
Inaddition,Monroeetal.(2008)makeuseofanestimateforthevarianceofthe
log–odds–ratio:
1 1
σ2 δˆ w(i −j)
≈ f wi +α w
+
f wj+α w
(25.10)
(cid:16) (cid:17)
Thefinalstatisticforawordisthenthez–scoreofitslog–odds–ratio:
δˆ w(i −j)
(25.11)
σ2 δˆ w(i −j)
(cid:114)
(cid:16) (cid:17)
TheMonroeetal.(2008)methodthusmodifiesthecommonlyusedlogoddsratio
intwoways:itusesthez-scoresofthelogoddsratio,whichcontrolsfortheamount
of variance in a word’s frequency, and it uses counts from a background corpus to
provideapriorcountforwords.
Fig. 25.12 shows the method applied to a dataset of restaurant reviews from
Yelp,comparingthewordsusedin1-starreviewstothewordsusedin5-starreviews
(Jurafskyetal.,2014).Thelargestdifferenceisinobvioussentimentwords,withthe
1-starreviewsusingnegativesentimentwordslikeworse,bad,awfulandthe5-star
reviewsusingpositivesentimentwordslikegreat,best,amazing.Butthereareother
illuminatingdifferences. 1-starreviewsuselogicalnegation(no, not), while5-star
reviewsuseemphaticsandemphasizeuniversality(very, highly, every, always). 1-
starreviewsusefirstpersonplurals(we,us,our)while5starreviewsusethesecond
person. 1-star reviews talk about people (manager, waiter, customer) while 5-star
reviews talk about dessert and properties of expensive restaurants like courses and
atmosphere. SeeJurafskyetal.(2014)formoredetails.510 CHAPTER25 • LEXICONSFORSENTIMENT,AFFECT,ANDCONNOTATION
Class Wordsin1-starreviews Class Wordsin5-starreviews
Negative worst, rude, terrible, horrible, bad, Positive great,best,love(d),delicious,amazing,
awful, disgusting, bland, tasteless, favorite, perfect, excellent, awesome,
gross, mediocre, overpriced, worse, friendly,fantastic,fresh,wonderful,in-
poor credible,sweet,yum(my)
Negation no,not Emphatics/ very,highly,perfectly,definitely,abso-
universals lutely,everything,every,always
1Plpro we,us,our 2pro you
3pro she,he,her,him Articles a,the
Pastverb was, were, asked, told, said, did, Advice try,recommend
charged,waited,left,took
Sequencersafter,then Conjunct also,as,well,with,and
Nouns manager, waitress, waiter, customer, Nouns atmosphere, dessert, chocolate, wine,
customers,attitude,waste,poisoning, course,menu
money,bill,minutes
Irrealis would,should Auxiliaries is/’s,can,’ve,are
modals
Comp to,that Prep,other in,of,die,city,mouth
Figure25.12 Thetop50wordsassociatedwithone–starandfive-starrestaurantreviewsinaYelpdatasetof
900,000reviews,usingtheMonroeetal.(2008)method(Jurafskyetal.,2014).
25.6 Using Lexicons for Sentiment Recognition
InChapter4weintroducedthenaiveBayesalgorithmforsentimentanalysis. The
lexiconswehavefocusedonthroughoutthechaptersofarcanbeusedinanumber
ofwaystoimprovesentimentdetection.
Inthesimplestcase,lexiconscanbeusedwhenwedon’thavesufficienttraining
data to build a supervised sentiment analyzer; it can often be expensive to have a
humanassignsentimenttoeachdocumenttotrainthesupervisedclassifier.
Insuchsituations,lexiconscanbeusedinarule-basedalgorithmforclassifica-
tion. Thesimplestversionisjusttousetheratioofpositivetonegativewords: ifa
documenthasmorepositivethannegativewords(usingthelexicontodecidethepo-
larityofeachwordinthedocument),itisclassifiedaspositive. Oftenathresholdλ
isused,inwhichadocumentisclassifiedaspositiveonlyiftheratioisgreaterthan
λ. If the sentiment lexicon includes positive and negative weights for each word,
θ+andθ ,thesecanbeusedaswell. Here’sasimplesuchsentimentalgorithm:
w w−
f+ = θ+count(w)
w
ws.t.w positivelexicon
∈(cid:88)
f− = θ w−count(w)
ws.t.w negativelexicon
∈(cid:88)
+ if
f+
>λ
f
−
sentiment =  if f− >λ (25.12)
−
0
othf e+
rwise.
Ifsupervisedtrainingdataisavailable,
thesecountscomputedfromsentimentlex-
icons,sometimesweightedornormalizedinvariousways,canalsobeusedasfea-
tures in a classifier along with other lexical or non-lexical features. We return to
suchalgorithmsinSection25.7.25.7 • USINGLEXICONSFORAFFECTRECOGNITION 511
25.7 Using Lexicons for Affect Recognition
Detectionofemotion(andtheotherkindsofaffectivemeaningdescribedbyScherer
(2000)) can be done by generalizing the algorithms described above for detecting
sentiment.
Themostcommonalgorithmsinvolvesupervisedclassification: atrainingsetis
labeledfortheaffectivemeaningtobedetected,andaclassifierisbuiltusingfeatures
extractedfromthetrainingset.Aswithsentimentanalysis,ifthetrainingsetislarge
enough, and the test set is sufficiently similar to the training set, simply using all
thewordsorallthebigramsasfeaturesinapowerfulclassifierlikeSVMorlogistic
regression, as described in Fig. 4.2 in Chapter 4, is an excellent algorithm whose
performanceishardtobeat. Thuswecantreataffectivemeaningclassificationofa
textsampleassimpledocumentclassification.
Somemodificationsarenonethelessoftennecessaryforverylargedatasets. For
example,theSchwartzetal.(2013)studyofpersonality,gender,andageusing700
million words of Facebook posts used only a subset of the n-grams of lengths 1-
3. Only words and phrases used by at least 1% of the subjects were included as
features,and2-gramsand3-gramswereonlykeptiftheyhadsufficientlyhighPMI
(PMIgreaterthan2 length,wherelengthisthenumberofwords):
∗
p(phrase)
pmi(phrase)=log (25.13)
p(w)
w ∈p (cid:89)hrase
Variousweightscanbeusedforthefeatures,includingtherawcountinthetraining
set, or some normalized probability or log probability. Schwartz et al. (2013), for
example, turn feature counts into phrase likelihoods by normalizing them by each
subject’stotalworduse.
freq(phrase,subject)
p(phrasesubject)= (25.14)
| freq(phrase(cid:48),subject)
phrase(cid:48) v(cid:88)ocab(subject)
∈
If the training data is sparser, or not as similar to the test set, any of the lexicons
we’vediscussedcanplayahelpfulrole,eitheraloneorincombinationwithallthe
wordsandn-grams.
Many possible values can be used for lexicon features. The simplest is just an
indicatorfunction,inwhichthevalueofafeature f takesthevalue1ifaparticular
L
texthasanywordfromtherelevantlexiconL. UsingthenotationofChapter4, in
whichafeaturevalueisdefinedforaparticularoutputclasscanddocumentx.
1 if w:w L & w x & class=c
f L(c,x) =
0
oth∃ erwise∈ ∈
(cid:26)
Alternatively the value of a feature f for a particular lexicon L can be the total
L
numberofwordtokensinthedocumentthatoccurinL:
f = count(w)
L
w L
(cid:88)∈
Forlexicainwhicheachwordisassociatedwithascoreorweight,thecountcanbe
multipliedbyaweightθL:
w
f = θLcount(w)
L w
w L
(cid:88)∈512 CHAPTER25 • LEXICONSFORSENTIMENT,AFFECT,ANDCONNOTATION
CountscanalternativelybeloggedornormalizedperwriterasinEq.25.14.
However they are defined, these lexicon features are then used in a supervised
classifier to predict the desired affective category for the text or document. Once
a classifier is trained, we can examine which lexicon features are associated with
which classes. For a classifier like logistic regression the feature weight gives an
indicationofhowassociatedthefeatureiswiththeclass.
25.8 Lexicon-based methods for Entity-Centric Affect
Whatifwewanttogetanaffectscorenotforanentiredocument,butforaparticular
entityinthetext? Theentity-centricmethodofFieldandTsvetkov(2019)combines
affectlexiconswithcontextualembeddingstoassignanaffectscoretoanentityin
text. Inthecontextofaffectaboutpeople,theyrelabeltheValence/Arousal/Domi-
nancedimensionasSentiment/Agency/Power. Thealgorithmfirsttrainsclassifiers
tomapembeddingstoscores:
1. Foreachwordwinthetrainingcorpus:
(a) Useoff-the-shelfpretrainedencoders(likeBERT)toextractacontextual
embeddingeforeachinstanceoftheword. Noadditionalfine-tuningis
done.
(b) Averageovertheeembeddingsofeachinstanceofwtoobtainasingle
embeddingvectorforonetrainingpointw.
(c) UsetheNRCVADLexicontogetS,A,andPscoresforw.
2. Train(three)regressionmodelsonallwordswtopredictV,A,Dscoresfrom
aword’saverageembedding.
Nowgivenanentitymentionminatext,weassignaffectscoresasfollows:
1. UsethesamepretrainedLMtogetcontextualembeddingsformincontext.
2. Feedthisembeddingthroughthe3regressionmodelstogetS,A,Pscoresfor
theentity.
Thisresultsina(S,A,P)tupleforagivenentitymention;Togetscoresfortherep-
resentationofanentityinacompletedocument,wecanruncoreferenceresolution
and average the (S,A,P) scores for all the mentions. Fig. 25.13 shows the scores
from their algorithm for characters from the movie The Dark Knight when run on
Wikipediaplotsummarytextswithgoldcoreference.
25.9 Connotation Frames
Thelexicons we’vedescribedso fardefine awordas apointin affectivespace. A
connotation connotationframe,bycontrast,isalexiconthatincorporatesaricherkindofgram-
frame
maticalstructure,bycombiningaffectivelexiconswiththeframesemanticlexicons
of Chapter 24. The basic insight of connotation frame lexicons is that a predicate
likeaverbexpressesconnotationsabouttheverb’sarguments(Rashkinetal.2016,
Rashkinetal.2017).
Considersentenceslike:
(25.15) CountryAviolatedthesovereigntyofCountryB25.9 • CONNOTATIONFRAMES 513
weakly Rachel Dent Gordan Batman Joker powerfully weakly Rachel Joker Dent Gordan Batmanpowerfully
Power Score Power Score
negative Joker Dent Gordan Rachel Batman positive negative Joker Gordan Batman Dent Rachel positive
Sentiment Score Sentiment Score
dull Rachel Dent GordanBatman Joker scary
dull Dent Gordan Rachel Batman Joker scary
Agency Score
Agency Score
Figure25.13 Power(dominance),sentiment(valence)andagency(arousal)forFchigaruarcete2rs:Power,sentiment,andagencyscoresforchar-
Figure1:Power,sentiment,andagencyscoresforchar-
inthemovieTheDarkKnightcomputedfromembeddingstrainedontheNRCVADacLteexriscoinn.TheDarkNight aslearnedthroughASPwith
actersinTheDarkNightaslearnedthroughtheregres-
Note the protagonist (Batman) and the antagonist (the Joker) have high power anEdLaMgeonceymbeddings. Thesescoresreflectthesamepat-
scoresbutdiffersiinonsemntoimdeenltw,withhilEeLthMelooveeminbteedredsitnRgas.chSelcohraesslogwenpeorwaellryandagencybut
terns as the regression model with greater separation
highsentiment.alignwithcharacterarchetypes,i.e. theantagonisthas
betweencharacters.
thelowestsentimentscore.
(25.16) theteenager... survivedtheBostonMarathonbombing”
Byusingtheverbviolatein(25.15),theauthorisexpressingtheirsympatvhieeyswDitehnt (ally to Batman who turns evil) and
ment have resulted in his effective removal from
Country B, portraying Country B as a victim, and expressing antagonismRatocwhealrdDawes (primary love interest). To facil-
the industry. While articles about the #MeToo
theagentCountryA.Bycontrast,inusingtheverbsurvive,theauthorof(25.16)is
itate extracting example sentences, we score each
movement portray men like Weinstein as unpow-
expressingthatthebombingisanegativeexperience,andthesubjectofthesentence,
instanceoftheseentitiesinthenarrativeseparately
theteenager, iserafusyl,mwpaethceatinccshpaercauctleart.eTthheastetahsepeccotsrpoofrcaonunsoetdatitoonareinherent
and average across instances to obtain an entity
inthemeaningtroafintheEvLeMrbosvainodlatBeEanRdTsuprovritvrea,yasthsehmowansinpoFwige.r2f5u.l1.4.
score for the document.9 To maximize our data
Thus, inacorpus wheretraditional powerroles
by capturing every mention of an entity, we per-
Connotation Frame for “Role1 suh rva ivv ese Rob lee 2”e n inverted, theCoenmnotbateiodnd Frianmge sfor e“Rxotler1a vciotleadtes Role2”
S(writer→ro +le1) S(roW le1r →ite rr
ole2)
f d tS( ur w _oro ritem erm
→
sr, ole2iE a )nsL ttM hh eeo y da aan tr ad e tbB hi eE a ysR eT ad retp oe tw rr S(af a wro iitr nerr d em →r so dl _e1) tw oh ne So (.rr ops W leo Fe 1r →wi utet rrh re otlra he2n e)s rtSr r+( wa u erin vcter →- - -role2)f a tho ll er ym , ub sc eao s- oer d fef Woe nre ikon iuc pe r edrr e ie ass uo dll t au s tt ai fo ro in nmb ty rT aah inba iln ned g3. ta hA s ed wd Ei e Lt li l Mon a os-
Role1 is a _ idence ofThtehrei sis exisRtosle1i ins thethe performan_ ce of the mRooled2 ies la(Petersetal.,2018),weuseELMoembed-
sympathetic Role1 Role2 some type antagonist Role1 Role2 sympathetic
dingsforouranalysis.
victim BERT-maofs hkaredsdhipembeddings - whereas these em- victim
+
be_ddings generally capture pow_ er poorly as co+m- Figures 1 and 2 show results. For refer-
Reader pared to the unmasked embeddingsRea(dTearble 2), ence, we show the entity scores as compared to
theyoutperformtheunmaskedembeddingsonthis one polar opposite pair identified by ASP. Both
(a) (b)
task, and even outperform the frequency baseline the regression model and ASP show similar pat-
Figure25.14 Connotationframesforsurviveandviolate.(a)Forsurvive,thewriterandreaderhavepositive
sentimenttowardRole1,thesubjecint,aonndenesgeattitvinegse.ntNimeevnetrtothwearldesRso,let2h,etyheddoirenctootbojeuctt.p(ebr)-Forvt ie olr an ts e,. thB eatman has high power, while Rachel has
writerandreaderhavepositivesenftoimrmentFiniestledadettoawl.ar(d2R01ol9e)2,,lthikeedliyrecbteocbajeucste. theydonot low power. Additionally, the Joker is associated
captureaffectinformationaswellastheunmasked with the most negative sentiment, but the high-
The conno eta mti bo en df dr ia nm ge sl (e Tx aic bo lens 2)o .f Rashkin et al. (2016) and Rashkeisnt eatgaeln.cy. Throughout the plot summary, the
(2017) also express other connotative aspects of the predicate toward eamchovairegup-rogresses by the Joker taking an aggres-
ment,includingtheeffect(somethingbadhappenedtox)value: (xisvaluable),and
4.3 QualitativeDocument-levelAnalysis sive action and the other characters responding.
mental state: (x is distressed by the event). Connotation frames can also mark the
We can see this dynamic reflected in the Joker’s
power differenFtiianlablleytw, ewenethqeuaalrigtautmiveenltys (aunsianlgyztheehvoewrb iwmepllloroeumreans that the
profile score, as a high-powered, high-agency,
themeargumenmtehtahsogdreactaeprtpuorweseratfhfaenctthdeiamgeennts)i,oannsdtbhyeaagneanlcyyzoinfgeachargument
(waitedislowagency). Fig.25.15showsavisualizationfromSapetal.(20lo1w7)-.sentiment character, who is the primary plot-
single documents in detail. We conduct this anal-
Connotationframescanbebuiltbyhand(Sapetal.,2017),ortheycanbderlievaernr.edIngeneral,ASPshowsagreaterseparation
ysisinadomainwhereweexpectentitiestofulfill
bysupervisedlearning(Rashkinetal.,2016),forexampleusinghand-labebleedtwtraeienn- charactersthantheregressionmodel. We
traditional power roles and where entity portray-
hypothesize that this occurs because ASP isolates
als are known. Following Bamman et al. (2013),
thedimensionsofinterest,whiletheregressionap-
we analyze the Wikipedia plot summary of the
proach captures other confounds, such as that hu-
movie The Dark Knight,7 focusing on Batman
(protagonist),8 theJoker(antagonist),JimGordan
9Whenweusedthisaveragingmetricinotherevaluations,
(law enforcement officer, ally to Batman), Har-
wefoundnosignificantchangeinresults.Thus,inothersce-
narios,wecomputescoresoveraveragedembeddings,rather
7http://bit.ly/2XmhRDR thanaveragingscoresseparatelycomputedforeachembed-
8WeconsiderBatman/BruceWaynetobethesameentity. dingtoreducecomputationallycomplexity.514 CHAPTER25 • LEXICONSFORSENTIMENT,AFFECT,ANDCONNOTATION
He implored the tribunal to show mercy.
power(AG<TH) power(AG>TH)
VERB
AGENT THEME
implore
power(AG < TH)
The princess waited for her prince.
VERB
AGENT THEME
wait agency(AG)= agency(AG)=+
 
agency(AG) = -
Figure25.15 TheconnotationframesofSapetal.(2017),showingthattheverbimplore
impliestheagenthasFloiwguerrep2ow: eTrhtheafnotrhmeathlenmoteat(iionncoonfttrhaset,csoanyn,owtaitthioanverblikedemanded),
andshowingthelowflreavmeleosfoafgepnocwyeorfathnedsaugbejencctyo.fTwhaeitefidr.sFtiegxuarmefprloemSapetal.(2017).
shows the relative power differential implied by
the verb “implored”, i.e., the agent (“he”) is in
ing data to superviasepocsliatisosnifioefrslefsosrpeoawcehrtohfanthteheintdheivmideu(a“lthreeltarit-ionsF,ieg.ugr.e, 3w:hSetahmeprleverbsintheconnotationframes
S(writer Role1)buisna+l”)o.rI-n,caonndtrathste,n“Himepdreomvainngdeadccthueratcryibuvniaalglowbaitlhchoingshtraaninnotstatoragreement. Sizeisindicative
→
acrossallrelations.showmercy”impliesthattheagenthasauthority of verb frequency in our corpus (bigger = more
over the theme. The second example shows the frequent),colordifferencesareonlyforlegibility.
lowlevelofagencyimpliedbytheverb“waited”.
one another. For example, if the agent “dom-
25.10 Summary
interactivedemowebsiteofourfindings(seeFig- inates” the theme (denoted as power(AG>TH)),
ure5intheappendixforascreenshot).2 Further- thentheagentisimpliedtohavealevelofcontrol
more, aswillbeseeninSection4.1, connotation over the theme. Alternatively, if the agent “hon-
• Manykindso ff raa mff ee sc oti fv fee rs nta ewtes inc sa ign hb tse td hi as tti cn og mu pis leh med en, tin ac nl du dd ein -gemorost”iothnes,tmheomoeds(,denotedaspower(AG<TH)), the
attitudes(wh vi ic ah tein frc ol mud te hese wn eti lm l-ke nn ot) w, nin Bte er cp he dr es lo tn esa tl (s Bta ecn hc de e, la ,ndpwerritseorniamlpitlyie.sthatthethemeismoreimportantor
• Emotion
can19b8e6)r.eprInesepnartetidcublayr,fiwxeedfiantdomthiactuhnigiths-aogfetnencycalleau dth bo ar sit ia ctiv ee m. oW -eusedAMTcrowdsourcingtola-
womenthroughthelensofconnotationframesare bel 1700 transitive verbs for power differentials.
tions,oraspointsinspacedefinedbydimensionslikevalenceandarousal.
rareinmodernfilms. Itis,inpart,becausesome Withthreeannotatorsperverb,theinter-annotator
• Words havemcoonvnieosta(et.igo.n,aSlnaowspeWcthsitree)laatcecdidteonttahlleysepaasfsfetchteive satgarteeesm, aenntdisth0i.s34(Krippendorff’s↵).
connotationaBleacshpdeeclttoefstwaonrddamlseoanbiencgaucsaenebveenremproevsieesntwedithinlexicons.
Agency Theagencyattributedtotheagentofthe
strongfemalecharactersarenotentirelyfreefrom
• Affective lexicons can be built by hand, using crowd sourcinvegrbtodleanboetelsthwehether the action being described
thedeeplyingrainedbiasesinsocialnorms.
affectivecontentofeachword. implies that the agent is powerful, decisive, and
capable of pushing forward their own storyline.
• Lexiconscan2beCbounilntowtiathtio sen mF ir -sa um pe es rvo if sP edo,wbeoroatsntrdappingfroFmorseexeadmwploer,dasperson who is described as “ex-
usingsimilaritymAegterniccsylikeembeddingcosine.
periencing”thingsdoesnotseemasactiveandde-
• Lexicons
canWbeecrleeaatrenetwdoinneawfucollnynostuaptioernvriesleadtiomnsa,npnoewr,erwhencis aiv ce oa ns vs eo nm iee no tnewhoisdescribedas“determin-
trainingsignaalndcaangbenecfyou(enxdaminptlehseiwnoFrilgdu,rseu3c)h,aassraantienxgpsaans-signi en dg” byth uin sg es r. soA nMT workers labeled 2000 transi-
sion of the existing connotation frame lexicons.3 tiveverbsforimplyinghigh/moderate/lowagency
areviewsite.
Three AMT crowdworkers annotated the verbs (inter-annotator agreement of 0.27). We denote
• Wordscanbewaisthsipglnaecdehwolediegrhsttsoinavaolidexgiecnodnebrybiuassiinngthvearcioonu-sfunhcitgihonasgeonfcwyoarsdagency(AG)=+, andlow agency
counts in tratienxitn(ge.tge.,xtXs,reasncduerdatYio;amneetxraimcsplleikteaslkogisoshdodwsnratiaosiangfeonrcmy(aAtiGv)e= .
 
Dirichletpriionrt.heappendixinFigure7). Wedefinetheanno-
Pairwise agreements on a hard constraint are
tatedconstructsasfollows:
• Affectcanbedetected,justlikesentiment,byusingstandards5u6p%ervainsded5t1e%xt for power and agency, respec-
classificationPotewcehrnDiqiuffeesr,eunstiinalgsallMthaneywvoerrdbssoimrpbliygrtahmesauin- atetixvtelays.feDaetuspreitse.this, agreements reach 96% and
Additionalfetahtourriteyslceavnelbseodfrtahwenagfreonmt acnodutnhtesmoefwreolartdivseintolexic9o4n%s.whenmoderatelabelsarecountedasagree-
ingwitheitherhighorlowlabels,showingthatan-
• Lexiconscanal2shotbtep:u/s/ehdotmoeds.ectse.cwtaasfhfiencgttinona.erduule/-˜bmassaepd/classinfioetratb oy rsp ri ac rk eli yng
stronglydisagreewithoneanother.
thesimplemmaojov 3ri Tie ht ey-b ls eie xan icst oi/ nm. se an ndtb aa ds ee md oo an rec ao vu an ilat bs leof atw ho tr td ps :i /n /eachSolemxeiccoonn.tributingfactorsinthelowerKAscores
• Connotationho frm ae ms.c es s.wexasphreisnsgtriocnh.eerdure/l˜amtsioanps/moofvaifefe-cbtiivaes/m. eanii nnc glu thd ae tt ahe prs eu db -tlety of choosing between neutral
icateencodesaboutitsarguments.BIBLIOGRAPHICALANDHISTORICALNOTES 515
Bibliographical and Historical Notes
TheideaofformallyrepresentingthesubjectivemeaningofwordsbeganwithOs-
good et al. (1957), the same pioneering study that first proposed the vector space
modelofmeaningdescribedinChapter6. Osgoodetal.(1957)hadparticipantsrate
wordsonvariousscales,andranfactoranalysisontheratings. Themostsignificant
factor they uncovered was the evaluative dimension, which distinguished between
pairslikegood/bad,valuable/worthless,pleasant/unpleasant. Thisworkinfluenced
thedevelopmentofearlydictionariesofsentimentandaffectivemeaninginthefield
ofcontentanalysis(Stoneetal.,1966).
subjectivity Wiebe(1994)begananinfluentiallineofworkondetectingsubjectivityintext,
beginningwiththetaskofidentifyingsubjectivesentencesandthesubjectivechar-
acters who are described in the text as holding private states, beliefs or attitudes.
Learned sentiment lexicons such as the polarity lexicons of Hatzivassiloglou and
McKeown(1997)wereshowntobeausefulfeatureinsubjectivitydetection(Hatzi-
vassiloglouandWiebe2000,Wiebe2000).
The term sentiment seems to have been introduced in 2001 by Das and Chen
(2001),todescribethetaskofmeasuringmarketsentimentbylookingatthewordsin
stocktradingmessageboards.InthesamepaperDasandChen(2001)alsoproposed
the use of a sentiment lexicon. The list of words in the lexicon was created by
hand,buteachwordwasassignedweightsaccordingtohowmuchitdiscriminated
a particular class (say buy versus sell) by maximizing across-class variation and
minimizing within-class variation. The term sentiment, and the use of lexicons,
caughtonquitequickly(e.g.,interalia,Turney2002).Pangetal.(2002)firstshowed
the power of using all the words without a sentiment lexicon; see also Wang and
Manning(2012).
Mostofthesemi-supervisedmethodswedescribeforextendingsentimentdic-
tionariesdrewontheearlyideathatsynonymsandantonymstendtoco-occurinthe
samesentence(MillerandCharles1991,JustesonandKatz1991,RiloffandShep-
herd 1997). Other semi-supervised methods for learning cues to affective mean-
ing rely on information extraction techniques, like the AutoSlog pattern extractors
(Riloff and Wiebe, 2003). Graph based algorithms for sentiment were first sug-
gestedbyHatzivassiloglouandMcKeown(1997),andgraphpropagationbecamea
standard method (Zhu and Ghahramani 2002, Zhu et al. 2003, Zhou et al. 2004a,
Velikovich et al. 2010). Crowdsourcing can also be used to improve precision by
filtering the result of semi-supervised lexicon learning (Riloff and Shepherd 1997,
Fastetal.2016).
Muchrecentworkfocusesonwaystolearnembeddingsthatdirectlyencodesen-
timentorotherproperties,suchasthe DENSIFIER algorithmofRotheetal.(2016)
thatlearnstotransformtheembeddingspacetofocusonsentiment(orother)infor-
mation.
Exercises
25.1 Show that the relationship between a word w and a category c in the Potts
Score in Eq. 25.6 is a variant of the pointwise mutual information pmi(w,c)
withoutthelogterm.516 CHAPTER26 • COREFERENCERESOLUTION
CHAPTER
26 Coreference Resolution
andevenStigand,thepatrioticarchbishopofCanterbury,founditadvisable–”’
‘FoundWHAT?’saidtheDuck.
‘FoundIT,’theMouserepliedrathercrossly:‘ofcourseyouknowwhat“it”means.’
‘Iknowwhat“it”meanswellenough,whenIfindathing,’saidtheDuck: ‘it’sgener-
allyafrogoraworm.Thequestionis,whatdidthearchbishopfind?’
LewisCarroll,AliceinWonderland
An important component of language processing is knowing who is being talked
aboutinatext. Considerthefollowingpassage:
(26.1) VictoriaChen,CFOofMegabucksBanking,sawherpayjumpto$2.3
million,asthe38-year-oldbecamethecompany’spresident. Itiswidely
knownthatshecametoMegabucksfromrivalLotsabucks.
Each of the underlined phrases in this passage is used by the writer to refer to
a person named Victoria Chen. We call linguistic expressions like her or Victoria
mention Chen mentions or referring expressions, and the discourse entity that is referred
referent to(VictoriaChen)thereferent. (Todistinguishbetweenreferringexpressionsand
theirreferents,weitalicizetheformer.)1 Twoormorereferringexpressionsthatare
corefer used to refer to the same discourse entity are said to corefer; thus, Victoria Chen
andshecoreferin(26.1).
Coreferenceisanimportantcomponentofnaturallanguageprocessing. Adia-
loguesystemthathasjusttoldtheuser“Thereisa2pmflightonUnitedanda4pm
oneonCathayPacific”mustknowwhichflighttheusermeansby“I’lltakethesec-
ond one”. A question answering system that uses Wikipedia to answer a question
aboutMarieCuriemustknowwhoshewasinthesentence“ShewasborninWar-
saw”.AndamachinetranslationsystemtranslatingfromalanguagelikeSpanish,in
whichpronounscanbedropped,mustusecoreferencefromtheprevioussentenceto
decidewhethertheSpanishsentence‘“Meencantaelconocimiento”,dice.’ should
be translated as ‘“I love knowledge”, he says’, or ‘“I love knowledge”, she says’.
Indeed, this example comes from an actual news article in El Pa´ıs about a female
professorandwasmistranslatedas“he”inmachinetranslationbecauseofinaccurate
coreferenceresolution(Schiebinger,2013).
Natural language processing systems (and humans) interpret linguistic expres-
discourse sions with respect to a discourse model (Karttunen, 1969). A discourse model
model
(Fig. 26.1) is a mental model that the understander builds incrementally when in-
terpreting a text, containing representations of the entities referred to in the text,
as well as properties of the entities and relations among them. When a referent is
evoked firstmentionedinadiscourse,wesaythatarepresentationforitisevokedintothe
accessed model. Uponsubsequentmention,thisrepresentationisaccessedfromthemodel.
1 Asaconvenientshorthand,wesometimesspeakofareferringexpressionreferringtoareferent,e.g.,
sayingthatshereferstoVictoriaChen. However,thereadershouldkeepinmindthatwhatwereally
meanisthatthespeakerisperformingtheactofreferringtoVictoriaChenbyutteringshe.517
Discourse Model
Lotsabucks
V
Megabucks $
refer (access)
pay
refer (evoke)
“Victoria” corefer “she”
Figure26.1 Howmentionsevokeandaccessdiscourseentitiesinadiscoursemodel.
Reference in a text to an entity that has been previously introduced into the
anaphora discourse is called anaphora, and the referring expression used is said to be an
anaphor anaphor, oranaphoric.2 Inpassage(26.1), thepronounssheandherandthedefi-
niteNPthe38-year-oldarethereforeanaphoric. Theanaphorcoreferswithaprior
antecedent mention(inthiscaseVictoriaChen)thatiscalledtheantecedent. Noteveryrefer-
ringexpressionisanantecedent. Anentitythathasonlyasinglementioninatext
singleton (likeLotsabucksin(26.1))iscalledasingleton.
coreference In this chapter we focus on the task of coreference resolution. Coreference
resolution
resolution is the task of determining whether two mentions corefer, by which we
meantheyrefertothesameentityinthediscoursemodel(thesamediscourseentity).
coreference Thesetofcoreferringexpressionsisoftencalledacoreferencechainoracluster.
chain
cluster For example, in processing (26.1), a coreference resolution algorithm would need
to find at least four coreference chains, corresponding to the four entities in the
discoursemodelinFig.26.1.
1. VictoriaChen,her,the38-year-old,She
{ }
2. MegabucksBanking,thecompany,Megabucks
{ }
3. herpay
{ }
4. Lotsabucks
{ }
Notethatmentionscanbenested; forexamplethementionherissyntactically
partofanothermention,herpay,referringtoacompletelydifferentdiscourseentity.
Coreference resolution thus comprises two tasks (although they are often per-
formedjointly): (1)identifyingthementions, and(2)clusteringthemintocorefer-
encechains/discourseentities.
We said that two mentions corefered if they are associated with the same dis-
courseentity. Butoftenwe’dliketogofurther,decidingwhichrealworldentityis
associated with this discourse entity. For example, the mention Washington might
refertotheUSstate,orthecapitalcity,orthepersonGeorgeWashington;theinter-
pretationofthesentencewillofcoursebeverydifferentforeachofthese. Thetask
entitylinking ofentitylinking(JiandGrishman,2011)orentityresolutionisthetaskofmapping
adiscourseentitytosomereal-worldindividual.3 Weusuallyoperationalizeentity
2 WewillfollowthecommonNLPusageofanaphortomeananymentionthathasanantecedent,rather
thanthemorenarrowusagetomeanonlymentions(likepronouns)whoseinterpretationdependsonthe
antecedent(underthenarrowerinterpretation,repeatednamesarenotanaphors).
3 Computationallinguistics/NLPthusdiffersinitsuseofthetermreferencefromthefieldofformal
semantics,whichusesthewordsreferenceandcoreferencetodescribetherelationbetweenamention
andareal-worldentity. Bycontrast,wefollowthefunctionallinguisticstraditioninwhichamention
referstoadiscourseentity(Webber,1978)andtherelationbetweenadiscourseentityandtherealworld
individualrequiresanadditionalstepoflinking.518 CHAPTER26 • COREFERENCERESOLUTION
linkingorresolutionbymappingtoanontology: alistofentitiesintheworld,like
a gazeteer (Chapter 19). Perhaps the most common ontology used for this task is
Wikipedia; each Wikipedia page acts as the unique id for a particular entity. Thus
theentitylinkingtaskofwikification(MihalceaandCsomai,2007)isthetaskofde-
cidingwhichWikipediapagecorrespondingtoanindividualisbeingreferredtoby
amention.Butentitylinkingcanbedonewithanyontology;forexampleifwehave
an ontology of genes, we can link mentions of genes in text to the disambiguated
genenameintheontology.
Inthenextsectionsweintroducethetaskofcoreferenceresolutioninmorede-
tail, and offer a variety of architectures for resolution, from simple deterministic
baselinealgorithmstostate-of-the-artneuralmodels.
Before turning to algorithms, however, we mention some important tasks we
willonlytouchonbrieflyattheendofthischapter. FirstarethefamousWinograd
Schemaproblems(so-calledbecausetheywerefirstpointedoutbyTerryWinograd
inhisdissertation). Theseentitycoreferenceresolutionproblemsaredesignedtobe
toodifficulttobesolvedbytheresolutionmethodswedescribeinthischapter,and
the kind of real-world knowledge they require has made them a kind of challenge
taskfornaturallanguageprocessing. Forexample,considerthetaskofdetermining
thecorrectantecedentofthepronountheyinthefollowingexample:
(26.2) Thecitycouncildeniedthedemonstratorsapermitbecause
a. theyfearedviolence.
b. theyadvocatedviolence.
Determiningthecorrectantecedentforthepronountheyrequiresunderstanding
that the second clause is intended as an explanation of the first clause, and also
that city councils are perhaps more likely than demonstrators to fear violence and
that demonstrators might be more likely to advocate violence. Solving Winograd
Schema problems requires finding way to represent or discover the necessary real
worldknowledge.
Aproblemwewon’tdiscussinthischapteristherelatedtaskofeventcorefer-
event ence, decidingwhethertwoeventmentions(suchasthebuyandtheacquisitionin
coreference
thesetwosentencesfromtheECB+corpus)refertothesameevent:
(26.3) AMDagreedto[buy]Markham,Ontario-basedATIforaround$5.4billion
incashandstock,thecompaniesannouncedMonday.
(26.4) The[acquisition]wouldturnAMDintooneoftheworld’slargestproviders
ofgraphicschips.
Eventmentionsaremuchhardertodetectthanentitymentions,sincetheycanbever-
balaswellasnominal. Oncedetected,thesamemention-pairandmention-ranking
modelsusedforentitiesareoftenappliedtoevents.
discoursedeixis Anevenmorecomplexkindofcoreferenceisdiscoursedeixis(Webber,1988),
inwhichananaphorrefersbacktoadiscoursesegment,whichcanbequitehardto
delimitorcategorize,liketheexamplesin(26.5)adaptedfromWebber(1991):
(26.5) AccordingtoSoleil,Beaujustopenedarestaurant
a. Butthatturnedouttobealie.
b. Butthatwasfalse.
c. Thatstruckmeasafunnywaytodescribethesituation.
The referent of that is a speech act (see Chapter 15) in (26.5a), a proposition in
(26.5b), and a manner of description in (26.5c). We don’t give algorithms in this
chapter for these difficult types of non-nominal antecedents, but see Kolhatkar
etal.(2018)forasurvey.26.1 • COREFERENCEPHENOMENA: LINGUISTICBACKGROUND 519
26.1 Coreference Phenomena: Linguistic Background
We now offer some linguistic background on reference phenomena. We introduce
the four types of referring expressions (definite and indefinite NPs, pronouns, and
names), describe how these are used to evoke and access entities in the discourse
model, and talk about linguistic features of the anaphor/antecedent relation (like
number/genderagreement,orpropertiesofverbsemantics).
26.1.1 TypesofReferringExpressions
IndefiniteNounPhrases: ThemostcommonformofindefinitereferenceinEn-
glishismarkedwiththedeterminera(oran),butitcanalsobemarkedbyaquan-
tifiersuchassomeoreventhedeterminerthis. Indefinitereferencegenerallyintro-
ducesintothediscoursecontextentitiesthatarenewtothehearer.
(26.6) a. Mrs. MartinwassoverykindastosendMrs. Goddardabeautifulgoose.
b. Hehadgoneroundonedaytobringhersomewalnuts.
c. Isawthisbeautifulcauliflowertoday.
DefiniteNounPhrases: Definitereference, suchasviaNPsthatusetheEnglish
article the, refers to an entity that is identifiable to the hearer. An entity can be
identifiable to the hearer because it has been mentioned previously in the text and
thusisalreadyrepresentedinthediscoursemodel:
(26.7) ItconcernsawhitestallionwhichIhavesoldtoanofficer. Butthepedigree
ofthewhitestallionwasnotfullyestablished.
Alternatively,anentitycanbeidentifiablebecauseitiscontainedinthehearer’s
set of beliefs about the world, or the uniqueness of the object is implied by the
description itself, in which case it evokes a representation of the referent into the
discoursemodel,asin(26.9):
(26.8) IreadaboutitintheNewYorkTimes.
(26.9) Haveyouseenthecarkeys?
These last uses are quite common; more than half of definite NPs in newswire
textsarenon-anaphoric,oftenbecausetheyarethefirsttimeanentityismentioned
(PoesioandVieira1998,BeanandRiloff1999).
Pronouns: Anotherformofdefinitereferenceispronominalization,usedforenti-
tiesthatareextremelysalientinthediscourse,(aswediscussbelow):
(26.10) Emmasmiledandchattedascheerfullyasshecould,
cataphora Pronounscanalsoparticipateincataphora,inwhichtheyarementionedbefore
theirreferentsare,asin(26.11).
(26.11) Evenbeforeshesawit,DorothyhadbeenthinkingabouttheEmeraldCity
everyday.
Here,thepronounssheanditbothoccurbeforetheirreferentsareintroduced.
Pronounsalsoappearinquantifiedcontextsinwhichtheyareconsideredtobe
bound bound,asin(26.12).
(26.12) Everydancerbroughtherleftarmforward.
Undertherelevantreading,herdoesnotrefertosomewomanincontext,butinstead
behaveslikeavariableboundtothequantifiedexpressioneverydancer. Wearenot
concernedwiththeboundinterpretationofpronounsinthischapter.520 CHAPTER26 • COREFERENCERESOLUTION
In some languages, pronouns can appear as clitics attached to a word, like lo
(‘it’)inthisSpanishexamplefromAnCora(RecasensandMart´ı,2010):
(26.13) Laintencio´nesreconocerelgranprestigioquetienelamarato´nyunirlo
conestagrancarrera.
‘TheaimistorecognizethegreatprestigethattheMarathonhasandjoinit
|
withthisgreatrace.”
Demonstrative Pronouns: Demonstrative pronouns this and that can appear ei-
theraloneorasdeterminers,forinstance,thisingredient,thatspice:
(26.14) IjustboughtacopyofThoreau’sWalden. Ihadboughtonefiveyearsago.
Thatonehadbeenverytattered;thisonewasinmuchbettercondition.
NotethatthisNPisambiguous;incolloquialspokenEnglish,itcanbeindefinite,
asin(26.6),ordefinite,asin(26.14).
Zero Anaphora: Instead ofusing apronoun, in somelanguages (includingChi-
nese, Japanese, and Italian) it is possible to have an anaphor that has no lexical
zeroanaphor realizationatall,calledazeroanaphororzeropronoun,asinthefollowingItalian
andJapaneseexamplesfromPoesioetal.(2016):
(26.15) EN [John] wenttovisitsomefriends. Ontheway[he] boughtsome
i i
wine.
IT [Giovanni] ando` afarvisitaadegliamici. Perviaφ compro` delvino.
i i
JA [John]-wayujin-ohoumon-sita. Tochu-deφ wain-oka-tta.
i i
orthisChineseexample:
(26.16) [我]前一会精神上太紧张。[0]现在比较平静了
[I]wastoonervousawhileago. ... [0]amnowcalmer.
Zeroanaphorscomplicatethetaskofmentiondetectionintheselanguages.
Names: Names(suchasofpeople,locations,ororganizations)canbeusedtorefer
tobothnewandoldentitiesinthediscourse:
(26.17) a. MissWoodhousecertainlyhadnotdonehimjustice.
b. InternationalBusinessMachinessoughtpatentcompensation
fromAmazon;IBMhadpreviouslysuedothercompanies.
26.1.2 InformationStatus
The way referring expressions are used to evoke new referents into the discourse
(introducingnewinformation),oraccessoldentitiesfromthemodel(oldinforma-
information tion),iscalledtheirinformationstatusorinformationstructure. Entitiescanbe
status
discourse-new discourse-new or discourse-old, and indeed it is common to distinguish at least
discourse-old threekindsofentitiesinformationally(Prince,1981):
newNPs:
brandnewNPs: theseintroduceentitiesthatarediscourse-newandhearer-
newlikeafruitorsomewalnuts.
unusedNPs: these introduce entities that are discourse-new but hearer-old
(likeHongKong,MarieCurie,ortheNewYorkTimes.
oldNPs: also called evoked NPs, these introduce entities that already in the dis-
coursemodel, hencearebothdiscourse-oldandhearer-old, likeitin“Iwent
toanewrestaurant. Itwas...”.26.1 • COREFERENCEPHENOMENA: LINGUISTICBACKGROUND 521
inferrables: these introduce entities that are neither hearer-old nor discourse-old,
but the hearer can infer their existence by reasoning based on other entities
thatareinthediscourse. Considerthefollowingexamples:
(26.18) Iwenttoasuperbrestaurantyesterday. Thechefhadjustopenedit.
(26.19) Mixflour,butterandwater. Kneadthedoughuntilshiny.
Neitherthechefnorthedoughwereinthediscoursemodelbasedonthefirst
bridging sentence of either example, but the reader can make a bridging inference
inference
thattheseentitiesshouldbeaddedtothediscoursemodelandassociatedwith
therestaurantandtheingredients,basedonworldknowledgethatrestaurants
have chefs and dough is the result of mixing flour and liquid (Haviland and
Clark1974,WebberandBaldwin1992,Nissimetal.2004,Houetal.2018).
The form of an NP gives strong clues to its information status. We often talk
given-new aboutanentity’spositiononthegiven-newdimension,theextenttowhichtherefer-
entisgiven(salientinthediscourse,easierforthehearertocalltomind,predictable
bythehearer),versusnew(non-salientinthediscourse,unpredictable)(Chafe1976,
accessible Prince 1981, Gundel et al. 1993). A referent that is very accessible (Ariel, 2001)
i.e.,verysalientinthehearer’smindoreasytocalltomind,canbereferredtowith
lesslinguisticmaterial. Forexamplepronounsareusedonlywhenthereferenthas
salience a high degree of activation or salience in the discourse model.4 By contrast, less
saliententities,likeanewreferentbeingintroducedtothediscourse,willneedtobe
introduced with a longer and more explicit referring expression to help the hearer
recoverthereferent.
Thus when an entity is first introduced into a discourse its mentions are likely
tohavefullnames, titlesorroles, orappositiveorrestrictiverelativeclauses, asin
the introduction of our protagonist in (26.1): Victoria Chen, CFO of Megabucks
Banking. Asanentityisdiscussedoveradiscourse,itbecomesmoresalienttothe
heareranditsmentionsonaveragetypicallybecomesshorterandlessinformative,
forexamplewithashortenedname(forexampleMs. Chen), adefinitedescription
(the38-year-old),orapronoun(sheorher)(Hawkins1978). However,thischange
in length is not monotonic, and is sensitive to discourse structure (Grosz 1977b,
Reichman1985,Fox1993).
26.1.3 Complications: Non-ReferringExpressions
Many noun phrases or other nominals are not referring expressions, although they
maybearaconfusingsuperficialresemblance. Forexampleinsomeoftheearliest
computationalworkonreference resolution, Karttunen (1969)pointedoutthatthe
NPacarinthefollowingexampledoesnotcreateadiscoursereferent:
(26.20) Janetdoesn’thaveacar.
andcannotbereferredbacktobyanaphoricitorthecar:
(26.21) *ItisaToyota.
(26.22) *Thecarisred.
We summarize here four common types of structures that are not counted as men-
tionsincoreferencetasksandhencecomplicatethetaskofmention-detection:
4 Pronounsalsousually(butnotalways)refertoentitiesthatwereintroducednofurtherthanoneortwo
sentencesbackintheongoingdiscourse,whereasdefinitenounphrasescanoftenreferfurtherback.522 CHAPTER26 • COREFERENCERESOLUTION
Appositives: An appositional structure is a noun phrase that appears next to a
headnounphrase,describingthehead.InEnglishtheyoftenappearincommas,like
“a unit of UAL” appearing in apposition to the NP United, or CFO of Megabucks
BankinginappositiontoVictoriaChen.
(26.23) VictoriaChen,CFOofMegabucksBanking,saw...
(26.24) United,aunitofUAL,matchedthefares.
AppositionalNPsarenotreferringexpressions,insteadfunctioningasakindof
supplementaryparentheticaldescriptionoftheheadNP.Nonetheless,sometimesit
is usefulto linkthese phrases toan entity theydescribe, andso somedatasets like
OntoNotesmarkappositionalrelationships.
Predicative and Prenominal NPs: Predicative or attributive NPs describe prop-
ertiesoftheheadnoun. InUnitedisaunitofUAL,theNPaunitofUALdescribes
a property of United, rather than referring to a distinct entity. Thus they are not
markedasmentionsincoreferencetasks; inourexampletheNPs$2.3millionand
the company’s president, are attributive, describing properties of her pay and the
38-year-old;Example(26.27)showsaChineseexampleinwhichthepredicateNP
(中国最大的城市;China’sbiggestcity)isnotamention.
(26.25) herpayjumpedto$2.3million
(26.26) the38-year-oldbecamethecompany’spresident
(26.27) 上海是[中国最大的城市] [ShanghaiisChina’sbiggestcity]
Expletives: ManyusesofpronounslikeitinEnglishandcorrespondingpronouns
expletive in other languages are not referential. Such expletive or pleonastic cases include
clefts it is raining, in idioms like hit it off, or in particular syntactic situations like clefts
(26.28a)orextraposition(26.28b):
(26.28) a. ItwasEmmaGoldmanwhofoundedMotherEarth
b. Itsurprisedmethattherewasaherringhangingonherwall.
Generics: Anotherkindofexpressionthatdoesnotreferbacktoanentityexplic-
itlyevokedinthetextisgenericreference. Consider(26.29).
(26.29) Ilovemangos. Theyareverytasty.
Here,theyrefers,nottoaparticularmangoorsetofmangos,butinsteadtotheclass
ofmangosingeneral. Thepronounyoucanalsobeusedgenerically:
(26.30) InJulyinSanFranciscoyouhavetowearajacket.
26.1.4 LinguisticPropertiesoftheCoreferenceRelation
Nowthatwehaveseenthelinguisticpropertiesofindividualreferringexpressions
weturntopropertiesoftheantecedent/anaphorpair. Understandingtheseproperties
ishelpfulbothindesigningnovelfeaturesandperformingerroranalyses.
Number Agreement: Referring expressions and their referents must generally
agreeinnumber;Englishshe/her/he/him/his/itaresingular,we/us/they/themareplu-
ral,andyouisunspecifiedfornumber. Soapluralantecedentlikethechefscannot
generally corefer with a singular anaphor like she. However, algorithms cannot
enforcenumberagreementtoostrictly. First,semanticallypluralentitiescanbere-
ferredtobyeitheritorthey:
(26.31) IBMannouncedanewmachinetranslationproductyesterday. Theyhave
beenworkingonitfor20years.26.1 • COREFERENCEPHENOMENA: LINGUISTICBACKGROUND 523
singularthey Second, singular they has become much more common, in which they is used to
describesingularindividuals,oftenusefulbecausetheyisgenderneutral. Although
recentlyincreasing,singulartheyisquiteold,partofEnglishformanycenturies.5
PersonAgreement: Englishdistinguishesbetweenfirst,second,andthirdperson,
and a pronoun’s antecedent must agree with the pronoun in person. Thus a third
personpronoun(he,she,they,him,her,them,his,her,their)musthaveathirdperson
antecedent(oneoftheaboveoranyothernounphrase). However,phenomenalike
quotationcancauseexceptions;inthisexampleI,my,andshearecoreferent:
(26.32) “IvotedforNaderbecausehewasmostalignedwithmyvalues,”shesaid.
GenderorNounClassAgreement: Inmanylanguages,allnounshavegrammat-
icalgenderornounclass6andpronounsgenerallyagreewiththegrammaticalgender
oftheirantecedent. InEnglishthisoccursonlywiththird-personsingularpronouns,
whichdistinguishbetween male(he, him, his), female(she, her), andnonpersonal
(it)grammaticalgenders.Non-binarypronounslikezeorhirmayalsooccurinmore
recenttexts.Knowingwhichgendertoassociatewithanameintextcanbecomplex,
andmayrequireworldknowledgeabouttheindividual. Someexamples:
(26.33) Maryamhasatheorem. Sheisexciting. (she=Maryam,notthetheorem)
(26.34) Maryamhasatheorem. Itisexciting. (it=thetheorem,notMaryam)
Binding Theory Constraints: The binding theory is a name for syntactic con-
straintsontherelationsbetweenamentionandanantecedentinthesamesentence
reflexive (Chomsky, 1981). Oversimplifying a bit, reflexive pronouns like himself and her-
selfcoreferwiththesubjectofthemostimmediateclausethatcontainsthem(26.35),
whereasnonreflexivescannotcoreferwiththissubject(26.36).
(26.35) Janetboughtherselfabottleoffishsauce. [herself=Janet]
(26.36) Janetboughtherabottleoffishsauce. [her=Janet]
(cid:54)
Recency: Entities introduced in recent utterances tend to be more salient than
those introduced from utterances further back. Thus, in (26.37), the pronoun it is
morelikelytorefertoJim’smapthanthedoctor’smap.
(26.37) Thedoctorfoundanoldmapinthecaptain’schest. Jimfoundaneven
oldermaphiddenontheshelf. Itdescribedanisland.
GrammaticalRole: Entitiesmentionedinsubjectpositionaremoresalientthan
those in object position, which are in turn more salient than those mentioned in
obliquepositions. Thusalthoughthefirstsentencein(26.38)and(26.39)expresses
roughly the same propositional content, the preferred referent for the pronoun he
varieswiththesubject—Johnin(26.38)andBillin(26.39).
(26.38) BillyBoneswenttothebarwithJimHawkins. Hecalledforaglassof
rum. [he=Billy]
(26.39) JimHawkinswenttothebarwithBillyBones. Hecalledforaglassof
rum. [he=Jim]
5 Here’saboundpronounexamplefromShakespeare’sComedyofErrors:There’snotamanImeetbut
dothsalutemeAsifIweretheirwell-acquaintedfriend
6 Theword“gender”isgenerallyonlyusedforlanguageswith2or3nounclasses,likemostIndo-
Europeanlanguages;manylanguages,liketheBantulanguagesorChinese,haveamuchlargernumber
ofnounclasses.524 CHAPTER26 • COREFERENCERESOLUTION
VerbSemantics: Someverbssemanticallyemphasizeoneoftheirarguments,bi-
asingtheinterpretationofsubsequentpronouns. Compare(26.40)and(26.41).
(26.40) JohntelephonedBill. Helostthelaptop.
(26.41) JohncriticizedBill. Helostthelaptop.
Theseexamplesdifferonlyintheverbusedinthefirstsentence,yet“he”in(26.40)
istypicallyresolvedtoJohn,whereas“he”in(26.41)isresolvedtoBill. Thismay
bepartlyduetothelinkbetweenimplicitcausalityandsaliency: theimplicitcause
of a “criticizing” event is its object, whereas the implicit cause of a “telephoning”
eventisitssubject.Insuchverbs,theentitywhichistheimplicitcausemaybemore
salient.
SelectionalRestrictions: Manyotherkindsofsemanticknowledgecanplayarole
inreferentpreference. Forexample,theselectionalrestrictionsthataverbplaceson
itsarguments(Chapter24)canhelpeliminatereferents,asin(26.42).
(26.42) Iatethesoupinmynewbowlaftercookingitforhours
Therearetwopossiblereferentsforit,thesoupandthebowl.Theverbeat,however,
requires that its direct object denote something edible, and this constraint can rule
outbowlasapossiblereferent.
26.2 Coreference Tasks and Datasets
Wecanformulatethetaskofcoreferenceresolutionasfollows: GivenatextT,find
all entities and the coreference links between them. We evaluate our task by com-
paring the links our system creates with those in human-created gold coreference
annotationsonT.
Let’sreturntoourcoreferenceexample,nowusingsuperscriptnumbersforeach
coreferencechain(cluster),andsubscriptlettersforindividualmentionsintheclus-
ter:
(26.43) [VictoriaChen]1,CFOof[MegabucksBanking]2,saw[[her]1pay]3jump
a a b a
to$2.3million,as[the38-year-old]1alsobecame[[thecompany]2’s
c b
president. Itiswidelyknownthat[she]1 cameto[Megabucks]2fromrival
d c
[Lotsabucks]4.
a
Assumingexample(26.43)wastheentiretyofthearticle,thechainsforherpayand
Lotsabucksaresingletonmentions:
1. VictoriaChen,her,the38-year-old,She
{ }
2. MegabucksBanking,thecompany,Megabucks
{ }
3. herpay
{ }
4. Lotsabucks
{ }
For most coreference evaluation campaigns, the input to the system is the raw
textofarticles,andsystemsmustdetectmentionsandthenlinkthemintoclusters.
Solving this task requires dealing with pronominal anaphora (figuring out that her
referstoVictoriaChen),filteringoutnon-referentialpronounslikethepleonasticIt
in It has been ten years), dealing with definite noun phrases to figure out that the
38-year-old is coreferent with Victoria Chen, and that the company is the same as
Megabucks. Andweneedtodealwithnames,torealizethatMegabucksisthesame
asMegabucksBanking.26.3 • MENTIONDETECTION 525
Exactlywhatcountsasamentionandwhatlinksareannotateddiffersfromtask
to task and dataset to dataset. For example some coreference datasets do not label
singletons,makingthetaskmuchsimpler.Resolverscanachievemuchhigherscores
oncorporawithoutsingletons,sincesingletonsconstitutethemajorityofmentionsin
runningtext,andtheyareoftenhardtodistinguishfromnon-referentialNPs. Some
tasks use gold mention-detection (i.e. the system is given human-labeled mention
boundariesandthetaskisjusttoclusterthesegoldmentions),whicheliminatesthe
needtodetectandsegmentmentionsfromrunningtext.
CoreferenceisusuallyevaluatedbytheCoNLLF1score,whichcombinesthree
metrics: MUC,B3,andCEAF;Section26.7givesthedetails.
e
Let’smentionafewcharacteristicsofonepopularcoreferencedataset,OntoNotes
(Pradhan et al. 2007c, Pradhan et al. 2007a), and the CoNLL 2012 Shared Task
based on it (Pradhan et al., 2012a). OntoNotes contains hand-annotated Chinese
and English coreference datasets of roughly one million words each, consisting of
newswire,magazinearticles,broadcastnews,broadcastconversations,webdataand
conversational speech data, as well as about 300,000 words of annotated Arabic
newswire. The most important distinguishing characteristic of OntoNotes is that
it does not label singletons, simplifying the coreference task, since singletons rep-
resent 60%-70% of all entities. In other ways, it is similar to other coreference
datasets. ReferringexpressionNPsthatarecoreferentaremarkedasmentions,but
genericsandpleonasticpronounsarenotmarked.Appositiveclausesarenotmarked
asseparatementions,buttheyareincludedinthemention.ThusintheNP,“Richard
Godown,presidentoftheIndustrialBiotechnologyAssociation”thementionisthe
entire phrase. Prenominal modifiers are annotated as separate entities only if they
arepropernouns. Thuswheatisnotanentityinwheatfields,butUNisanentityin
UNpolicy(butnotadjectiveslikeAmericaninAmericanpolicy).
A number of corpora mark richer discourse phenomena. The ISNotes corpus
annotatesaportionofOntoNotesforinformationstatus,includebridgingexamples
(Houetal.,2018). TheLitBankcoreferencecorpus(Bammanetal.,2020)contains
coreference annotations for 210,532 tokens from 100 different literary novels, in-
cludingsingletonsandquantifiedandnegatednounphrases.TheAnCora-COcoref-
erencecorpus(RecasensandMart´ı,2010)contains400,000wordseachofSpanish
(AnCora-CO-Es) and Catalan (AnCora-CO-Ca) news data, and includes labels for
complex phenomena like discourse deixis in both languages. The ARRAU corpus
(Uryupinaetal.,2020)contains350,000wordsofEnglishmarkingallNPs,which
means singleton clusters are available. ARRAU includes diverse genres like dialog
(theTRAINSdata)andfiction(thePearStories),andhaslabelsforbridgingrefer-
ences,discoursedeixis,generics,andambiguousanaphoricrelations.
26.3 Mention Detection
mention The first stage of coreference is mention detection: finding the spans of text that
detection
constitute each mention. Mention detection algorithms are usually very liberal in
proposingcandidatementions(i.e.,emphasizingrecall),andonlyfilteringlater. For
examplemanysystemsrunparsersandnamedentitytaggersonthetextandextract
everyspanthatiseitheranNP,apossessivepronoun,oranamedentity.
Doingsofromoursampletextrepeatedin(26.44):
(26.44) VictoriaChen,CFOofMegabucksBanking,sawherpayjumpto$2.3526 CHAPTER26 • COREFERENCERESOLUTION
million,asthe38-year-oldalsobecamethecompany’spresident. Itis
widelyknownthatshecametoMegabucksfromrivalLotsabucks.
mightresultinthefollowinglistof13potentialmentions:
VictoriaChen $2.3million she
CFOofMegabucksBanking the38-year-old Megabucks
MegabucksBanking thecompany Lotsabucks
her thecompany’spresident
herpay It
Morerecentmentiondetectionsystemsareevenmoregenerous;thespan-based
algorithm we will describe in Section 26.6 first extracts literally all n-gram spans
of words up to N=10. Of course recall from Section 26.1.3 that many NPs—and
theoverwhelmingmajorityofrandomn-gramspans—arenotreferringexpressions.
Thereforeallsuchmentiondetectionsystemsneedtoeventuallyfilteroutpleonas-
tic/expletive pronouns like It above, appositives like CFO of Megabucks Banking
Inc,orpredicatenominalslikethecompany’spresidentor$2.3million.
Someofthisfilteringcanbedonebyrules. Earlyrule-basedsystemsdesigned
regularexpressionstodealwithpleonasticit, likethefollowingrulesfromLappin
andLeass(1994)thatusedictionariesofcognitiveverbs(e.g.,believe,know,antic-
ipate)tocapturepleonasticitin“Itisthoughtthatketchup...”, ormodaladjectives
(e.g., necessary, possible, certain, important), for, e.g., “It is likely that I...”. Such
rulesaresometimesusedaspartofmodernsystems:
It is Modaladjective that S
It is Modaladjective (for NP) to VP
It is Cogv-ed that S
It seems/appears/means/follows (that) S
Mention-detectionrulesaresometimesdesignedspecificallyforparticulareval-
uationcampaigns. ForOntoNotes,forexample,mentionsarenotembeddedwithin
larger mentions, and while numeric quantities are annotated, they are rarely coref-
erential. Thus for OntoNotes tasks like CoNLL 2012 (Pradhan et al., 2012a), a
commonfirstpassrule-basedmentiondetectionalgorithm(Leeetal.,2013)is:
1. TakeallNPs,possessivepronouns,andnamedentities.
2. Remove numeric quantities (100 dollars, 8%), mentions embedded in
largermentions,adjectivalformsofnations,andstopwords(likethere).
3. Removepleonasticitbasedonregularexpressionpatterns.
Rule-based systems, however, are generally insufficient to deal with mention-
detection, andsomodernsystemsincorporatesomesortoflearnedmentiondetec-
tion component, such as a referentiality classifier, an anaphoricity classifier—
detectingwhetheranNPisananaphor—oradiscourse-newclassifier—detecting
whetheramentionisdiscourse-newandapotentialantecedentforafutureanaphor.
anaphoricity Ananaphoricitydetector,forexample,candrawitspositivetrainingexamples
detector
from any span that is labeled as an anaphoric referring expression in hand-labeled
datasetslikeOntoNotes,ARRAU,orAnCora. AnyotherNPornamedentitycanbe
markedasanegativetrainingexample. Anaphoricityclassifiersusefeaturesofthe
candidatementionsuchasitsheadword,surroundingwords,definiteness,animacy,
length, position in the sentence/discourse, many of which were first proposed in
earlyworkbyNgandCardie(2002a);seeSection26.5formoreonfeatures.26.3 • MENTIONDETECTION 527
Referentialityoranaphoricitydetectorscanberunasfilters,inwhichonlymen-
tionsthatareclassifiedasanaphoricorreferentialarepassedontothecoreference
system. Theendresultofsuchafilteringmentiondetectionsystemonourexample
abovemightbethefollowingfilteredsetof9potentialmentions:
VictoriaChen herpay she
MegabucksBank the38-year-old Megabucks
her thecompany Lotsabucks
It turns out, however, that hard filtering of mentions based on an anaphoricity
or referentiality classifier leads to poor performance. If the anaphoricity classifier
thresholdissettoohigh,toomanymentionsarefilteredoutandrecallsuffers. Ifthe
classifier threshold is set too low, too many pleonastic or non-referential mentions
areincludedandprecisionsuffers.
Themodernapproachisinsteadtoperformmentiondetection,anaphoricity,and
coreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge
2007, Rahman and Ng 2009). For example mention detection in the Lee et al.
(2017b),2018systemisbasedonasingleend-to-endneuralnetworkthatcomputes
ascoreforeachmentionbeingreferential,ascorefortwomentionsbeingcorefer-
ence,andcombinesthemtomakeadecision,trainingallthesescoreswithasingle
end-to-endloss. We’lldescribethismethodindetailinSection26.6. 7
Despitetheseadvances,correctlydetectingreferentialmentionsseemstostillbe
an unsolved problem, since systems incorrectly marking pleonastic pronouns like
itandothernon-referentialNPsascoreferentisalargesourceoferrorsofmodern
coreferenceresolutionsystems(KummerfeldandKlein2013,MartschatandStrube
2014,MartschatandStrube2015,Wisemanetal.2015,Leeetal.2017a).
Mention,referentiality,oranaphoricitydetectionisthusanimportantopenarea
ofinvestigation. Othersourcesofknowledgemayturnouttobehelpful,especially
in combination with unsupervised and semisupervised algorithms, which also mit-
igate the expense of labeled datasets. In early work, for example Bean and Riloff
(1999)learnedpatternsforcharacterizinganaphoricornon-anaphoricNPs;(byex-
tracting and generalizing over the first NPs in a text, which are guaranteed to be
non-anaphoric). Chang et al. (2012) look for head nouns that appear frequently in
thetrainingdatabutneverappearasgoldmentionstohelpfindnon-referentialNPs.
Bergsmaetal.(2008b)usewebcountsasasemisupervisedwaytoaugmentstandard
featuresforanaphoricitydetectionforEnglishit,animportanttaskbecauseitisboth
commonandambiguous;betweenaquarterandhalfitexamplesarenon-anaphoric.
Considerthefollowingtwoexamples:
(26.45) Youcanmake[it]inadvance. [anaphoric]
(26.46) Youcanmake[it]inHollywood. [non-anaphoric]
Theitinmakeitisnon-anaphoric,partoftheidiommakeit. Bergsmaetal.(2008b)
turnthecontextaroundeachexampleintopatterns,like“make*inadvance”from
(26.45),and“make*inHollywood”from(26.46).TheythenuseGooglen-gramsto
enumerateallthewordsthatcanreplaceitinthepatterns. Non-anaphoriccontexts
tendtoonlyhaveitinthewildcardpositions, whileanaphoriccontextsoccurwith
manyotherNPs(forexamplemaketheminadvanceisjustasfrequentintheirdata
7 Somesystemstrytoavoidmentiondetectionoranaphoricitydetectionaltogether. Fordatasetslike
OntoNoteswhichdon’tlabelsingletons,analternativetofilteringoutnon-referentialmentionsistorun
coreferenceresolution,andthensimplydeleteanycandidatementionswhichwerenotcoreferedwith
anothermention.Thislikelydoesn’tworkaswellasexplicitlymodelingreferentiality,andcannotsolve
theproblemofdetectingsingletons,whichisimportantfortaskslikeentitylinking.528 CHAPTER26 • COREFERENCERESOLUTION
as make it in advance, but make them in Hollywood did not occur at all). These
n-gramcontextscanbeusedasfeaturesinasupervisedanaphoricityclassifier.
26.4 Architectures for Coreference Algorithms
Modern systems for coreference are based on supervised neural machine learning,
supervisedfromhand-labeleddatasetslikeOntoNotes. Inthissectionweoverview
thevariousarchitectureofmodernsystems, usingthecategorizationofNg(2010),
whichdistinguishesalgorithmsbasedonwhethertheymakeeachcoreferencedeci-
sioninawaythatisentity-based—representingeachentityinthediscoursemodel—
oronlymention-based—consideringeachmentionindependently,andwhetherthey
use ranking models to directly compare potential antecedents. Afterwards, we go
intomoredetailononestate-of-the-artalgorithminSection26.6.
26.4.1 TheMention-PairArchitecture
mention-pair We begin with the mention-pair architecture, the simplest and most influential
coreference architecture, which introduces many of the features of more complex
mention-pair algorithms, even though other architectures perform better. The mention-pair ar-
chitecture is based around a classifier that— as its name suggests—is given a pair
of mentions, a candidate anaphor and a candidate antecedent, and makes a binary
classificationdecision: coreferringornot.
Let’sconsiderthetaskofthisclassifierforthepronounsheinourexample,and
assumetheslightlysimplifiedsetofpotentialantecedentsinFig.26.2.
p(coref|”Victoria Chen”,”she”)
Victoria Chen Megabucks Banking her her pay the 37-year-old she
p(coref|”Megabucks Banking”,”she”)
Figure26.2 Foreachpairofamention(likeshe),andapotentialantecedentmention(like
VictoriaChenorher),themention-pairclassifierassignsaprobabilityofacoreferencelink.
Foreachpriormention(VictoriaChen,MegabucksBanking,her,etc.),thebinary
classifier computes a probability: whether or not the mention is the antecedent of
she. Wewantthisprobabilitytobehighforactualantecedents(VictoriaChen,her,
the38-year-old)andlowfornon-antecedents(MegabucksBanking,herpay).
Earlyclassifiersusedhand-builtfeatures(Section26.5); morerecentclassifiers
useneuralrepresentationlearning(Section26.6)
Fortraining,weneedaheuristicforselectingtrainingsamples;sincemostpairs
of mentions in a document are not coreferent, selecting every pair would lead to
a massive overabundance of negative samples. The most common heuristic, from
(Soonetal.,2001),istochoosetheclosestantecedentasapositiveexample,andall
pairsinbetweenasthenegativeexamples.Moreformally,foreachanaphormention
m wecreate
i
• onepositiveinstance(m,m )wherem istheclosestantecedenttom,and
i j j i26.4 • ARCHITECTURESFORCOREFERENCEALGORITHMS 529
• anegativeinstance(m,m )foreachm betweenm andm
i k k j i
Thus for the anaphor she, we would choose (she, her) as the positive example
andnonegativeexamples. Similarly,fortheanaphorthecompanywewouldchoose
(thecompany,Megabucks)asthepositiveexampleand(thecompany,she)(thecom-
pany, the 38-year-old) (the company, her pay) and (the company, her) as negative
examples.
Once the classifier is trained, it is applied to each test sentence in a clustering
step.Foreachmentioniinadocument,theclassifierconsiderseachofthepriori 1
−
mentions. Inclosest-firstclustering(Soonetal.,2001),theclassifierisrunrightto
left(frommentioni 1downtomention1)andthefirstantecedentwithprobability
−
>.5islinkedtoi. Ifnoantecedenthasprobably>0.5,noantecedentisselectedfor
i. Inbest-firstclustering,theclassifierisrunonalli 1antecedentsandthemost
−
probableprecedingmentionischosenastheantecedentfori. Thetransitiveclosure
ofthepairwiserelationistakenasthecluster.
Whilethemention-pairmodelhastheadvantageofsimplicity, ithastwomain
problems. First, the classifier doesn’t directly compare candidate antecedents to
eachother,soit’snottrainedtodecide,betweentwolikelyantecedents,whichone
isinfact better. Second, itignoresthediscourse model, lookingonlyatmentions,
notentities. Eachclassifierdecisionismadecompletelylocallytothepair,without
being able to take into account other mentions of the same entity. The next two
modelseachaddressoneofthesetwoflaws.
26.4.2 TheMention-RankArchitecture
Thementionrankingmodeldirectlycomparescandidateantecedentstoeachother,
choosingthehighest-scoringantecedentforeachanaphor.
Inearlyformulations,formentioni,theclassifierdecideswhichofthe 1,...,i
{ −
1 prior mentions is the antecedent (Denis and Baldridge, 2008). But suppose i is
}
infactnotanaphoric,andnoneoftheantecedentsshouldbechosen? Suchamodel
wouldneedtorunaseparateanaphoricityclassifieroni. Instead,itturnsouttobe
bettertojointlylearnanaphoricitydetectionandcoreferencetogetherwithasingle
loss(RahmanandNg,2009).
Soinmodernmention-rankingsystems,fortheithmention(anaphor),wehave
anassociatedrandomvariabley rangingoverthevaluesY(i)= 1,...,i 1,(cid:15) . The
i
{ − }
value(cid:15)isaspecialdummymentionmeaningthatidoesnothaveanantecedent(i.e.,
iseitherdiscourse-newandstartsanewcorefchain,orisnon-anaphoric).
}
p(”Victoria Chen”|”she”) p(”her”|she”) p(”the 37-year-old”|she”)
One or more
of these
should be high
ϵ Victoria Chen Megabucks Banking her her pay the 37-year-old she}
All of these
should be low
p(ϵ|”she”) p(”Megabucks Banking”|she”) p(”her pay”|she”)
Figure26.3 Foreachcandidateanaphoricmention(likeshe),themention-rankingsystemassignsaproba-
bilitydistributionoverallpreviousmentionsplusthespecialdummymention(cid:15).
Attesttime,foragivenmentionithemodelcomputesonesoftmaxoverallthe
antecedents (plus (cid:15)) giving a probability for each candidate antecedent (or none).530 CHAPTER26 • COREFERENCERESOLUTION
Fig. 26.3 shows an example of the computation for the single candidate anaphor
she.
Oncetheantecedentisclassifiedforeachanaphor,transitiveclosurecanberun
overthepairwisedecisionstogetacompleteclustering.
Trainingistrickierinthemention-rankingmodelthanthemention-pairmodel,
becauseforeachanaphorwedon’tknowwhichofallthepossiblegoldantecedents
to use for training. Instead, the best antecedent for each mention is latent; that
is, for each mention we have a whole cluster of legal gold antecedents to choose
from. Earlyworkusedheuristicstochooseanantecedent,forexamplechoosingthe
closest antecedent as the gold antecedent and all non-antecedents in a window of
twosentencesasthenegativeexamples(DenisandBaldridge,2008). Variouskinds
ofwaystomodellatentantecedentsexist(Fernandesetal.2012,Changetal.2013,
DurrettandKlein2013). Thesimplestwayistogivecredittoanylegalantecedent
by summing over all of them, with a loss function that optimizes the likelihood of
all correct antecedents from the gold clustering (Lee et al., 2017b). We’ll see the
detailsinSection26.6.
Mention-ranking models can be implemented with hand-build features or with
neural representation learning (which might also incorporate some hand-built fea-
tures). we’llexplorebothdirectionsinSection26.5andSection26.6.
26.4.3 Entity-basedModels
Boththemention-pairandmention-rankingmodelsmaketheirdecisionsaboutmen-
tions. Bycontrast,entity-basedmodelslinkeachmentionnottoapreviousmention
buttoapreviousdiscourseentity(clusterofmentions).
A mention-ranking model can be turned into an entity-ranking model simply
by having the classifier make its decisions over clusters of mentions rather than
individualmentions(RahmanandNg,2009).
Fortraditionalfeature-basedmodels,thiscanbedonebyextractingfeaturesover
clusters. The size of a cluster is a useful feature, as is its ‘shape’, which is the
list of types of the mentions in the cluster i.e., sequences of the tokens (P)roper,
(D)efinite,(I)ndefinite,(Pr)onoun,sothataclustercomposedof Victoria,her,the
{
38-year-old wouldhavetheshapeP-Pr-D(Bjo¨rkelundandKuhn,2014).Anentity-
}
basedmodelthatincludesamention-pairclassifiercanuseasfeaturesaggregatesof
mention-pairprobabilities,forexamplecomputingtheaverageprobabilityofcoref-
erenceoverallmention-pairsinthetwoclusters(ClarkandManning2015).
Neural models can learn representations of clusters automatically, for example
byusinganRNNoverthesequenceofclustermentionstoencodeastatecorrespond-
ingtoaclusterrepresentation(Wisemanetal.,2016),orbylearningdistributedrep-
resentationsforpairsofclustersbypoolingoverlearnedrepresentationsofmention
pairs(ClarkandManning,2016b).
However,althoughentity-basedmodelsaremoreexpressive,theuseofcluster-
levelinformationinpracticehasnotledtolargegainsinperformance,somention-
rankingmodelsarestillmorecommonlyused.
26.5 Classifiers using hand-built features
Hand-designed features play an important role in coreference, whether as the sole
inputtoclassificationinpre-neuralclassifiers,orasaugmentationstotheautomatic26.5 • CLASSIFIERSUSINGHAND-BUILTFEATURES 531
representationlearningusedinstate-of-the-artneuralsystemsliketheonewe’llde-
scribeinSection26.6.
Inthissectionwedescribefeaturescommonlyusedinlogisticregression,SVM,
orrandomforestclassifiersforcoreferenceresolution.
Given an anaphor mention and a potential antecedent mention, most feature
basedclassifiersmakeuseofthreetypesoffeatures: (i)featuresoftheanaphor,(ii)
features of the candidate antecedent, and (iii) features of the relationship between
thepair.Entity-basedmodelscanmakeadditionaluseoftwoadditionalclasses:(iv)
feature of all mentions from the antecedent’s entity cluster, and (v) features of the
relationbetweentheanaphorandthementionsintheantecedententitycluster.
FeaturesoftheAnaphororAntecedentMention
First(last)word Victoria/she Firstorlastword(orembedding)ofantecedent/anaphor
Headword Victoria/she Headword(orheadembedding)ofantecedent/anaphor
Attributes Sg-F-A-3-PER/ The number, gender, animacy, person, named entity type
Sg-F-A-3-PER attributesof(antecedent/anaphor)
Length 2/1 lengthinwordsof(antecedent/anaphor)
Grammaticalrole Sub/Sub The grammatical role—subject, direct object, indirect
object/PP—of(antecedent/anaphor)
Mentiontype P/Pr Type: (P)roper, (D)efinite, (I)ndefinite, (Pr)onoun)ofan-
tecedent/anaphor
FeaturesoftheAntecedentEntity
Entityshape P-Pr-D The ‘shape’ or list of types of the mentions in the
antecedent entity (cluster), i.e., sequences of (P)roper,
(D)efinite,(I)ndefinite,(Pr)onoun.
Entityattributes Sg-F-A-3-PER The number, gender, animacy, person, named entity type
attributesoftheantecedententity
Ant.clustersize 3 Numberofmentionsintheantecedentcluster
FeaturesofthePairofMentions
Longeranaphor F Trueifanaphorislongerthanantecedent
Pairsofanyfeatures Victoria/she, For each individual feature, pair of type of antecedent+
2/1,P/Pr,etc. typeofanaphor
Sentencedistance 1 Thenumberofsentencesbetweenantecedentandanaphor
Mentiondistance 4 Thenumberofmentionsbetweenantecedentandanaphor
i-within-i F Anaphorhasi-within-irelationwithantecedent
Cosine Cosinebetweenantecedentandanaphorembeddings
Appositive F Trueiftheanaphorisinthesyntacticappositionrelationto
theantecedent. Usefulevenifappositivesaren’tmentions
(toknowtoattachtheappositivetoaprecedinghead)
FeaturesofthePairofEntities
ExactStringMatch F Trueifthestringsofanytwomentionsfromtheantecedent
andanaphorclustersareidentical.
HeadWordMatch F True if any mentions from antecedent cluster has same
headwordasanymentioninanaphorcluster
WordInclusion F Allwordsinanaphorclusterincludedinantecedentcluster
FeaturesoftheDocument
Genre/source N Thedocumentgenre—(D)ialog,(N)ews,etc,
Figure26.4 Feature-based coreference: sample feature values for anaphor “she” and potential antecedent
“VictoriaChen”.
Figure26.4showsaselectionofcommonlyusedfeatures,andshowsthevalue532 CHAPTER26 • COREFERENCERESOLUTION
that would be computed for the potential anaphor “she” and potential antecedent
“VictoriaChen”inourexamplesentence,repeatedbelow:
(26.47) VictoriaChen,CFOofMegabucksBanking,sawherpayjumpto$2.3
million,asthe38-year-oldalsobecamethecompany’spresident. Itis
widelyknownthatshecametoMegabucksfromrivalLotsabucks.
Features that prior work has found to be particularly useful are exact string
match,entityheadwordagreement,mentiondistance,aswellas(forpronouns)exact
attributematchandi-within-i,and(fornominalsandpropernames)wordinclusion
andcosine.Forlexicalfeatures(likeheadwords)itiscommontoonlyusewordsthat
appear enough times (perhaps more than 20 times), backing off to parts of speech
forrarewords.
Itiscrucialinfeature-basedsystemstouseconjunctionsoffeatures;oneexper-
imentsuggestedthatmovingfromindividualfeaturesinaclassifiertoconjunctions
ofmultiplefeaturesincreasedF1by4points(Leeetal.,2017a). Specificconjunc-
tionscanbedesignedbyhand(DurrettandKlein,2013),allpairsoffeaturescanbe
conjoined(BengtsonandRoth,2008),orfeatureconjunctionscanbelearnedusing
decisiontreeorrandomforestclassifiers(NgandCardie2002a,Leeetal.2017a).
Finally,someofthesefeaturescanalsobeusedinneuralmodelsaswell.Modern
neuralsystems(Section26.6)usecontextualwordembeddings,sotheydon’tbenefit
fromaddingshallowfeatureslikestringorheadmatch,grammaticalrole,ormention
types. However other features like mention length, distance between mentions, or
genrecancomplementneuralcontextualembeddingmodelsnicely.
26.6 A neural mention-ranking algorithm
In this section we describe the neural e2e-coref algorithms of Lee et al. (2017b)
(simplified and extended a bit, drawing on Joshi et al. (2019) and others). This is
amention-rankingalgorithmthatconsidersallpossiblespansoftextinthedocu-
ment,assignsamention-scoretoeachspan,prunesthementionsbasedonthisscore,
thenassignscoreferencelinkstotheremainingmentions.
More formally, given a document D with T words, the model considers all of
the
T(T+1)
text spans in D (unigrams, bigrams, trigrams, 4-grams, etc; in practice
2
we only consider spans up a maximum length around 10). The task is to assign
to each span i an antecedent y, a random variable ranging over the valuesY(i)=
i
1,...,i 1,(cid:15) ; each previous span and a special dummy token (cid:15). Choosing the
{ − }
dummytokenmeansthatidoesnothaveanantecedent,eitherbecauseiisdiscourse-
newandstartsanewcoreferencechain,orbecauseiisnon-anaphoric.
For each pair of spans i and j, the system assigns a score s(i,j) for the coref-
erencelinkbetweenspaniandspan j. ThesystemthenlearnsadistributionP(y)
i
overtheantecedentsforspani:
exp(s(i,y))
i
P(y i) = (26.48)
exp(s(i,y))
y Y(i) (cid:48)
(cid:48)∈
Thisscores(i,j)includesthreefacto(cid:80)rsthatwe’lldefinebelow: m(i);whetherspan
i is a mention; m(j); whether span j is a mention; and c(i,j); whether j is the
antecedentofi:
s(i,j)=m(i)+m(j)+c(i,j) (26.49)26.6 • ANEURALMENTION-RANKINGALGORITHM 533
For the dummy antecedent (cid:15), the score s(i,(cid:15)) is fixed to 0. This way if any non-
dummyscoresarepositive,themodelpredictsthehighest-scoringantecedent,butif
allthescoresarenegativeitabstains.
26.6.1 Computingspanrepresentations
Tocomputethetwofunctionsm(i)andc(i,j)whichscoreaspaniorapairofspans
(i,j), we’ll need a way to represent a span. The e2e-coref family of algorithms
represents each span by trying to capture 3 words/tokens: the first word, the last
word, and the most important word. We first run each paragraph or subdocument
through an encoder (like BERT) to generate embeddings h for each token i. The
i
spaniisthenrepresentedbyavectorg thatisaconcatenationoftheencoderoutput
i
embeddingforthefirst(start)tokenofthespan,theencoderoutputforthelast(end)
tokenofthespan,andathirdvectorwhichisanattention-basedrepresentation:
g i=[h START(i),h END(i),h ATT(i)] (26.50)
The goal of the attention vector is to represent which word/token is the likely
syntactic head-word of the span; we saw in the prior section that head-words are
a useful feature; a matching head-word is a good indicator of coreference. The
attentionrepresentationiscomputedasusual;thesystemlearnsaweightvectorw ,
α
andcomputesitsdotproductwiththehiddenstateh transformedbyaFFN:
t
α t =w α FFNα(h t) (26.51)
·
Theattentionscoreisnormalizedintoadistributionviaasoftmax:
exp(α)
t
a i,t = END(i) (26.52)
exp(α )
k=START(i) k
And then the attention distribution(cid:80) is used to create a vector h which is an
ATT(i)
attention-weightedsumoftheembeddingse ofeachofthewordsinspani:
t
END(i)
h ATT(i) = a i,t ·e t (26.53)
t=ST (cid:88)ART(i)
Fig. 26.5 shows the computation of the span representation and the mention
score.
General Electric Electric said the the Postal Service Service contacted the the company
Mention score (m)
Span representation (g)
Span head (h ) + + + + +
ATT
Encodings (h)
Encoder
… General Electric said the Postal Service contacted the company
Figure26.5 Computationofthespanrepresentationg(andthementionscorem)inaBERTversionofthe
e2e-corefmodel(Leeetal.2017b,Joshietal.2019).Themodelconsidersallspansuptoamaximumwidthof
say10;thefigureshowsasmallsubsetofthebigramandtrigramspans.534 CHAPTER26 • COREFERENCERESOLUTION
26.6.2 Computingthementionandantecedentscoresmandc
Now that we know how to compute the vector g for representing span i, we can
i
seethedetailsofthetwoscoringfunctionsm(i)andc(i,j). Botharecomputedby
feedforwardnetworks:
m(i) = w m FFN m(g i) (26.54)
·
c(i,j) = w c FFN c([g i,g j,g i g j,]) (26.55)
· ◦
Atinferencetime,thismentionscoremisusedasafiltertokeeponlythebestfew
mentions.
Wethencomputetheantecedentscoreforhigh-scoringmentions.Theantecedent
scorec(i,j)takesasinputarepresentationofthespansiand j,butalsotheelement-
wise similarity of the two spans to each other g g (here is element-wise mul-
i j
◦ ◦
tiplication). Fig. 26.6 shows the computation of the score s for the three possible
antecedentsofthecompanyintheexamplesentencefromFig.26.5.
Figure26.6 Thecomputationofthescoresforthethreepossibleantecedentsofthecom-
panyintheexamplesentencefromFig.26.5.FigureafterLeeetal.(2017b).
Given the set of mentions, the joint distribution of antecedents for each docu-
ment is computed in a forward pass, and we can then do transitive closure on the
antecedentstocreateafinalclusteringforthedocument.
Fig. 26.7 shows example predictions from the model, showing the attention
weights, which Lee et al. (2017b) find correlate with traditional semantic heads.
Notethatthemodelgetsthesecondexamplewrong,presumablybecauseattendants
andpilotlikelyhavenearbywordembeddings.
Figure26.7 Sample predictions from the Lee et al. (2017b) model, with one cluster per
example,showingonecorrectexampleandonemistake.Bold,parenthesizedspansaremen-
tionsinthepredictedcluster. Theamountofredcoloronawordindicatesthehead-finding
attentionweightai,t in(26.52).FigureadaptedfromLeeetal.(2017b).26.7 • EVALUATIONOFCOREFERENCERESOLUTION 535
26.6.3 Learning
For training, we don’t have a single gold antecedent for each mention; instead the
coreference labeling only gives us each entire cluster of coreferent mentions; so a
mention only has a latent antecedent. We therefore use a loss function that maxi-
mizesthesumofthecoreferenceprobabilityofanyofthelegalantecedents. Fora
givenmentioniwithpossibleantecedentsY(i), let GOLD(i)bethesetofmentions
inthegoldclustercontainingi. SincethesetofmentionsoccurringbeforeiisY(i),
thesetofmentionsinthatgoldclusterthatalsooccurbeforeiisY(i) GOLD(i).We
∩
thereforewanttomaximize:
P(yˆ) (26.56)
yˆ ∈Y(i) ∩(cid:88)GOLD(i)
IfamentioniisnotinagoldclusterGOLD(i)=(cid:15).
To turn this probability into a loss function, we’ll use the cross-entropy loss
functionwedefinedinEq.5.22inChapter5,bytakingthe logoftheprobability.
−
Ifwethensumoverallmentions,wegetthefinallossfunctionfortraining:
N
L= log P(yˆ) (26.57)
−
(cid:88)i=2 yˆ ∈Y(i) ∩(cid:88)GOLD(i)
26.7 Evaluation of Coreference Resolution
Weevaluatecoreferencealgorithmsmodel-theoretically,comparingasetofhypoth-
esischainsorclustersH producedbythesystemagainstasetofgoldorreference
chainsorclustersRfromahumanlabeling,andreportingprecisionandrecall.
However,thereareawidevarietyofmethodsfordoingthiscomparison. Infact,
thereare5commonmetricsusedtoevaluatecoreferencealgorithms: thelinkbased
MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy 2011, Luo et al. 2014)
metrics,thementionbasedB3 metric(BaggaandBaldwin,1998),theentitybased
CEAFmetric(Luo,2005),andthelinkbasedentityawareLEAmetric(Moosaviand
Strube,2016).
MUC Let’sjustexploretwoofthemetrics. TheMUCF-measure(Vilainetal.,1995)
F-measure
isbasedonthenumberofcoreferencelinks(pairsofmentions)commontoH and
R. Precision is the number of common links divided by the number of links in H.
Recall is the number of common links divided by the number of links in R; This
makes MUC biased toward systems that produce large chains (and fewer entities),
anditignoressingletons,sincetheydon’tinvolvelinks.
3 B3 is mention-based rather than link-based. For each mention in the reference
B
chain,wecomputeaprecisionandrecall,andthenwetakeaweightedsumoverall
Nmentionsinthedocumenttocomputeaprecisionandrecallfortheentiretask.For
agivenmentioni,letRbethereferencechainthatincludesi,andH thehypothesis
chainthathasi. ThesetofcorrectmentionsinH isH R. Precisionformentioni
H R H R ∩
isthus | ∩ |,andrecallformentionithus | ∩ |. Thetotalprecisionistheweighted
H R
sumofth|e|precisionformentioni,weighted| |byaweightw. Thetotalrecallisthe
i536 CHAPTER26 • COREFERENCERESOLUTION
weightedsumoftherecallformentioni,weightedbyaweightw. Equivalently:
i
N
#ofcorrectmentionsinhypothesischaincontainingentity
i
Precision = w
i
#ofmentionsinhypothesischaincontainingentity
i
i=1
(cid:88)
N
#ofcorrectmentionsinhypothesischaincontainingentity
i
Recall = w
i
#ofmentionsinreferencechaincontainingentity
i
i=1
(cid:88)
The weight w for each entity can be set to different values to produce different
i
versionsofthealgorithm.
FollowingaproposalfromDenisandBaldridge(2009),theCoNLLcoreference
competitionswerescoredbasedontheaverageofMUC,CEAF-e,andB3 (Pradhan
etal.2011,Pradhanetal.2012b),andsoitiscommoninmanyevaluationcampaigns
toreportanaverageofthese3metrics. SeeLuoandPradhan(2016)foradetailed
description of the entire set of metrics; reference implementations of these should
beusedratherthanattemptingtoreimplementfromscratch(Pradhanetal.,2014).
Alternativemetricshavebeenproposedthatdealwithparticularcoreferencedo-
mains or tasks. For example, consider the task of resolving mentions to named
entities(persons,organizations,geopoliticalentities),whichmightbeusefulforin-
formation extraction or knowledge base completion. A hypothesis chain that cor-
rectlycontainsallthepronounsreferringtoanentity,buthasnoversionofthename
itself,orislinkedwithawrongname,isnotusefulforthistask. Wemightinstead
wantametricthatweightseachmentionbyhowinformativeitis(withnamesbeing
most informative) (Chen and Ng, 2013) or a metric that considers a hypothesis to
match a gold chain only if it contains at least one variant of a name (the NEC F1
metricofAgarwaletal.(2019)).
26.8 Winograd Schema problems
From early on in the field, researchers have noted that some cases of coreference
are quite difficult, seeming to require world knowledge or sophisticated reasoning
tosolve. TheproblemwasmostfamouslypointedoutbyWinograd(1972)withthe
followingexample:
(26.58) Thecitycouncildeniedthedemonstratorsapermitbecause
a. theyfearedviolence.
b. theyadvocatedviolence.
Winograd noticed that the antecedent that most readers preferred for the pro-
nountheyincontinuation(a)wasthecitycouncil,butin(b)wasthedemonstrators.
He suggested that this requires understanding that the second clause is intended
as an explanation of the first clause, and also that our cultural frames suggest that
city councils are perhaps more likely than demonstrators to fear violence and that
demonstratorsmightbemorelikelytoadvocateviolence.
InanattempttogetthefieldofNLPtofocusmoreonmethodsinvolvingworld
knowledge and common-sense reasoning, Levesque (2011) proposed a challenge
Winograd taskcalledtheWinogradSchemaChallenge.8 Theproblemsinthechallengetask
schema
8 Levesque’scallwasquicklyfollowedupbyLevesqueetal.(2012)andRahmanandNg(2012), a
competitionattheIJCAIconference(Davisetal.,2017),andanaturallanguageinferenceversionofthe
problemcalledWNLI(Wangetal.,2018a).26.9 • GENDERBIASINCOREFERENCE 537
arecoreferenceproblemsdesignedtobeeasilydisambiguatedbythehumanreader,
but hopefully not solvable by simple techniquessuch as selectional restrictions, or
otherbasicwordassociationmethods.
The problemsare framedas apair ofstatements thatdiffer ina singleword or
phrase,andacoreferencequestion:
(26.59) Thetrophydidn’tfitintothesuitcasebecauseitwastoolarge.
Question: Whatwastoolarge? Answer: Thetrophy
(26.60) Thetrophydidn’tfitintothesuitcasebecauseitwastoosmall.
Question: Whatwastoosmall? Answer: Thesuitcase
Theproblemshavethefollowingcharacteristics:
1. Theproblemseachhavetwoparties
2. Apronounpreferentiallyreferstooneoftheparties,butcouldgrammatically
alsorefertotheother
3. Aquestionaskswhichpartythepronounrefersto
4. Ifonewordinthequestionischanged,thehuman-preferredanswerchanges
totheotherparty
The kind of world knowledge that might be needed to solve the problems can
vary. Inthetrophy/suitcaseexample,itisknowledgeaboutthephysicalworld;that
abiggerobjectcannotfitintoasmallerobject. IntheoriginalWinogradsentence,
itisstereotypesaboutsocialactorslikepoliticiansandprotesters. Inexampleslike
thefollowing,itisknowledgeabouthumanactionsliketurn-takingorthanking.
(26.61) BillpassedthegameboytoJohnbecausehisturnwas[over/next]. Whose
turnwas[over/next]? Answers: Bill/John
(26.62) JoanmadesuretothankSusanforallthehelpshehad[given/received].
Whohad[given/received]help? Answers: Susan/Joan.
Although the Winograd Schema was designed to require common-sense rea-
soning, a large percentage of the original set of problem can be solved by pre-
trainedlanguagemodels,fine-tunedonWinogradSchemasentences(Kocijanetal.,
2019). Largepretrainedlanguagemodelsencodeanenormousamountofworldor
common-sense knowledge! The current trend is therefore to propose new datasets
withincreasinglydifficultWinograd-likecoreferenceresolutionproblemslikeKNOWREF
(Emamietal.,2019),withexampleslike:
(26.63) MarcusisundoubtedlyfasterthanJarrettrightnowbutin[his]primethe
gapwasn’tallthatbig.
Intheend,itseemslikelythatsomecombinationoflanguagemodelingandknowl-
edgewillprovefruitful; indeed, itseemsthatknowledge-basedmodelsoverfitless
tolexicalidiosyncraciesinWinogradSchematrainingsets(Trichelairetal.,2018),
26.9 Gender Bias in Coreference
Aswithotheraspectsoflanguageprocessing,coreferencemodelsexhibitgenderand
otherbiases(Zhaoetal.2018a,Rudingeretal.2018,Websteretal.2018).Forexam-
pletheWinoBiasdataset(Zhaoetal.,2018a)usesavariantoftheWinogradSchema
paradigmtotesttheextenttowhichcoreferencealgorithmsarebiasedtowardlink-
inggenderedpronounswithantecedentsconsistentwithculturalstereotypes. Aswe538 CHAPTER26 • COREFERENCERESOLUTION
summarizedinChapter6,embeddingsreplicatesocietalbiasesintheirtrainingtest,
suchasassociatingmenwithhistoricallysterotypicalmaleoccupationslikedoctors,
and women with stereotypical female occupations like secretaries (Caliskan et al.
2017,Gargetal.2018).
A WinoBias sentence contain two mentions corresponding to stereotypically-
male and stereotypically-female occupations and a gendered pronoun that must be
linkedtooneofthem. Thesentencecannotbedisambiguatedbythegenderofthe
pronoun, but a biased model might be distracted by this cue. Here is an example
sentence:
(26.64) Thesecretarycalledthephysician andtoldhim aboutanewpatient
i i
[pro-stereotypical]
(26.65) Thesecretarycalledthephysician andtoldher aboutanewpatient
i i
[anti-stereotypical]
Zhaoetal.(2018a)consideracoreferencesystemtobebiasedifitismoreaccu-
rateatlinkingpronounsconsistentwithgenderstereotypicaloccupations(e.g.,him
withphysicianin(26.64))thanlinkingpronounsinconsistentwithgender-stereotypical
occupations (e.g., her with physician in (26.65)). They show that coreference sys-
tems of all architectures (rule-based, feature-based machine learned, and end-to-
end-neural)allshowsignificantbias, performingonaverage21F pointsworsein
1
theanti-stereotypicalcases.
One possible source of this bias is that female entities are significantly un-
derrepresented in the OntoNotes dataset, used to train most coreference systems.
Zhao et al. (2018a) propose a way to overcome this bias: they generate a second
gender-swapped dataset in which all male entities in OntoNotes are replaced with
femaleonesandviceversa,andretraincoreferencesystemsonthecombinedorig-
inalandswappedOntoNotesdata,alsousingdebiasedGloVEembeddings(Boluk-
basi et al., 2016). The resulting coreference systems no longer exhibit bias on the
WinoBiasdataset,withoutsignificantlyimpactingOntoNotescoreferenceaccuracy.
In a follow-up paper, Zhao et al. (2019) show that the same biases exist in ELMo
contextualized word vector representations and coref systems that use them. They
showedthatretrainingELMowithdataaugmentationagainreducesorremovesbias
incoreferencesystemsonWinoBias.
Websteretal.(2018)introducesanotherdataset,GAP,andthetaskofGendered
Pronoun Resolution as a tool for developing improved coreference algorithms for
gendered pronouns. GAP is a gender-balanced labeled corpus of 4,454 sentences
with gendered ambiguous pronouns (by contrast, only 20% of the gendered pro-
nouns in the English OntoNotes training data are feminine). The examples were
createdbydrawingonnaturallyoccurringsentencesfromWikipediapagestocreate
hardtoresolvecaseswithtwonamedentitiesofthesamegenderandanambiguous
pronounthatmayrefertoeitherperson(orneither),likethefollowing:
(26.66) InMay,FujisawajoinedMariMotohashi’srinkastheteam’sskip,moving
backfromKaruizawatoKitamiwhereshehadspentherjuniordays.
Websteretal.(2018)showthatmoderncoreferencealgorithmsperformsignif-
icantly worse on resolving feminine pronouns than masculine pronouns in GAP.
Kuritaetal.(2019)showsthatasystembasedonBERTcontextualizedwordrepre-
sentationsshowssimilarbias.26.10 • SUMMARY 539
26.10 Summary
Thischapterintroducedthetaskofcoreferenceresolution.
• Thisisthetaskoflinkingtogethermentionsintextwhichcorefer,i.e. refer
to the same discourse entity in the discourse model, resulting in a set of
coreferencechains(alsocalledclustersorentities).
• Mentions can be definite NPs or indefinite NPs, pronouns (including zero
pronouns)ornames.
• The surface form of an entity mention is linked to its information status
(new,old,orinferrable),andhowaccessibleorsalienttheentityis.
• SomeNPsarenotreferringexpressions,suchaspleonasticitinItisraining.
• Many corpora have human-labeled coreference annotations that can be used
forsupervisedlearning,includingOntoNotesforEnglish,Chinese,andAra-
bic,ARRAUforEnglish,andAnCoraforSpanishandCatalan.
• Mention detection can start with all nouns and named entities and then use
anaphoricityclassifiersorreferentialityclassifierstofilteroutnon-mentions.
• Threecommonarchitecturesforcoreferencearemention-pair,mention-rank,
andentity-based,eachofwhichcanmakeuseoffeature-basedorneuralclas-
sifiers.
• Modern coreference systems tend to be end-to-end, performing mention de-
tectionandcoreferenceinasingleend-to-endarchitecture.
• Algorithmslearnrepresentationsfortextspansandheads,andlearntocom-
pareanaphorspanswithcandidateantecedentspans.
• Coreferencesystemsareevaluatedbycomparingwithgoldentitylabelsusing
precision/recallmetricslikeMUC,B3,CEAF,BLANC,orLEA.
• The Winograd Schema Challenge problems are difficult coreference prob-
lemsthatseemtorequireworldknowledgeorsophisticatedreasoningtosolve.
• Coreferencesystemsexhibitgenderbiaswhichcanbeevaluatedusingdatasets
likeWinobiasandGAP.
Bibliographical and Historical Notes
Coreference has been part of natural language processing since the 1970s (Woods
etal.1972,Winograd1972). Thediscoursemodelandtheentity-centricfoundation
of coreference was formulated by Karttunen (1969) (at the 3rd COLING confer-
ence), playing a role also in linguistic semantics (Heim 1982, Kamp 1981). But
it was Bonnie Webber’s 1978 dissertation and following work (Webber 1983) that
explored the model’s computational aspects, providing fundamental insights into
howentitiesarerepresentedinthediscoursemodelandthewaysinwhichtheycan
licensesubsequentreference. Manyoftheexamplessheprovidedcontinuetochal-
lengetheoriesofreferencetothisday.
Hobbs The Hobbs algorithm9 is a tree-search algorithm that was the first in a long
algorithm
seriesofsyntax-basedmethodsforidentifyingreferencerobustlyinnaturallyoccur-
ring text. The input to the Hobbs algorithm is a pronoun to be resolved, together
9 ThesimpleroftwoalgorithmspresentedoriginallyinHobbs(1978).540 CHAPTER26 • COREFERENCERESOLUTION
with a syntactic (constituency) parse of the sentences up to and including the cur-
rentsentence. Thedetailsofthealgorithmdependonthegrammarused,butcanbe
understood from a simplified version due to Kehler et al. (2004) that just searches
through the list of NPs in the current and prior sentences. This simplified Hobbs
algorithm searches NPs in the following order: “(i) in the current sentence from
right-to-left,startingwiththefirstNPtotheleftofthepronoun,(ii)intheprevious
sentencefromleft-to-right,(iii)intwosentencespriorfromleft-to-right,and(iv)in
thecurrentsentencefromleft-to-right,startingwiththefirstnoungrouptotheright
of the pronoun (for cataphora). The first noun group that agrees with the pronoun
with respect to number, gender, and person is chosen as the antecedent” (Kehler
etal.,2004).
LappinandLeass(1994)wasaninfluentialentity-basedsystemthatusedweights
tocombinesyntacticandotherfeatures,extendedsoonafterbyKennedyandBogu-
raev(1996)whosesystemavoidstheneedforfullsyntacticparses.
Approximately contemporaneously centering (Grosz et al., 1995) was applied
to pronominal anaphora resolution by Brennan et al. (1987), and a wide variety of
workfollowedfocusedoncentering’suseincoreference(Kameyama1986,DiEu-
genio 1990, Walker et al. 1994, Di Eugenio 1996, Strube and Hahn 1996, Kehler
1997a,Tetreault2001,Iidaetal.2003). KehlerandRohde(2013)showhowcenter-
ingcanbeintegratedwithcoherence-driventheoriesofpronouninterpretation. See
Chapter27fortheuseofcenteringinmeasuringdiscoursecoherence.
Coreference competitions as part of the US DARPA-sponsored MUC confer-
encesprovidedearlylabeledcoreferencedatasets(the1995MUC-6and1998MUC-
7 corpora), and set the tone for much later work, choosing to focus exclusively
onthesimplestcasesofidentitycoreference(ignoringdifficultcaseslikebridging,
metonymy,andpart-whole)anddrawingthecommunitytowardsupervisedmachine
learningandmetricsliketheMUCmetric(Vilainetal.,1995). ThelaterACEeval-
uationsproducedlabeledcoreferencecorporainEnglish, Chinese, andArabicthat
werewidelyusedformodeltrainingandevaluation.
ThisDARPAworkinfluencedthecommunitytowardsupervisedlearningbegin-
ninginthemid-90s(Connollyetal.1994, AoneandBennett1995, McCarthyand
Lehnert1995).Soonetal.(2001)laidoutasetofbasicfeatures,extendedbyNgand
Cardie(2002b),andaseriesofmachinelearningmodelsfollowedoverthenext15
years. These often focused separately on pronominal anaphora resolution (Kehler
etal.2004,BergsmaandLin2006),fullNPcoreference(CardieandWagstaff1999,
NgandCardie2002b,Ng2005a)anddefiniteNPreference(PoesioandVieira1998,
VieiraandPoesio2000),aswellasseparateanaphoricitydetection(BeanandRiloff
1999,BeanandRiloff2004,NgandCardie2002a,Ng2004),orsingletondetection
(deMarneffeetal.,2015).
Themovefrommention-pairtomention-rankingapproacheswaspioneeredby
Yang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods,
thenextendedbyDenisandBaldridge(2008)whoproposedtodorankingviaasoft-
maxoverallpriormentions. Theideaofdoingmentiondetection,anaphoricity,and
coreferencejointlyinasingleend-to-endmodelgrewoutoftheearlyproposalofNg
(2005b)touseadummyantecedentformention-ranking,allowing‘non-referential’
tobeachoicefor coreferenceclassifiers, DenisandBaldridge’s2007jointsystem
combining anaphoricity classifier probabilities with coreference probabilities, the
Denis and Baldridge (2008) ranking model, and the Rahman and Ng (2009) pro-
posaltotrainthetwomodelsjointlywithasingleobjective.
Simplerule-basedsystemsforcoreferencereturnedtoprominenceinthe2010s,BIBLIOGRAPHICALANDHISTORICALNOTES 541
partlybecauseoftheirabilitytoencodeentity-basedfeaturesinahigh-precisionway
(Zhou et al. 2004b, Haghighi and Klein 2009, Raghunathan et al. 2010, Lee et al.
2011, Lee et al. 2013, Hajishirzi et al. 2013) but in the end they suffered from an
inabilitytodealwiththesemanticsnecessarytocorrectlyhandlecasesofcommon
nouncoreference.
Areturntosupervisedlearningledtoanumberofadvancesinmention-ranking
models which were also extended into neural architectures, for example using re-
inforcementlearningtodirectlyoptimizecoreferenceevaluationmodelsClarkand
Manning (2016a), doing end-to-end coreference all the way from span extraction
(Lee et al. 2017b, Zhang et al. 2018). Neural models also were designed to take
advantage of globalentity-level information (Clark and Manning2016b, Wiseman
etal.2016,Leeetal.2018).
CoreferenceisalsorelatedtothetaskofentitylinkingdiscussedinChapter14.
Coreference can help entity linking by giving more possible surface forms to help
link to the right Wikipedia page, and conversely entity linking can help improve
coreferenceresolution. ConsiderthisexamplefromHajishirzietal.(2013):
(26.67) [MichaelEisner] and[DonaldTsang] announcedthegrandopeningof
1 2
[[HongKong] Disneyland] yesterday. [Eisner] thanked[thePresident]
3 4 1 2
andwelcomed[fans] to[thepark] .
5 4
Integrating entity linking into coreference can help draw encyclopedic knowl-
edge(likethefactthatDonaldTsangisapresident)tohelpdisambiguatethemen-
tionthePresident. PonzettoandStrube(2006)2007andRatinovandRoth(2012)
showedthatsuchattributesextractedfromWikipediapagescouldbeusedtobuild
richermodelsofentitymentionsincoreference. Morerecentresearchshowshowto
dolinkingandcoreferencejointly(Hajishirzietal.2013,Zhengetal.2013)oreven
jointlywithnamedentitytaggingaswell(DurrettandKlein2014).
Thecoreferencetaskasweintroduceditinvolvesasimplifyingassumptionthat
the relationship between an anaphor and its antecedent is one of identity: the two
coreferringmentionsrefertotheidenticaldiscoursereferent. Inrealtexts,therela-
tionship can be more complex, where different aspects of a discourse referent can
beneutralizedorrefocused. Forexample(26.68)(Recasensetal.,2011)showsan
metonymy exampleofmetonymy,inwhichthecapitalcityWashingtonisusedmetonymically
torefertotheUS.(26.69-26.70)showotherexamples(Recasensetal.,2011):
(26.68) astrictinterpretationofapolicyrequiresTheU.S.tonotifyforeign
dictatorsofcertaincoupplots... Washingtonrejectedthebid...
(26.69) IoncecrossedthatborderintoAshgh-AbadonNowruz,thePersianNew
Year. IntheSouth,everyonewascelebratingNewYear;totheNorth,it
wasaregularday.
(26.70) InFrance,thepresidentiselectedforatermofsevenyears,whileinthe
UnitedStatesheiselectedforatermoffouryears.
For further linguistic discussions of these complications of coreference see Puste-
jovsky(1991),vanDeemterandKibble(2000),Poesioetal.(2006),Fauconnierand
Turner(2008),Versley(2008),andBarker(2010).
Ng(2017)offersausefulcompacthistoryofmachinelearningmodelsincoref-
erenceresolution. Therearethreeexcellentbook-lengthsurveysofanaphora/coref-
erence resolution, covering different time periods: Hirst (1981) (early work until
about1981),Mitkov(2002)(1986-2001),andPoesioetal.(2016)(2001-2015).
AndyKehlerwrotetheDiscoursechapterforthe2000firsteditionofthistext-
book,whichweusedasthestartingpointforthesecond-editionchapter,andthere542 CHAPTER26 • COREFERENCERESOLUTION
aresomeremnantsofAndy’slovelyprosestillinthisthird-editioncoreferencechap-
ter.
ExercisesCHAPTER
27 Discourse Coherence
Andeveninourwildestandmostwanderingreveries,nayinourverydreams,
we shall find, if we reflect, that the imagination ran not altogether at adven-
tures, but that there was still a connection upheld among the different ideas,
which succeeded each other. Were the loosest and freest conversation to be
transcribed,therewouldimmediatelybetranscribed,therewouldimmediately
beobservedsomethingwhichconnecteditinallitstransitions.
DavidHume,Anenquiryconcerninghumanunderstanding,1748
OrsonWelles’movieCitizenKanewasgroundbreakinginmanyways,perhapsmost
notably in its structure. The story of the life of fictional media magnate Charles
Foster Kane, the movie does not proceed in chronological order through Kane’s
life. Instead, thefilmbeginswithKane’sdeath(famouslymurmuring“Rosebud”)
and is structured around flashbacks to his life inserted among scenes of a reporter
investigatinghisdeath. Thenovelideathatthestructureofamoviedoesnothave
tolinearlyfollowthestructureoftherealtimelinemadeapparentfor20thcentury
cinematography the infinite possibilities and impact of different kinds of coherent
narrativestructures.
But coherent structure is not just a fact about movies or works of art. Like
movies, language does not normally consist of isolated, unrelated sentences, but
instead of collocated, structured, coherent groups of sentences. We refer to such
discourse a coherent structured group of sentences as a discourse, and we use the word co-
coherence herence to refer to the relationship between sentences that makes real discourses
differentthanjustrandomassemblagesofsentences. Thechapteryouarenowread-
ing is an example of a discourse, as is a news article, a conversation, a thread on
socialmedia,aWikipediapage,andyourfavoritenovel.
Whatmakesadiscoursecoherent? Ifyoucreatedatextbytakingrandomsen-
tenceseachfrommanydifferentsourcesandpastedthemtogether,wouldthatbea
local coherentdiscourse? Almostcertainlynot. Realdiscoursesexhibitbothlocalcoher-
global enceandglobalcoherence. Let’sconsiderthreewaysinwhichrealdiscoursesare
locallycoherent;
First, sentences or clauses in real discourses are related to nearby sentences in
systematicways. ConsiderthisexamplefromHobbs(1979):
(27.1) JohntookatrainfromParistoIstanbul. Helikesspinach.
This sequence is incoherent because it is unclear to a reader why the second
sentence follows the first; what does liking spinach have to do with train trips? In
fact,areadermightgotosomeefforttotrytofigureouthowthediscoursecouldbe
coherent;perhapsthereisaFrenchspinachshortage? Theveryfactthathearerstry
toidentifysuchconnectionssuggeststhathumandiscoursecomprehensioninvolves
theneedtoestablishthiskindofcoherence.
Bycontrast,inthefollowingcoherentexample:
(27.2) JanetookatrainfromParistoIstanbul. Shehadtoattendaconference.544 CHAPTER27 • DISCOURSECOHERENCE
the second sentence gives a REASON for Jane’s action in the first sentence. Struc-
tured relationships like REASON that hold between text units are called coherence
coherence relations,andcoherentdiscoursesarestructuredbymanysuchcoherencerelations.
relations
CoherencerelationsareintroducedinSection27.1.
Asecondwayadiscoursecanbelocallycoherentisbyvirtueofbeing“about”
someone or something. In a coherent discourse some entities are salient, and the
discoursefocusesonthemanddoesn’tgobackandforthbetweenmultipleentities.
Thisiscalledentity-basedcoherence. Considerthefollowingincoherentpassage,
in which the salient entity seems to wildly swing from John to Jenny to the piano
storetothelivingroom,backtoJenny,thenthepianoagain:
(27.3) Johnwantedtobuyapianoforhislivingroom.
Jennyalsowantedtobuyapiano.
Hewenttothepianostore.
Itwasnearby.
Thelivingroomwasonthesecondfloor.
Shedidn’tfindanythingsheliked.
Thepianoheboughtwashardtogetuptothatfloor.
Entity-basedcoherencemodelsmeasurethiskindofcoherencebytrackingsalient
Centering entitiesacrossadiscourse. ForexampleCenteringTheory(Groszetal.,1995),the
Theory
most influential theory of entity-based coherence, keeps track of which entities in
the discourse model are salient at any point (salient entities are more likely to be
pronominalizedortoappearinprominentsyntacticpositionslikesubjectorobject).
In Centering Theory, transitions between sentences that maintain the same salient
entityareconsideredmorecoherentthanonesthatrepeatedlyshiftbetweenentities.
entitygrid The entity grid model of coherence (Barzilay and Lapata, 2008) is a commonly
usedmodelthatrealizessomeoftheintuitionsoftheCenteringTheoryframework.
Entity-basedcoherenceisintroducedinSection27.3.
topically Finally,discoursescanbelocallycoherentbybeingtopicallycoherent: nearby
coherent
sentences are generally about the same topic and use the same or similar vocab-
ulary to discuss these topics. Because topically coherent discourses draw from a
single semantic field or topic, they tend to exhibit the surface property known as
lexicalcohesion lexical cohesion (Halliday and Hasan, 1976): the sharing of identical or semanti-
callyrelatedwordsinnearbysentences. Forexample,thefactthatthewordshouse,
chimney, garret, closet, and window— all of which belong to the same semantic
field— appear in the two sentences in (27.4), or that they share the identical word
shingled,isacuethatthetwoaretiedtogetherasadiscourse:
(27.4) BeforewinterIbuiltachimney,andshingledthesidesofmyhouse...
Ihavethusatightshingledandplasteredhouse... withagarretanda
closet,alargewindowoneachside....
In addition to the local coherence between adjacent or nearby sentences, dis-
courses also exhibit global coherence. Many genres of text are associated with
particularconventionaldiscoursestructures. Academicarticlesmighthavesections
describingtheMethodologyorResults. Storiesmightfollowconventionalplotlines
or motifs. Persuasive essays have a particular claim they are trying to argue for,
andanessaymightexpressthisclaimtogetherwithastructuredsetofpremisesthat
support the argument and demolish potential counterarguments. We’ll introduce
versionsofeachofthesekindsofglobalcoherence.
Why dowe careabout thelocal orglobal coherenceof adiscourse? Sinceco-
herenceisapropertyofawell-writtentext,coherencedetectionplaysapartinany27.1 • COHERENCERELATIONS 545
taskthatrequiresmeasuringthequalityofatext. Forexamplecoherencecanhelp
inpedagogicaltaskslikeessaygradingoressayqualitymeasurementthataretrying
togradehowwell-writtenahumanessayis(Somasundaranetal.2014,Fengetal.
2014,LaiandTetreault2018).Coherencecanalsohelpforsummarization;knowing
thecoherencerelationshipbetweensentencescanhelpknowhowtoselectinforma-
tion from them. Finally, detecting incoherent text may even play a role in mental
healthtaskslikemeasuringsymptomsofschizophreniaorotherkindsofdisordered
language (Ditman and Kuperberg 2010, Elveva˚g et al. 2007, Bedi et al. 2015, Iter
etal.2018).
27.1 Coherence Relations
Recallfromtheintroductionthedifferencebetweenpassages(27.5)and(27.6).
(27.5) JanetookatrainfromParistoIstanbul. Shelikesspinach.
(27.6) JanetookatrainfromParistoIstanbul. Shehadtoattendaconference.
Thereason(27.6)ismorecoherentisthatthereadercanformaconnectionbe-
tweenthetwosentences,inwhichthesecondsentenceprovidesapotentialREASON
for the first sentences. This link is harder to form for (27.5). These connections
coherence betweentextspansinadiscoursecanbespecifiedasasetofcoherencerelations.
relation
Thenexttwosectionsdescribetwocommonlyusedmodelsofcoherencerelations
andassociatedcorpora: RhetoricalStructureTheory(RST),andthePennDiscourse
TreeBank(PDTB).
27.1.1 RhetoricalStructureTheory
ThemostcommonlyusedmodelofdiscourseorganizationisRhetoricalStructure
RST Theory(RST)(MannandThompson,1987). InRSTrelationsaredefinedbetween
nucleus two spans of text, generally a nucleus and a satellite. The nucleus is the unit that
satellite is more central to the writer’s purpose and that is interpretable independently; the
satelliteislesscentralandgenerallyisonlyinterpretablewithrespecttothenucleus.
Somesymmetricrelations,however,holdbetweentwonuclei.
BelowareafewexamplesofRSTcoherencerelations,withdefinitionsadapted
fromtheRSTTreebankManual(CarlsonandMarcu,2001).
Reason: Thenucleusisanactioncarriedoutbyananimateagentandthesatellite
isthereasonforthenucleus.
(27.7) [ JanetookatrainfromParistoIstanbul.] [ Shehadtoattenda
NUC SAT
conference.]
Elaboration: Thesatellitegivesadditionalinformationordetailaboutthesituation
presentedinthenucleus.
(27.8) [ DorothywasfromKansas.] [ Shelivedinthemidstofthegreat
NUC SAT
Kansasprairies.]
Evidence: The satellite gives additional information or detail about the situation
presentedinthenucleus. Theinformationispresentedwiththegoalofconvincethe
readertoaccepttheinformationpresentedinthenucleus.
(27.9) [ Kevinmustbehere.] [ Hiscarisparkedoutside.]
NUC SAT546 CHAPTER27 • DISCOURSECOHERENCE
Attribution: Thesatellitegivesthesourceofattributionforaninstanceofreported
speechinthenucleus.
(27.10) [ Analystsestimated][ thatsalesatU.S.storesdeclinedinthe
SAT NUC
quarter,too]
List: In this multinuclear relation, a series of nuclei is given, without contrast or
explicitcomparison:
(27.11) [ BillyBoneswasthemate;][ LongJohn,hewasquartermaster]
NUC NUC
RSTrelationsaretraditionallyrepresentedgraphically;theasymmetricNucleus-
Satelliterelationisrepresentedwithanarrowfromthesatellitetothenucleus:
evidence
Kevin must be here. His car is parked outside
Wecanalsotalkaboutthecoherenceofalargertextbyconsideringthehierar-
chicalstructurebetweencoherencerelations.Figure27.1showstherhetoricalstruc-
ture of a paragraph from Marcu (2000a) for the text in (27.12) from the Scientific
Americanmagazine.
(27.12) Withitsdistantorbit–50percentfartherfromthesunthanEarth–andslim
atmosphericblanket,Marsexperiencesfrigidweatherconditions. Surface
temperaturestypicallyaverageabout-60degreesCelsius(-76degrees
Fahrenheit)attheequatorandcandipto-123degreesCnearthepoles. Only
themiddaysunattropicallatitudesiswarmenoughtothawiceonoccasion,
butanyliquidwaterformedinthiswaywouldevaporatealmostinstantly
becauseofthelowatmosphericpressure.
Title 2-9
(1)
Mars evidence
2-3 4-9
background elaboration-additional
(2) (3) 4-5 6-9
WIth its Mars
distant orbit experiences
<p> -- 50 frigid weather List Contrast
percent conditions.
farther from (4) (5) 6-7 8-9
the sun than Surface and can dip
Earth -- </p> temperatures to -123 purpose explanation-argumentative
and slim typically average degrees C
atmospheric about -60 near the (6) (7) (8) (9)
blanket, degrees Celsius poles. Only the to thaw ice but any liquid water because of
<p> (-76 degrees midday sun at on occasion, formed in this way the low
Fahrenheit)</p> tropical latitudes would evaporate atmospheric
at the equator is warm enough almost instantly pressure.
Figure27.1 A discourse tree for the Scientific American text in (27.12), from Marcu (2000a). Note that
asymmetricrelationsarerepresentedwithacurvedarrowfromthesatellitetothenucleus.
TheleavesintheFig.27.1treecorrespondtotextspansofasentence,clauseor
EDU phrasethatarecalledelementarydiscourseunitsorEDUsinRST;theseunitscan
alsobereferredtoasdiscoursesegments. Becausetheseunitsmaycorrespondto
arbitraryspansoftext, determiningtheboundariesofanEDUisanimportanttask
for extracting coherence relations. Roughly speaking, one can think of discourse27.1 • COHERENCERELATIONS 547
segmentsasbeinganalogoustoconstituentsinsentencesyntax,andindeedaswe’ll
seeinSection27.2wegenerallydrawonparsingalgorithmstoinferdiscoursestruc-
ture.
There are corpora for many discourse coherence models; the RST Discourse
TreeBank (Carlson et al., 2001) is the largest available discourse corpus. It con-
sistsof385EnglishlanguagedocumentsselectedfromthePennTreebank,withfull
RSTparsesforeachone,usingalargesetof78distinctrelations, groupedinto16
classes.RSTtreebanksexistalsoforSpanish,German,Basque,DutchandBrazilian
Portuguese(Braudetal.,2017).
Now that we’ve seen examples of coherence, we can see more clearly how a
coherencerelationcanplayaroleinsummarizationorinformationextraction. For
example, the nuclei of a text presumably express more important information than
thesatellites,whichmightbedroppedinasummary.
27.1.2 PennDiscourseTreeBank(PDTB)
PDTB The Penn Discourse TreeBank (PDTB) is a second commonly used dataset that
embodiesanothermodelofcoherencerelations(Miltsakakietal.2004,Prasadetal.
2008, Prasad et al. 2014). PDTB labeling is lexically grounded. Instead of asking
annotatorstodirectlytagthecoherencerelationbetweentextspans,theyweregiven
discourse alistofdiscourseconnectives,wordsthatsignaldiscourserelations,likebecause,
connectives
although,when,since,orasaresult. Inapartofatextwherethesewordsmarkeda
coherencerelationbetweentwotextspans,theconnectiveandthespanswerethen
annotated,asinFig.27.13,wherethephraseasaresultsignalsacausalrelationship
between what PDTB calls Arg1 (the first two sentences, here in italics) and Arg2
(thethirdsentence,hereinbold).
(27.13) Jewelrydisplaysindepartmentstoreswereoftenclutteredanduninspired.
Andthemerchandisewas,well,fake. Asaresult,marketersoffauxgems
steadilylostspaceindepartmentstorestomorefashionable
rivals—cosmeticsmakers.
(27.14) InJuly,theEnvironmentalProtectionAgencyimposedagradualbanon
virtuallyallusesofasbestos. (implicit=asaresult)By1997,almostall
remainingusesofcancer-causingasbestoswillbeoutlawed.
Notallcoherencerelationsaremarkedbyanexplicitdiscourseconnective,and
sothePDTBalsoannotatespairsofneighboringsentenceswithnoexplicitsignal,
like(27.14). Theannotatorfirstchoosesthewordorphrasethatcouldhavebeenits
signal(inthiscaseasaresult),andthenlabelsitssense. Forexamplefortheam-
biguousdiscourseconnectivesinceannotatorsmarkedwhetheritisusingaCAUSAL
oraTEMPORALsense.
Thefinaldatasetcontainsroughly18,000explicitrelationsand16,000implicit
relations.Fig.27.2showsexamplesfromeachofthe4majorsemanticclasses,while
Fig.27.3showsthefulltagset.
UnliketheRSTDiscourseTreebank,whichintegratesthesepairwisecoherence
relationsintoaglobaltreestructurespanninganentirediscourse,thePDTBdoesnot
annotateanythingabovethespan-pairlevel,makingnocommitmentwithrespectto
higher-leveldiscoursestructure.
There are also treebanks using similar methods for other languages; (27.15)
shows an example from the Chinese Discourse TreeBank (Zhou and Xue, 2015).
Because Chinese has a smaller percentage of explicit discourse connectives than
English (only 22% of all discourse relations are marked with explicit connectives,548 CHAPTER27 • DISCOURSECOHERENCE
Class Type Example
TEMPORAL SYNCHRONOUS TheparishionersofSt. MichaelandAllAngelsstoptochatat
thechurchdoor,asmembersherealwayshave. (Implicitwhile)
In the tower, five men and women pull rhythmically on ropes
attachedtothesamefivebellsthatfirstsoundedherein1614.
CONTINGENCY REASON AlsounlikeMr. Ruder,Mr. Breedenappearstobeinaposition
togetsomewherewithhisagenda.(implicit=because)Asafor-
mer White House aide who worked closely with Congress,
heissavvyinthewaysofWashington.
COMPARISON CONTRAST The U.S. wants the removal of what it perceives as barriers to
investment;Japandeniestherearerealbarriers.
EXPANSION CONJUNCTION Notonlydotheactorsstandoutsidetheircharactersandmake
itcleartheyareatoddswiththem,buttheyoftenliterallystand
ontheirheads.
Figure27.2 Thefourhigh-levelsemanticdistinctionsinthePDTBsensehierarchy
Temporal Comparison
Asynchronous Contrast(Juxtaposition,Opposition)
• •
Synchronous(Precedence,Succession) PragmaticContrast(Juxtaposition,Opposition)
• •
Concession(Expectation,Contra-expectation)
•
PragmaticConcession
•
Contingency Expansion
Cause(Reason,Result) Exception
• •
PragmaticCause(Justification) Instantiation
• •
Condition (Hypothetical, General, Unreal Restatement(Specification,Equivalence,Generalization)
• •
Present/Past,FactualPresent/Past)
PragmaticCondition(Relevance,ImplicitAs- Alternative(Conjunction, Disjunction, ChosenAlterna-
• •
sertion) tive)
List
•
Figure27.3 ThePDTBsensehierarchy. Therearefourtop-levelclasses,16types,and23subtypes(notall
¯
typeshavesubtypes).11ofthe16typesarecommonlyusedforimplicitargumentclassification;the5typesin
italicsaretoorareinimplicitlabelingtobeused.
compared to 47% in English), annotators labeled this corpus by directly mapping
pairsofsentencesto11sensetags,withoutstartingwithalexicaldiscourseconnec-
tor.
(27.15) [Conn为] [Arg2推动图们江地区开发]，[Arg1韩国捐款一百万美元
设立了图们江发展基金]
“[Inorderto][Arg2promotethedevelopmentoftheTumenRiverregion],
[Arg1SouthKoreadonatedonemilliondollarstoestablishtheTumen
RiverDevelopmentFund].”
Thesediscoursetreebankshavebeenusedforsharedtasksonmultilingualdis-
courseparsing(Xueetal.,2016).
27.2 Discourse Structure Parsing
Givenasequenceofsentences,howcanweautomaticallydeterminethecoherence
discourse relations between them? This task is often called discourse parsing (even though
parsing
for PDTB we are only assigning labels to leaf spans and not building a full parse27.2 • DISCOURSESTRUCTUREPARSING 549
treeaswedoforRST).
27.2.1 EDUsegmentationforRSTparsing
RST parsing is generally done in two stages. The first stage, EDU segmentation,
extractsthestartandendofeachEDU.Theoutputofthisstagewouldbealabeling
likethefollowing:
(27.16) [Mr. Rambosays] [thata3.2-acreproperty] [overlookingtheSan
e1 e2
FernandoValley] [ispricedat$4million] [becausethelateactorErroll
e3 e4
Flynnoncelivedthere.]
e5
SinceEDUsroughlycorrespondtoclauses,earlymodelsofEDUsegmentation
first ran a syntactic parser, and then post-processed the output. Modern systems
generallyuseneuralsequencemodelssupervisedbythegoldEDUsegmentationin
datasetsliketheRSTDiscourseTreebank. Fig.27.4showsanexamplearchitecture
simplified from the algorithm of Lukasik et al. (2020) that predicts for each token
whether or not it is a break. Here the input sentence is passed through an encoder
and then passed through a linear layer and a softmax to produce a sequence of 0s
and1,where1indicatesthestartofanEDU.
EDU break 0 0 0 1
softmax
linear layer
ENCODER
Mr. Rambo says that …
Figure27.4 PredictingEDUsegmentbeginningsfromencodedtext.
27.2.2 RSTparsing
ToolsforbuildingRSTcoherencestructureforadiscoursehavelongbeenbasedon
syntacticparsingalgorithmslikeshift-reduceparsing(Marcu,1999). Manymodern
RSTparserssinceJiandEisenstein(2014)drawontheneuralsyntacticparserswe
saw in Chapter 18, using representation learning to build representations for each
span, and training a parser to choose the correct shift and reduce actions based on
thegoldparsesinthetrainingset.
We’lldescribetheshift-reduceparserofYuetal.(2018). Theparserstatecon-
sistsofastackandaqueue,andproducesthisstructurebytakingaseriesofactions
onthestates. Actionsinclude:
• shift: pushesthefirstEDUinthequeueontothestackcreatingasingle-node
subtree.
• reduce(l,d):mergesthetoptwosubtreesonthestack,wherelisthecoherence
relationlabel,andd isthenuclearitydirection,d NN,NS,SN .
∈{ }
Aswellasthepoprootoperation,toremovethefinaltreefromthestack.
Fig.27.6showstheactionstheparsertakestobuildthestructureinFig.27.5.550 CHAPTER27 • DISCOURSECOHERENCE
elab e1:AmericanTelephone&TelegraphCo.saidit
e2:willlayoff75to85technicianshere,effectiveNov.1.
e3:Theworkersinstall,maintainandrepairitsprivatebranchexchanges,
attr elab e4:whicharelargeintracompanytelephonenetworks.
e1 e2 e3 e4
FiguFreig1u:reA27n.5exaEmxapmleploefRRSTSTdisdciosucroseurtsreee,trseheo,wwingheforeurEeD,Ues.,Feigu,reefroamreYuEDetUals.,(2a0t18t)r. and elab are
1 2 3 4
{ }
discourserelationlabels,andarrowsindicatethenuclearitiesofdiscourserelations.
Step Stack Queue Action Relation
RSTdiscourseparsing.
O1 therstudie? sstilladoe p1, te d2is, ce r3e, te e4syntaxfeaS tuH resproposedbysta?
tisticalmodels,
2 e 1 e 2,e 3,e 4 SH ?
feedingthemintoneuralnetworkmodels(Braudetal.,2016;Braudetal.,2017).
3 e 1,e 2 e 3,e 4 RD(attr,SN) ?
Theaboveapproaches4modelsynet 1a :2xtreesinane 3e,xep 4licitway,requSirHingdiscretesyntaxe 1p ea 2rsingoutputs
asinputsforRSTparsing5.Theseea 1p :p 2r,oea 3chesmaysue 4fferfromtheerroSrHpropagationproblee 1me 2.Syntaxtrees
producedbyasupervised6syntaex1:2pa,res3i,neg4modelco?uldhaveeRrrDo(rse,lwahbic,hNSm)aypropagaet de1ei 2ntodiscourse
parsingmodels. Theprob7lemcoeu1l:d2b,ee 3e:4xtremelyse?riouswheRnDin(peultsaobf,dSisNc)oursepaers1iend2g,eh3aev4edifferent
distributionswiththetrai8 ningdatae o1f:4thesupervise?dsyntaxparser. RP eR cently,Zhae n1ge 2e, te a3lde .4(, 2e 0\11:27e )3s:4uggest
an alternative method, which extracts syntax features from a Bi-Affine dependencydparsedr (Dozat and
TableFi1gu:rAe2n7.e6xaPmarpsilnegothfetehxeamtrpalenosfitFiiogn.2-b7.a5suesdingsyasstheifmt-refdourceRdpSarTserdd.iFsicgouruerfsreompaYrusing.
Manning, 2016), and the method gives competitive performances on relation extraction. It actually
etal.(2018).
representssyntaxtreesimplicitly,thusitcanreducetheerrorpropagationproblem.
In this work, we inveTsthiegaYteutehteailm. (p2l0ic1i8t)syunsetasxanfeaetnucroedeexr-tdraecctoidoenraaprpchroitaecchtufroer, RwSheTrepathrseinegn.coIdneard-
dTithioen,inwiteiaplrosptaotsee airseptraraennsseeintmitosnpt-htbyeasisnetpdautnete,suparananldmotfohwdeeolfirdfnosaraltnhsditsaEtteDasUrkes,pwursehisinceghnatisshaiaebrlfaeurcltholircienaslcuobrliptL.oSrTTatMhee.vraTerhioeaures three kinds of
faecattuiorenssflienxiobulyr.tWraefinerssxittpiblooiLnitShsTyieMsrtaelracmyhe:icrarlebpri-edsiernetcstitohneawlLorSdTsMinssi(dBei-aLnSETDMUs,)atnodenthceodseectoenxdts,reapnrdesfeunrttsher
enhance the transitiothne-bEaDseUdsmeqoudeenlcwe.ithGidvyennaamniicnpouratcsleen.teBnacseewd1o,nw 2th,.e..,pwromp,otsheedwmoroddsecl,anwebestruedpyre-the
effectivSehniefsts(oSfHou),rwsperhnotipecodhsaersdeumimsuopavlliec(sbitytshsyetnattfiaicxrseftmeEabtDuedrUedsi.ningWst,ehcecoomqnubdieuncuatetieooxnpnsetworimitthheencthssatoaranccktae,rsfteoamnrmdbaeirdnddgiRnaSgsTsiondrigsl-e-nodesubtree.
• tags,orcontextualembeddings)resultinginaninputwordrepresentationsequence
course TreeBank (Carlson et al., 2003). First, we evaluate the performance of our proposed transition-
Reduce (RD xw) ,( xlw,d ,.) ., .,w xwh .i Tch hem ree sr ug lte os ft th hee wto op rdt -w levo els bu ib Lt Sr Tee Ms io sn tht eh ne as st ea qc uk en, cw eh ofer hewl vais lua esd :iscourse relation
base•d baseline, findin1g t2hat themmodel is able to achieve strong performances after applying dynamic
label,andd NN,NS,SN indicatestherelationnuclearity(nuclear(N)orsatellite(S)).
oracle. Thenweeval2ua{tetheeffective}neshswo,hfwim,.p..l,ihcwits=yntbaxiLfSeTatMur(exswe,xxtwr,a.c.t.e,xdwf)romaBi-Affin(e27d.e1p7e)n-
1 2 m 1 2 m
dencyPpaorpserR. Roeostul(tPsRsh)o,wwthhaicththepiomppsliociuttsythnteaxtofepatturereesoarneethffeecstitvaec,kg,ivminagrbkeinttegrtpheerfodremcaondciensgthbaneing completed,
expl• iciwtThreene-tLhSeTsMtaA (cLn kiE heD toU aldl.o ,sf 2os 0p n1a l5n ybw )o.s n, Ow eus+ sru1c, bo.. td. r, eew setwt ah inle ldn bh teha rs eeb lqeiL auS seeT udM efoio sru petp umu bt lpir cte yp u.r ne dse en rt ta hti eo Anh paw sc, hh ew s+L1, ic.. e. n,h setw,
andisrepresentedbyaveragepooling:
2.0athttps://github.com/yunan4nlp/NNDisParser.
InGsiuvmenmathrye,wReSmTatirnelyemasaksehtohwefnolilnowFiniggutwreo1c,onittrcibaun1tibonesgint entheirsawteodrkb:y(1)thweefporlolpoowseinagtraancstiitoionn-sequence: SH,
xe= hw (27.18) {
b Sa Hse ,d Rne Dur (a alR tS tT rd ,is Sco Nu )rs ,e Sp Har ,si Sng Hm
,
o Rd Del (w eit lh ady bn ,a Nm Stic )os,ra+Rcl 1De, ((2 e) lwkae bco ,m Sp Na )re ,t Phr Reed .if Tfe ar ben lets 1yn sta hc oti wc
s the decoding
integrationapproachesproposedbyus. Therestofthep−aperiso(cid:88)kr=gsanizedasfollows.}Section2describes
processindetail. Bythisway, wenaturallyconvert RSTdiscourseparsingintopredicting asequenceof
our proposed modelTshinecsleucdoinndgltahyeertruasnessittihoins-ibnapsuetdtonceoumraplumteoadefiln,atlhreepdryensaemntiactioornaocflethsetrsaeteqguyenacnedofthe
it mra pn lis ci it ti so yn nta ac xti fo ean ts uE, reDwUeh xe trrer ape crtee isoaencnhtaaptl piion rnoesacihn he .c:l Su ed ce tis ona 3st pa rt ee sea nn tsd tn heex et xps ete rip ma ec nt tsio tn oer ve af le ur ar ti eng out ro mth ode et lr se .e.
Section4showstherelatedwork. Finally,hsee ,chteio ,.n ..,5hdera =ws bc io Ln Sc Tlu Msi (oxnes ,.xe,...,xe)
(27.19)
2.2 Encoder-Decoder 1 2 n 1 2 n
2 Transition-basTehdeDdeicsocdoeurrissethPeanrasifneegdforward network W that outputs an action o based on a
Previous transition-based RST discourse parsing studies exploit statistical models, using manually-
concatenationofthetopthreesubtreesonthestack(s ,s ,s )plusthefirstEDUin
o 1 2
WdeesfioglnloewdJdiiasncdreEtiesefnesatetuinre(2s0(1S4a),geaxep,lo2i0ti0n9g;aHtreainlsmitiaonn-abnasdedSafrgaamee,w2o0r1k5fo;rWRaSnTgdeistcaolu.r,s2e0p1ar7s)in.gI.n this work, we
thequeue(q ):
0
Tphreopfroasmeewaotrrkanissictoinocne-pbtauasleldysnimeuprlaelanmdofldeexlibfloertoRsSuTppdoritscarobuitrrsaerypfaerastuinregs,,wwhhiicchhhafoslbleoewnswaidneleyncoder-decoder
t t t e
ufsreadminewaonrukm.bGerivoefnNaLnPitnapskust(sZehquueetnacle.,o2f01o E3D;=DUysWer( eeht s0a,l, e.h, s2,10., .1h .5,s2;e,Zhh qa,0n) tgheeteanlc.,o2d0e1r6)c.oImnpadu(dt2 ei7ts.i2ot0nh),eainputrepresen-
1 2 n
t str iaa mn ti is o li at n rio s tn o{-b h sa ee 1s q,e ud h enme 2c,o e. -d w. t.e o, hl -h e sf ro ee ne qrm }uth, ea e nali cnz r eee dps mrta eh osc e dee endr ltt saea ti c pin oo rn ot da pes o ok rf sepi tn h drt eeo rd eEp ciDr c e{e ntUd s ti lc ynot nei (n x Bg tth aa e hst ds qee aupq neu aua ue ec}n t ec hi te oe qan 0o lf s .,ca oc 2c mo 0ti n 1eo 4sdn )s id .t, iiw ro Ie nnh ci te tc l hdyh eofi fs rn ooe lm ls t ohs we tehn iet nei ga nl ,coderoutputs.
encoder, and the three hidden vectors representing partial trees are computed by
wefirstdescribethetransitionsystemforRSTdiscourseparsing,andthenintroduceourneuralnetwork
2.2.1 Encoder
averagepoolingovertheencoderoutputfortheEDUsinthosetrees:
modelbyitsencoderanddecoderparts,respectively. Thirdly,wepresentourproposeddynamicoracle
We follow Li et al. (2016), using hierarchical Bi-LSTMs to encode the source EDU inputs, where the
strategy aiming to enhance the transition-based model. Then we ijntroduce the integration method of
ifimrpslti-cliatysyenrtiasxufesaetdurteos.rFepinraelslyenwtesdeeqsucreinbecitahlewtrhaot sirndi=nsginm jseidt1 ihe +odo 1fofEoDuhrUe knse,uaranldntehtweosrekcmonoddellas.y(2e7r.2i1s)usedtorepresent
sequencial EDUs. Given an input sentence w 1,w− 2,..., (cid:88)kw=im , first we represent each word by its form
2.1 TheTransition-basedSystem { }
(e.g., w ) and POS tag (e.g. t ), concatenating their neural embeddings. By this way, the input vectors
i i
T oh fe tt hra en fisi rti so tn -l- ab yas ee rd Bfr ia -m Le Sw To Mrkc ao renver xtswa ,s xtrwu ,ct .u ..r ,a xllwearn ,i wng hp er ro ebl xewmi =nto ea mse bq (u wen )ceof ea mct bio (n tp )r ,e ad nic d-
then we apply
tions,whosekeypointisatransitions{yst1
em.
A2 transitim on}systemconsi istsoftwoparti s: statesandai
ctions.
Bi-LSTMdirectly,obtaining:
The states are used to store partially-parsed results and the actions are used to control state transitions.
hw,hw,...,hw560= Bi-LSTM( xw,xw,...,xw ) (1)
1 2 m 1 2 m
{ } { }
The second-layer Bi-LSTM is built over sequential EDUs. We should first obtain a suitable representa-
tion for each EDU, which is composed by a span of words inside a certain sentence. Assuming an EDU
withitswordsby w ,w ,...,w ,afterapplyingthefirst-layerBi-LSTM,weobtaintheirrepresenta-
s s+1 t
{ }
tionsby hw,hw ...,hw ,thenwecalculatetheEDUrepresentationbyaveragepooling:
{ s s+1 t }
t
1
xe = hw (2)
t s+1 k
  s
X
WhentheEDUrepresentationsareready,weapplythesecond-layerBi-LSTMdirectly,resulting:
he,he,...,he = Bi-LSTM( xe,xe,...,xe ) (3)
1 2 n 1 2 n
{ } { }
56127.2 • DISCOURSESTRUCTUREPARSING 551
TrainingfirstmapseachRSTgoldparsetreeintoasequenceoforacleactions,and
thenusesthestandardcross-entropyloss(withl regularization)totrainthesystem
2
totakesuchactions. GiveastateSandoracleactiona,wefirstcomputethedecoder
outputusingEq.27.20,applyasoftmaxtogetprobabilities:
exp(o )
a
p a = exp(o ) (27.22)
a (cid:48)∈A a (cid:48)
andthencomputingthecross-entropy(cid:80)loss:
λ
L CE() = log(p a)+ Θ 2 (27.23)
− 2|| ||
RSTdiscourseparsersareevaluatedonthetestsectionoftheRSTDiscourseTree-
bank,eitherwithgoldEDUsorend-to-end,usingtheRST-Parevalmetrics(Marcu,
2000b). ItisstandardtofirsttransformthegoldRSTtreesintoright-branchingbi-
nary trees, and to report four metrics: trees with no labels (S for Span), labeled
withnuclei(N),withrelations(R),orboth(FforFull),foreachmetriccomputing
micro-averaged F over all spans from all documents (Marcu 2000b, Morey et al.
1
2017).
27.2.3 PDTBdiscourseparsing
PDTB discourse parsing, the task of detecting PDTB coherence relations between
shallow
discourse spans,issometimescalledshallowdiscourseparsingbecausethetaskjustinvolves
parsing
flatrelationshipsbetweentextspans,ratherthanthefulltreesofRSTparsing.
The setoffour subtasksfor PDTBdiscourse parsingwas laidout byLin etal.
(2014)inthefirstcompletesystem, withseparatetasksforexplicit(tasks1-3)and
implicit(task4)connectives:
1. Findthediscourseconnectives(disambiguatingthemfromnon-discourseuses)
2. Findthetwospansforeachconnective
3. Labeltherelationshipbetweenthesespans
4. Assignarelationbetweeneveryadjacentpairofsentences
ManysystemshavebeenproposedforTask4:takingapairofadjacentsentences
asinputandassignacoherencerelationsenselabelasoutput. Thesetupoftenfol-
lowsLinetal.(2009)inassuminggoldsentencespanboundariesandassigningeach
adjacentspanoneofthe11second-levelPDTBtagsornone(removingthe5very
raretagsofthe16showninitalicsinFig.27.3).
A simple but very strong algorithm for Task 4 is to represent each of the two
spans by BERT embeddings and take the last layer hidden state corresponding to
the position of the [CLS] token, pass this through a single layer tanh feedforward
networkandthenasoftmaxforsenseclassification(Nieetal.,2019).
Each of the other tasks also have been addressed. Task 1 is to disambiguat-
ing discourse connectives from their non-discourse use. For example as Pitler and
Nenkova (2009) point out, the word and is a discourse connective linking the two
clauses by an elaboration/expansion relation in (27.24) while it’s a non-discourse
NPconjunctionin(27.25):
(27.24) Sellingpickedupaspreviousbuyersbailedoutoftheirpositionsand
aggressiveshortsellers—anticipatingfurtherdeclines—movedin.
(27.25) Myfavoritecolorsareblueandgreen.552 CHAPTER27 • DISCOURSECOHERENCE
Similarly,onceisadiscourseconnectiveindicatingatemporalrelationin(27.26),
butsimplyanon-discourseadverbmeaning‘formerly’andmodifyingusedin(27.27):
(27.26) Theasbestosfiber,crocidolite,isunusuallyresilientonceitentersthe
lungs,withevenbriefexposurestoitcausingsymptomsthatshowup
decadeslater,researcherssaid.
(27.27) AformofasbestosonceusedtomakeKentcigarettefiltershascauseda
highpercentageofcancerdeathsamongagroupofworkersexposedtoit
morethan30yearsago,researchersreported.
Determining whether a word is a discourse connective is thus a special case
of word sense disambiguation. Early work on disambiguation showed that the 4
PDTB high-level sense classes could be disambiguated with high (94%) accuracy
used syntactic features from gold parse trees (Pitler and Nenkova, 2009). Recent
work performs the task end-to-end from word inputs using a biLSTM-CRF with
BIOoutputs(B-CONN,I-CONN,O)(Yuetal.,2019).
Fortask2,PDTBspanscanbeidentifiedwiththesamesequencemodelsusedto
findRSTEDUs: abiLSTMsequencemodelwithpretrainedcontextualembedding
(BERT)inputs(Mulleretal.,2019). Simpleheuristicsalsodoprettywellasabase-
line at finding spans, since 93% of relations are either completely within a single
sentenceorspantwoadjacentsentences,withoneargumentineachsentence(Biran
andMcKeown,2015).
27.3 Centering and Entity-Based Coherence
Asecondwayadiscoursecanbecoherentisbyvirtueofbeing“about”someentity.
Thisideathatateachpointinthediscoursesomeentityissalient, andadiscourse
iscoherentbycontinuingtodiscussthesameentity,appearsearlyinfunctionallin-
guisticsandthepsychologyofdiscourse(Chafe1976,KintschandVanDijk1978),
and soon made its way to computational models. In this section we introduce two
entity-based models of this kind of entity-based coherence: Centering Theory (Grosz et al.,
1995),andtheentitygridmodelofBarzilayandLapata(2008).
27.3.1 Centering
Centering Centering Theory (Grosz et al., 1995) is a theory of both discourse salience and
Theory
discoursecoherence. Asamodelofdiscourse salience, Centeringproposesthatat
anygivenpointinthediscourseoneoftheentitiesinthediscoursemodelissalient:
it is being “centered” on. As a model of discourse coherence, Centering proposes
thatdiscoursesinwhichadjacentsentencesCONTINUEtomaintainthesamesalient
entity are more coherent than those which SHIFT back and forth between multiple
entities(wewillseethatCONTINUEandSHIFTaretechnicaltermsinthetheory).
The following two texts from Grosz et al. (1995) which have exactly the same
propositional content but different saliences, can help in understanding the main
Centeringintuition.
(27.28) a. Johnwenttohisfavoritemusicstoretobuyapiano.
b. Hehadfrequentedthestoreformanyyears.
c. Hewasexcitedthathecouldfinallybuyapiano.
d. Hearrivedjustasthestorewasclosingfortheday.27.3 • CENTERINGANDENTITY-BASEDCOHERENCE 553
(27.29) a. Johnwenttohisfavoritemusicstoretobuyapiano.
b. ItwasastoreJohnhadfrequentedformanyyears.
c. Hewasexcitedthathecouldfinallybuyapiano.
d. ItwasclosingjustasJohnarrived.
Whilethesetwotextsdifferonlyinhowthetwoentities(Johnandthestore)are
realizedinthesentences, thediscoursein(27.28)isintuitivelymorecoherentthan
the one in (27.29). As Grosz et al. (1995) point out, this is because the discourse
in(27.28)isclearlyaboutoneindividual,John,describinghisactionsandfeelings.
Thediscoursein(27.29),bycontrast,focusesfirstonJohn,thenthestore,thenback
toJohn,thentothestoreagain. Itlacksthe“aboutness”ofthefirstdiscourse.
Centering Theory realizesthis intuition by maintainingtwo representations for
backward-
looking each utteranceU n. The backward-looking center ofU n, denoted asC b(U n), rep-
center
resentsthecurrentsaliententity,theonebeingfocusedoninthediscourseafterU
n
forward-looking is interpreted. The forward-looking centers ofU n, denoted asC f(U n), are a set
center
ofpotentialfuturesaliententities,thediscourseentitiesevokedbyU anyofwhich
n
couldserveasC (thesaliententity)ofthefollowingutterance,i.e.C (U ).
b b n+1
Thesetofforward-lookingcentersC (U )arerankedaccordingtofactorslike
f n
discourse salience and grammatical role (for example subjects are higher ranked
thanobjects,whicharehigherrankedthanallothergrammaticalroles). Wecallthe
highest-ranked forward-looking centerC (for “preferred center”). C is a kind of
p p
predictionaboutwhatentitywillbetalkedaboutnext. Sometimesthenextutterance
indeedtalksaboutthisentity,butsometimesanotherentitybecomessalientinstead.
We’ll use here the algorithm for centering presented in Brennan et al. (1987),
whichdefinesfourintersententialrelationshipsbetweenapairofutterancesU and
n
U that depend on the relationship between C (U ), C (U ), and C (U );
n+1 b n+1 b n p n+1
theseareshowninFig.27.7.
C (U )=C (U ) C (U )=C (U )
b n+1 b n b n+1 b n
(cid:54)
orundefinedC (U )
b n
C (U )=C (U ) Continue Smooth-Shift
b n+1 p n+1
C (U )=C (U ) Retain Rough-Shift
b n+1 p n+1
(cid:54)
Figure27.7 CenteringTransitionsforRule2fromBrennanetal.(1987).
Thefollowingrulesareusedbythealgorithm:
Rule1: IfanyelementofC f(U n)isrealizedbyapronouninutterance
U ,thenC (U )mustberealizedasapronounalso.
n+1 b n+1
Rule2: Transitionstatesareordered. ContinueispreferredtoRetainis
preferredtoSmooth-ShiftispreferredtoRough-Shift.
Rule 1 captures the intuition that pronominalization (including zero-anaphora)
is a common way to mark discourse salience. If there are multiple pronouns in an
utterancerealizingentitiesfromthepreviousutterance,oneofthesepronounsmust
realizethebackwardcenterC ;ifthereisonlyonepronoun,itmustbeC .
b b
Rule2capturestheintuitionthatdiscoursesthatcontinuetocenterthesameen-
tityaremorecoherentthanonesthatrepeatedlyshifttoothercenters. Thetransition
tableisbasedontwofactors: whetherthebackward-lookingcenterC isthesame
b
fromU toU and whether this discourse entity is the one that is preferred (C )
n n+1 p
inthenewutteranceU n+1. Ifbothofthesehold,a CONTINUErelation,thespeaker
has been talking about the same entity and is going to continue talking about that554 CHAPTER27 • DISCOURSECOHERENCE
entity. InaRETAINrelation,thespeakerintendstoSHIFTtoanewentityinafuture
utterance and meanwhile places the current entity in a lower rankC f. In a SHIFT
relation,thespeakerisshiftingtoanewsaliententity.
Let’s walk though the start of (27.28) again, repeated as (27.30), showing the
representationsaftereachutteranceisprocessed.
(27.30) Johnwenttohisfavoritemusicstoretobuyapiano. (U )
1
Hewasexcitedthathecouldfinallybuyapiano. (U )
2
Hearrivedjustasthestorewasclosingfortheday. (U )
3
ItwasclosingjustasJohnarrived(U )
4
UsingthegrammaticalrolehierarchytoordertheC ,forsentenceU weget:
f 1
C (U ): John,musicstore,piano
f 1
{ }
C (U ): John
p 1
C (U ): undefined
b 1
andthenforsentenceU :
2
C (U ): John,piano
f 2
{ }
C (U ): John
p 2
C (U ): John
b 2
Result: Continue (C (U )=C (U );C (U )undefined)
p 2 b 2 b 1
ThetransitionfromU
1
toU
2
isthusa CONTINUE. Completingthisexampleisleft
asexercise(1)forthereader
27.3.2 EntityGridmodel
Centering embodies a particular theory of how entity mentioning leads to coher-
ence: thatsaliententitiesappearinsubjectpositionorarepronominalized,andthat
discourses are salient by means of continuing to mention the same entity in such
ways.
entitygrid The entity grid model of Barzilay and Lapata (2008) is an alternative way to
captureentity-basedcoherence: insteadofhavingatop-downtheory,theentity-grid
modelusingmachinelearningtoinducethepatternsofentitymentioningthatmake
adiscoursemorecoherent.
The model is based around an entity grid, a two-dimensional array that repre-
sentsthedistributionofentitymentionsacrosssentences. Therowsrepresentsen-
tences,andthecolumnsrepresentdiscourseentities(mostversionsoftheentitygrid
modelfocusjustonnominalmentions).Eachcellrepresentsthepossibleappearance
ofanentityinasentence,andthevaluesrepresentwhethertheentityappearsandits
grammaticalrole. Grammaticalrolesaresubject(S),object(O),neither(X),orab-
sent(–);intheimplementationofBarzilayandLapata(2008),subjectsofpassives
arerepresentedwith O,leadingtoarepresentationwithsomeofthecharacteristics
ofthematicroles.
Fig. 27.8 from Barzilay and Lapata (2008) shows a grid for the text shown in
Fig.27.9. Thereisonerowforeachofthesixsentences. Thesecondcolumn, for
theentity‘trial’,is O – – – X,showingthatthetrialappearsinthefirstsentenceas
direct object, in the last sentence as an oblique, and does not appear in the middle
sentences. Thethirdcolumn,fortheentityMicrosoft,showsthatitappearsassub-
jectinsentence1(italsoappearsastheobjectoftheprepositionagainst,butentities
thatappearmultipletimesarerecordedwiththeirhighest-rankedgrammaticalfunc-
tion). ComputingtheentitygridsrequiresextractingentitiesanddoingcoreferenceComputationalLinguistics Volume34,Number1
thesepatternscanbeencodedasfeaturevectorsappropriateforperformingcoherence-
relatedrankingandclassificationtasks.
3.1TheEntity-GridDiscourseRepresentation
Each text is represented by an entity grid, a two-dimensional array that captures
thedistributionofdiscourseentitiesacrosstextsentences.WefollowMiltsakakiand
Kukich(2000)inassumingthatourunitofanalysisisthetraditionalsentence(i.e.,a
main clause with accompanying subordinate and adjunct clauses). The rows of the
grid correspond to sentences, and the columns correspond to discourse entities. By
discourseentitywemeanaclassofcoreferentnounphrases(weexplaininSection3.3
howcoreferententitiesareidentified).Foreachoccurrenceofadiscourseentityinthe
text, the corresponding grid cell contains information about its presence or absence
inasequenceofsentences.Inaddition,forentitiespresentinagivensentence,grid
cellscontaininformationabouttheirsyntacticrole.Suchinformationcanbeexpressed
in many ways (e.g., using constituent labels or thematic role information). Because
grammaticalrelationsfigureprominentlyinentity-basedtheoriesoflocalcoherence(see
Section2),theyserveasalogicalpointofdeparture.Eachgridcellthuscorrespondsto
astringfromasetofcategoriesreflectingwhethertheentityinquestionisasubject(S),
object (O), or neither (X). Entities absent from a sentence are signaled by gaps (–).
Grammatical role information can be extracted from the output of a broad-coverage
dependencyparser(Lin2001;BriscoeandCarroll2002)oranystate-of-theartstatistical
parser(Collins1997;Charniak2000).Wediscusshowthisinformationwascomputed
forourexperimentsinSection3.3.
Table1illustratesafragmentofanentitygridconstructedforthetextinTable2.
Becausethetextcontainssixsentences,thegridcolumnsareoflengthsix.Consider
forinstancethegridcolumnfortheentitytrial, [O – – – – X].Itrecordsthattrialis
presentinsentences1and6(asOandX,respectively)butisabsentfromtherestofthe
sentences.AlsonotethatthegridinTable1takescoreferenceresolutionintoaccount.
Eventhoughthesameentityappearsindifferentlinguisticforms,forexample,Microsoft
Corp., Microsoft, and the company, it is mapped to a single entry in the grid (see the
columnintroducedbyMicrosoftinTable1).
ComputationalLinguistics Volume34,Number1
afeaturespacewithtransitionsoflengthtwoisillustratedinTable3.Thesecondrow
27.3 •TablCe1ENTERINGANDENTITY-BASEDCOHERENCE 555
(introducedbyd 1)isthAeffreaagtmuernetvofetchteoernrteitpyrgerside.nNtaoutinopnhorafsethsearegrreidpreinsenTtaebdlbey1th.eirheadnouns.Gridcells
correspondtogrammaticalroles:subjects(S),objects(O),orneither(X).
3.3GridConstruction:LinguisticDimensions
One of the central research issues in developing entity-based models of coherence is
determiningwhatsour1cesSoOfliSngXuOist–ic–kn–ow– l–ed–ge–a–re–es–se1ntialforaccurateprediction,
andhowtoencodethe2ms–uc–ciOnc–tly–iXnaSdOis–co–ur–se–re–p–res–en2tation.Previousapproaches
tend to agree on the 3 fea– tu– resS oOf– en– tit– y– dSistOribOu– tio– n– re– la3 ted to local coherence—the
4 – – S – – – – – – – – S – – – 4
BdairszaiglaryeeamndenLtapliaetsainthe5w–ay–th–e–se–fe–atu–re–s–ar–em–o–dSeleOd–. 5 ModelingLocalCoherence
Our study of alte6rna–tivXeSe–nc–od–in–gs– i–s –no–t–a–m–erOe6duplication of previous ef-
Fif go ur rts e( 2P 7o .8esioPeatrtaol.f2t0h0e4e)ntthitaytgforicdufsoornthleintegxutiisntiFciags.p2e7c.9ts.oEfnptiatireasmareetelrisizteadtiboyn.thBeeicrahuesaedwe
areinterestedinanautomaticallyconstructedmodel,wehavetotakeintoaccountcom-
noun;eachcellrepresentsw6hetheranentityappearsassubject(S),object(O),neither(X),or
putationalandlearningissueswhenconsideringalternativerepresentations.Therefore,
isTaabbsleen2t(–).FigurefromBarzilayandLapata(2008).
Souumrmexapryloaruagtimonenotefdthweitphasryanmtaectteicrasnpnaocteatiisongsufiodregdribdycothmrpeeutcaotinosni.derations:thelinguistic
importanceofaparameter,theaccuracyofitsautomaticcomputation,andthesizeofthe
1 [TheJusticeDepartment] isconductingan[anti-trusttrial] against[MicrosoftCorp.]
resultingfeaturespace.FSromthelinguisticside,wefocOusonpropertiesofentityX distri-
with[evidence] that[thecompany] isincreasinglyattemptingtocrush[competitors] .
butionthataretXightlylinkedtolocSalcoherence,andatthesametimeallowformO ultiple
2 [Microsoft] isaccusedoftryingtoforcefullybuyinto[markets] where[itsown
inpterorpdruecttas]tioO anresndoutrcionmgptehteitievneceondoiungghptoroucnessesa.tC[eosmtabpluisthaetdiobnraaXlncdosn]s.iderationspreventus
S O
3fro[Tmheccoanssei]derervinoglvedsisacroouunrsde[ervepidreensceen]taotifo[nMsictrhoastofcta]nangogtrebsesicvoelmyppuretsesdurrienlgiablybyexist-
S O S
in[gNteotsoclasp.eF]oOrinintostmanercgei,nwge[bcroowulsdernsootftewxapree]rOim. entwiththegranularityofanutterance—
4 [Microsoft] claims[itstactics] arecommonplaceandgoodeconomically.
sentenceverSsusclause—becaSuseavailableclauseseparatorsintroducesubstantialnoise
5 [Thegovernment] mayfile[acivilsuit] rulingthat[conspiracy] tocurb[competition]
intoagridconstruSction.Finally,weexcOluderepresentationsthSatwillexplodethesOizeof
through[collusion] is[aviolationoftheShermanAct] .
6th[eMfiecartousoreft]spcaocnet,inthuXeersetboyshinocwre[ainscirnegastehdeeaamrnoinugnst]ofddeO saptaitere[qthueirtreidal]fo.rtrainingthemodel.
S O X
Figure27.9 A discourse with the entities marked and annotated with grammatical func-
Entity Extraction.Theaccuratecomputationofentityclassesiskeytocomputingmean-
tio in ns g. fFWui lgheuenrne titfayrongmoriuB dnsa .riz Isnilaa ptytreeasvntideodL ua smp imaotra pe( le2thm0a0 en8 n).otantcioenwsoitfhenatditiyff-ebraesnetdgmraomdemlsa,tcilcaaslsreosloeficnortehfe-
searmenetsneonutennsceh,awveedbeefeanuletxttorathcteedrolmeawniuthaltlhye(hMigilhtseastkgakraimanmdatKicuaklircahnk2i0n0g0:;sKubarjeacmtsaanries
rerseaotnlauklte.ido2n0h0itg4o;hcePlruoteshtseaiornteohtbejmaelc.tisn2,0two04hd)ii,csbhcuoiuntrttshueirsneniastritenioersatn(akCnehdoapphttiiegorhn2er6fo)trhaaosnuwtrhemellroaedssetp.l.aFrAosrinnegxoabtmhveipolues,
setsnhote elunetcnioeti snty tfooMrgi eicdtreo gns rotafit mfyi msinm agte iecnnatti liotryn oe lcedlast .swseisceisintoSeemntpelnocyea1nwaiuthtotmheatgicracmormefaetriecnaclerorelessolx ut(ifoonr
MtoIioc nlrotths hoaeftt rdeCesoturelprt.mi) naignnegdsris wd,h(f ciocorhlutnh moeunc snom tphphaartnaaysre) e,sb dreu eft nesris etor (e ltiphkreeess tahenmeteced oelnuotmnitlynyifbnoy ras Mdoi inccurt omh seeonfg ttr ).id in-(see
TableCsu1rraenndt2a)p.proaches recast coreference resolution as a classification task. A pair
dicateentitiesthatarementionedofteninthetexts;sparsecolumns(likethecolumn
of NPs is classified as coreferring or not based on constraints that are learned from
forearnings)indicateentitiesthatarementionedrarely.
3a.n2EannntiotytaGterdidcsorapsuFse.aAturseepVaercattoersclustering mechanism then coordinates the possibly
coI nn trt ah de ie ctn ot rit yy pg ar ii rd wm iso ed ce ll a, sc so ifih ce ar te in oc ne si as nm de ca os nu sr te rd ucb ty
s
apa ptt ae rr tn its ioo nflooncathleensetittyoftrNaPns-.
In
sit Aoiuo fnru. nedxFpao merr eimnex teaanlmtasp s,slwe u, meD pemte ip opa nlrot umyne dNn egt rlyias innda gsCou uab rrjdaeipcet p’sri on(2a0s ce h0n2i)t sectn hoc areetft1ehr, eeandncied strrt ieh bse uon tluiontnioot onfmseye nnstti-etimes.
tioiTnnhecedoshiynesrtseeenmntttedenxeccteside2ex;shtwihbihistesitshceethrrteatwitnroarnNesgiPtuisolaanrrie[tiSecso–rr]ee.ffleTerehcnteetdtbraiynnesgixtrpiiodlonitstoiapnrgoeloathgwyu.seSasoletmhqueoefonlfceetxhsiceasel,
Srge,rOgaumXla,mr–iatiteincsawla,hrseiecmfhoarcnmatniacl,bizaeendedxiptnroasCcitteeiondnteaarlsinfcegaotnTuthrineesuo.orIyutsiassctercalolisnnesfdtrroaominntteshaecohnMcUtroCalnu(sm6i–tni7o.)ndEsaaotcafhstehtes
{ }
tralaonncsdaitlyifoioenclduhssassintaatapedr-ojoabfc-aethbneitl-isateyrnt;tptehenercfeopsrr.moGbaarnibdcielsi(to7yf0oc.4ofFh[- Semr–een]atisntuertxhetesongarrMeidlUifkCreo-lm6yatFnoidgh.6a23v.7e4.s8oonimsM0eU.d0Ce8n-7s)e.
(itcoolcucmurnss6(it.iem.,ecsooluumtonfstwheit7h5jutosttaaltfreawnsigtaiopnss, soufclhenagsthMtiwcroo)s.ofFtiign.2T7ab.1le0s1h)oawndstmheany
dissptrairbsueticoonluomvenrstwrahniscihtiwonilslocofnlesinsgttmho2stfloyrotfhgeatpesxt(soefeFmiagr.k2et7s.9an(dsheoawrnninagssitnheTafibrlset1).
Onewouldfurtherexpectthatentitiescorrespondingtodensecolumnsaremoreoften
roTwabdle)3,and2otherdocuments.
1
sEuxbajmecptlseoorfaobfejeactutsre.-Tvhecetsoercdhoacruamcteenrtisretipcrsesweniltlabtieonleusssinpgroanlloturanncseitdioinnsloofwle-ncgothhetrwenocgeivteenxts.
syntIanctsipcicraetdegboyrieCseSn,tOe,riXn,ganTdh–e.ory,ouranalysisrevolvesaroundpatternsoflocalentity
transitions. A local entity transition is a sequence S,O,X,– n that represents entity
SS SO SX S– OS OO OX O– XS XO{ XX X–} –S –O –X ––
occurrences and their syntactic roles in n adjacent sentences. Local transitions can be
eda1sily.0o1bta.i0n1ed0from.0a8gri.d01as0conti0nuou.s09sub0sequ0ences0ofea.0c3hc.o0l5um.0n7.Ea.c0h3tr.a59nsition
wd 2ill h.0a2ve.0a1cer.t0a1in.0p2ro0babilit.y07in0a gi.v0e2n g.1r4id. F.1o4r in.0s6tan.0c4e,.t0h3e p.0r7ob0a.b1ility.36of the
d .02 0 0 .03 .09 0 .09 .06 0 0 0 .05 .03 .07 .17 .39
tr3ansition [S–] in the grid from Table 1 is 0.08 (computed as a ratio of its frequency
Fi[gi.uer.,es2ix7].1d0ividAefdeabtuyrethveecttootraflonrurempbreesrenotfintgradnoscituimonesntosfusleinnggtahllttwranos[itii.oe.n,s7o5f])l.enEgatchh2t.ext
DocacunmthenutsdbeisvitehweteedxtaisnaFdigi.st2r7ib.9u.tFioignudreefifrnoemdBoavrezriltaryanansditiLoanptaytap(e2s0.08).
8 1
Wecannowgoonestepfurtherandrepresenteachtextbyafixedsetoftransition
sequencesusingastandardfeaturevectornotation.Eachgridrenderingjofadocument
Thetransitionsandtheirprobabilitiescanthenbeusedasfeaturesforamachine
d corresponds to a feature vector Φ(x )=(p (x ),p (x ),...,p (x )), where m is the
leairningmodel.Thismodelcanbeatextijclassifi1erijtrain2edijtoprodmuceijhuman-labeled
number of all predefined entity transitions, and p(x ) the probability of transition t
t ij
coherencescores(forexamplefromhumanslabelingeachtextascoherentorinco-
ingridx .Thisfeaturevectorrepresentationisusefullyamenabletomachinelearning
ij
hearlegnotr)i.thBmusts(usecehdoautraeixspeexrpimenesnivtseitnogSaetchtieorn.sB4a–rz6i)l.aFyuarnthdeLrmapoartea,(i2t0a0ll5o)winstrthoeduccoendsid-
aesriamtipolnifyoifnlgarignennovuamtiboenr:s ocfohtrearnesnictieonmsowdehlischcaconubldeptroatiennetdialblyyusnelcfo-vsuerpenrovvieslioenn:tity
tradiinsetrdibtuotiodnisptiantgteurinshsrtehleevnaanttufroarlcoorhiegriennacleoarsdseerssomfesnetnotreontcheesricnohaerdeinscceo-urerslaetefdrotmasks.
Notethatconsiderablelatitudeisavailablewhenspecifyingthetransitiontypesto
beincludedinafeaturevector.Thesecanbealltransitionsofagivenlength(e.g.,two
orthree)orthemostfrequenttransitionswithinadocumentcollection.Anexampleof
7
tnemtrapeD
lairT
tfosorciM ecnedivE
srotitepmoC
stekraM stcudorP
sdnarB
esaC
epacsteN erawtfoS
scitcaT
tnemnrevoG
tiuS
sgninraE556 CHAPTER27 • DISCOURSECOHERENCE
amodifiedorder(suchasarandomizedorder). Weturntotheseevaluationsinthe
nextsection.
27.3.3 EvaluatingNeuralandEntity-basedcoherence
Entity-based coherence models, as well as the neural models we introduce in the
nextsection,aregenerallyevaluatedinoneoftwoways.
First,wecanhavehumansratethecoherenceofadocumentandtrainaclassifier
topredictthesehumanratings,whichcanbecategorial(high/low,orhigh/mid/low)
orcontinuous. Thisisthebestevaluationtouseifwehavesomeendtaskinmind,
likeessaygrading,wherehumanratersarethecorrectdefinitionofthefinallabel.
Alternatively, since it’s very expensive to get human labels, and we might not
yet have an end-task in mind, we can use natural texts to do self-supervision. In
self-supervisionwepairupanaturaldiscoursewithapseudo-documentcreatedby
changing the ordering. Since naturally-ordered discourses are more coherent than
randompermutation(Linetal.,2011),asuccessfulcoherencealgorithmshouldpre-
fertheoriginalordering.
Self-supervision has been implemented in 3 ways. In the sentence order dis-
criminationtask(BarzilayandLapata,2005),wecompareadocumenttoarandom
permutation of its sentence. A model is considered correct for an (original, per-
muted)testpairifitrankstheoriginaldocumenthigher. Givenkdocuments,wecan
computenpermutations,resultinginknpairseachwithoneoriginaldocumentand
onepermutation,touseintrainingandtesting.
Inthesentenceinsertiontask(Chenetal.,2007)wetakeadocument,remove
oneofthensentencess,andcreaten 1copiesofthedocumentwithsinsertedinto
−
each position. The task is to decide which of the n documents is the one with the
originalordering,distinguishingtheoriginalpositionforsfromallotherpositions.
Insertionisharderthandiscriminationsincewearecomparingdocumentsthatdiffer
byonlyonesentence.
Finally, in the sentence order reconstruction task (Lapata, 2003), we take a
document, randomize the sentences, and train the model to put them back in the
correctorder. Againgivenkdocuments,wecancomputenpermutations,resulting
inknpairseachwithoneoriginaldocumentandonepermutation,touseintraining
andtesting. Reorderingisofcourseamuchhardertaskthansimpleclassification.
27.4 Representation learning models for local coherence
Thethirdkindoflocalcoherenceistopicalorsemanticfieldcoherence. Discourses
cohere by talking about the same topics and subtopics, and drawing on the same
semanticfieldsindoingso.
Thefieldwaspioneeredbyaseriesofunsupervisedmodelsinthe1990softhis
lexicalcohesion kind of coherence that made use of lexical cohesion (Halliday and Hasan, 1976):
the sharing of identical or semantically related words in nearby sentences. Morris
andHirst(1991)computedlexicalchainsofwords(likepine,bushtrees,trunk)that
occurredthroughadiscourseandthatwererelatedinRoget’sThesaurus(bybeingin
thesamecategory,orlinkedcategories). Theyshowedthatthenumberanddensity
TextTiling of chain correlated with the topic structure. The TextTiling algorithm of Hearst
(1997) computed the cosine between neighboring text spans (the normalized dot
productofvectorsofrawwordcounts),againshowingthatsentencesorparagraphin27.4 • REPRESENTATIONLEARNINGMODELSFORLOCALCOHERENCE 557
asubtopichavehighcosinewitheachother,butnotwithsentencesinaneighboring
subtopic.
Athirdearlymodel, theLSACoherencemethodofFoltzetal.(1998)wasthe
first to use embeddings, modeling the coherencebetween two sentences as the co-
sinebetweentheirLSAsentenceembeddingvectors1,computingembeddingsfora
sentencesbysummingtheembeddingsofitswordsw:
sim(s,t) = cos(s,t)
= cos( w, w) (27.31)
w s w t
(cid:88)∈ (cid:88)∈
anddefiningtheoverallcoherenceofatextastheaveragesimilarityoverallpairsof
adjacentsentencess ands :
i i+1
n 1
1 −
coherence(T) = cos(s i,s i+1) (27.32)
n 1
− i=1
(cid:88)
Modernneuralrepresentation-learningcoherencemodels,beginningwithLietal.
(2014),drawontheintuitionsoftheseearlyunsupervisedmodelsforlearningsen-
tence representations and measuring how they change between neighboring sen-
tences. ButthenewmodelsalsodrawontheideapioneeredbyBarzilayandLapata
(2005) of self-supervision. That is, unlike say coherence relation models, which
trainonhand-labeledrepresentationsforRSTorPDTB,thesemodelsaretrainedto
distinguishnaturaldiscoursesfromunnaturaldiscoursesformedbyscramblingthe
order of sentences, thus using representation learning to discover the features that
matterforatleasttheorderingaspectofcoherence.
Herewepresentonesuchmodel,thelocalcoherencediscriminator(LCD)(Xu
et al., 2019). Like early models, LCD computes the coherence of a text as the av-
erage of coherence scores between consecutive pairs of sentences. But unlike the
earlyunsupervisedmodels,LCDisaself-supervisedmodeltrainedtodiscriminate
consecutivesentencepairs(s,s )inthetrainingdocuments(assumedtobecoher-
i i+1
ent) from (constructed) incoherent pairs (s,s). All consecutive pairs are positive
i (cid:48)
examples,andthenegative(incoherent)partnerforasentences isanothersentence
i
uniformlysampledfromthesamedocumentass.
i
Fig. 27.11 describes the architecture of the model f , which takes a sentence
θ
pair and returns a score, higher scores for more coherent pairs. Given an input
sentencepairsandt,themodelcomputessentenceembeddingssandt(usingany
sentenceembeddingsalgorithm),andthenconcatenatesfourfeaturesofthepair:(1)
theconcatenationofthetwovectors(2)theirdifferences t;(3)theabsolutevalue
−
of their difference s t; (4) their element-wise product s t. These are passed
| − | (cid:12)
throughaone-layerfeedforwardnetworktooutputthecoherencescore.
Themodelistrainedtomakethiscoherencescorehigherforrealpairsthanfor
negativepairs. Moreformally,thetrainingobjectiveforacorpusCofdocumentsd,
eachofwhichconsistsofalistofsentencess,is:
i
L θ = E [L(f θ(s i,s i+1),f θ(s i,s(cid:48)))] (27.33)
(cid:88)d ∈C
(cid:88)si∈dp(s (cid:48)|si)
dE ip ti( os
(cid:48)
n|s ei) dis onth se :ex gp ive ec ntat aio sn enw teit nh cere ssp te hc et t ao lgt oh re ithn meg sat si av me ps la em sp ali nn eg gd ati is vtr eib su et nio ten ncc eon s-
i i (cid:48)
1 See Chapter 6 for more on LSA embeddings; they are computed by applying SVD to the term-
documentmatrix(eachcellweightedbylogfrequencyandnormalizedbyentropy),andthenthefirst
300dimensionsareusedastheembedding.558 CHAPTER27 • DISCOURSECOHERENCE
Loss function: The role of the loss function is
to encourage f+ = f (s ,s ) to be high while
✓ i i+1
f = f (s ,s)tobelow. Commonlossessuchas
  ✓ i 0
marginorloglosscanallbeused. Throughexper-
imental validation, we found that margin loss to
be superior for this problem. Specifically, L takes
ontheform: L(f+,f ) = max(0,⌘ f++f )
   
 
where⌘ isthemarginhyperparameter.
Negative samples: Technically, we are free to
choose any sentence s to form a negative pair
0
with s . However, because of potential differ-
i
ences in genre, topic and writing style, such neg-
Figure27.11 The architecture of the LCD model of document coherence, showing the
atives might cause the discriminative model to Figure1: Genericarchitectureforourproposedmodel.
computationofthescoreforapairofsentencessandt.FigurefromXuetal.(2019).
learn cues unrelated to coherence. Therefore, we
only select sentences from the same documuneifnotrmtolyovertheothersentencesinthesamedocument. Lisalossfunctionthat
construct negative pairs. Specifically,
supt pa oke ss etwsosco4re.s2,onPerfoer-tarpaoinsietidveGpeainrearnadtoivneefMoroadneelgaatsivtehpeair,withthegoalof
i
comes from document d with length
nen ,co thur ea nging f+= fSθe(ns it,es in+c1)etEonbceohdigehrand f −= f θ(s i,s (cid:48)))tobelow. Fig.27.11
k uksethemarginlossl(f+,f )=max(0,η f++f )whereη isthemarginhyper-
− −
p(s s ) is a uniform distribution over the n 1 Our model can work w−ith any pre-trained sen-
0 i parkameter.
|  
sentences s j j =i fromd k. ForadocumentwXuithet al.t(e2n01c9e)eanlscoogdievre,araunsegfiunlgbafsreolimnetahlgeomritohmsttshiamt iptslieslftihcas quite high
{ } 6
n sentences, there are n 1 positive paipresr,foarnmdanceainvemraegaseurGinlgoVpeerp(lPexeintyn:intrgationnanetRaNl.N, 2la0n1g4u)ageemmboedde-l on the data,
 
(n 1) (n 2)/2 negative pairs. It turns aonudtcthomatputedthienglosgtloikemlihooroedsoofpshenistetinccaetes idinstuwpoewrvaiysse,donocreguinvseun-thepreceding
  ⇤   context (conditional log likelihood) and once with no context (marginal log likeli-
the quadratic number of negatives provides a rich pervised pre-trained sentence encoders (Conneau
hood). Thedifferencebetweenthesevaluestellsushowmuchtheprecedingcontext
enough learning signal, while at the same time, is et al., 2017). As mentioned in the introduction,
improvedthepredictabilityofs,apredictabilitymeasureofcoherence.
i
not too prohibitively large to be effectively Tco rav in- ing msiondceelsgteonperreadtiicvtelomngoedreclsonctaexntsotfhtaennjbuesttcuornnseedcuitinvteopairs of sen-
ered by a sampling procedure. In practtiecnec,eswcean ressuelnttiennecveenensctroodnegre,rgdeisnceoruartsieverecporehseernetantcioenms.oFdoerlecxaanmple a Trans-
sample a new set of negatives each timefworemesreleanguabgeemleovdeerlagtreaidnebdywiothuracmoontdraesltivtoesbeennteenficet ofbrojemctivtehetopredicttext
uptoadistanceof 2sentencesimprovesperformanceonvariousdiscoursecoher-
a document, hence after many epochs, we can ef- advantages of both generative and discriminative
±
encetasks(Iteretal.,2020).
fectively cover the space for even very long doc- training,similarto(Kirosetal.,2015;Petersetal.,
Language-model style models are generally evaluated by the methods of Sec-
uments. Section 5.7 discusses further details on 2018). After initialization, we freeze the genera-
tion 27.3.3, although they can also be evaluated on the RST and PDTB coherence
sampling. relationtaskst.ivemodelparameterstoavoidoverfitting.
In Section 5, we will experimentally show that
4.1 ModelArchitecture
while we do benefit from strong pre-trained en-
27.5 Global Coherence
Thespecificneuralarchitecturethatweuseforf coders,thefactthatourlocaldiscriminativemodel
✓
is illustrated in Figure 1. We assume the use of improvesoverpreviousmethodsisindependentof
some pre-trained sentence encoder, whichAisdidscios-urse mthuestcahlosoicceoohefrseegnltoebnaclleyernatchoedrethr.an just at the level of pairs of sen-
cussedinthenextsection. tences. Consider stories, for example. The narrative structure of stories is one of
Given an input sentence pair, the
sentet nh ce eol ed ne -stkin5dsofEgxlopbearlicmoheernetnscetobestudied. InhisinfluentialMorphologyof
the Folktale, Propp (1968) models the discourse structure of Russian folktales via
codermapsthesentencestoreal-valuedvectorsS
akindofplotgrammar. Hismodelincludesasetofcharactercategorieshecalled
5.1 EvaluationTasks
and T. We then compute the concatenation of the
dramatis personae, like Hero, Villain, Donor, or Helper, and a set of events he
following features: (1) concatenation of tchaelletdwfounctioFnosll(oliwkein“gViNllaginuyceonmmanitdskJoidtnyap(2p0in1g7”),“aDnodnoorthteesrtspHree-ro”,or“Hero
vectors(S,T);(2)element-wisedifferenceisSpursTu;ed”) vthiaotuhsavweotroko,cwcuer einvaplauratitceuloarurormdeor,daellosngonwitthheotdhiesr-components.
 
(3)element-wiseproductS T;(4)absolutP ero vp ap lus ehowscthraimtthineaptliootnsoafnedacihnsoefrtthieonfaitraysktasle.sAhedsdtiutdioiensaclalyn,bweerepresentedas
⇤
of element-wise difference S T . The concate- evaluate on the paragraph reconstruction task in
|   |
nated feature representation is then fed to a one- open-domain settings, in a similar manner to Li
layerMLPtooutputthecoherencescore. andJurafsky(2017).
In practice, we make our overall coherence In the discrimination task, a document is com-
model bidirectional, by training a forward model pared to a random permutation of its sentences,
with input (S,T) and a backward model with in- andthemodelisconsideredcorrectifitscoresthe
put (T,S) with the same architecture but separate original document higher than the permuted one.
parameters. The coherence score is then the aver- Twentypermutationsareusedinthetestsetinac-
agefromthetwomodels. cordancewithpreviouswork.
68127.5 • GLOBALCOHERENCE 559
asequenceofthesefunctions,differenttaleschoosingdifferentsubsetsoffunctions,
but always in the same order. Indeed Lakoff (1972b) showed that Propp’s model
amountedtoadiscoursegrammarofstories,andinrecentcomputationalworkFin-
layson(2016)demonstratesthatsomeoftheseProppianfunctionscouldbeinduced
from corpora of folktale texts by detecting events that have similar actions across
stories. Bamman et al. (2013) showed that generalizations over dramatis personae
could be induced from movie plot summaries on Wikipedia. Their model induced
latentpersonaefromfeaturesliketheactionsthecharactertakes(e.g.,Villainsstran-
gle),theactionsdonetothem(e.g.,Villainsarefoiledandarrested)orthedescriptive
wordsusedofthem(Villainsareevil).
In this section we introduce two kinds of such global discourse structure that
have been widely studied computationally. The first is the structure of arguments:
the way people attempt to convince each other in persuasive essays by offering
claims and supporting premises. The second is somewhat related: the structure of
scientificpapers,andthewayauthorspresenttheirgoals,results,andrelationshipto
priorworkintheirpapers.
27.5.1 ArgumentationStructure
Thefirsttypeofglobaldiscoursestructureisthestructureofarguments. Analyzing
argumentation people’sargumentationcomputationallyisoftencalledargumentationmining.
mining
ThestudyofargumentsdatesbacktoAristotle,whoinhisRhetoricsdescribed
pathos three components of a good argument: pathos (appealing to the emotions of the
ethos listener),ethos(appealingtothespeaker’spersonalcharacter),andlogos(thelogical
logos structureoftheargument).
Mostofthediscoursestructurestudiesofargumentationhavefocusedonlogos,
particularlyviabuildingandtrainingonannotateddatasetsofpersuasiveessaysor
other arguments (Reed et al. 2008, Stab and Gurevych 2014a, Peldszus and Stede
2016, Habernal and Gurevych 2017, Musi et al. 2018). Such corpora, for exam-
claims ple,oftenincludeannotationsofargumentativecomponentslikeclaims(thecentral
premises component of the argument that is controversial and needs support) and premises
(the reasons given by the author to persuade the reader by supporting or attacking
argumentative theclaimorotherpremises),aswellastheargumentativerelationsbetweenthem
relations
likeSUPPORTandATTACK.
ConsiderthefollowingexampleofapersuasiveessayfromStabandGurevych
(2014b). The first sentence (1) presents a claim (in bold). (2) and (3) present two
premisessupportingtheclaim. (4)givesapremisesupportingpremise(3).
“(1)Museumsandartgalleriesprovideabetterunderstanding
about arts than Internet. (2) In most museums and art galleries, de-
tailed descriptions in terms of the background, history and author are
provided. (3) Seeing an artwork online is not the same as watching it
withourowneyes, as(4)thepictureonlinedoesnotshowthetexture
orthree-dimensionalstructureoftheart,whichisimportanttostudy.”
Thusthisexamplehasthreeargumentativerelations: SUPPORT(2,1),SUPPORT(3,1)
and SUPPORT(4,3). Fig. 27.12 shows the structure of a much more complex argu-
ment.
While argumentation mining is clearly related to rhetorical structure and other
kindsofcoherencerelations,argumentstendtobemuchlesslocal;oftenapersua-
siveessaywillhaveonlyasinglemainclaim,withpremisesspreadthroughoutthe
text,withoutthelocalcoherenceweseeincoherencerelations.StabandGurevych ParsingArgumentationStructures
cloning.Thisexampleillustratesthatknowingargumentativerelationsisimportantfor
separating several arguments in a paragraph. The example also shows that argument
componentsfrequentlyexhibitprecedingtextunitsthatarenotrelevanttotheargument
buthelpfulforrecognizingtheargumentcomponenttype.Forexample,precedingdis-
course connectors like “therefore”, “consequently”, or “thus” can signal a subsequent
claim. Discourse markers like “because”, “since”, or “furthermore” could indicate a
premise.Formally,theseprecedingtokensofanargumentcomponentstartingattoken
t are defined as the tokens t ,...,t that are not covered by another argument
i i m i 1
componentinthesentences=t 
,t
,...,t 
where1 i nandi m 1.Thethirdbody
1 2 n      
paragraphillustratesacontraargumentandargumentativeattackrelations:
Admittedly, [cloning could be misused for military purposes] . For example,
Claim5
[it could be used to manipulate human genes in order to create obedient soldiers
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
with extraordinary abilities] . However, because [moral and ethical values are
:::::::::::::::::::::::Premise9 ::::::::::::::::::::::::
internationally shared] , [it is very unlikely that cloning will be misused for
:::::::::::::::::: Premise10 ::::::::::::::::::::::::::::::::::::::::::::
militantobjectives] .
:::::::::::::::Premise11
TheparagraphbeginswithClaim ,whichattacksthestanceoftheauthor.Itissupported
5
by Premise in the second sentence. The third sentence includes two premises, both of
9
which defend the stance of the author. Premise is an attack of Claim , and Premise
11 5 10
supports Premise . The last paragraph (conclusion) restates the major claim and sum-
11
marizesthemainaspectsoftheessay:
To sum up, although [permitting cloning might bear some risks like misuse for
military purposes] , I strongly believe that [this technology is beneficial to
Claim6
humanity] .Itislikelythat[thistechnologybearssomeimportantcureswhich
MajorClaim2
willsignificantlyimprovelifeconditions] .
Claim7
Theconclusionoftheessaystartswithanattackingclaimfollowedbytherestatementof
themajorclaim.Thelastsentenceincludesanotherclaimthatsummarizesthemostim-
portantpointsoftheauthor’sargumentation.Figure2showstheentireargumentation
structureoftheexampleessay.
560 CHAPTER27 • DISCOURSECOHERENCE
Figure2
Figure27.12 Argumentation structure of a persuasive essay. Arrows indicate argumentation relations, ei-
Argumentationstructureoftheexampleessay.Arrowsindicateargumentativerelations.
therofSUPPORT(witharrowheads)orATTACK(withcircleheads);Pdenotespremises. FigurefromStaband
Arrowheadsdenoteargumentativesupportrelationsandcircleheadsattackrelations.Dashed
Gurevych(2017).
linesindicaterelationsthatareencodedinthestanceattributesofclaims.“P”denotespremises.
Algorithms for detecting argumentation structure often include classifiers for
distinguishingclaims, premises, ornon-argumentation, togetherwithrelation6c2l9as-
sifiers for deciding if two spans have the SUPPORT, ATTACK, or neither relation
(PeldszusandStede,2013). Whilethesearethemainfocusofmuchcomputational
work, there is also preliminary efforts on annotating and detecting richer semantic
relationships(ParkandCardie2014,Hideyetal.2017)suchasdetectingargumen-
argumentation tationschemes,larger-scalestructuresforargumentlikeargumentfromexample,
schemes
or argument from cause to effect, or argument from consequences (Feng and
Hirst,2011).
Anotherimportantlineofresearchisstudyinghowtheseargumentstructure(or
other features) are associated with the success or persuasiveness of an argument
(HabernalandGurevych2016, Tanetal.2016, Hideyetal.2017. Indeed, whileit
isAristotle’slogosthatismostrelatedtodiscoursestructure, Aristotle’sethosand
pathos techniques are particularly relevant in the detection of mechanisms of this
persuasion sortofpersuasion. Forexamplescholarshaveinvestigatedthelinguisticrealization
offeaturesstudiedbysocialscientistslikereciprocity(peoplereturnfavors),social
proof (people follow others’ choices), authority (people are influenced by those
with power), and scarcity (people value things that are scarce), all of which can
bebroughtupinapersuasiveargument(Cialdini,1984). RosenthalandMcKeown
(2017)showedthatthesefeaturescouldbecombinedwithargumentationstructure
to predict who influences whom on social media, Althoff et al. (2014) found that
linguistic models of reciprocity and authority predicted success in online requests,
whilethesemisupervisedmodelofYangetal.(2019)detectedmentionsofscarcity,
commitment,andsocialidentitytopredictthesuccessofpeer-to-peerlendingplat-
forms.
SeeStedeandSchneider(2018)foracomprehensivesurveyofargumentmining.
27.5.2 Thestructureofscientificdiscourse
Scientificpapershaveaveryspecificglobalstructure: somewhereinthecourseof
the paper the authors must indicate a scientific goal, develop a method for a solu-
tion, provide evidence for the solution, and compare to prior work. One popular27.6 • SUMMARY 561
annotation scheme for modeling these rhetorical goals is the argumentative zon-
argumentative ingmodelofTeufeletal.(1999)andTeufeletal.(2009),whichisinformedbythe
zoning
ideathateachscientificpapertriestomakeaknowledgeclaimaboutanewpiece
of knowledge being added to the repository of the field (Myers, 1992). Sentences
inascientificpapercanbeassignedoneof15tags;Fig.27.13shows7(shortened)
examplesoflabeledsentences.
Category Description Example
AIM Statement of specific research goal, or “Theaimofthisprocessistoexaminetherolethat
hypothesisofcurrentpaper trainingplaysinthetaggingprocess”
OWN METHOD New Knowledge claim, own work: “Inorderforittobeusefulforourpurposes,the
methods followingextensionsmustbemade:”
OWN RESULTS Measurable/objective outcome of own “Allthecurveshaveagenerallyupwardtrendbut
work alwaysliefarbelowbackoff(51%errorrate)”
USE Otherworkisusedinownwork “We use the framework for the allocation and
transferofcontrolofWhittaker....”
GAP WEAK Lack of solution in field, problem with “Here, we will produce experimental evidence
othersolutions suggestingthatthissimplemodelleadstoserious
overestimates”
SUPPORT Otherworksupportscurrentworkoris “Worksimilartothatdescribedherehasbeencar-
supportedbycurrentwork riedoutbyMerialdo(1994),withbroadlysimilar
conclusions.”
ANTISUPPORT Clashwithother’sresultsortheory;su- “Thisresultchallengestheclaimsof...”
periorityofownwork
Figure27.13 Examplesfor7ofthe15labelsfromtheArgumentativeZoninglabelset(Teufeletal.,2009).
Teufeletal.(1999)andTeufeletal.(2009)developlabeledcorporaofscientific
articlesfromcomputationallinguisticsandchemistry,whichcanbeusedassupervi-
sionfortrainingstandardsentence-classificationarchitecturetoassignthe15labels.
27.6 Summary
Inthischapterweintroducedlocalandglobalmodelsfordiscoursecoherence.
• Discoursesarenotarbitrarycollectionsofsentences; theymustbecoherent.
Among the factors that make a discourse coherent are coherence relations
betweenthesentences,entity-basedcoherence,andtopicalcoherence.
• Varioussetsofcoherencerelationsandrhetoricalrelationshavebeenpro-
posed. The relations in Rhetorical Structure Theory (RST) hold between
spans of text and are structured into a tree. Because of this, shift-reduce
and other parsing algorithms are generally used to assign these structures.
ThePennDiscourseTreebank(PDTB)labelsonlyrelationsbetweenpairsof
spans,andthelabelsaregenerallyassignedbysequencemodels.
• Entity-based coherence captures the intuition that discourses are about an
entity, and continue mentioning the entity from sentence to sentence. Cen-
teringTheoryisafamilyofmodelsdescribinghowsalienceismodeledfor
discourseentities,andhencehowcoherenceisachievedbyvirtueofkeeping
thesamediscourseentitiessalientoverthediscourse. Theentitygridmodel
gives a more bottom-up way to compute which entity realization transitions
leadtocoherence.562 CHAPTER27 • DISCOURSECOHERENCE
• Many different genres have different types of global coherence. Persuasive
essays have claims and premises that are extracted in the field of argument
mining,scientificarticleshavestructurerelatedtoaims,methods,results,and
comparisons.
Bibliographical and Historical Notes
Coherencerelationsarosefromtheindependentdevelopmentofanumberofschol-
ars,includingHobbs(1979)ideathatcoherencerelationsplayaninferentialrolefor
the hearer, and the investigations by Mann and Thompson (1987) of the discourse
structure of large texts. Other approaches to coherence relations and their extrac-
SDRT tionincludeSegmentedDiscourseRepresentationTheory(SDRT)(AsherandLas-
carides 2003, Baldridge et al. 2007) and the Linguistic Discourse Model (Polanyi
1988, SchaandPolanyi1988, Polanyietal.2004). WolfandGibson(2005)argue
thatcoherencestructureincludescrossedbracketings, whichmakeitimpossibleto
represent as a tree, and propose a graph representation instead. A compendium of
over 350 relations that have been proposed in the literature can be found in Hovy
(1990).
RSTparsingwasfirstproposedbyMarcu(1997),andearlyworkwasrule-based,
focusedondiscoursemarkers(Marcu,2000a). ThecreationoftheRSTDiscourse
TreeBank (Carlson et al. 2001, Carlson and Marcu 2001) enabled a wide variety
of machine learning algorithms, beginning with the shift-reduce parser of Marcu
(1999)thatuseddecisiontreestochooseactions,andcontinuingwithawidevariety
ofmachinelearnedparsingmethods(SoricutandMarcu2003,Sagae2009,Hernault
etal.2010,FengandHirst2014,Surdeanuetal.2015,Jotyetal.2015)andchunkers
(SporlederandLapata,2005).SubbaandDiEugenio(2009)integratedsophisticated
semanticinformationintoRSTparsing. JiandEisenstein(2014)firstappliedneural
models to RST parsing neural models, leading to the modern set of neural RST
models(Lietal.2014,Lietal.2016d,Braudetal.2017,Yuetal.2018,interalia)
aswellasneuralsegmenters(Wangetal.2018b). andneuralPDTBparsingmodels
(JiandEisenstein2015,Qinetal.2016,Qinetal.2017).
Barzilay and Lapata (2005) pioneered the idea of self-supervision for coher-
ence: training a coherence model to distinguish true orderings of sentences from
randompermutations. Lietal.(2014)firstappliedthisparadigmtoneuralsentence-
representation, and many neural self-supervised models followed (Li and Jurafsky
2017, Logeswaran et al. 2018, Lai and Tetreault 2018, Xu et al. 2019, Iter et al.
2020)
Anotheraspectofglobalcoherenceistheglobaltopicstructureofatext,theway
thetopicsshiftoverthecourseofthedocument.BarzilayandLee(2004)introduced
an HMM model for capturing topics for coherence, and later work expanded this
intuition(SoricutandMarcu2006,Elsneretal.2007,LouisandNenkova2012,Li
andJurafsky2017).
The relationship between explicit and implicit discourse connectives has been
a fruitful one for research. Marcu and Echihabi (2002) first proposed to use sen-
tenceswithexplicitrelationstohelpprovidetrainingdataforimplicitrelations,by
removing the explicit relations and trying to re-predict them as a way of improv-
ing performance on implicit connectives; this idea was refined by Sporleder and
Lascarides(2005), (Pitleretal.,2009), andRutherfordandXue(2015). Thisrela-BIBLIOGRAPHICALANDHISTORICALNOTES 563
tionship can also be used as a way to create discourse-aware representations. The
DisSentalgorithm(Nieetal.,2019)createsthetaskofpredictingexplicitdiscourse
markersbetweentwosentences. Theyshowthatrepresentationslearnedtobegood
at this task also function as powerful sentence representations for other discourse
tasks.
Theideaofentity-basedcoherenceseemstohaveariseninmultiplefieldsinthe
mid-1970s, in functional linguistics (Chafe, 1976), in the psychology of discourse
processing(KintschandVanDijk,1978),andintheroughlycontemporaneouswork
of Grosz, Sidner, Joshi, and their colleagues. Grosz (1977a) addressed the focus
ofattentionthatconversationalparticipantsmaintainasthediscourseunfolds. She
defined two levels of focus; entities relevant to the entire discourse were said to
be in global focus, whereas entities that are locally in focus (i.e., most central to
a particular utterance) were said to be in immediate focus. Sidner 1979; 1983 de-
scribedamethodfortracking(immediate)discoursefociandtheiruseinresolving
pronounsanddemonstrativenounphrases. Shemadeadistinctionbetweenthecur-
rentdiscoursefocusandpotentialfoci,whicharethepredecessorstothebackward-
andforward-lookingcentersofCenteringtheory,respectively.Thenameandfurther
rootsofthecenteringapproachlieinpapersbyJoshiandKuhn(1979)andJoshiand
Weinstein(1981),whoaddressedtherelationshipbetweenimmediatefocusandthe
inferencesrequiredtointegratethecurrentutteranceintothediscoursemodel.Grosz
etal.(1983)integratedthisworkwiththepriorworkofSidnerandGrosz. Thisled
to a manuscript on centering which, while widely circulated since 1986, remained
unpublished until Grosz et al. (1995). A collection of centering papers appears in
Walker et al. (1998b). See Karamanis et al. (2004) and Poesio et al. (2004) for a
deeperexplorationofcenteringanditsparameterizations,andtheHistorysectionof
Chapter26formoreontheuseofcenteringoncoreference.
The grid model of entity-based coherence was first proposed by Barzilay and
Lapata (2005) drawing on earlier work by Lapata (2003) and Barzilay, and then
extended by them Barzilay and Lapata (2008) and others with additional features
(Elsner and Charniak 2008, 2011, Feng et al. 2014, Lin et al. 2011) a model that
projectsentitiesintoaglobalgraphforthediscourse(GuinaudeauandStrube2013,
MesgarandStrube2016),andaconvolutionalmodeltocapturelonger-rangeentity
dependencies(NguyenandJoty,2017).
Theoriesofdiscoursecoherencehavealsobeenusedinalgorithmsforinterpret-
ing discourse-level linguistic phenomena, including verb phrase ellipsis and gap-
ping (Asher 1993, Kehler 1993), and tense interpretation (Lascarides and Asher
1993, Kehler 1994, Kehler 2000). An extensive investigation into the relationship
between coherence relations and discourse connectives can be found in Knott and
Dale(1994).
Useful surveys of discourse processing and structure include Stede (2011) and
Webberetal.(2012).
AndyKehlerwrotetheDiscoursechapterforthe2000firsteditionofthistext-
book,whichweusedasthestartingpointforthesecond-editionchapter,andthere
aresomeremnantsofAndy’slovelyprosestillinthisthird-editioncoherencechap-
ter.564 CHAPTER27 • DISCOURSECOHERENCE
Exercises
27.1 FinishtheCenteringTheoryprocessingofthelasttwoutterancesof(27.30),
andshowhow(27.29)wouldbeprocessed. Doesthealgorithmindeedmark
(27.29)aslesscoherent?
27.2 Select an editorial column from your favorite newspaper, and determine the
discourse structure for a 10–20 sentence portion. What problems did you
encounter? Were you helped by superficial cues the speaker included (e.g.,
discourseconnectives)inanyplaces?CHAPTER
28 Phonetics
Thecharactersthatmakeupthetextswe’vebeendiscussinginthisbookaren’tjust
randomsymbols. Theyarealsoanamazingscientificinvention: atheoreticalmodel
oftheelementsthatmakeuphumanspeech.
The earliest writing systems we know of (Sumerian, Chinese, Mayan) were
mainly logographic: one symbol representing a whole word. But from the ear-
liest stages we can find, some symbols were also used to represent the sounds
that made up words. The cuneiform sign to the right pro-
nounced ba and meaning “ration” in Sumerian could also
functionpurelyasthesound/ba/. TheearliestChinesechar-
acters we have, carved into bones for divination, similarly
contain phonetic elements. Purely sound-based writing systems, whether syllabic
(likeJapanesehiragana),alphabetic(liketheRomanalphabet),orconsonantal(like
Semitic writing systems), trace back to these early logo-syllabic systems, often as
twoculturescametogether. Thus,theArabic,Aramaic,Hebrew,Greek,andRoman
systemsallderivefromaWestSemiticscriptthatispresumedtohavebeenmodified
byWesternSemiticmercenariesfromacursiveformofEgyptianhieroglyphs. The
JapanesesyllabariesweremodifiedfromacursiveformofChinesephoneticcharac-
ters,whichthemselveswereusedinChinesetophoneticallyrepresenttheSanskrit
intheBuddhistscripturesthatcametoChinaintheTangdynasty.
Thisimplicitideathatthespokenwordiscomposedofsmallerunitsofspeech
underliesalgorithmsforbothspeechrecognition(transcribingwaveformsintotext)
andtext-to-speech(convertingtextintowaveforms). Inthischapterwegiveacom-
phonetics putational perspective on phonetics, the study of the speech sounds used in the
languagesoftheworld, howtheyareproducedinthehumanvocaltract, howthey
arerealizedacoustically,andhowtheycanbedigitizedandprocessed.
28.1 Speech Sounds and Phonetic Transcription
A letter like ‘p’ or ‘a’ is already a useful model of the sounds of human speech,
and indeed we’ll see in Chapter 16 how to map between letters and waveforms.
Nonetheless,itishelpfultorepresentsoundsslightlymoreabstractly. We’llrepre-
phone sent the pronunciation of a word as a string of phones, which are speech sounds,
eachrepresentedwithsymbolsadaptedfromtheRomanalphabet.
The standard phonetic representation for transcribing the world’s languages is
IPA theInternationalPhoneticAlphabet(IPA),anevolvingstandardfirstdevelopedin
1888,Butinthischapterwe’llinsteadrepresentphoneswiththeARPAbet(Shoup,
1980),asimplephoneticalphabet(Fig.28.1)thatconvenientlyusesASCIIsymbols
torepresentanAmerican-EnglishsubsetoftheIPA.
Many of the IPA and ARPAbet symbols are equivalent to familiar Roman let-
ters. So,forexample,theARPAbetphone[p]representstheconsonantsoundatthe566 CHAPTER28 • PHONETICS
ARPAbet IPA ARPAbet ARPAbet IPA ARPAbet
Symbol Symbol Word Transcription Symbol Symbol Word Transcription
[p] [p] parsley [paarsliy] [iy] [i] lily [lihliy]
[t] [t] tea [tiy] [ih] [I] lily [lihliy]
[k] [k] cook [kuhk] [ey] [eI] daisy [deyziy]
[b] [b] bay [bey] [eh] [E] pen [pehn]
[d] [d] dill [dihl] [ae] [æ] aster [aestaxr]
[g] [g] garlic [gaarlixk] [aa] [A] poppy [paapiy]
[m] [m] mint [mihnt] [ao] [O] orchid [aorkixd]
[n] [n] nutmeg [nahtmehg] [uh] [U] wood [wuhd]
[ng] [N] baking [beykixng] [ow] [oU] lotus [lowdxaxs]
[f] [f] flour [flawaxr] [uw] [u] tulip [tuwlixp]
[v] [v] clove [klowv] [ah] [2] butter [bahdxaxr]
[th] [T] thick [thihk] [er] [Ç] bird [berd]
[dh] [D] those [dhowz] [ay] [aI] iris [ayrixs]
[s] [s] soup [suwp] [aw] [aU] flower [flawaxr]
[z] [z] eggs [ehgz] [oy] [oI] soil [soyl]
[sh] [S] squash [skwaash] [ax] [@] pita [piytax]
[zh] [Z] ambrosia [aembrowzhax]
[ch] [tS] cherry [chehriy]
[jh] [dZ] jar [jhaar]
[l] [l] licorice [lihkaxrixsh]
[w] [w] kiwi [kiywiy]
[r] [r] rice [rays]
[y] [j] yellow [yehlow]
[h] [h] honey [hahniy]
Figure28.1 ARPAbetandIPAsymbolsforEnglishconsonants(left)andvowels(right).
beginningofplatypus,puma,andplantain,themiddleofleopard,ortheendofan-
telope. Ingeneral,however,themappingbetweenthelettersofEnglishorthography
andphonesisrelativelyopaque; asinglelettercanrepresentverydifferentsounds
indifferentcontexts. TheEnglishletterccorrespondstophone[k]incougar[kuw
gaxr],butphone[s]incell[sehl]. Besidesappearingascandk,thephone[k]can
appearaspartofx(fox[faaks]),asck(jackal[jhaekel])andascc(raccoon[rae
kuwn]). Manyotherlanguages,forexample,Spanish,aremuchmoretransparent
intheirsound-orthographymappingthanEnglish.
28.2 Articulatory Phonetics
articulatory Articulatoryphoneticsisthestudyofhowthesephonesareproducedasthevarious
phonetics
organsinthemouth,throat,andnosemodifytheairflowfromthelungs.
TheVocalOrgans
Figure28.2showstheorgansofspeech. Soundisproducedbytherapidmovement
ofair. Humansproducemostsoundsinspokenlanguagesbyexpellingairfromthe
lungs through the windpipe (technically, the trachea) and then out the mouth or
nose. Asitpassesthroughthetrachea,theairpassesthroughthelarynx,commonly
known as the Adam’s apple or voice box. The larynx contains two small folds of28.2 • ARTICULATORYPHONETICS 567
Figure28.2 The vocal organs, shown in side view. (Figure from OpenStax University
Physics,CCBY4.0)
muscle,thevocalfolds(oftenreferredtonon-technicallyasthevocalcords),which
can be moved together or apart. The space between these two folds is called the
glottis glottis. Ifthefoldsareclosetogether(butnottightlyclosed),theywillvibrateasair
passesthroughthem;iftheyarefarapart,theywon’tvibrate. Soundsmadewiththe
voicedsound vocalfoldstogetherandvibratingarecalledvoiced;soundsmadewithoutthisvocal
unvoicedsound cordvibrationarecalledunvoicedorvoiceless. Voicedsoundsinclude[b],[d],[g],
[v],[z],andalltheEnglishvowels,amongothers. Unvoicedsoundsinclude[p],[t],
[k],[f],[s],andothers.
Theareaabovethetracheaiscalledthevocaltract;itconsistsoftheoraltract
andthenasaltract. Aftertheairleavesthetrachea,itcanexitthebodythroughthe
mouthorthenose. Mostsoundsaremadebyairpassingthroughthemouth. Sounds
nasal made by air passing through the nose are called nasal sounds; nasal sounds (like
English[m],[n],and[ng])useboththeoralandnasaltractsasresonatingcavities.
consonant Phonesaredividedintotwomainclasses: consonantsandvowels. Bothkinds
vowel ofsoundsareformedbythemotionofairthroughthemouth,throatornose. Con-
sonantsaremadebyrestrictionorblockingoftheairflowinsomeway,andcanbe
voicedorunvoiced. Vowelshavelessobstruction,areusuallyvoiced,andaregen-
erallylouderandlonger-lastingthanconsonants. Thetechnicaluseofthesetermsis
muchlikethecommonusage;[p],[b],[t],[d],[k],[g],[f],[v],[s],[z],[r],[l],etc.,
are consonants; [aa], [ae], [ao], [ih], [aw], [ow], [uw], etc., are vowels. Semivow-
els (such as [y] and [w]) have some of the properties of both; they are voiced like
vowels,buttheyareshortandlesssyllabiclikeconsonants.568 CHAPTER28 • PHONETICS
Consonants: PlaceofArticulation
Becauseconsonantsaremadebyrestrictingairflow,wecangroupthemintoclasses
placeof bytheirpointofmaximumrestriction,theirplaceofarticulation(Fig.28.3).
articulation
(nasal tract)
alveolar
dental
palatal velar
bilabial
glottal
Figure28.3 MajorEnglishplacesofarticulation.
labial Labial: Consonantswhosemainrestrictionisformedbythetwolipscomingto-
gether have a bilabial place of articulation. In English these include [p] as
in possum, [b] as in bear, and [m] as in marmot. The English labiodental
consonants[v]and[f]aremadebypressingthebottomlipagainsttheupper
rowofteethandlettingtheairflowthroughthespaceintheupperteeth.
dental Dental: Soundsthataremadebyplacingthetongueagainsttheteetharedentals.
ThemaindentalsinEnglisharethe[th]ofthingandthe[dh]ofthough,which
aremadebyplacingthetonguebehindtheteethwiththetipslightlybetween
theteeth.
alveolar Alveolar: Thealveolarridgeistheportionoftheroofofthemouthjustbehindthe
upperteeth. MostspeakersofAmericanEnglishmakethephones[s],[z],[t],
and[d]byplacingthetipofthetongueagainstthealveolarridge. Theword
coronalisoftenusedtorefertobothdentalandalveolar.
palatal Palatal: The roof of the mouth (the palate) rises sharply from the back of the
palate alveolar ridge. The palato-alveolar sounds [sh] (shrimp), [ch] (china), [zh]
(Asian),and[jh](jar)aremadewiththebladeofthetongueagainsttherising
backofthealveolarridge. Thepalatalsound[y]ofyakismadebyplacingthe
frontofthetongueupclosetothepalate.
velar Velar:Thevelum,orsoftpalate,isamovablemuscularflapattheverybackofthe
roofofthemouth. Thesounds[k](cuckoo),[g](goose),and[N](kingfisher)
aremadebypressingthebackofthetongueupagainstthevelum.
glottal Glottal: Theglottalstop[q]ismadebyclosingtheglottis(bybringingthevocal
foldstogether).
Consonants: MannerofArticulation
Consonantsarealsodistinguishedbyhowtherestrictioninairflowismade,forex-
ample,byacompletestoppageofairorbyapartialblockage. Thisfeatureiscalled
mannerof themannerofarticulationofaconsonant. Thecombinationofplaceandmanner
articulation
ofarticulationisusuallysufficienttouniquelyidentifyaconsonant. Followingare
themajormannersofarticulationforEnglishconsonants:
stop A stop is a consonant in which airflow is completely blocked for a short time.
Thisblockageisfollowedbyanexplosivesoundastheairisreleased. Theperiod
of blockage is called the closure, and the explosion is called the release. English28.2 • ARTICULATORYPHONETICS 569
hasvoicedstopslike[b],[d],and[g]aswellasunvoicedstopslike[p],[t],and[k].
Stopsarealsocalledplosives.
nasal Thenasalsounds[n],[m],and[ng]aremadebyloweringthevelumandallow-
ingairtopassintothenasalcavity.
fricatives In fricatives, airflow is constricted but not cut off completely. The turbulent
airflowthatresultsfromtheconstrictionproducesacharacteristic“hissing”sound.
The English labiodental fricatives [f] and [v] are produced by pressing the lower
lip against the upper teeth, allowing a restricted airflow between the upper teeth.
Thedentalfricatives[th]and[dh]allowairtoflowaroundthetonguebetweenthe
teeth. The alveolar fricatives [s] and [z] are produced with the tongue against the
alveolarridge,forcingairovertheedgeoftheteeth. Inthepalato-alveolarfricatives
[sh] and [zh], the tongue is at the back of the alveolar ridge, forcing air through a
grooveformedinthetongue. Thehigher-pitchedfricatives(inEnglish[s],[z],[sh]
sibilants and[zh])arecalledsibilants. Stopsthatarefollowedimmediatelybyfricativesare
calledaffricates;theseincludeEnglish[ch](chicken)and[jh](giraffe).
approximant Inapproximants,thetwoarticulatorsareclosetogetherbutnotcloseenoughto
causeturbulentairflow. InEnglish[y](yellow),thetonguemovesclosetotheroof
ofthemouthbutnotcloseenoughtocausetheturbulencethatwouldcharacterizea
fricative. InEnglish[w](wood), thebackofthetonguecomesclosetothevelum.
American [r] can be formed in at least two ways; with just the tip of the tongue
extendedandclosetothepalateorwiththewholetonguebunchedupnearthepalate.
[l]isformedwiththetipofthetongueupagainstthealveolarridgeortheteeth,with
one or both sides of the tongue lowered to allow air to flow over it. [l] is called a
lateralsoundbecauseofthedropinthesidesofthetongue.
tap Ataporflap[dx]isaquickmotionofthetongueagainstthealveolarridge.The
consonantinthemiddleofthewordlotus([lowdxaxs])isatapinmostdialectsof
AmericanEnglish;speakersofmanyU.K.dialectswouldusea[t]instead.
Vowels
Likeconsonants, vowelscanbecharacterizedby thepositionofthearticulatorsas
they are made. The three most relevant parameters for vowels are what is called
vowel height, which correlates roughly with the height of the highest part of the
tongue, vowelfrontnessorbackness, indicatingwhetherthishighpointistoward
the front or back of the oral tract and whether the shape of the lips is rounded or
not. Figure28.4showsthepositionofthetonguefordifferentvowels.
tongue palate
closed
velum
beet [iy] bat [ae] boot [uw]
Figure28.4 TonguepositionsforEnglishhighfront[iy],lowfront[ae]andhighback[uw].
In the vowel [iy], for example, the highest point of the tongue is toward the
front of the mouth. In the vowel [uw], by contrast, the high-point of the tongue is
locatedtowardthebackofthemouth. Vowelsinwhichthetongueisraisedtoward
Frontvowel the front are called front vowels; those in which the tongue is raised toward the570 CHAPTER28 • PHONETICS
backvowel back are called back vowels. Note that while both [ih] and [eh] are front vowels,
thetongueishigherfor[ih]thanfor[eh]. Vowelsinwhichthehighestpointofthe
highvowel tongueiscomparativelyhigharecalledhighvowels;vowelswithmidorlowvalues
ofmaximumtongueheightarecalledmidvowelsorlowvowels,respectively.
high
iy y uw
uw
ih
uh
w
ey oy ax o
front back
eh w
a ao
ah
ay
ae aa
low
Figure28.5 Theschematic“vowelspace”forEnglishvowels.
Figure28.5showsaschematiccharacterizationoftheheightofdifferentvowels.
Itisschematicbecausetheabstractpropertyheightcorrelatesonlyroughlywithac-
tualtonguepositions;itis,infact,amoreaccuratereflectionofacousticfacts. Note
thatthecharthastwokindsofvowels: thoseinwhichtongueheightisrepresented
asapointandthoseinwhichitisrepresentedasapath.Avowelinwhichthetongue
diphthong positionchangesmarkedlyduringtheproductionofthevowelisadiphthong. En-
glishisparticularlyrichindiphthongs.
Thesecondimportantarticulatorydimensionforvowelsistheshapeofthelips.
Certain vowels are pronounced with the lips rounded (the same lip shape used for
roundedvowel whistling). Theseroundedvowelsinclude[uw],[ao],and[ow].
Syllables
syllable Consonantsandvowelscombinetomakeasyllable. Asyllableisavowel-like(or
sonorant) sound together with some of the surrounding consonants that are most
closelyassociatedwithit. Theworddoghasonesyllable,[daag](inourdialect);
the word catnip has two syllables, [k ae t] and [n ih p]. We call the vowel at the
nucleus coreofasyllablethenucleus.Initialconsonants,ifany,arecalledtheonset.Onsets
onset withmorethanoneconsonant(asinstrike[strayk]),arecalledcomplexonsets.
coda Thecodaistheoptionalconsonantorsequenceofconsonantsfollowingthenucleus.
rime Thus[d]istheonsetofdog,and[g]isthecoda. Therime,orrhyme,isthenucleus
pluscoda. Figure28.6showssomesamplesyllablestructures.
Thetaskofautomaticallybreakingupawordintosyllablesiscalledsyllabifica-
syllabification tion.Syllablestructureisalsocloselyrelatedtothephonotacticsofalanguage.The
phonotactics termphonotacticsmeanstheconstraintsonwhichphonescanfolloweachotherin
a language. For example, English has strong constraints on what kinds of conso-
nantscanappeartogetherinanonset;thesequence[zdr],forexample,cannotbea
legalEnglishsyllableonset. Phonotacticscanberepresentedbyalanguagemodel
orfinite-statemodelofphonesequences.28.3 • PROSODY 571
σ σ σ
Onset Rime Onset Rime Rime
h Nucleus Coda g r Nucleus Coda Nucleus Coda
ae m iy n eh g z
Figure28.6 Syllablestructureofham,green,eggs.σ=syllable.
28.3 Prosody
prosody Prosody is the study of the intonational and rhythmic aspects of language, and in
particular the use of F0, energy, and duration to convey pragmatic, affective, or
conversation-interactional meanings.1 We’ll introduce these acoustic quantities in
detail in the next section when we turn to acoustic phonetics, but briefly we can
think of energy as the acoustic quality that we perceive as loudness, and F0 as the
frequency of the sound that is produced, the acoustic quality that we hear as the
pitch of an utterance. Prosody can be used to mark discourse structure, like the
differencebetweenstatementsandquestions,orthewaythataconversationisstruc-
tured. Prosodyisusedtomarkthesaliencyofaparticularwordorphrase. Prosody
is heavily used for paralinguistic functions like conveying affective meanings like
happiness, surprise, or anger. And prosody plays an important role in managing
turn-takinginconversation.
28.3.1 ProsodicProminence: Accent,StressandSchwa
prominence InanaturalutteranceofAmericanEnglish,somewordssoundmoreprominentthan
others, and certain syllables in these words are also more prominent than others.
Whatwemeanbyprominenceisthatthesewordsorsyllablesareperceptuallymore
salient to the listener. Speakers make a word or syllable more salient in English
bysayingitlouder, sayingitslower(soithasalongerduration), orbyvaryingF0
duringtheword,makingithigherormorevariable.
pitchaccent Accent Werepresentprominenceviaalinguisticmarkercalledpitchaccent.Words
orsyllablesthatareprominentaresaidtobear(beassociatedwith)apitchaccent.
Thusthisutterancemightbepronouncedbyaccentingtheunderlinedwords:
(28.1) I’malittlesurprisedtohearitcharacterizedashappy.
LexicalStress Thesyllablesthatbearpitchaccentarecalledaccentedsyllables.
Noteverysyllableofawordcanbeaccented: pitchaccenthastoberealizedonthe
lexicalstress syllablethathaslexicalstress. Lexicalstressisapropertyoftheword’spronuncia-
tionindictionaries;thesyllablethathaslexicalstressistheonethatwillbelouder
orlongerifthewordisaccented. Forexample,thewordsurprisedisstressedonits
secondsyllable,notitsfirst. (TrystressingtheothersyllablebysayingSURprised;
hopefully that sounds wrong to you). Thus, if the word surprised receives a pitch
accentinasentence,itisthesecondsyllablethatwillbestronger. Thefollowingex-
1 Thewordisusedinadifferentbutrelatedwayinpoetry,tomeanthestudyofversemetricalstructure.572 CHAPTER28 • PHONETICS
ampleshowsunderlinedaccentedwordswiththestressedsyllablebearingtheaccent
(thelouder,longersyllable)inboldface:
(28.2) I’malittlesurprisedtohearitcharacterizedashappy.
Stress is marked in dictionaries. The CMU dictionary (CMU, 1993), for ex-
ample, marks vowels with 0 (unstressed) or 1 (stressed) as in entries for counter:
[K AW1 N T ER0], or table: [T EY1 B AH0 L]. Difference in lexical stress can
affectwordmeaning;thenouncontentispronounced[KAA1NTEH0NT],while
theadjectiveispronounced[KAA0NTEH1NT].
ReducedVowelsandSchwa Unstressedvowelscanbeweakenedevenfurtherto
reducedvowel reducedvowels,themostcommonofwhichisschwa([ax]),asinthesecondvowel
schwa of parakeet: [p ae r ax k iy t]. In a reduced vowel the articulatory gesture isn’t as
completeasforafullvowel. Notallunstressedvowelsarereduced;anyvowel,and
diphthongsinparticular, canretainitsfullqualityeveninunstressedposition. For
example,thevowel[iy]canappearinstressedpositionasinthewordeat[iyt]orin
unstressedpositionasinthewordcarry[kaeriy].
prominence Insummary,thereisacontinuumofprosodicprominence,forwhichitisoften
usefultorepresentlevelslikeaccented,stressed,fullvowel,andreducedvowel.
28.3.2 ProsodicStructure
Spokensentenceshaveprosodicstructure: somewordsseemtogroupnaturallyto-
gether, while some words seem to have a noticeable break or disjuncture between
prosodic them. Prosodic structure is often described in terms of prosodic phrasing, mean-
phrasing
ing that an utterance has a prosodic phrase structure in a similar way to it having
a syntactic phrase structure. For example, the sentence I wanted to go to London,
intonation but could only get tickets for France seems to have two main intonation phrases,
phrase
theirboundaryoccurringatthecomma.Furthermore,inthefirstphrase,thereseems
to be another set of lesser prosodic phrase boundaries (often called intermediate
intermediate phrases) that split up the words as I wanted to go to London. These kinds of
phrase | |
intonation phrases are often correlated with syntactic structure constituents (Price
etal.1991,BennettandElfner2019).
Automatically predicting prosodic boundaries can be important for tasks like
TTS.Modernapproachesusesequencemodelsthattakeeitherrawtextortextan-
notatedwithfeatureslikeparsetreesasinput,andmakeabreak/no-breakdecision
ateachwordboundary. Theycanbetrainedondatalabeledforprosodicstructure
liketheBostonUniversityRadioNewsCorpus(Ostendorfetal.,1995).
28.3.3 Tune
Two utterances with the same prominence and phrasing patterns can still differ
tune prosodically by having different tunes. The tune of an utterance is the rise and
fallofits F0overtime. A veryobviousexampleoftune isthedifferencebetween
statementsandyes-noquestionsinEnglish. Thesamewordscanbesaidwithafinal
questionrise F0risetoindicateayes-noquestion(calledaquestionrise):
You know what I mean ?
finalfall orafinaldropinF0(calledafinalfall)toindicateadeclarativeintonation:28.4 • ACOUSTICPHONETICSANDSIGNALS 573
You know what I mean .
Languages make wide use of tune to express meaning (Xu, 2005). In English,
forexample,besidesthiswell-knownriseforyes-noquestions,aphrasecontaining
a list of nouns separated by commas often has a short rise called a continuation
continuation riseaftereachnoun. OtherexamplesincludethecharacteristicEnglishcontoursfor
rise
expressingcontradictionandexpressingsurprise.
LinkingProminenceandTune
Pitchaccentscomeindifferentvarietiesthatarerelatedtotune;highpitchedaccents,
for example, have different functions than low pitched accents. There are many
typologiesofaccentclassesindifferentlanguages. Onesuchtypologyispartofthe
ToBI ToBI (Tone andBreak Indices) theory ofintonation (Silverman etal. 1992). Each
word in ToBI can be associated with one of five types of pitch accents shown in
inFig.28.7. EachutteranceinToBIconsistsofasequenceofintonationalphrases,
boundarytone eachofwhichendsinoneoffourboundarytonesshowninFig.28.7,representing
theutterancefinalaspectsoftune. ThereareversionofToBIformanylanguages.
PitchAccents BoundaryTones
H* peakaccent L-L% “finalfall”:“declarativecontour”ofAmerican
English
L* lowaccent L-H% continuationrise
L*+H scoopedaccent H-H% “question rise”: cantonical yes-no question
contour
L+H* risingpeakaccent H-L% finallevelplateau
H+!H* stepdown
Figure28.7 TheaccentandboundarytoneslabelsfromtheToBItranscriptionsystemfor
AmericanEnglishintonation(BeckmanandAyers1997,BeckmanandHirschberg1994).
28.4 Acoustic Phonetics and Signals
Webeginwithaverybriefintroductiontotheacousticwaveformanditsdigitization
andfrequencyanalysis;theinterestedreaderisencouragedtoconsultthereferences
attheendofthechapter.
28.4.1 Waves
Acoustic analysis is based on the sine and cosine functions. Figure 28.8 shows a
plotofasinewave,inparticularthefunction
y=A sin(2πft) (28.3)
∗
wherewehavesettheamplitudeAto1andthefrequency f to10cyclespersecond.
Recallfrombasicmathematicsthattwoimportantcharacteristicsofawaveare
frequency itsfrequencyandamplitude. Thefrequencyisthenumberoftimesasecondthata
amplitude waverepeatsitself,thatis,thenumberofcycles. Weusuallymeasurefrequencyin
cyclespersecond. ThesignalinFig.28.8repeatsitself5timesin.5seconds,hence
Hertz 10cyclespersecond. Cyclespersecondareusuallycalledhertz(shortenedtoHz),574 CHAPTER28 • PHONETICS
1.0
0
–1.0
00 0.1 0.2 0.3 0.4 00..55
Time (s)
Figure28.8 Asinewavewithafrequencyof10Hzandanamplitudeof1.
sothefrequencyinFig.28.8wouldbedescribedas10Hz. TheamplitudeAofa
period sinewaveisthemaximumvalueontheYaxis.TheperiodT ofthewaveisthetime
ittakesforonecycletocomplete,definedas
1
T = (28.4)
f
EachcycleinFig.28.8lastsatenthofasecond;henceT =.1seconds.
28.4.2 SpeechSoundWaves
Let’s turn from hypothetical waves to sound waves. The input to a speech recog-
nizer,liketheinputtothehumanear,isacomplexseriesofchangesinairpressure.
These changes in air pressure obviously originate with the speaker and are caused
bythespecificwaythatairpassesthroughtheglottisandouttheoralornasalcav-
ities. We represent sound waves by plotting the change in air pressure over time.
Onemetaphorwhichsometimeshelpsinunderstandingthesegraphsisthatofaver-
tical plate blocking the air pressure waves (perhaps in a microphone in front of a
speaker’smouth,ortheeardruminahearer’sear). Thegraphmeasurestheamount
of compression or rarefaction (uncompression) of the air molecules at this plate.
Figure28.9showsashortsegmentofawaveformtakenfromtheSwitchboardcorpus
oftelephonespeechofthevowel[iy]fromsomeonesaying“shejusthadababy”.
0.02283
0
–0.01697
0 0.03875
Time (s)
Figure28.9 A waveform of the vowel [iy] from an utterance shown later in Fig. 28.13 on page 578. The
y-axisshowsthelevelofairpressureaboveandbelownormalatmosphericpressure. Thex-axisshowstime.
Noticethatthewaverepeatsregularly.
The first step in digitizing a sound wave like Fig. 28.9 is to convert the analog
representations(firstairpressureandthenanalogelectricsignalsinamicrophone)
sampling intoadigitalsignal.Thisanalog-to-digitalconversionhastwosteps:samplingand
quantization. Tosampleasignal,wemeasureitsamplitudeataparticulartime;the
samplingrateisthenumberofsamplestakenpersecond. Toaccuratelymeasurea
wave,wemusthaveatleasttwosamplesineachcycle: onemeasuringthepositive28.4 • ACOUSTICPHONETICSANDSIGNALS 575
part of the wave and one measuring the negative part. More than two samples per
cycleincreasestheamplitudeaccuracy,butfewerthantwosamplescausesthefre-
quencyofthewavetobecompletelymissed. Thus, themaximumfrequencywave
that can be measured is one whose frequency is half the sample rate (since every
cycle needs two samples). This maximum frequency for a given sampling rate is
Nyquist calledtheNyquistfrequency. Mostinformationinhumanspeechisinfrequencies
frequency
below 10,000 Hz; thus, a 20,000 Hz sampling rate would be necessary for com-
pleteaccuracy. Buttelephonespeechisfilteredbytheswitchingnetwork,andonly
frequencies less than 4,000 Hz are transmitted by telephones. Thus, an 8,000 Hz
sampling rate is sufficient for telephone-bandwidth speech like the Switchboard
corpus,while16,000Hzsamplingisoftenusedformicrophonespeech.
Evenan8,000Hzsamplingraterequires8000amplitudemeasurementsforeach
second of speech, so it is important to store amplitude measurements efficiently.
They are usually stored as integers, either 8 bit (values from -128–127) or 16 bit
(values from -32768–32767). This process of representing real-valued numbers as
quantization integersiscalledquantizationbecausethedifferencebetweentwointegersactsas
aminimumgranularity(aquantumsize)andallvaluesthatareclosertogetherthan
thisquantumsizearerepresentedidentically.
Once data is quantized, it is stored in various formats. One parameter of these
formats is the sample rate and sample size discussed above; telephone speech is
often sampled at 8 kHz and stored as 8-bit samples, and microphone data is often
sampledat16kHzandstoredas16-bitsamples.Anotherparameteristhenumberof
channel channels.Forstereodataorfortwo-partyconversations,wecanstorebothchannels
inthesamefileorwecanstoretheminseparatefiles.Afinalparameterisindividual
samplestorage—linearlyorcompressed.Onecommoncompressionformatusedfor
telephone speech is µ-law (often written u-law but still pronounced mu-law). The
intuition of log compression algorithms like µ-law is that human hearing is more
sensitive at small intensities than large ones; the log represents small values with
morefaithfulnessattheexpenseofmoreerroronlargevalues.Thelinear(unlogged)
PCM valuesaregenerallyreferredtoaslinearPCMvalues(PCMstandsforpulsecode
modulation,butnevermindthat).Here’stheequationforcompressingalinearPCM
samplevaluexto8-bitµ-law,(whereµ=255for8bits):
sgn(x)log(1+µ x)
F(x) = | | 1 x 1 (28.5)
log(1+µ) − ≤ ≤
Thereareanumberofstandardfileformatsforstoringtheresultingdigitizedwave-
file,suchasMicrosoft’s.wavandApple’sAIFFallofwhichhavespecialheaders;
simpleheaderless“raw”filesarealsoused. Forexample,the.wavformatisasubset
ofMicrosoft’sRIFFformatformultimediafiles; RIFFisageneralformatthatcan
represent a series of nested chunks of data and control information. Figure 28.10
showsasimple.wavfilewithasingledatachunktogetherwithitsformatchunk.
Figure28.10 Microsoftwavefileheaderformat,assumingsimplefilewithonechunk.Fol-
lowingthis44-byteheaderwouldbethedatachunk.576 CHAPTER28 • PHONETICS
28.4.3 FrequencyandAmplitude;PitchandLoudness
Soundwaves,likeallwaves,canbedescribedintermsoffrequency,amplitude,and
the other characteristics that we introduced earlier for pure sine waves. In sound
waves, thesearenotquiteassimpletomeasureastheywereforsinewaves. Let’s
consider frequency. Notein Fig. 28.9 that although not exactly asine, the wave is
nonethelessperiodic,repeating10timesinthe38.75milliseconds(.03875seconds)
capturedinthefigure. Thus,thefrequencyofthissegmentofthewaveis10/.03875
or258Hz.
Where does this periodic 258 Hz wave come from? It comes from the speed
of vibration of the vocal folds; since the waveform in Fig. 28.9 is from the vowel
[iy], it is voiced. Recall that voicing is caused by regular openings and closing of
thevocalfolds. Whenthevocalfoldsareopen,airispushingupthroughthelungs,
creatinga regionofhighpressure. Whenthe foldsareclosed, there isnopressure
fromthelungs. Thus, whenthevocalfoldsarevibrating, weexpecttoseeregular
peaksinamplitudeofthekindweseeinFig.28.9,eachmajorpeakcorresponding
to an opening of the vocal folds. The frequency of the vocal fold vibration, or the
fundamental frequencyofthecomplexwave,iscalledthefundamentalfrequencyofthewave-
frequency
F0 form,oftenabbreviatedF0. WecanplotF0overtimeinapitchtrack. Figure28.11
pitchtrack shows the pitch track of a short question, “Three o’clock?” represented below the
waveform. NotetheriseinF0attheendofthequestion.
500 Hz
0 Hz
three o’clock
0 0.544375
Time (s)
Figure28.11 Pitchtrackofthequestion“Threeo’clock?”,shownbelowthewavefile.Note
theriseinF0attheendofthequestion.Notethelackofpitchtraceduringtheveryquietpart
(the“o’”of“o’clock”;automaticpitchtrackingisbasedoncountingthepulsesinthevoiced
regions,anddoesn’tworkifthereisnovoicing(orinsufficientsound).
The vertical axis in Fig. 28.9 measures the amount of air pressure variation;
pressureisforceperunitarea,measuredinPascals(Pa).Ahighvalueonthevertical
axis(ahighamplitude)indicatesthatthereismoreairpressureatthatpointintime,
azerovaluemeansthereisnormal(atmospheric)airpressure,andanegativevalue
meansthereislowerthannormalairpressure(rarefaction).
In addition to this value of the amplitude at any point in time, we also often
need to know the average amplitude over some time range, to give us some idea
of how great the average displacement of air pressure is. But we can’t just take
the average of the amplitude values over a range; the positive and negative values
would (mostly) cancel out, leaving us with a number close to zero. Instead, we
generally use the RMS (root-mean-square) amplitude, which squares each number28.4 • ACOUSTICPHONETICSANDSIGNALS 577
beforeaveraging(makingitpositive),andthentakesthesquarerootattheend.
N
1
RMSamplitudeN = x2 (28.6)
i=1 (cid:118)N i
(cid:117) i=1
(cid:117) (cid:88)
(cid:116)
power Thepowerofthesignalisrelatedtothesquareoftheamplitude. Ifthenumber
ofsamplesofasoundisN,thepoweris
N
1
Power= x2 (28.7)
N i
i=1
(cid:88)
intensity Rather than power, we more often refer to the intensity of the sound, which
normalizesthepowertothehumanauditorythresholdandismeasuredindB.IfP
0
istheauditorythresholdpressure=2 10 5Pa,thenintensityisdefinedasfollows:
−
×
N
1
Intensity=10log x2 (28.8)
10NP i
0
i=1
(cid:88)
Figure28.12showsanintensityplotforthesentence“Isitalongmovie?” from
theCallHomecorpus,againshownbelowthewaveformplot.
is it a long movie?
0 1.1675
Time (s)
Figure28.12 Intensityplotforthesentence“Isitalongmovie?”.Notetheintensitypeaks
ateachvowelandtheespeciallyhighpeakforthewordlong.
Two important perceptual properties, pitch and loudness, are related to fre-
pitch quency and intensity. The pitch of a sound is the mental sensation, or perceptual
correlate,offundamentalfrequency; ingeneral,ifasoundhasahigherfundamen-
talfrequencyweperceiveitashavingahigherpitch. Wesay“ingeneral”because
therelationshipisnotlinear,sincehumanhearinghasdifferentacuitiesfordifferent
frequencies. Roughly speaking, human pitch perception is most accurate between
100Hzand1000Hzandinthisrangepitchcorrelateslinearlywithfrequency. Hu-
man hearing represents frequencies above 1000 Hz less accurately, and above this
range, pitch correlates logarithmically with frequency. Logarithmic representation
meansthatthedifferencesbetweenhighfrequenciesarecompressedandhencenot
asaccuratelyperceived. Therearevariouspsychoacousticmodelsofpitchpercep-
Mel tionscales. Onecommonmodelisthemelscale(Stevensetal.1937, Stevensand578 CHAPTER28 • PHONETICS
Volkmann 1940). A mel is a unit of pitch defined such that pairs of sounds which
areperceptuallyequidistantinpitchareseparatedbyanequalnumberofmels. The
melfrequencymcanbecomputedfromtherawacousticfrequencyasfollows:
f
m=1127ln(1+ ) (28.9)
700
As we’ll see in Chapter 16, the mel scale plays an important role in speech
recognition.
Theloudnessofasoundistheperceptualcorrelateofthepower.Sosoundswith
higher amplitudes are perceived as louder, but again the relationship is not linear.
First of all, as we mentioned above when we defined µ-law compression, humans
have greater resolution in the low-power range; the ear is more sensitive to small
powerdifferences. Second,itturnsoutthatthereisacomplexrelationshipbetween
power, frequency, and perceived loudness; sounds in certain frequency ranges are
perceivedasbeinglouderthanthoseinotherfrequencyranges.
VariousalgorithmsexistforautomaticallyextractingF0. Inaslightabuseofter-
pitchextraction minology,thesearecalledpitchextractionalgorithms. Theautocorrelationmethod
of pitch extraction, for example, correlates the signal with itself at various offsets.
The offset that gives the highest correlation gives the period of the signal. There
are various publicly available pitch extractiontoolkits; for example, an augmented
autocorrelationpitchtrackerisprovidedwithPraat(BoersmaandWeenink,2005).
28.4.4 InterpretationofPhonesfromaWaveform
Muchcanbelearnedfromavisualinspectionofawaveform. Forexample,vowels
areprettyeasytospot. Recallthatvowelsarevoiced;anotherpropertyofvowelsis
thattheytendtobelongandarerelativelyloud(aswecanseeintheintensityplot
inFig. 28.12). Lengthin time manifestsitselfdirectly onthe x-axis, andloudness
isrelatedto(thesquareof)amplitudeonthey-axis. Wesawintheprevioussection
thatvoicingisrealizedbyregularpeaksinamplitudeofthekindwesawinFig.28.9,
eachmajorpeakcorrespondingtoanopeningofthevocalfolds.Figure28.13shows
thewaveformoftheshortsentence“shejusthadababy”.Wehavelabeledthiswave-
formwithwordandphonelabels. NoticethateachofthesixvowelsinFig.28.13,
[iy],[ax],[ae],[ax],[ey],[iy],allhaveregularamplitudepeaksindicatingvoicing.
she just had a baby
sh iy j ax s h ae dx ax b ey b iy
0 1.059
Time (s)
Figure28.13 Awaveformofthesentence“Shejusthadababy”fromtheSwitchboardcorpus(conversation
4325).Thespeakerisfemale,was20yearsoldin1991,whichisapproximatelywhentherecordingwasmade,
andspeakstheSouthMidlandsdialectofAmericanEnglish.
Forastopconsonant,whichconsistsofaclosurefollowedbyarelease,wecan
oftenseeaperiodofsilenceornearsilencefollowedbyaslightburstofamplitude.
Wecanseethisforbothofthe[b]’sinbabyinFig.28.13.28.4 • ACOUSTICPHONETICSANDSIGNALS 579
Anotherphonethatisoftenquiterecognizableinawaveformisafricative. Re-
call that fricatives, especially very strident fricatives like [sh], are made when a
narrow channel for airflow causes noisy, turbulent air. The resulting hissy sounds
haveanoisy,irregularwaveform.ThiscanbeseensomewhatinFig.28.13;it’seven
clearerinFig.28.14,wherewe’vemagnifiedjustthefirstwordshe.
she
sh iy
0 0.257
Time (s)
Figure28.14 Amoredetailedviewofthefirstword“she”extractedfromthewavefileinFig.28.13. Notice
thedifferencebetweentherandomnoiseofthefricative[sh]andtheregularvoicingofthevowel[iy].
28.4.5 SpectraandtheFrequencyDomain
Whilesomebroadphoneticfeatures(suchasenergy,pitch,andthepresenceofvoic-
ing,stopclosures,orfricatives)canbeinterpreteddirectlyfromthewaveform,most
computational applications such as speech recognition (as well as human auditory
processing)arebasedonadifferentrepresentationofthesoundintermsofitscom-
ponentfrequencies. TheinsightofFourieranalysisisthateverycomplexwavecan
berepresentedasasumofmanysinewavesofdifferentfrequencies. Considerthe
waveforminFig.28.15. Thiswaveformwascreated(inPraat)bysummingtwosine
waveforms,oneoffrequency10Hzandoneoffrequency100Hz.
1
0
–1
0 0.5
Time (s)
Figure28.15 A waveform that is the sum of two sine waveforms, one of frequency 10
Hz(notefiverepetitionsinthehalf-secondwindow)andoneoffrequency100Hz, bothof
amplitude1.
spectrum Wecanrepresentthesetwocomponentfrequencieswithaspectrum. Thespec-
trum of a signal is a representation of each of its frequency components and their
amplitudes. Figure 28.16 shows the spectrum of Fig. 28.15. Frequency in Hz is
on the x-axis and amplitude on the y-axis. Note the two spikes in the figure, one
at10Hzandoneat100Hz. Thus, thespectrumisanalternativerepresentationof
the original waveform, and we use the spectrum as a tool to study the component
frequenciesofasoundwaveataparticulartimepoint.580 CHAPTER28 • PHONETICS
80
60
40
1 2 5 10 20 50 100 200
Frequency (Hz)
Figure28.16 ThespectrumofthewaveforminFig.28.15.
Let’slooknowatthefrequencycomponentsofaspeechwaveform.Figure28.17
shows part of the waveform for the vowel [ae] of the word had, cut out from the
sentenceshowninFig.28.13.
0.04968
0
–0.05554
0 0.04275
Time (s)
Figure28.17 Thewaveformofpartofthevowel[ae]fromthewordhadcutoutfromthe
waveformshowninFig.28.13.
Notethatthereisacomplexwavethatrepeatsabouttentimesinthefigure;but
thereisalsoasmallerrepeatedwavethatrepeatsfourtimesforeverylargerpattern
(notice the four small peaks inside each repeated wave). The complex wave has a
frequencyofabout234Hz(wecanfigurethisoutsinceitrepeatsroughly10times
in.0427seconds,and10cycles/.0427seconds=234Hz).
The smaller wave then should have a frequency of roughly four times the fre-
quencyofthelargerwave,orroughly936Hz. Then,ifyoulookcarefully,youcan
seetwolittlewavesonthepeakofmanyofthe936Hzwaves. Thefrequencyofthis
tiniestwavemustberoughlytwicethatofthe936Hzwave,hence1872Hz.
Figure28.18showsasmoothedspectrumforthewaveforminFig.28.17,com-
putedwithadiscreteFouriertransform(DFT).
20
0
–20
000 1000 22000000 3000 444000000000
Frequency (Hz)
Figure28.18 Aspectrumforthevowel[ae]fromthewordhadinthewaveformofShejust
hadababyinFig.28.13.
)zH/Bd(
level
erusserp
dnuoS
)zH/Bd(
level
erusserp
dnuoS28.4 • ACOUSTICPHONETICSANDSIGNALS 581
The x-axis of a spectrum shows frequency, and the y-axis shows some mea-
sureofthemagnitudeofeachfrequencycomponent(indecibels(dB),alogarithmic
measureofamplitudethatwesawearlier). Thus, Fig.28.18showssignificantfre-
quency components at around 930 Hz, 1860 Hz, and 3020 Hz, along with many
otherlower-magnitudefrequencycomponents. Thesefirsttwocomponentsarejust
whatwenoticedinthetimedomainbylookingatthewaveinFig.28.17!
Why is a spectrum useful? It turns out that these spectral peaks that are easily
visibleinaspectrumarecharacteristicofdifferentphones;phoneshavecharacteris-
ticspectral“signatures”.Justaschemicalelementsgiveoffdifferentwavelengthsof
lightwhentheyburn,allowingustodetectelementsinstarsbylookingatthespec-
trumof thelight, we candetectthecharacteristic signatureofthe differentphones
bylookingatthespectrumofawaveform. Thisuseofspectralinformationisessen-
tialtobothhumanandmachinespeechrecognition. Inhumanaudition,thefunction
cochlea ofthecochlea, orinnerear, istocomputeaspectrumoftheincomingwaveform.
Similarly, the acoustic features used in speech recognition are spectral representa-
tions.
Let’slookatthespectrumofdifferentvowels. Sincesomevowelschangeover
time, we’ll use a different kind of plot called a spectrogram. While a spectrum
spectrogram showsthefrequencycomponentsofawaveatonepointintime,aspectrogramisa
wayofenvisioninghowthedifferentfrequenciesthatmakeupawaveformchange
over time. The x-axis shows time, as it did for the waveform, but the y-axis now
showsfrequenciesinhertz. Thedarknessofapointonaspectrogramcorresponds
totheamplitudeofthefrequencycomponent.Verydarkpointshavehighamplitude,
lightpointshavelowamplitude.Thus,thespectrogramisausefulwayofvisualizing
thethreedimensions(timexfrequencyxamplitude).
Figure28.19showsspectrogramsofthreeAmericanEnglishvowels,[ih],[ae],
and [ah]. Note that each vowel has a set of dark bars at various frequency bands,
slightly different bands for each vowel. Each of these represents the same kind of
spectralpeakthatwesawinFig.28.17.
5000
0
0 2.81397
Time (s)
Figure28.19 SpectrogramsforthreeAmericanEnglishvowels,[ih],[ae],and[uh]
formant Each dark bar (or spectral peak) is called a formant. As we discuss below, a
formantisafrequencybandthatisparticularlyamplifiedbythevocaltract. Since
different vowels are produced with the vocal tract in different positions, they will
produce different kinds of amplifications or resonances. Let’s look at the first two
formants,calledF1andF2. NotethatF1,thedarkbarclosesttothebottom,isina
different position for the three vowels; it’s low for [ih] (centered at about 470 Hz)
and somewhat higher for [ae] and [ah] (somewhere around 800 Hz). By contrast,
F2,theseconddarkbarfromthebottom,ishighestfor[ih],inthemiddlefor[ae],
andlowestfor[ah].
We can see the same formants in running speech, although the reduction and
)zH(
ycneuqerF582 CHAPTER28 • PHONETICS
coarticulation processes make them somewhat harder to see. Figure 28.20 shows
thespectrogramof“shejusthadababy”,whosewaveformwasshowninFig.28.13.
F1andF2(andalsoF3)areprettyclearforthe[ax]ofjust,the[ae]ofhad,andthe
[ey]ofbaby.
she just had a baby
sh iy j ax s h ae dx ax b ey b iy
0 1.059
Time (s)
Figure28.20 Aspectrogramofthesentence“shejusthadababy”whosewaveformwasshowninFig.28.13.
Wecanthinkofaspectrogramasacollectionofspectra(timeslices),likeFig.28.18placedendtoend.
What specific clues can spectral representations give for phone identification?
First,sincedifferentvowelshavetheirformantsatcharacteristicplaces,thespectrum
candistinguishvowelsfromeachother.We’veseenthat[ae]inthesamplewaveform
had formants at 930 Hz, 1860 Hz, and 3020 Hz. Consider the vowel [iy] at the
beginning of the utterance in Fig. 28.13. The spectrum for this vowel is shown in
Fig.28.21. Thefirstformantof[iy]is540Hz,muchlowerthanthefirstformantfor
[ae],andthesecondformant(2581Hz)ismuchhigherthanthesecondformantfor
[ae]. Ifyoulookcarefully,youcanseetheseformantsasdarkbarsinFig.28.20just
around0.5seconds.
80
70
60
50
40
30
20
10
0
−10
0 1000 2000 3000
Figure28.21 Asmoothed(LPC)spectrumforthevowel[iy]atthestartofShejusthada
baby.Notethatthefirstformant(540Hz)ismuchlowerthanthefirstformantfor[ae]shown
inFig.28.18,andthesecondformant(2581Hz)ismuchhigherthanthesecondformantfor
[ae].
Thelocationofthefirsttwoformants(calledF1andF2)playsalargeroleinde-
terminingvowelidentity,althoughtheformantsstilldifferfromspeakertospeaker.
Higher formants tend to be caused more by general characteristics of a speaker’s
vocaltractratherthanbyindividualvowels. Formantsalsocanbeusedtoidentify
thenasalphones[n],[m],and[ng]andtheliquids[l]and[r].28.4 • ACOUSTICPHONETICSANDSIGNALS 583
28.4.6 TheSource-FilterModel
Whydodifferentvowelshavedifferentspectralsignatures?Aswebrieflymentioned
above, theformantsarecausedbytheresonantcavitiesofthemouth. Thesource-
source-filter filter model is a way of explaining the acoustics of a sound by modeling how the
model
pulsesproducedbytheglottis(thesource)areshapedbythevocaltract(thefilter).
Let’sseehowthisworks. Wheneverwehaveawavesuchasthevibrationinair
harmonic caused by the glottal pulse, the wave also has harmonics. A harmonic is another
wavewhosefrequencyisamultipleofthefundamentalwave. Thus,forexample,a
115Hzglottalfoldvibrationleadstoharmonics(otherwaves)of230Hz, 345Hz,
460Hz, andsoonon. Ingeneral, eachofthesewaveswillbeweaker, thatis, will
havemuchlessamplitudethanthewaveatthefundamentalfrequency.
It turns out, however, that the vocal tract acts as a kind of filter or amplifier;
indeedanycavity,suchasatube,causeswavesofcertainfrequenciestobeamplified
and others to be damped. This amplification process is caused by the shape of the
cavity;agivenshapewillcausesoundsofacertainfrequencytoresonateandhence
be amplified. Thus, by changing the shape of the cavity, we can cause different
frequenciestobeamplified.
When we produce particular vowels, we are essentially changing the shape of
the vocal tract cavity by placing the tongue and the other articulators in particular
positions. Theresultisthatdifferentvowelscausedifferentharmonicstobeampli-
fied. Soawaveofthesamefundamentalfrequencypassedthroughdifferentvocal
tractpositionswillresultindifferentharmonicsbeingamplified.
Wecanseetheresultofthisamplificationbylookingattherelationshipbetween
the shape of the vocal tract and the corresponding spectrum. Figure 28.22 shows
thevocaltractpositionforthreevowelsandatypicalresultingspectrum. Thefor-
mantsareplacesinthespectrumwherethevocaltracthappenstoamplifyparticular
harmonicfrequencies.
20
20
[iy] (tea) [uw] (moo)
0
F1
[ae] (cat)
F2
F1 F2
0 0 –20 F1 F2
0 268 2416 4000 0 903 1695 4000 0 295 817 4000
Frequency (Hz) Frequency (Hz) Frequency (Hz)
[iy] (tea) [ae] (cat) [uw] (moo)
Figure28.22 Visualizingthevocaltractpositionasafilter:thetonguepositionsforthreeEnglishvowelsand
theresultingsmoothedspectrashowingF1andF2.
)zH/Bd(
level
erusserp
dnuoS
)zH/Bd(
level
erusserp
dnuoS
)zH/Bd(
level
erusserp
dnuoS584 CHAPTER28 • PHONETICS
28.5 Phonetic Resources
Awidevarietyofphoneticresourcescanbedrawnonforcomputationalwork. On-
pronunciation line pronunciation dictionaries give phonetic transcriptions for words. The LDC
dictionary
distributes pronunciation lexicons for Egyptian Arabic, Dutch, English, German,
Japanese, Korean, Mandarin, and Spanish. For English, the CELEX dictionary
(Baayen et al., 1995) has pronunciations for 160,595 wordforms, with syllabifica-
tion, stress, and morphological and part-of-speech information. The open-source
CMUPronouncingDictionary(CMU,1993)haspronunciationsforabout134,000
wordforms, while the fine-grained 110,000 word UNISYN dictionary (Fitt, 2002),
freelyavailableforresearchpurposes,givessyllabifications,stress,andalsopronun-
ciationsfordozensofdialectsofEnglish.
Another useful resource is a phonetically annotated corpus, in which a col-
lectionofwaveformsishand-labeledwiththecorrespondingstringofphones. The
TIMITcorpus(NIST,1990),originallyajointprojectbetweenTexasInstruments
(TI),MIT,andSRI,isacorpusof6300readsentences,with10sentenceseachfrom
630speakers. The6300sentencesweredrawnfromasetof2342sentences,some
selectedtohaveparticulardialectshibboleths,otherstomaximizephoneticdiphone
coverage. Eachsentenceinthecorpuswasphoneticallyhand-labeled,thesequence
of phones was automatically aligned with the sentence wavefile, and then the au-
tomatic phone boundaries were manually hand-corrected (Seneff and Zue, 1988).
time-aligned Theresultisatime-alignedtranscription: atranscriptioninwhicheachphoneis
transcription
associatedwithastartandendtimeinthewaveform,liketheexampleinFig.28.23.
she had your dark suit in greasy wash water all year
shiy hvaedcl jhaxr dcldaarkcl suxq en gclgriysix waash qwaadxaxrq aal yixaxr
Figure28.23 PhonetictranscriptionfromtheTIMITcorpus,usingspecialARPAbetfeaturesfornarrowtran-
scription,suchasthepalatalizationof[d]inhad,unreleasedfinalstopindark,glottalizationoffinal[t]insuit
to[q],andflapof[t]inwater.TheTIMITcorpusalsoincludestime-alignments(notshown).
The Switchboard Transcription Project phonetically annotated corpus consists
of3.5hoursofsentencesextractedfromtheSwitchboardcorpus(Greenbergetal.,
1996), together with transcriptions time-aligned at the syllable level. Figure 28.24
showsanexample.
0.470 0.640 0.720 0.900 0.953 1.279 1.410 1.630
dher kaa nax vihm bix twiyn ray naw
Figure28.24 PhonetictranscriptionoftheSwitchboardphrasethey’rekindofinbetween
rightnow. Notevowelreductioninthey’reandof, codadeletioninkindandright, andre-
syllabification(the[v]ofofattachesastheonsetofin). Timeisgiveninnumberofseconds
fromthebeginningofsentencetothestartofeachsyllable.
The Buckeye corpus (Pitt et al. 2007, Pitt et al. 2005) is a phonetically tran-
scribed corpus of spontaneous American speech, containing about 300,000 words
from 40 talkers. Phonetically transcribed corpora are also available for other lan-
guages,includingtheKielcorpusofGermanandMandarincorporatranscribedby
theChineseAcademyofSocialSciences(Lietal.,2000).
Inadditiontoresourceslikedictionariesandcorpora,therearemanyusefulpho-
netic software tools. Many of the figures in this book were generated by the Praat
package(BoersmaandWeenink,2005),whichincludespitch,spectral,andformant
analysis,aswellasascriptinglanguage.28.6 • SUMMARY 585
28.6 Summary
Thischapterhasintroducedmanyoftheimportantconceptsofphoneticsandcom-
putationalphonetics.
• Wecanrepresentthepronunciationofwordsintermsofunitscalledphones.
The standard system for representing phones is the International Phonetic
AlphabetorIPA.Themostcommoncomputationalsystemfortranscription
ofEnglishistheARPAbet,whichconvenientlyusesASCIIsymbols.
• Phonescanbedescribedbyhowtheyareproducedarticulatorilybythevocal
organs;consonantsaredefinedintermsoftheirplaceandmannerofarticu-
lationandvoicing;vowelsbytheirheight,backness,androundness.
• Speech sounds can also be described acoustically. Sound waves can be de-
scribedintermsoffrequency,amplitude,ortheirperceptualcorrelates,pitch
andloudness.
• Thespectrumofasounddescribesitsdifferentfrequencycomponents.While
some phoneticproperties arerecognizable fromthe waveform, both humans
andmachinesrelyonspectralanalysisforphonedetection.
• A spectrogram is a plot of a spectrum over time. Vowels are described by
characteristicharmonicscalledformants.
Bibliographical and Historical Notes
The major insights of articulatory phonetics date to the linguists of 800–150 B.C.
India. Theyinventedtheconceptsofplaceandmannerofarticulation, workedout
theglottalmechanismofvoicing, andunderstoodtheconceptofassimilation. Eu-
ropeansciencedidnotcatchupwiththeIndianphoneticiansuntilover2000years
later, in the late 19th century. The Greeks did have some rudimentary phonetic
knowledge;bythetimeofPlato’sTheaetetusandCratylus,forexample,theydistin-
guishedvowelsfromconsonants,andstopconsonantsfromcontinuants. TheStoics
developedtheideaofthesyllableandwereawareofphonotacticconstraintsonpos-
siblewords.AnunknownIcelandicscholarofthe12thcenturyexploitedtheconcept
of the phoneme and proposed a phonemic writing system for Icelandic, including
diacriticsforlengthandnasality. Buthistextremainedunpublisheduntil1818and
even then was largely unknown outside Scandinavia (Robins, 1967). The modern
era of phonetics is usually said to have begun with Sweet, who proposed what is
essentiallythephonemeinhisHandbookofPhonetics1877. Healsodevisedanal-
phabetfortranscriptionanddistinguishedbetweenbroadandnarrowtranscription,
proposing many ideas that were eventually incorporated into the IPA. Sweet was
considered the best practicing phonetician of his time; he made the first scientific
recordings of languages for phonetic purposes and advanced the state of the art of
articulatory description. He was also infamously difficult to get along with, a trait
thatiswellcapturedinHenryHiggins,thestagecharacterthatGeorgeBernardShaw
modeled after him. The phoneme was first named by the Polish scholar Baudouin
deCourtenay,whopublishedhistheoriesin1894.
IntroductoryphoneticstextbooksincludeLadefoged(1993)andClarkandYal-
lop(1995).Wells(1982)isthedefinitivethree-volumesourceondialectsofEnglish.
Many of the classic insights in acoustic phonetics had been developed by the
late 1950s or early 1960s; just a few highlights include techniques like the sound586 CHAPTER28 • PHONETICS
spectrograph (Koenig et al., 1946), theoretical insights like the working out of the
source-filtertheoryandotherissuesinthemappingbetweenarticulationandacous-
tics((Fant,1960),Stevensetal.1953,StevensandHouse1955,HeinzandStevens
1961, Stevens and House 1961) the F1xF2 space of vowel formants (Peterson and
Barney, 1952), the understanding of the phonetic nature of stress and the use of
duration and intensity as cues (Fry, 1955), and a basic understanding of issues in
phoneperception(MillerandNicely1955,Libermanetal.1952). Lehiste(1967)is
acollectionofclassicpapersonacousticphonetics. Manyoftheseminalpapersof
GunnarFanthavebeencollectedinFant(2004).
Excellent textbooks on acoustic phonetics include Johnson (2003) and Lade-
foged (1996). Coleman (2005) includes an introduction to computational process-
ing of acoustics and speech from a linguistic perspective. Stevens (1998) lays out
an influential theory of speech sound production. There are a number of software
packagesforacousticphoneticanalysis.ProbablythemostwidelyusedoneisPraat
(BoersmaandWeenink,2005).
Exercises
28.1 FindthemistakesintheARPAbettranscriptionsofthefollowingwords:
a. “three”[dhri] d. “study”[stuhdi] g. “slight”[sliyt]
b. “sing”[sihng] e. “though”[thow]
c. “eyes”[ays] f. “planning”[pplaanihng]
28.2 Ira Gershwin’s lyric for Let’s Call the Whole Thing Off talks about two pro-
nunciations(each)ofthewords“tomato”,“potato”,and“either”. Transcribe
intotheARPAbetbothpronunciationsofeachofthesethreewords.
28.3 TranscribethefollowingwordsintheARPAbet:
1. dark
2. suit
3. greasy
4. wash
5. water
28.4 Takeawavefileofyourchoice. Someexamplesareonthetextbookwebsite.
DownloadthePraatsoftware,anduseittotranscribethewavefilesattheword
level and into ARPAbet phones, using Praat to help you play pieces of each
wavefileandtolookatthewavefileandthespectrogram.
28.5 RecordyourselfsayingfiveoftheEnglishvowels: [aa],[eh],[ae],[iy],[uw].
FindF1andF2foreachofyourvowels.Bibliography
Abadi, M., A. Agarwal, P. Barham, Aho,A.V.andJ.D.Ullman.1972.The Asher,N.1993. ReferencetoAbstract
E. Brevdo, Z. Chen, C. Citro, TheoryofParsing,Translation,and ObjectsinDiscourse.StudiesinLin-
G. S. Corrado, A. Davis, J. Dean, Compiling,volume1.PrenticeHall. guisticsandPhilosophy(SLAP)50,
M. Devin, S. Ghemawat, I. Good- Alberti, C., K. Lee, and M. Collins. Kluwer.
fellow, A. Harp, G. Irving, M. Is- 2019.ABERTbaselineforthenatu- Asher,N.andA.Lascarides.2003.Log-
ard,Y.Jia,R.Jozefowicz,L.Kaiser, ralquestions.http://arxiv.org/ icsofConversation.CambridgeUni-
M.Kudlur,J.Levenberg,D.Mane´, abs/1901.08634. versityPress.
R. Monga, S. Moore, D. Murray,
C. Olah, M. Schuster, J. Shlens, Algoet, P. H. and T. M. Cover. 1988. Atal, B. S. and S. Hanauer. 1971.
B. Steiner, I. Sutskever, K. Talwar, A sandwich proof of the Shannon- Speech analysis and synthesis by
P.Tucker,V.Vanhoucke,V.Vasude- McMillan-Breiman theorem. The predictionofthespeechwave.JASA,
van, F.Vie´gas, O.Vinyals, P.War- Annals of Probability, 16(2):899– 50:637–655.
den, M. Wattenberg, M. Wicke, 909. Austin,J.L.1962. HowtoDoThings
Y.Yu,andX.Zheng.2015. Tensor- Allen,J.1984. Towardsageneralthe- with Words. Harvard University
Flow:Large-scalemachinelearning oryofactionandtime. ArtificialIn- Press.
onheterogeneoussystems.Software telligence,23(2):123–154.
availablefromtensorflow.org. Awadallah, A. H., R. G. Kulkarni,
Allen,J.andC.R.Perrault.1980. An-
U. Ozertem, and R. Jones. 2015.
Abney, S. P., R. E. Schapire, and alyzingintentioninutterances.Arti- Charaterizing and predicting voice
Y. Singer. 1999. Boosting ap- ficialIntelligence,15:143–178. queryreformulation.CIKM-15.
pliedtotaggingandPPattachment. Allen, J., M. S. Hunnicut, and D. H.
EMNLP/VLC. Klatt. 1987. From Text to Speech: Ba,J.L.,J.R.Kiros,andG.E.Hinton.
2016.Layernormalization.NeurIPS
TheMITalksystem.CambridgeUni-
Agarwal, O., S. Subramanian, workshop.
versityPress.
A. Nenkova, and D. Roth. 2019.
Baayen, R. H. 2001. Word frequency
Evaluationofnamedentitycorefer- Althoff, T., C. Danescu-Niculescu-
distributions.Springer.
ence. WorkshoponComputational Mizil,andD.Jurafsky.2014. How
ModelsofReference,Anaphoraand to ask for a favor: A case study Baayen, R. H., R. Piepenbrock, and
Coreference. onthesuccessofaltruisticrequests. L. Gulikers. 1995. The CELEX
ICWSM2014. LexicalDatabase(Release2)[CD-
Aggarwal, C. C. and C. Zhai. 2012.
A survey of text classification al- Amsler, R.A.1981. Ataxonomyfor ROM].LinguisticDataConsortium,
gorithms. In C. C. Aggarwal and Englishnounsandverbs.ACL. UniversityofPennsylvania[Distrib-
utor].
C. Zhai, editors, Mining text data, An, J., H. Kwak, and Y.-Y. Ahn.
pages163–222.Springer. 2018. SemAxis: A lightweight Baccianella,S.,A.Esuli,andF.Sebas-
framework to characterize domain- tiani. 2010. Sentiwordnet 3.0: An
Agichtein, E. and L. Gravano. 2000.
specificwordsemanticsbeyondsen- enhancedlexicalresourceforsenti-
Snowball: Extractingrelationsfrom
timent.ACL. ment analysis and opinion mining.
large plain-text collections. Pro-
LREC.
ceedings of the 5th ACM Interna- Anastasopoulos, A. and G. Neubig.
tional Conference on Digital Li- 2020. Shouldallcross-lingualem- Bach,K.andR.Harnish.1979.Linguis-
braries. beddingsspeakEnglish? ACL. ticcommunicationandspeechacts.
MITPress.
Agirre,E.andO.L.deLacalle.2003. Antoniak, M. and D. Mimno.
Clustering WordNet word senses. 2018. Evaluating the stability of Backus, J. W. 1959. The syntax
RANLP2003. embedding-based word similarities. and semantics of the proposed in-
TACL,6:107–119. ternationalalgebraiclanguageofthe
Agirre,E.,C.Banea,C.Cardie,D.Cer, Aone,C.andS.W.Bennett.1995.Eval- Zurich ACM-GAMM Conference.
M. Diab, A. Gonzalez-Agirre, uatingautomatedandmanualacqui- Information Processing: Proceed-
W.Guo, I.Lopez-Gazpio, M.Mar- sitionofanaphoraresolutionstrate- ingsoftheInternationalConference
itxalar, R. Mihalcea, G. Rigau, gies.ACL. on Information Processing, Paris.
L. Uria, and J. Wiebe. 2015. UNESCO.
SemEval-2015 task 2: Semantic Ariel, M. 2001. Accessibility the-
textual similarity, English, Span- ory: An overview. In T. Sanders, Backus, J. W. 1996. Transcript of
ish and pilot on interpretability. J.Schilperoord,andW.Spooren,ed- question and answer session. In
SemEval-15. itors,TextRepresentation: Linguis- R. L. Wexelblat, editor, History of
tic and Psycholinguistic Aspects, ProgrammingLanguages,page162.
Agirre, E., M. Diab, D. Cer, pages29–87.Benjamins. AcademicPress.
and A. Gonzalez-Agirre. 2012.
SemEval-2012task6:Apilotonse- Artetxe, M. and H. Schwenk. 2019. Bada,M.,M.Eckert,D.Evans,K.Gar-
mantictextualsimilarity. SemEval- Massivelymultilingualsentenceem- cia, K.Shipley, D.Sitnikov, W.A.
12. beddingsforzero-shotcross-lingual Baumgartner,K.B.Cohen,K.Ver-
transferandbeyond. TACL,7:597– spoor,J.A.Blake,andL.E.Hunter.
Agirre, E. and P. Edmonds, editors. 610. 2012. Concept annotation in the
2006. Word Sense Disambigua- craft corpus. BMC bioinformatics,
Artstein, R., S. Gandhe, J. Gerten,
tion: Algorithms and Applications. 13(1):161.
A. Leuski, and D. Traum. 2009.
Kluwer.
Semi-formal evaluation of conver- Bagga, A. and B. Baldwin. 1998.
Agirre, E. and D. Martinez. 2001. sational characters. In Languages: Algorithms for scoring coreference
Learning class-to-class selectional FromFormaltoNatural,pages22– chains.LRECWorkshoponLinguis-
preferences.CoNLL. 35.Springer. ticCoreference.
587588 Bibliography
Bahdanau,D.,K.H.Cho,andY.Ben- Ban˜o´n, M., P. Chen, B. Haddow, II (1966), pages 271–298. Univer-
gio.2015. Neuralmachinetransla- K. Heafield, H. Hoang, M. Espla`- sityofChicagoPress.
tionbyjointlylearningtoalignand Gomis,M.L.Forcada,A.Kamran, Bean, D. and E. Riloff. 1999.
translate.ICLR2015. F. Kirefu, P. Koehn, S. Ortiz Ro- Corpus-basedidentificationofnon-
Bahdanau, D., J. Chorowski, jas, L. Pla Sempere, G. Ram´ırez- anaphoricnounphrases.ACL.
D.Serdyuk, P.Brakel, andY.Ben- Sa´nchez, E. Sarr´ıas, M. Strelec, Bean, D. and E. Riloff. 2004. Unsu-
gio. 2016. End-to-end attention- B. Thompson, W. Waites, D. Wig- pervisedlearningofcontextualrole
based large vocabulary speech gins, and J. Zaragoza. 2020. knowledge for coreference resolu-
recognition.ICASSP. ParaCrawl: Web-scale acquisition tion.HLT-NAACL.
ofparallelcorpora.ACL.
Bahl, L. R. and R. L. Mercer. 1976. Beckman, M. E. and G. M. Ayers.
Partofspeechassignmentbyasta- Bar-Hillel, Y. 1960. The present sta- 1997. Guidelines for ToBI la-
tisticaldecisionalgorithm.Proceed- tus of automatic translation of lan- belling. Unpublished manuscript,
ingsIEEEInternationalSymposium guages. InF.Alt,editor,Advances Ohio State University, http:
onInformationTheory. inComputers1,pages91–163.Aca- //www.ling.ohio-state.edu/
demicPress.
Bahl, L. R., F. Jelinek, and R. L. research/phonetics/E_ToBI/.
Mercer. 1983. A maximum likeli- Barker, C. 2010. Nominals don’t Beckman, M. E. and J. Hirschberg.
hoodapproachtocontinuousspeech provide criteria of identity. In 1994. TheToBIannotationconven-
recognition. IEEETransactionson M. Rathert and A. Alexiadou, edi- tions. Manuscript,OhioStateUni-
PatternAnalysisandMachineIntel- tors, TheSemanticsofNominaliza- versity.
ligence,5(2):179–190. tionsacrossLanguagesandFrame-
works,pages9–24.Mouton. Bedi,G.,F.Carrillo,G.A.Cecchi,D.F.
Baker, C. F., C. J. Fillmore, and Slezak, M. Sigman, N. B. Mota,
J. B. Lowe. 1998. The Berkeley Barrett, L. F., B. Mesquita, K. N. S.Ribeiro,D.C.Javitt,M.Copelli,
FrameNetproject.COLING/ACL. Ochsner,andJ.J.Gross.2007. The and C. M. Corcoran. 2015. Auto-
experienceofemotion. AnnualRe-
Baker,J.K.1975a.TheDRAGONsys- mated analysis of free speech pre-
viewofPsychology,58:373–403.
tem–Anoverview. IEEETransac- dicts psychosis onset in high-risk
tionsonAcoustics,Speech,andSig- Barzilay,R.andM.Lapata.2005.Mod- youths.npjSchizophrenia,1.
nalProcessing,ASSP-23(1):24–29. eling local coherence: An entity- Bejcˇek, E., E. Hajicˇova´, J. Hajicˇ,
basedapproach.ACL.
Baker, J. K. 1975b. Stochastic mod- P. J´ınova´, V. Kettnerova´,
eling for automatic speech under- Barzilay,R.andM.Lapata.2008.Mod- V. Kola´ˇrova´, M. Mikulova´,
standing. In D. R. Reddy, edi- eling local coherence: An entity- J. M´ırovsky´, A. Nedoluzhko,
tor, Speech Recognition. Academic basedapproach.ComputationalLin- J. Panevova´, L. Pola´kova´,
Press. guistics,34(1):1–34. M. Sˇevcˇ´ıkova´, J. Sˇteˇpa´nek, and
Baldridge,J.,N.Asher,andJ.Hunter. Barzilay,R.andL.Lee.2004.Catching Sˇ. Zika´nova´. 2013. Prague de-
2007. Annotation for and robust thedrift:Probabilisticcontentmod- pendency treebank 3.0. Technical
parsing of discourse structure on els, with applications to generation report, Institute of Formal and Ap-
unrestricted texts. Zeitschrift fu¨r andsummarization.HLT-NAACL. pliedLinguistics,CharlesUniversity
Sprachwissenschaft,26:213–239. Basile,P.,A.Caputo,andG.Semeraro. in Prague. LINDAT/CLARIN dig-
ital library at Institute of Formal
Bamman, D., O.Lewke, andA.Man- 2014.AnenhancedLeskwordsense
and Applied Linguistics, Charles
soor. 2020. An annotated dataset disambiguationalgorithmthrougha
UniversityinPrague.
ofcoreferenceinEnglishliterature. distributionalsemanticmodel.COL-
LREC. ING. Bellegarda, J. R. 1997. A latent se-
manticanalysisframeworkforlarge-
Bamman,D.,B.O’Connor,andN.A. Baum,L.E.andJ.A.Eagon.1967.An
span language modeling. EU-
Smith. 2013. Learning latent per- inequality with applications to sta-
ROSPEECH.
sonasoffilmcharacters.ACL. tistical estimation for probabilistic
functions of Markov processes and Bellegarda, J.R.2000. Exploitingla-
Bamman, D., S. Popat, and S. Shen.
toamodelforecology. Bulletinof tentsemanticinformationinstatisti-
2019. Anannotateddatasetofliter-
theAmericanMathematicalSociety, callanguagemodeling.Proceedings
aryentities.NAACLHLT.
73(3):360–363. oftheIEEE,89(8):1279–1296.
Banarescu, L., C. Bonial, S. Cai,
Baum,L.E.andT.Petrie.1966.Statis- Bellegarda, J. R. 2013. Natural lan-
M. Georgescu, K. Griffitt, U. Her-
ticalinferenceforprobabilisticfunc- guagetechnologyinmobiledevices:
mjakob, K. Knight, P. Koehn,
tions of finite-state Markov chains. Twogroundingframeworks. InMo-
M.Palmer,andN.Schneider.2013.
Annals of Mathematical Statistics, bile Speech and Advanced Natu-
Abstractmeaningrepresentationfor
37(6):1554–1563. ralLanguageSolutions,pages185–
sembanking. 7thLinguisticAnnota-
196.Springer.
tion Workshop and Interoperability Baum, L.F.1900. TheWizardofOz.
withDiscourse. AvailableatProjectGutenberg. Bellman,R.1957. DynamicProgram-
ming.PrincetonUniversityPress.
Banerjee,S.andA.Lavie.2005. ME- Bayes,T.1763.AnEssayTowardSolv-
TEOR:AnautomaticmetricforMT ing a Problem in the Doctrine of Bellman, R. 1984. Eye of the Hurri-
evaluation with improved correla- Chances, volume53. Reprintedin cane:anautobiography.WorldSci-
tion with human judgments. Pro- FacsimilesofTwoPapersbyBayes, entificSingapore.
ceedings of ACL Workshop on In- HafnerPublishing,1963. Bender,E.M.2019.The#BenderRule:
trinsic and Extrinsic Evaluation Bazell, C. E. 1952/1966. The corre- Onnamingthelanguageswestudy
Measures for MT and/or Summa- spondence fallacy in structural lin- andwhyitmatters.
rization. guistics.InE.P.Hamp,F.W.House- Bender,E.M.andB.Friedman.2018.
Banko,M.,M.Cafarella,S.Soderland, holder, and R. Austerlitz, editors, Datastatementsfornaturallanguage
M.Broadhead,andO.Etzioni.2007. Studies by Members of the English processing: Towardmitigatingsys-
Openinformationextractionforthe Department,IstanbulUniversity(3), tembiasandenablingbetterscience.
web.IJCAI. reprintedinReadingsinLinguistics TACL,6:587–604.Bibliography 589
Bender, E. M. and A. Koller. 2020. Bhat, I., R. A. Bhat, M. Shrivastava, procedureforquantitativelycompar-
Climbing towards NLU: On mean- and D. Sharma. 2017. Joining ingthesyntacticcoverageofEnglish
ing,form,andunderstandinginthe hands:Exploitingmonolingualtree- grammars.SpeechandNaturalLan-
ageofdata.ACL. banks for parsing of code-mixing guageWorkshop.
Bengio, Y., A. Courville, and P. Vin- data.EACL. Blei, D. M., A. Y. Ng, and M. I. Jor-
cent. 2013. Representation learn- Bickel, B. 2003. Referential density dan.2003. LatentDirichletalloca-
ing: A review and new perspec- indiscourseandsyntactictypology. tion.JMLR,3(5):993–1022.
tives. IEEETransactionsonPattern Language,79(2):708–736. Blodgett, S. L., S. Barocas,
Analysis and Machine Intelligence, Bickmore,T.W.,H.Trinh,S.Olafsson, H.Daume´III,andH.Wallach.2020.
35(8):1798–1828. T.K.O’Leary,R.Asadi,N.M.Rick- Language(technology)ispower: A
Bengio, Y., R. Ducharme, P. Vincent, les,andR.Cruz.2018. Patientand critical survey of “bias” in NLP.
andC.Jauvin.2003. Aneuralprob- consumer safety risks when using ACL.
abilistic language model. JMLR, conversationalassistantsformedical Blodgett, S. L., L. Green, and
3:1137–1155. information:Anobservationalstudy B. O’Connor. 2016. Demographic
of Siri, Alexa, and Google Assis- dialectal variation in social media:
Bengio, Y., P. Lamblin, D. Popovici, tant.JournalofMedicalInternetRe- A case study of African-American
and H. Larochelle. 2007. Greedy search,20(9):e11510. English.EMNLP.
layer-wise training of deep net-
works.NeurIPS. Bikel, D. M., S. Miller, R. Schwartz, Blodgett,S.L.andB.O’Connor.2017.
andR.Weischedel.1997. Nymble: Racialdisparityinnaturallanguage
Bengio,Y.,H.Schwenk,J.-S.Sene´cal, Ahigh-performancelearningname- processing: A case study of so-
F. Morin, and J.-L. Gauvain. 2006. finder.ANLP. cial media African-American En-
Neuralprobabilisticlanguagemod- glish. Fairness,Accountability,and
els. In Innovations in Machine Biran, O. and K. McKeown. 2015. Transparency in Machine Learning
Learning,pages137–186.Springer. PDTBdiscourseparsingasatagging (FAT/ML)Workshop,KDD.
task: The two taggers approach.
Bengtson, E.andD.Roth.2008. Un- SIGDIAL. Bloomfield,L.1914.AnIntroductionto
derstandingthevalueoffeaturesfor theStudyofLanguage. HenryHolt
Bird,S.,E.Klein,andE.Loper.2009.
coreferenceresolution.EMNLP. andCompany.
Natural Language Processing with
Bennett, R. and E. Elfner. 2019. The Python.O’Reilly. Bloomfield,L.1933. Language. Uni-
syntax–prosody interface. Annual versityofChicagoPress.
Bisani, M. and H. Ney. 2004. Boot-
ReviewofLinguistics,5:151–171. strapestimatesforconfidenceinter- Bobrow,D.G.,R.M.Kaplan,M.Kay,
vanBenthem,J.andA.terMeulen,ed- valsinASRperformanceevaluation. D. A. Norman, H. Thompson, and
itors.1997. HandbookofLogicand ICASSP. T.Winograd.1977. GUS,Aframe
Language.MITPress. drivendialogsystem. ArtificialIn-
Bishop, C.M.2006. Patternrecogni- telligence,8:155–173.
Bentivogli,L.,M.Cettolo,M.Federico, tionandmachinelearning.Springer.
Bobrow, D. G. and D. A. Norman.
andC.Federmann.2018. Machine
Bisk, Y., A. Holtzman, J. Thomason, 1975. Some principles of mem-
translationhumanevaluation:anin-
J. Andreas, Y. Bengio, J. Chai, ory schemata. In D. G. Bobrow
vestigation of evaluation based on
M. Lapata, A. Lazaridou, J. May, andA.Collins,editors,Representa-
post-editinganditsrelationwithdi-
A. Nisnevich, N. Pinto, and tion and Understanding. Academic
rectassessment.ICSLT.
J.Turian.2020.Experiencegrounds Press.
Berant, J., A. Chou, R. Frostig, and language.EMNLP.
Bobrow,D.G.andT.Winograd.1977.
P. Liang. 2013. Semantic parsing Bizer, C., J. Lehmann, G. Kobilarov, AnoverviewofKRL,aknowledge
on freebase from question-answer S. Auer, C. Becker, R. Cyganiak, representation language. Cognitive
pairs.EMNLP. andS.Hellmann.2009. DBpedia— Science,1(1):3–46.
Berg-Kirkpatrick, T., D. Burkett, and A crystallization point for the Web Boersma, P. and D. Weenink. 2005.
D.Klein.2012. Anempiricalinves- of Data. Web Semantics: science, Praat: doing phonetics by com-
tigationofstatisticalsignificancein services and agents on the world puter (version 4.3.14). [Computer
NLP.EMNLP. wideweb,7(3):154–165. program].RetrievedMay26,2005,
Berger,A.,S.A.DellaPietra,andV.J. Bjo¨rkelund, A. and J. Kuhn. 2014. fromhttp://www.praat.org/.
DellaPietra.1996. Amaximumen- Learningstructuredperceptronsfor Boguraev, B. K. and T. Briscoe, ed-
tropy approach to natural language coreference resolution with latent itors. 1989. Computational Lexi-
processing. ComputationalLinguis- antecedents and non-local features. cographyforNaturalLanguagePro-
tics,22(1):39–71. ACL. cessing.Longman.
Bergsma, S.andD.Lin.2006. Boot- Black, A. W. and P. Taylor. 1994. Bohus, D. and A. I. Rudnicky. 2005.
strapping path-based pronoun reso- CHATR:Agenericspeechsynthesis Sorry,Ididn’tcatchthat! —Anin-
lution.COLING/ACL. system.COLING. vestigationofnon-understandinger-
Black, E. 1988. An experiment in rors and recovery strategies. SIG-
Bergsma, S., D. Lin, and R. Goebel. computationaldiscriminationofEn- DIAL.
2008a. Discriminative learning of
selectional preference from unla- glish word senses. IBM Jour- Bojanowski,P.,E.Grave,A.Joulin,and
beledtext.EMNLP. nal of Research and Development, T. Mikolov. 2017. Enriching word
32(2):185–194. vectors with subword information.
Bergsma, S., D. Lin, and R. Goebel. Black, E., S. P. Abney, D. Flickinger, TACL,5:135–146.
2008b. Distributionalidentification
C. Gdaniec, R. Grishman, P. Har- Bollacker, K., C. Evans, P. Paritosh,
ofnon-referentialpronouns.ACL.
rison, D. Hindle, R. Ingria, F. Je- T. Sturge, and J. Taylor. 2008.
Bethard, S. 2013. ClearTK-TimeML: linek,J.L.Klavans,M.Y.Liberman, Freebase: a collaboratively created
AminimalistapproachtoTempEval M. P. Marcus, S. Roukos, B. San- graph database for structuring hu-
2013.SemEval-13. torini,andT.Strzalkowski.1991. A manknowledge.SIGMOD2008.590 Bibliography
Bolukbasi, T., K.-W. Chang, J. Zou, Brin,S.1998. Extractingpatternsand Budzianowski, P., T.-H. Wen, B.-
V.Saligrama,andA.T.Kalai.2016. relationsfromtheWorldWideWeb. H. Tseng, I. Casanueva, S. Ultes,
Manistocomputerprogrammeras Proceedings World Wide Web and O. Ramadan, and M. Gasˇic´. 2018.
womanistohomemaker?Debiasing Databases International Workshop, MultiWOZ - a large-scale multi-
wordembeddings.NeurIPS. Number1590inLNCS.Springer. domain wizard-of-Oz dataset for
Booth, T. L. 1969. Probabilistic Brockmann, C. and M. Lapata. 2003. task-oriented dialogue modelling.
representation of formal languages. Evaluating and combining ap-
EMNLP.
IEEEConferenceRecordofthe1969 proaches to selectional preference Bullinaria, J. A. and J. P. Levy. 2007.
TenthAnnualSymposiumonSwitch- acquisition.EACL. Extracting semantic representations
ingandAutomataTheory. from word co-occurrence statistics:
Broschart,J.1997. WhyTongandoes
Acomputationalstudy.Behaviorre-
Bordes,A.,N.Usunier,S.Chopra,and it differently. Linguistic Typology,
searchmethods,39(3):510–526.
J. Weston. 2015. Large-scale sim- 1:123–165.
ple question answering with mem- Bullinaria, J. A. and J. P. Levy.
ory networks. ArXiv preprint Brown, P. F., J. Cocke, S. A. 2012. Extracting semantic repre-
arXiv:1506.02075. DellaPietra,V.J.DellaPietra,F.Je- sentationsfromwordco-occurrence
linek, J. D. Lafferty, R. L. Mercer, statistics: stop-lists, stemming, and
Borges,J.L.1964. Theanalyticallan-
and P. S. Roossin. 1990. A statis- SVD. Behavior research methods,
guage of John Wilkins. University
tical approach to machine transla- 44(3):890–907.
of Texas Press. Trans. Ruth L. C.
tion. Computational Linguistics,
Simms. Bulyko, I., K. Kirchhoff, M. Osten-
16(2):79–85.
dorf,andJ.Goldberg.2005. Error-
Bostrom,K.andG.Durrett.2020.Byte
Brown, P. F., S. A. Della Pietra, V. J. sensitive response generation in a
pairencodingissuboptimalforlan-
DellaPietra,andR.L.Mercer.1993. spoken language dialogue system.
guagemodel pretraining. Findings
The mathematics of statistical ma- SpeechCommunication,45(3):271–
ofEMNLP.
chine translation: Parameter esti- 288.
Bourlard, H. and N. Morgan. 1994. mation. ComputationalLinguistics,
Caliskan, A., J. J. Bryson, and
Connectionist Speech Recognition: 19(2):263–311.
A.Narayanan.2017. Semanticsde-
AHybridApproach.Kluwer.
Brown, T., B. Mann, N. Ryder, rived automatically from language
Bowman, S.R., L.Vilnis, O.Vinyals, M. Subbiah, J. Kaplan, P. Dhari- corpora contain human-like biases.
A. M. Dai, R. Jozefowicz, and wal, A. Neelakantan, P. Shyam, Science,356(6334):183–186.
S. Bengio. 2016. Generating sen- G. Sastry, A. Askell, S. Agar- Callison-Burch, C., M. Osborne, and
tences from a continuous space. wal, A. Herbert-Voss, G. Krueger, P. Koehn. 2006. Re-evaluating the
CoNLL. T.Henighan,R.Child,A.Ramesh, roleofBLEUinmachinetranslation
Boyd-Graber, J., S. Feng, and P. Ro- D. M. Ziegler, J. Wu, C. Win- research.EACL.
driguez. 2018. Human-computer ter, C. Hesse, M. Chen, E. Sigler,
Canavan, A., D. Graff, and G. Zip-
question answering: The case for M. Litwin, S. Gray, B. Chess,
perlen.1997. CALLHOMEAmeri-
quizbowl. In S. Escalera and J. Clark, C. Berner, S. McCan-
canEnglishspeechLDC97S42.Lin-
M. Weimer, editors, The NIPS ’17 dlish,A.Radford,I.Sutskever,and
guisticDataConsortium.
Competition: Building Intelligent D.Amodei.2020. Languagemod-
Systems.Springer. elsarefew-shotlearners. NeurIPS, Cardie,C.1993.Acase-basedapproach
volume33. toknowledgeacquisitionfordomain
Brachman, R. J. 1979. On the epis-
specificsentenceanalysis.AAAI.
temogical status of semantic net- Bruce,B.C.1975. Generationasaso-
works. InN.V.Findler,editor,As- cialaction.ProceedingsofTINLAP- Cardie, C. 1994. Domain-Specific
sociativeNetworks: Representation 1 (Theoretical Issues in Natural KnowledgeAcquisitionforConcep-
andUseofKnowledgebyComput- LanguageProcessing). tual Sentence Analysis. Ph.D. the-
ers,pages3–50.AcademicPress. sis, University of Massachusetts,
Brysbaert, M., A. B. Warriner, and Amherst, MA. Available as CMP-
Brachman,R.J.andH.J.Levesque,ed- V. Kuperman. 2014. Concrete- SCITechnicalReport94-74.
itors.1985. ReadingsinKnowledge ness ratings for 40 thousand gen-
Representation.MorganKaufmann. erally known English word lem- Cardie, C. and K. Wagstaff. 1999.
Nounphrasecoreferenceascluster-
Brachman, R. J. and J. G. Schmolze. mas. Behavior Research Methods,
ing.EMNLP/VLC.
1985. An overview of the KL- 46(3):904–911.
ONEknowledgerepresentationsys- Bu, H., J. Du, X. Na, B. Wu, and Carlini, N., F. Tramer, E. Wal-
tem. Cognitive Science, 9(2):171– H.Zheng.2017. AISHELL-1: An lace,M.Jagielski,A.Herbert-Voss,
216. open-source Mandarin speech cor- K. Lee, A. Roberts, T. Brown,
D. Song, U. Erlingsson, A. Oprea,
Brants,T.2000.TnT:Astatisticalpart- pus and a speech recognition base-
and C. Raffel1. 2020. Extract-
of-speechtagger.ANLP. line.O-COCOSDAProceedings.
ing training data from large lan-
Brants, T., A. C. Popat, P. Xu, F. J. Buchholz,S.andE.Marsi.2006.Conll- guage models. ArXiv preprint
Och,andJ.Dean.2007. Largelan- xsharedtaskonmultilingualdepen- arXiv:2012.07805.
guage models in machine transla- dencyparsing.CoNLL. Carlson,G.N.1977.Referencetokinds
tion.EMNLP/CoNLL.
Buck, C., K. Heafield, and in English. Ph.D. thesis, Univer-
Braud, C., M. Coavoux, and B.VanOoyen.2014.N-gramcounts sityofMassachusetts,Amherst.For-
A. Søgaard. 2017. Cross-lingual andlanguagemodelsfromthecom- ward.
RSTdiscourseparsing.EACL. moncrawl.LREC. Carlson,L.andD.Marcu.2001. Dis-
Bre´al,M.1897. EssaideSe´mantique: Budanitsky, A. and G. Hirst. 2006. course tagging manual. Technical
Sciencedessignifications.Hachette. Evaluating WordNet-based mea- ReportISI-TR-545,ISI.
Brennan, S. E., M. W. Friedman, and sures of lexical semantic related- Carlson, L., D. Marcu, and M. E.
C. Pollard. 1987. A centering ap- ness. Computational Linguistics, Okurowski. 2001. Building a
proachtopronouns.ACL. 32(1):13–47. discourse-tagged corpus in theBibliography 591
framework of rhetorical structure Chen, D., A. Fisch, J. Weston, and Choe, D. K. and E. Charniak. 2016.
theory.SIGDIAL. A. Bordes. 2017a. Reading Wiki- Parsing as language modeling.
Carreras, X. and L. Ma`rquez. 2005. pediatoansweropen-domainques- EMNLP. Association for Compu-
Introduction to the CoNLL-2005 tions.ACL. tationalLinguistics.
sharedtask: Semanticrolelabeling. Chen,D.andC.Manning.2014.Afast Choi,J.D.andM.Palmer.2011a. Get-
CoNLL. andaccuratedependencyparserus- tingthemostoutoftransition-based
Chafe, W. L. 1976. Givenness, con- ingneuralnetworks.EMNLP. dependencyparsing.ACL.
trastiveness, definiteness, subjects, Chen, E., B. Snyder, and R. Barzi- Choi, J. D. and M. Palmer. 2011b.
topics,andpointofview. InC.N. lay.2007. Incrementaltextstructur- Transition-based semantic role la-
Li,editor,SubjectandTopic,pages ingwithonlinehierarchicalranking. beling using predicate argument
25–55.AcademicPress. EMNLP/CoNLL. clustering. ProceedingsoftheACL
2011WorkshoponRelationalMod-
Chambers,N.2013. NavyTime: Event Chen, J. N. and J. S. Chang. 1998. elsofSemantics.
and time ordering from raw text. Topical clustering of MRD senses
SemEval-13. basedoninformationretrievaltech- Choi, J.D., J.Tetreault, andA.Stent.
2015. It depends: Dependency
Chambers,N.,T.Cassidy,B.McDow- niques. ComputationalLinguistics, parser comparison using a web-
ell, and S. Bethard. 2014. Dense 24(1):61–96. basedevaluationtool.ACL.
eventorderingwithamulti-passar- Chen,S.F.andJ.Goodman.1998. An
Chomsky, N.1956. Threemodelsfor
chitecture.TACL,2:273–284. empirical study of smoothing tech-
the description of language. IRE
Chambers, N. and D. Jurafsky. 2010. niquesforlanguagemodeling.Tech- Transactions on Information The-
Improvingtheuseofpseudo-words nical Report TR-10-98, Computer ory,2(3):113–124.
for evaluating selectional prefer- ScienceGroup,HarvardUniversity.
Chomsky, N. 1956/1975. The Logi-
ences.ACL. Chen, S. F. and J. Goodman. 1999. cal Structure of Linguistic Theory.
Chambers, N. and D. Jurafsky. 2011. An empirical study of smoothing Plenum.
Template-based information extrac- techniques for language modeling. Chomsky, N. 1957. Syntactic Struc-
tionwithoutthetemplates.ACL. Computer Speech and Language, tures.Mouton,TheHague.
13:359–394.
Chan, W., N. Jaitly, Q. Le, and Chomsky, N. 1963. Formal proper-
O. Vinyals. 2016. Listen, at- Chen,X.,Z.Shi,X.Qiu,andX.Huang. ties of grammars. In R. D. Luce,
tend and spell: A neural network 2017b. Adversarial multi-criteria R. Bush, and E. Galanter, editors,
for large vocabulary conversational learningforChinesewordsegmen- HandbookofMathematicalPsychol-
speechrecognition.ICASSP. tation.ACL. ogy,volume2,pages323–418.Wi-
Chandioux, J. 1976. ME´TE´O: un Cheng, J., L. Dong, and M. La- ley.
syste`me ope´rationnel pour la tra- pata. 2016. Long short-term Chomsky, N.1981. LecturesonGov-
duction automatique des bulletins memory-networksformachineread- ernmentandBinding.Foris.
me´te´orologiques destine´s au grand ing.EMNLP. Chorowski, J., D. Bahdanau, K. Cho,
public.Meta,21:127–133. Chiang,D.2005.Ahierarchicalphrase- and Y. Bengio. 2014. End-to-end
Chang,A.X.andC.D.Manning.2012. basedmodelforstatisticalmachine continuousspeechrecognitionusing
SUTime: Alibraryforrecognizing translation.ACL. attention-based recurrent NN: First
and normalizing time expressions. results.NeurIPSDeepLearningand
Chierchia,G.andS.McConnell-Ginet.
LREC. RepresentationLearningWorkshop.
1991.MeaningandGrammar.MIT
Chang, K.-W., R. Samdani, and Press. Chou,W.,C.-H.Lee,andB.H.Juang.
D. Roth. 2013. A constrained la- 1993. Minimum error rate train-
Chinchor,N.,L.Hirschman,andD.L.
tent variable model for coreference ing based on n-best string models.
Lewis. 1993. Evaluating Message
resolution.EMNLP. ICASSP.
Understanding systems: An analy-
Chang, K.-W., R. Samdani, A. Ro- sisofthethirdMessageUnderstand- Christodoulopoulos, C., S. Goldwa-
zovskaya, M. Sammons, and ingConference.ComputationalLin- ter, and M. Steedman. 2010. Two
D. Roth. 2012. Illinois-Coref: guistics,19(3):409–449. decades of unsupervised POS in-
TheUIsystemintheCoNLL-2012 duction: How far have we come?
sharedtask.CoNLL. Chiticariu, L., M. Danilevsky, Y. Li, EMNLP.
F. Reiss, and H. Zhu. 2018. Sys-
Chaplot, D. S. and R. Salakhutdinov. temT: Declarative text understand- Chu,Y.-J.andT.-H.Liu.1965. Onthe
2018. Knowledge-basedwordsense ingforenterprise.NAACLHLT,vol- shortest arborescence of a directed
disambiguationusingtopicmodels. ume3. graph. Science Sinica, 14:1396–
AAAI. 1400.
Chiticariu, L., Y. Li, and F. R. Reiss. Chu-Carroll, J. 1998. A statistical
Charniak, E. 1997. Statistical pars- 2013. Rule-BasedInformationEx- modelfordiscourseactrecognition
ingwithacontext-freegrammarand traction is Dead! Long Live Rule- in dialogue interactions. Applying
wordstatistics.AAAI. Based Information Extraction Sys- MachineLearningtoDiscoursePro-
Charniak, E., C. Hendrickson, N. Ja- tems! EMNLP. cessing.Papersfromthe1998AAAI
cobson, and M. Perkowitz. 1993. Chiu, J. P. C. and E. Nichols. 2016. Spring Symposium. Tech. rep. SS-
Equations for part-of-speech tag- Namedentityrecognitionwithbidi- 98-01.AAAIPress.
ging.AAAI. rectional LSTM-CNNs. TACL, Chu-Carroll, J.andS.Carberry.1998.
Che,W.,Z.Li,Y.Li,Y.Guo,B.Qin, 4:357–370. Collaborativeresponsegenerationin
and T. Liu. 2009. Multilingual Cho, K., B. van Merrie¨nboer, C. Gul- planningdialogues. Computational
dependency-basedsyntacticandse- cehre, D. Bahdanau, F. Bougares, Linguistics,24(3):355–400.
manticparsing.CoNLL. H. Schwenk, and Y. Bengio. 2014. Chu-Carroll,J.andB.Carpenter.1999.
Chen, C. and V. Ng. 2013. Linguis- Learningphraserepresentationsus- Vector-based natural language call
ticallyawarecoreferenceevaluation ingRNNencoder–decoderforstatis- routing. ComputationalLinguistics,
metrics.IJCNLP. ticalmachinetranslation.EMNLP. 25(3):361–388.592 Bibliography
Church, A.1940. Aformulationofa Clark, J. H., E. Choi, M. Collins, UScensus. SpeechCommunication,
simple theory of types. Journal of D. Garrette, T. Kwiatkowski, 23:243–260.
SymbolicLogic,5:56–68. V. Nikolaev, and J. Palomaki. Coleman,J.2005. IntroducingSpeech
Church,K.W.1988. Astochasticparts 2020. TyDi QA: A benchmark and Language Processing. Cam-
programandnounphraseparserfor for information-seeking question bridgeUniversityPress.
unrestrictedtext.ANLP. a lan ns gw ue ar gin eg s.Tin ACty Lp ,o 8l :o 4g 5i 4ca –l 4ly 70.diverse Collins,M.1999. Head-DrivenStatis-
Church,K.W.1989. Astochasticparts tical Models for Natural Language
Clark, K. and C. D. Manning. 2015.
programandnounphraseparserfor Parsing. Ph.D.thesis,Universityof
Entity-centriccoreferenceresolution
unrestrictedtext.ICASSP. Pennsylvania,Philadelphia.
withmodelstacking.ACL.
Church, K. W. 1994. Unix for Poets. Collobert,R.andJ.Weston.2007.Fast
Clark, K. and C. D. Manning. 2016a.
Slides from 2nd ELSNET Summer semantic extraction using a novel
Deep reinforcement learning for
Schoolandunpublishedpaperms. neuralnetworkarchitecture.ACL.
mention-ranking coreference mod-
Church,K.W.andW.A.Gale.1991.A els.EMNLP. Collobert, R. and J. Weston. 2008.
comparisonoftheenhancedGood- A unified architecture for natural
Clark, K. and C. D. Manning. 2016b.
Turinganddeletedestimationmeth- language processing: Deep neural
Improvingcoreferenceresolutionby
ods for estimating probabilities of networks with multitask learning.
learningentity-leveldistributedrep-
Englishbigrams. ComputerSpeech ICML.
resentations.ACL.
andLanguage,5:19–54. Collobert, R., J. Weston, L. Bottou,
Clark, P., I. Cowhey, O. Et-
Church, K. W. and P. Hanks. 1989. M. Karlen, K. Kavukcuoglu, and
zioni, T. Khot, A. Sabharwal,
Wordassociationnorms,mutualin- P. Kuksa. 2011. Natural language
C.Schoenick,andO.Tafjord.2018.
formation,andlexicography.ACL. processing (almost) from scratch.
Think you have solved question
JMLR,12:2493–2537.
Church, K. W. and P. Hanks. 1990. answering? TryARC,theAI2rea-
Wordassociationnorms,mutualin- soning challenge. ArXiv preprint Comrie,B.1989.LanguageUniversals
formation,andlexicography. Com- arXiv:1803.05457. and Linguistic Typology, 2nd edi-
putationalLinguistics,16(1):22–29. tion.Blackwell.
Clark, P., O. Etzioni, D. Khashabi,
Church, K. W., T. Hart, and J. Gao. T.Khot,B.D.Mishra,K.Richard- Connolly, D., J. D. Burger, and D. S.
2007. Compressing trigram lan- son, A. Sabharwal, C. Schoenick, Day.1994. Amachinelearningap-
guagemodelswithGolombcoding. O. Tafjord, N. Tandon, S. Bhak- proachtoanaphoricreference. Pro-
EMNLP/CoNLL. thavatsalam, D. Groeneveld, ceedings of the International Con-
Cialdini, R. B. 1984. Influence: The M.Guerquin,andM.Schmitz.2019. ference on New Methods in Lan-
psychologyofpersuasion.Morrow. From’F’to’A’ontheNYRegents guageProcessing(NeMLaP).
Science Exams: An overview of Cooley, J. W. and J. W. Tukey. 1965.
Ciaramita, M. and Y. Altun. 2006. the Aristo project. ArXiv preprint An algorithm for the machine cal-
Broad-coverage sense disambigua- arXiv:1909.01958. culation of complex Fourier se-
tion and information extraction
with a supersense sequence tagger. Clark,S.,J.R.Curran,andM.Osborne. ries. MathematicsofComputation,
EMNLP. 2003. Bootstrapping POS-taggers 19(90):297–301.
usingunlabelleddata.CoNLL. Cooper, F. S., A. M. Liberman, and
Ciaramita, M. and M. Johnson. 2003.
CMU.1993.TheCarnegieMellonPro- J.M.Borst.1951. Theinterconver-
Supersense tagging of unknown
nouncingDictionaryv0.1. Carnegie sionofaudibleandvisiblepatterns
nounsinWordNet.EMNLP-2003.
MellonUniversity. as a basis for research in the per-
Cieri, C., D. Miller, and K. Walker. ception of speech. Proceedings of
Coccaro,N.andD.Jurafsky.1998. To-
2004.TheFishercorpus:Aresource the National Academy of Sciences,
wards better integration of seman-
for the next generations of speech- 37(5):318–325.
ticpredictorsinstatisticallanguage
to-text.LREC.
modeling.ICSLP. Copestake, A. and T. Briscoe. 1995.
Clark, E.1987. Theprincipleofcon- Semi-productive polysemy and
Cohen, M. H., J. P. Giangola, and
trast: Aconstraintonlanguageac- sense extension. Journal of Se-
J.Balogh.2004. VoiceUserInter-
quisition. InB.MacWhinney, edi- mantics,12(1):15–68.
faceDesign.Addison-Wesley.
tor,Mechanismsoflanguageacqui-
Cottrell, G. W. 1985. A Connection-
sition,pages1–33.LEA. Cohen,P.R.andC.R.Perrault.1979.
istApproachtoWordSenseDisam-
Elementsofaplan-basedtheoryof
Clark, H. H. 1996. Using Language. biguation. Ph.D.thesis, University
speech acts. Cognitive Science,
CambridgeUniversityPress. of Rochester, Rochester, NY. Re-
3(3):177–212.
Clark,H.H.andJ.E.FoxTree.2002. vised version published by Pitman,
Using uh and um in spontaneous Colby,K.M.,F.D.Hilf,S.Weber,and 1989.
H.C.Kraemer.1972.Turing-likein-
speaking.Cognition,84:73–111. Cover,T.M.andJ.A.Thomas.1991.
distinguishability tests for the vali-
Clark, H. H. and C. Marshall. 1981. dation of a computer simulation of Elements of Information Theory.
Definite reference and mutual paranoidprocesses. ArtificialIntel- Wiley.
knowledge. In A. K. Joshi, B. L. ligence,3:199–221. Covington, M. 2001. A fundamen-
Webber,andI.A.Sag,editors,Ele- tal algorithm for dependency pars-
Colby,K.M.,S.Weber,andF.D.Hilf.
ments of Discourse Understanding, ing. Proceedingsofthe39thAnnual
1971. Artificialparanoia. Artificial
pages10–63.Cambridge. ACMSoutheastConference.
Intelligence,2(1):1–25.
Clark, H. H. and D. Wilkes-Gibbs.
Cole,R.A.,D.G.Novick,P.J.E.Ver-
Cox,D.1969.AnalysisofBinaryData.
1986. Referring as a collaborative meulen,S.Sutton,M.Fanty,L.F.A. ChapmanandHall,London.
process.Cognition,22:1–39.
Wessels,J.H.deVilliers,J.Schalk- Craven, M. and J. Kumlien. 1999.
Clark, J.andC.Yallop.1995. AnIn- wyk, B. Hansen, and D. Burnett. Constructing biological knowledge
troductiontoPhoneticsandPhonol- 1997. Experiments with a spo- bases by extracting information
ogy,2ndedition.Blackwell. ken dialogue system for taking the fromtextsources.ISMB-99.Bibliography 593
Crawford, K. 2017. The trouble with 400-millionwordCorpusofHistor- Denis,P.andJ.Baldridge.2007. Joint
bias.KeynoteatNeurIPS. ical American English. Corpora, determination of anaphoricity and
7(2):121–157. coreferenceresolutionusinginteger
Croft,W.1990. TypologyandUniver-
sals.CambridgeUniversityPress. Davies, M. 2015. The Wiki- programming.NAACL-HLT.
pedia Corpus: 4.6 million arti- Denis,P.andJ.Baldridge.2008. Spe-
Cross, J. and L. Huang. 2016. Span-
cles, 1.9 billion words. Adapted cialized models and ranking for
based constituency parsing with a
from Wikipedia. https://www. coreferenceresolution.EMNLP.
structure-labelsystemandprovably
optimaldynamicoracles. EMNLP. english-corpora.org/wiki/. Denis,P.andJ.Baldridge.2009.Global
AssociationforComputationalLin- Davies, M. 2020. The Corpus jointmodelsforcoreferenceresolu-
guistics. of Contemporary American En- tionandnamedentityclassification.
glish (COCA): One billion words, Procesamiento del Lenguaje Natu-
Cruse, D. A. 2004. Meaning in Lan- 1990-2019. https://www. ral,42.
guage:anIntroductiontoSemantics
english-corpora.org/coca/. DeRose,S.J.1988. Grammaticalcat-
andPragmatics. OxfordUniversity
Press.Secondedition. Davis, E., L. Morgenstern, and C. L. egory disambiguation by statistical
Ortiz. 2017. The first Winograd optimization. Computational Lin-
Cucerzan, S. 2007. Large-scale schema challenge at IJCAI-16. AI guistics,14:31–39.
named entity disambiguation based
Magazine,38(3):97–98. Devlin, J., M.-W.Chang, K.Lee, and
onWikipediadata.EMNLP/CoNLL.
K. Toutanova. 2019. BERT: Pre-
Davis,K.H.,R.Biddulph,andS.Bal-
Dagan, I., S. Marcus, and trainingofdeepbidirectionaltrans-
ashek.1952. Automaticrecognition
S. Markovitch. 1993. Contextual formersforlanguageunderstanding.
ofspokendigits. JASA,24(6):637–
wordsimilarityandestimationfrom NAACLHLT.
642.
sparsedata.ACL. DiEugenio,B.1990. Centeringtheory
Davis, S. and P. Mermelstein. 1980.
Dahl, G. E., T. N. Sainath, and G. E. and the Italian pronominal system.
Comparison of parametric repre-
Hinton. 2013. Improving deep COLING.
sentations for monosyllabic word
neural networks for LVCSR using recognition in continuously spoken Di Eugenio, B. 1996. The discourse
rectified linear units and dropout. sentences. IEEE Transactions on functionsofItaliansubjects:Acen-
ICASSP. Acoustics, Speech, andSignalPro- teringapproach.COLING.
Dahl, G. E., D. Yu, L. Deng, and cessing,28(4):357–366. Diab,M.andP.Resnik.2002.Anunsu-
A.Acero.2012. Context-dependent Deerwester,S.C.,S.T.Dumais,G.W. pervisedmethodforwordsensetag-
pre-trained deep neural networks Furnas, R. A. Harshman, T. K. gingusingparallelcorpora.ACL.
for large-vocabulary speech recog- Landauer, K. E. Lochbaum, and Dinan, E., G. Abercrombie, A. S.
nition. IEEE Transactions on au- L.Streeter.1988. Computerinfor- Bergman,S.Spruit,D.Hovy,Y.-L.
dio, speech, andlanguageprocess- mationretrievalusinglatentseman- Boureau,andV.Rieser.2021.Antic-
ing,20(1):30–42. ticstructure:USPatent4,839,853. ipatingsafetyissuesine2econver-
Danescu-Niculescu-Mizil, C. and Deerwester,S.C.,S.T.Dumais,T.K. sationalai: Frameworkandtooling.
L.Lee.2011. Chameleonsinimag- Landauer, G.W.Furnas, andR.A. ArXiv,abs/2107.03451.
inedconversations:Anewapproach Harshman. 1990. Indexing by la- Dinan,E.,A.Fan,A.Williams,J.Ur-
to understanding coordination of tent semantics analysis. JASIS, banek, D. Kiela, and J. Weston.
linguistic style in dialogs. 2nd 41(6):391–407. 2020.Queensarepowerfultoo:Mit-
Workshop on Cognitive Modeling igatinggenderbiasindialoguegen-
Deibel,D.andR.Evanhoe.2021.Con-
andComputationalLinguistics. eration.EMNLP.
versationswithThings: UXDesign
Danieli,M.andE.Gerbino.1995.Met- forChatandVoice.Rosenfeld. Dinan, E., S. Roller, K. Shuster,
rics for evaluating dialogue strate- A. Fan, M. Auli, and J. We-
DeJong, G. F. 1982. An overview of
gies in a spoken language system. ston. 2019. Wizard of Wikipedia:
theFRUMPsystem. InW.G.Lehn-
AAAISpringSymposiumonEmpir- Knowledge-powered conversational
ertandM.H.Ringle,editors,Strate-
icalMethodsinDiscourseInterpre- agents.ICLR.
giesforNaturalLanguageProcess-
tationandGeneration. Ditman, T. and G. R. Kuperberg.
ing,pages149–176.LEA.
Das,S.R.andM.Y.Chen.2001. Ya- 2010.Buildingcoherence:Aframe-
hoo! forAmazon: Sentimentpars- Demberg,V.2006. Letter-to-phoneme work for exploring the breakdown
ingfromsmalltalkontheweb.EFA conversion for a German text-to- oflinksacrossclauseboundariesin
2001BarcelonaMeetings.http:// speechsystem.DiplomarbeitNr.47, schizophrenia. Journalofneurolin-
ssrn.com/abstract=276189. Universita¨tStuttgart. guistics,23(3):254–269.
David, Jr., E. E. and O. G. Selfridge. Denes,P.1959. Thedesignandoper- Dixon,L.,J.Li,J.Sorensen,N.Thain,
1962. Eyesandearsforcomputers. ationofthemechanicalspeechrec- andL.Vasserman.2018.Measuring
ProceedingsoftheIRE(Instituteof ognizer at University College Lon- and mitigating unintended bias in
RadioEngineers),50:1093–1101. don. JournaloftheBritishInstitu- textclassification. 2018AAAI/ACM
tionofRadioEngineers,19(4):219– ConferenceonAI,Ethics,andSoci-
Davidson,D.1967.Thelogicalformof 234.Appearstogetherwithcompan- ety.
actionsentences. InN.Rescher,ed- ionpaper(Fry1959).
Dixon,N.andH.Maxey.1968. Termi-
itor,TheLogicofDecisionandAc-
Deng,L.,G.Hinton,andB.Kingsbury. nal analog synthesis of continuous
tion.UniversityofPittsburghPress.
2013. New types of deep neural speechusingthediphonemethodof
Davidson, T., D. Bhattacharya, and network learning for speech recog- segment assembly. IEEE Transac-
I.Weber.2019. Racialbiasinhate nition and related applications: An tionsonAudioandElectroacoustics,
speechandabusivelanguagedetec- overview.ICASSP. 16(1):40–50.
tion datasets. Third Workshop on
Deng, Y.andW.Byrne.2005. HMM Do, Q. N. T., S. Bethard, and M.-F.
AbusiveLanguageOnline.
wordandphrasealignmentforsta- Moens. 2017. Improving implicit
Davies, M. 2012. Expanding hori- tistical machine translation. HLT- semanticrolelabelingbypredicting
zonsinhistoricallinguisticswiththe EMNLP. semanticframearguments.IJCNLP.594 Bibliography
Doddington,G.2002. Automaticeval- Duda,R.O.andP.E.Hart.1973. Pat- Emami, A., P.Trichelair, A.Trischler,
uationofmachinetranslationquality ternClassificationandSceneAnaly- K.Suleman,H.Schulz,andJ.C.K.
using n-gram co-occurrence statis- sis.JohnWileyandSons. Cheung. 2019. The KNOWREF
tics.HLT. coreferencecorpus: Removinggen-
Durrett, G.andD.Klein.2013. Easy
Dolan, B. 1994. Word sense am- victoriesanduphillbattlesincoref- der and number cues for diffi-
biguation:Clusteringrelatedsenses. erenceresolution.EMNLP. cult pronominal anaphora resolu-
COLING. tion.ACL.
Durrett,G.andD.Klein.2014. Ajoint
Dong, L. and M. Lapata. 2016. Lan- model for entity analysis: Corefer- Erk, K. 2007. A simple, similarity-
guagetologicalformwithneuralat- ence, typing, and linking. TACL, based model for selectional prefer-
tention.ACL. 2:477–490. ences.ACL.
Dorr,B.1994. Machinetranslationdi- Earley, J.1968. AnEfficientContext- van Esch, D. and R. Sproat. 2018.
vergences:Aformaldescriptionand Free Parsing Algorithm. Ph.D. Anexpandedtaxonomyofsemiotic
proposed solution. Computational thesis, Carnegie Mellon University, classes for text normalization. IN-
Linguistics,20(4):597–633. Pittsburgh,PA. TERSPEECH.
Ethayarajh, K., D. Duvenaud, and
Dostert, L. 1955. The Georgetown- Earley, J. 1970. An efficient context-
I.B.M. experiment. In Machine free parsing algorithm. CACM, G. Hirst. 2019a. Towards un-
TranslationofLanguages:Fourteen 6(8):451–455. derstanding linear word analogies.
Essays,pages124–135.MITPress.
ACL.
Ebden, P. and R. Sproat. 2015. The Ethayarajh, K., D. Duvenaud, and
Dowty,D.R.1979.WordMeaningand KestrelTTStextnormalizationsys- G.Hirst.2019b. Understandingun-
MontagueGrammar.D.Reidel. tem. Natural Language Engineer- desirable word embedding associa-
Dowty,D.R.,R.E.Wall,andS.Peters. ing,21(3):333. tions.ACL.
1981. IntroductiontoMontagueSe-
Edmonds, J. 1967. Optimum branch- Etzioni, O., M.Cafarella, D.Downey,
mantics.D.Reidel.
ings. Journal of Research of the A.-M.Popescu,T.Shaked,S.Soder-
Dozat, T. and C. D. Manning. 2017. National Bureau of Standards B, land, D. S. Weld, and A. Yates.
Deepbiaffineattentionforneuralde- 71(4):233–240. 2005. Unsupervised named-entity
pendencyparsing.ICLR. Edunov, S., M. Ott, M. Auli, and extractionfromtheweb:Anexperi-
Dozat, T. and C. D. Manning. 2018. D. Grangier. 2018. Understanding mentalstudy. ArtificialIntelligence,
Simplerbutmoreaccuratesemantic back-translationatscale.EMNLP. 165(1):91–134.
dependencyparsing.ACL. Efron,B.andR.J.Tibshirani.1993.An Evans, N. 2000. Word classes in the
Dozat, T., P. Qi, and C. D. Manning. introductiontothebootstrap. CRC world’s languages. In G. Booij,
2017. Stanford’sgraph-basedneu- press. C. Lehmann, and J. Mugdan, ed-
raldependencyparserattheCoNLL itors, Morphology: A Handbook
Egghe, L.2007. UntanglingHerdan’s
2017sharedtask.Proceedingsofthe on Inflection and Word Formation,
lawandHeaps’law: Mathematical
CoNLL2017SharedTask:Multilin- pages708–732.Mouton.
andinformetricarguments. JASIST,
gualParsingfromRawTexttoUni- 58(5):702–709. Fader,A.,S.Soderland,andO.Etzioni.
versalDependencies. 2011. Identifyingrelationsforopen
Dror, R., G. Baumer, M. Bogomolov, Eisner,J.1996.Threenewprobabilistic informationextraction.EMNLP.
andR.Reichart.2017. Replicabil- modelsfordependencyparsing: An Fan, A., S. Bhosale, H. Schwenk,
ityanalysisfornaturallanguagepro-
exploration.COLING.
Z. Ma, A. El-Kishky, S. Goyal,
cessing: Testing significance with Ekman, P. 1999. Basic emotions. In M. Baines, O. Celebi, G. Wenzek,
multiple datasets. TACL, 5:471– T. Dalgleish and M. J. Power, ed- V. Chaudhary, N. Goyal, T. Birch,
–486. itors, Handbook of Cognition and V.Liptchinsky,S.Edunov,M.Auli,
Dror,R.,L.Peled-Cohen,S.Shlomov, Emotion,pages45–60.Wiley. and A. Joulin. 2021. Beyond
and R. Reichart. 2020. Statisti- Elman,J.L.1990. Findingstructurein english-centric multilingual ma-
calSignificanceTestingforNatural time. Cognitivescience,14(2):179– chinetranslation.JMLR,22(107):1–
LanguageProcessing,volume45of 211. 48.
Synthesis Lectures on Human Lan- Elsner,M.,J.Austerweil,andE.Char- Fano,R.M.1961. TransmissionofIn-
guage Technologies. Morgan & niak. 2007. A unified local and formation: A Statistical Theory of
Claypool. global model for discourse coher- Communications.MITPress.
Dryer, M. S. and M. Haspelmath, ed- ence.NAACL-HLT. Fant,G.M.1951. Speechcommunica-
itors. 2013. The World Atlas of Elsner, M. and E. Charniak. 2008. tionresearch. Ing.VetenskapsAkad.
Language Structures Online. Max Coreference-inspired coherence Stockholm,Sweden,24:331–337.
Planck Institute for Evolutionary
modeling.ACL. Fant,G.M.1960. AcousticTheoryof
Anthropology, Leipzig. Available
SpeechProduction.Mouton.
onlineathttp://wals.info. Elsner,M.andE.Charniak.2011. Ex-
tending the entity grid with entity- Fant,G.M.1986.Glottalflow:Models
DuBois,J.W.,W.L.Chafe,C.Meyer,
specificfeatures.ACL. andinteraction. JournalofPhonet-
S. A. Thompson, R. Englebretson,
ics,14:393–399.
andN.Martey.2005. SantaBarbara Elveva˚g, B., P. W. Foltz, D. R.
corpusofspokenAmericanEnglish, Weinberger, and T. E. Goldberg. Fant,G.M.2004.SpeechAcousticsand
Parts1-4. Philadelphia: Linguistic 2007. Quantifying incoherence in Phonetics.Kluwer.
DataConsortium. speech: anautomatedmethodology Faruqui, M., J. Dodge, S. K. Jauhar,
Dua, D., Y. Wang, P. Dasigi, andnovelapplicationtoschizophre- C.Dyer,E.Hovy,andN.A.Smith.
G. Stanovsky, S. Singh, and nia. Schizophrenia research, 93(1- 2015. Retrofitting word vectors to
M.Gardner.2019.DROP:Areading 3):304–316. semanticlexicons.NAACLHLT.
comprehension benchmark requir- Emami,A.andF.Jelinek.2005.Aneu- Fast,E.,B.Chen,andM.S.Bernstein.
ing discrete reasoning over para- ral syntactic language model. Ma- 2016.Empath:UnderstandingTopic
graphs.NAACLHLT. chinelearning,60(1):195–227. SignalsinLarge-ScaleText.CHI.Bibliography 595
Fauconnier, G. and M. Turner. 2008. Table,volume17ofMonographSe- Foland, W. and J. H. Martin. 2016.
The way we think: Conceptual ries on Language and Linguistics, CU-NLP at SemEval-2016 task 8:
blending and the mind’s hidden pages 19–34. Georgetown Univer- AMRparsingusingLSTM-basedre-
complexities.BasicBooks. sityPress. currentneuralnetworks. SemEval-
Fazel-Zarandi, M., S.-W. Li, J. Cao, Fillmore,C.J.1968.Thecaseforcase.
2016.
J.Casale,P.Henderson,D.Whitney, InE.W.BachandR.T.Harms,ed- Foland, Jr., W. R. and J. H. Martin.
andA.Geramifard.2017. Learning itors, Universals in Linguistic The- 2015. Dependency-based seman-
robustdialogpoliciesinnoisyenvi- ory, pages 1–88. Holt, Rinehart & ticrolelabelingusingconvolutional
ronments. ConversationalAIWork- Winston. neuralnetworks.*SEM2015.
shop(NIPS). Fillmore,C.J.1985.Framesandthese- Foltz,P.W.,W.Kintsch,andT.K.Lan-
Feldman, J. A. and D. H. Ballard. manticsofunderstanding.Quaderni dauer. 1998. The measurement of
1982. Connectionist models and diSemantica,VI(2):222–254. textualcoherencewithlatentseman-
theirproperties. CognitiveScience, tic analysis. Discourse processes,
6:205–254. Fillmore, C.J.2003. Valencyandse- 25(2-3):285–307.
mantic roles: the concept of deep
Fellbaum, C., editor. 1998. WordNet: ,W.Nekoto,V.Marivate,T.Matsila,
structure case. In V. Agel, L. M. ∀
An Electronic Lexical Database. T. Fasubaa, T. Kolawole, T. Fag-
Eichinger,H.W.Eroms,P.Hellwig,
MITPress. bohungbe, S. O. Akinola, S. H.
H. J. Heringer, and H. Lobin, ed-
Muhammad, S. Kabongo, S. Osei,
Feng,V.W.andG.Hirst.2011. Classi- itors, Dependenz und Valenz: Ein
S. Freshia, R. A. Niyongabo,
fyingargumentsbyscheme.ACL. internationales Handbuch der zeit-
R.M.P.Ogayo, O.Ahia, M.Mer-
Feng, V. W. and G. Hirst. 2014. geno¨ssischenForschung,chapter36, essa, M. Adeyemi, M. Mokgesi-
A linear-time bottom-up discourse pages457–475.WalterdeGruyter. Selinga, L. Okegbemi, L. J. Mar-
parser with constraints and post- Fillmore, C. J. 2012. ACL life- tinus, K. Tajudeen, K. Degila,
editing.ACL. time achievement award: Encoun- K.Ogueji,K.Siminyu,J.Kreutzer,
Feng,V.W.,Z.Lin,andG.Hirst.2014. terswithlanguage. Computational J. Webster, J. T. Ali, J. A. I.
Theimpactofdeephierarchicaldis- Linguistics,38(4):701–718. Orife, I. Ezeani, I. A. Dangana,
coursestructuresintheevaluationof Fillmore, C.J.andC.F.Baker.2009. H. Kamper, H. Elsahar, G. Duru,
textcoherence.COLING. Aframesapproachtosemanticanal- G. Kioko, E. Murhabazi, E. van
Biljon, D. Whitenack, C. Onye-
Fensel, D., J. A. Hendler, H. Lieber- ysis. In B. Heine and H. Narrog,
man,andW.Wahlster,editors.2003. editors, The Oxford Handbook of fuluchi, C. Emezue, B. Dossou,
Spinning the Semantic Web: Bring LinguisticAnalysis,pages313–340. B.Sibanda,B.I.Bassey,A.Olabiyi,
theWorldWideWebtoitsFullPo- OxfordUniversityPress.
A.Ramkilowan,A.O¨ktem,A.Akin-
tential.MITPress,Cambridge,MA. faderin,andA.Bashir.2020.Partic-
Fillmore, C. J., C. R. Johnson, and ipatory research for low-resourced
Fernandes,E.R.,C.N.dosSantos,and M.R.L.Petruck.2003.Background machine translation: A case study
R. L. Milidiu´. 2012. Latent struc- toFrameNet. Internationaljournal in African languages. Findings of
ture perceptron with feature induc- oflexicography,16(3):235–250. EMNLP.
tionforunrestrictedcoreferenceres-
olution.CoNLL. Finkelstein,L.,E.Gabrilovich,Y.Ma- Forchini, P. 2013. Using movie
tias, E. Rivlin, Z. Solan, G. Wolf- corpora to explore spoken Ameri-
Ferragina, P. and U. Scaiella. 2011. man,andE.Ruppin.2002. Placing can English: Evidence from multi-
Fastandaccurateannotationofshort searchincontext:Theconceptrevis- dimensionalanalysis.InJ.Bamford,
texts with wikipedia pages. IEEE ited.ACMTransactionsonInforma- S. Cavalieri, and G. Diani, editors,
Software,29(1):70–75. tionSystems,20(1):116—-131. Variation and Change in Spoken
Ferro,L.,L.Gerber,I.Mani,B.Sund- Finlayson, M. A. 2016. Inferring andWrittenDiscourse:Perspectives
heim, and G. Wilson. 2005. Tides Propp’sfunctionsfromsemantically fromcorpuslinguistics,pages123–
2005standardfortheannotationof annotatedtext.TheJournalofAmer- 136.Benjamins.
temporalexpressions. Technicalre- icanFolklore,129(511):55–77. Fox, B.A.1993. DiscourseStructure
port,MITRE.
andAnaphora:WrittenandConver-
Firth,J.R.1935. Thetechniqueofse-
Ferrucci, D. A. 2012. Introduction sationalEnglish.Cambridge.
mantics. Transactionsofthephilo-
to “This is Watson”. IBM Jour-
logicalsociety,34(1):36–73. Francis, W. N. and H. Kucˇera. 1982.
nal of Research and Development,
Frequency Analysis of English Us-
56(3/4):1:1–1:15. Firth,J.R.1957.Asynopsisoflinguis-
age.HoughtonMifflin,Boston.
tictheory1930–1955. InStudiesin
Fessler,L.2017.WetestedbotslikeSiri
LinguisticAnalysis.PhilologicalSo- Franz, A. and T. Brants. 2006.
and Alexa to see who would stand
ciety. ReprintedinPalmer,F.(ed.) All our n-gram are belong to
Fup ebto 22s ,ex 2u 0a 1l 7h .ar ha ts tsm pse :n /t. /qzQ .u ca or mtz /. 1968.SelectedPapersofJ.R.Firth. y bo lu o. gsph ot tt .p c: o/ m/ /g 2o 0o 0g 6l /e 0r 8e /search.
911681/. Longman,Harlow.
all-our-n-gram-are-belong-to-you.
Field,A.andY.Tsvetkov.2019.Entity- Fitt, S. 2002. Unisyn lexicon. html.
http://www.cstr.ed.ac.uk/
c Ae Cn Ltr .iccontextualaffectiveanalysis. projects/unisyn/. Fra Sse imr, uN la. tM ing.a sn pd eeG c. hN s. ysG teil mbe sr .t.1 C9 o9 m1 -.
Fikes, R. E. and N. J. Nilsson. 1971. Flanagan,J.L.1972. SpeechAnalysis, puter Speech and Language, 5:81–
STRIPS: A new approach to the Synthesis,andPerception.Springer. 99.
application of theorem proving to Flanagan,J.L.,K.Ishizaka,andK.L. Friedman, B., D. G. Hendry, and
problem solving. Artificial Intelli- Shipley.1975. Synthesisofspeech A. Borning. 2017. A survey
gence,2:189–208.
fromadynamicmodelofthevocal of value sensitive design methods.
Fillmore,C.J.1966. Aproposalcon- cordsandvocaltract. TheBellSys- FoundationsandTrendsinHuman-
cerningEnglishprepositions.InF.P. tem Technical Journal, 54(3):485– Computer Interaction, 11(2):63–
Dinneen,editor,17thannualRound 506. 125.596 Bibliography
Fry, D. B. 1955. Duration and inten- Garside,R.,G.Leech,andA.McEnery. Gladkova, A., A. Drozd, and S. Mat-
sityasphysicalcorrelatesoflinguis- 1997. Corpus Annotation. Long- suoka. 2016. Analogy-based de-
ticstress.JASA,27:765–768. man. tection of morphological and se-
Fry, D. B. 1959. Theoretical as- Gebru, T., J. Morgenstern, B. Vec- mantic relations with word embed-
pectsofmechanicalspeechrecogni- chione, J. W. Vaughan, H. Wal- dings:whatworksandwhatdoesn’t.
tion. JournaloftheBritishInstitu- lach, H. Daume´ III, and K. Craw- NAACLStudentResearchWorkshop.
tionofRadioEngineers,19(4):211– ford.2020. Datasheetsfordatasets. AssociationforComputationalLin-
218.Appearstogetherwithcompan- ArXiv. guistics.
ionpaper(Denes1959). Glenberg, A. M. and D. A. Robert-
Gehman, S., S. Gururangan, M. Sap,
Furnas, G. W., T. K. Landauer, L. M. Y.Choi,andN.A.Smith.2020.Re- son. 2000. Symbol grounding and
Gomez, and S. T. Dumais. 1987. alToxicityPrompts: Evaluatingneu- meaning: A comparison of high-
Thevocabularyprobleminhuman- ral toxic degeneration in language dimensionalandembodiedtheories
system communication. Commu- models.FindingsofEMNLP. ofmeaning. Journalofmemoryand
nications of the ACM, 30(11):964– language,43(3):379–401.
Gerber, M.andJ.Y.Chai.2010. Be-
971. yondnombank: Astudyofimplicit Godfrey, J., E. Holliman, and J. Mc-
Gabow,H.N.,Z.Galil,T.Spencer,and arguments for nominal predicates. Daniel. 1992. SWITCHBOARD:
R. E.Tarjan. 1986. Efficient algo- ACL. Telephone speech corpus for re-
rithms for finding minimum span- searchanddevelopment.ICASSP.
Gers, F. A., J. Schmidhuber, and
ningtreesinundirectedanddirected F.Cummins.2000. Learningtofor- Goffman,E.1974. Frameanalysis:An
graphs. Combinatorica, 6(2):109– get: Continualpredictionwithlstm. essayontheorganizationofexperi-
122. Neural computation, 12(10):2451– ence.HarvardUniversityPress.
Gaddy, D., M. Stern, and D. Klein. 2471. Goldberg, J., M. Ostendorf, and
2018. What’s going on in neural
constituency parsers? an analysis. Gil, D. 2000. Syntactic categories, K.Kirchhoff.2003. Theimpactof
NAACLHLT. cross-linguisticvariationanduniver- responsewordinginerrorcorrection
sal grammar. In P. M. Vogel and subdialogs. ISCATutorialandRe-
Gale, W.A.andK.W.Church.1994. B. Comrie, editors, Approaches to searchWorkshoponErrorHandling
What is wrong with adding one? theTypologyofWordClasses,pages inSpokenDialogueSystems.
InN.OostdijkandP.deHaan, ed-
173–216.Mouton. Goldberg, Y. 2017. Neural Network
itors, Corpus-Based Research into
Language,pages189–198.Rodopi. Gildea,D.andD.Jurafsky.2000. Au- MethodsforNaturalLanguagePro-
tomatic labeling of semantic roles. cessing,volume10ofSynthesisLec-
Gale, W.A.andK.W.Church.1991. ACL. tures on Human Language Tech-
Aprogramforaligningsentencesin nologies.Morgan&Claypool.
bilingualcorpora.ACL. Gildea, D. and D. Jurafsky. 2002.
Automatic labeling of semantic Gonen,H.andY.Goldberg.2019.Lip-
Gale, W.A.andK.W.Church.1993. roles. Computational Linguistics, stick on a pig: Debiasing methods
Aprogramforaligningsentencesin 28(3):245–288. coverupsystematicgenderbiasesin
bilingual corpora. Computational wordembeddingsbutdonotremove
Linguistics,19:75–102. Gildea, D. and M. Palmer. 2002. them.NAACLHLT.
The necessity of syntactic parsing
Gale, W. A., K. W. Church, and
for predicate argument recognition. Good, M. D., J. A. Whiteside, D. R.
D.Yarowsky.1992a.Estimatingup-
ACL. Wixon,andS.J.Jones.1984.Build-
perandlowerboundsontheperfor-
ingauser-derivedinterface. CACM,
mance of word-sense disambigua- Giles, C. L., G. M. Kuhn, and R. J.
27(10):1032–1043.
tionprograms.ACL. Williams.1994. Dynamicrecurrent
Gale, W. A., K. W. Church, and neuralnetworks: Theoryandappli- Goodfellow, I., Y. Bengio, and
D.Yarowsky.1992b. Onesenseper cations. IEEETrans.NeuralNetw. A. Courville. 2016. Deep Learn-
discourse.HLT. LearningSyst.,5(2):153–156. ing.MITPress.
Gale, W. A., K. W. Church, and Gillick,L.andS.J.Cox.1989. Some Goodman, J. 2006. A bit of progress
D.Yarowsky.1992c. Workonsta- statistical issues in the comparison in language modeling: Extended
tisticalmethodsforwordsensedis- of speech recognition algorithms. version. Technical Report MSR-
ambiguation. AAAIFallSymposium ICASSP. TR-2001-72,MachineLearningand
onProbabilisticApproachestoNat- Ginzburg, J.andI.A.Sag.2000. In- AppliedStatisticsGroup,Microsoft
uralLanguage. terrogativeInvestigations:theForm, Research,Redmond,WA.
Gao, S., A. Sethi, S. Aggarwal, MeaningandUseofEnglishInter- Goodwin,C.1996. Transparentvision.
T. Chung, and D. Hakkani-Tu¨r. rogatives.CSLI. In E. Ochs, E. A. Schegloff, and
2019. Dialog state tracking: A Girard, G. 1718. La justesse de la S. A. Thompson, editors, Interac-
neural reading comprehension ap- languefranc¸oise: oulesdiffe´rentes tionandGrammar,pages370–404.
proach.SIGDIAL. significations des mots qui passent CambridgeUniversityPress.
Garg, N., L.Schiebinger, D.Jurafsky, pour synonimes. Laurent d’Houry, Gopalakrishnan, K., B. Hedayatnia,
andJ.Zou.2018.Wordembeddings Paris. Q. Chen, A. Gottardi, S. Kwa-
quantify 100 years of gender and Giuliano, V. E. 1965. The inter- tra, A. Venkatesh, R. Gabriel, and
ethnic stereotypes. Proceedings of pretation of word associations. D. Hakkani-Tu¨r. 2019. Topical-
the National Academy of Sciences, Statistical Association Methods chat: Towardsknowledge-grounded
115(16):E3635–E3644. For Mechanized Documentation. open-domainconversations.INTER-
SPEECH.
Garside,R.1987. TheCLAWSword- Symposium Proceedings. Wash-
tagging system. In R. Garside, ington, D.C., USA, March 17, Gould, J. D., J. Conti, and T. Ho-
G.Leech,andG.Sampson,editors, 1964. https://nvlpubs.nist. vanyecz. 1983. Composing let-
TheComputationalAnalysisofEn- gov/nistpubs/Legacy/MP/ terswithasimulatedlisteningtype-
glish,pages30–41.Longman. nbsmiscellaneouspub269.pdf. writer.CACM,26(4):295–308.Bibliography 597
Gould,J.D.andC.Lewis.1985. De- test. Journalofpersonalityandso- 2020.Don’tstoppretraining:Adapt
signingforusability:Keyprinciples cialpsychology,74(6):1464–1480. language models to domains and
and what designers think. CACM, Grenager,T.andC.D.Manning.2006. tasks.ACL.
28(3):300–311. Unsuperviseddiscoveryofastatisti- Gusfield, D. 1997. Algorithms on
Gould,S.J.1980.ThePanda’sThumb. calverblexicon.EMNLP. Strings, Trees, and Sequences:
PenguinGroup. Computer Science and Computa-
Grice, H.P.1975. Logicandconver-
Graff, D. 1997. The 1996 Broadcast sation. In P. Cole and J. L. Mor- tionalBiology. CambridgeUniver-
News speech and language-model gan, editors, Speech Acts: Syntax sityPress.
corpus.ProceedingsDARPASpeech andSemanticsVolume3,pages41– Guyon, I.andA.Elisseeff.2003. An
RecognitionWorkshop. 58.AcademicPress. introductiontovariableandfeature
Gravano, A., J. Hirschberg, and Grice,H.P.1978.Furthernotesonlogic selection.JMLR,3:1157–1182.
Sˇ. Benˇusˇ. 2012. Affirmative cue andconversation. InP.Cole,editor, Haber, J. and M. Poesio. 2020. As-
words in task-oriented dialogue. Pragmatics: Syntax and Semantics sessing polyseme sense similarity
ComputationalLinguistics,38(1):1– Volume9,pages113–127.Academic throughco-predicationacceptability
39. Press. and contextualised embedding dis-
tance.*SEM.
Graves, A. 2012. Sequence transduc- Grishman,R.andB.Sundheim.1995.
tionwithrecurrentneuralnetworks. Design of the MUC-6 evaluation. Habernal, I. and I. Gurevych. 2016.
ICASSP. MUC-6. Which argument is more convinc-
ing? Analyzingandpredictingcon-
Graves, A. 2013. Generating se- Grosz,B.J.1977a. Therepresentation
vincingnessofWebargumentsusing
quences with recurrent neural net- anduseoffocusinasystemforun-
bidirectionalLSTM.ACL.
works.ArXiv. derstandingdialogs.IJCAI-77.Mor-
ganKaufmann. Habernal, I. and I. Gurevych. 2017.
Graves, A., S. Ferna´ndez, F. Gomez,
Argumentation mining in user-
and J. Schmidhuber. 2006. Con- Grosz,B.J.1977b.TheRepresentation
generatedwebdiscourse. Computa-
nectionist temporal classification: and Use of Focus in Dialogue Un-
tionalLinguistics,43(1):125–179.
Labelling unsegmented sequence derstanding. Ph.D. thesis, Univer-
datawithrecurrentneuralnetworks. sityofCalifornia,Berkeley. Haghighi, A. and D. Klein. 2009.
ICML. Grosz,B.J.,A.K.Joshi,andS.Wein- Simple coreference resolution with
richsyntacticandsemanticfeatures.
Graves, A., S. Ferna´ndez, M. Li- stein.1983. Providingaunifiedac-
EMNLP.
wicki,H.Bunke,andJ.Schmidhu- countofdefinitenounphrasesinEn-
ber. 2007. Unconstrained on-line glish.ACL. Hajishirzi, H., L. Zilles, D. S. Weld,
handwritingrecognitionwithrecur- Grosz,B.J.,A.K.Joshi,andS.Wein- and L. Zettlemoyer. 2013. Joint
rentneuralnetworks.NeurIPS. stein.1995.Centering:Aframework coreference resolution and named-
entitylinkingwithmulti-passsieves.
Graves,A.andN.Jaitly.2014.Towards formodelingthelocalcoherenceof
EMNLP.
end-to-end speech recognition with discourse. Computational Linguis-
recurrentneuralnetworks.ICML. tics,21(2):203–225. Hajicˇ,J.1998.BuildingaSyntactically
AnnotatedCorpus: ThePragueDe-
Graves, A., A.-r. Mohamed, and Grosz, B. J. and C. L. Sidner. 1980.
pendencyTreebank,pages106–132.
G.Hinton.2013.Speechrecognition Plansfordiscourse. InP.R.Cohen,
Karolinum.
withdeeprecurrentneuralnetworks. J. Morgan, and M. E. Pollack, ed-
ICASSP. itors, IntentionsinCommunication, Hajicˇ,J.2000. Morphologicaltagging:
pages417–444.MITPress. Datavs.dictionaries.InNAACL.
Graves, A. and J. Schmidhuber. 2005.
Framewise phoneme classification Gruber, J.S.1965. StudiesinLexical Hajicˇ, J., M. Ciaramita, R. Johans-
with bidirectional LSTM and other Relations.Ph.D.thesis,MIT. son, D. Kawahara, M. A. Mart´ı,
neuralnetworkarchitectures. Neu- Gru¨newald, S., A. Friedrich, and L. Ma`rquez, A. Meyers, J. Nivre,
ralNetworks,18(5-6):602–610. J.Kuhn.2021. ApplyingOccam’s S. Pado´, J. Sˇteˇpa´nek, P. Stranaˇk´,
Graves, A., G. Wayne, and I. Dani- razor to transformer-based depen- M.Surdeanu,N.Xue,andY.Zhang.
helka. 2014. Neural Turing ma- dency parsing: What works, what 2009. The conll-2009 shared task:
chines.ArXiv. doesn’t, and what is really neces- Syntactic and semantic dependen-
sary.IWPT. ciesinmultiplelanguages.CoNLL.
Green, B. F., A. K. Wolf, C. Chom-
sky,andK.Laughery.1961. Base- Guinaudeau, C. and M. Strube. 2013. Hakkani-Tu¨r, D., K. Oflazer, and
ball: An automatic question an- Graph-basedlocalcoherencemodel- G.Tu¨r.2002.Statisticalmorpholog-
swerer. ProceedingsoftheWestern ing.ACL. icaldisambiguationforagglutinative
JointComputerConference19. Guindon, R. 1988. A multidis- l aa nn dgu Ha ug mes a. nitiJ eo su ,r 3n 6a (l 4)o :3f 8C 1–o 4m 1p 0u .ters
Greenberg,S.,D.Ellis,andJ.Hollen- ciplinary perspective on dialogue
back.1996.Insightsintospokenlan- structure in user-advisor dialogues. Halliday,M.A.K.andR.Hasan.1976.
guage gleaned from phonetic tran- In R. Guindon, editor, Cogni- CohesioninEnglish.Longman.En-
scriptionoftheSwitchboardcorpus. tive Science and Its Applications glishLanguageSeries,TitleNo.9.
ICSLP. for Human-Computer Interaction, Hamilton,W.L.,K.Clark,J.Leskovec,
Greene,B.B.andG.M.Rubin.1971. pages163–200.LawrenceErlbaum. and D. Jurafsky. 2016a. Inducing
Automatic grammatical tagging of Gundel, J. K., N. Hedberg, and domain-specific sentiment lexicons
English. Department of Linguis- R.Zacharski.1993.Cognitivestatus fromunlabeledcorpora.EMNLP.
tics, BrownUniversity, Providence, andtheformofreferringexpressions Hamilton, W. L., J. Leskovec, and
RhodeIsland. indiscourse. Language,69(2):274– D.Jurafsky.2016b.Diachronicword
Greenwald,A.G.,D.E.McGhee,and 307. embeddingsrevealstatisticallawsof
J. L. K. Schwartz. 1998. Measur- Gururangan, S., A. Marasovic´, semanticchange.ACL.
ingindividualdifferencesinimplicit S. Swayamdipta, K. Lo, I. Belt- Hancock,B.,A.Bordes,P.-E.Mazare´,
cognition: the implicit association agy, D. Downey, and N. A. Smith. andJ.Weston.2019. Learningfrom598 Bibliography
dialogue after deployment: Feed Heafield, K., I. Pouzyrevsky, J. H. R.Lowe,andJ.Pineau.2017. Eth-
yourself,chatbot! ACL. Clark, and P. Koehn. 2013. Scal- ical challenges in data-driven dia-
Hannun,A.2017. Sequencemodeling ablemodifiedKneser-Neylanguage loguesystems.AAAI/ACMAIEthics
withCTC.Distill,2(11). modelestimation.ACL. andSocietyConference.
Hannun, A. Y., A. L. Maas, D. Juraf- Heaps, H. S. 1978. Information re- Hendrickx,I.,S.N.Kim,Z.Kozareva,
sky,andA.Y.Ng.2014. First-pass trieval.Computationalandtheoret- P.Nakov,D.O´ Se´aghdha, S.Pado´,
largevocabularycontinuousspeech icalaspects.AcademicPress. M. Pennacchiotti, L. Romano, and
recognition using bi-directional re- Hearst,M.A.1991. Nounhomograph S.Szpakowicz.2009.Semeval-2010
current DNNs. ArXiv preprint disambiguation. Proceedingsofthe task 8: Multi-way classification of
arXiv:1408.2873. 7thConferenceoftheUniversityof semanticrelationsbetweenpairsof
nominals. 5th International Work-
Harris, C. M. 1953. A study of the Waterloo Centre for the New OED
shoponSemanticEvaluation.
building blocks in speech. JASA, andTextResearch.
25(5):962–969. Hendrix,G.G.,C.W.Thompson,and
Hearst,M.A.1992a. Automaticacqui-
J.Slocum.1973.Languageprocess-
Harris, R. A. 2005. Voice Interaction sition of hyponyms from large text
ingviacanonicalverbsandsemantic
Design: Crafting the New Conver- corpora.COLING.
models.ProceedingsofIJCAI-73.
sational Speech Systems. Morgan
Kaufmann. Hearst,M.A.1992b.Automaticacqui- Henrich,V.,E.Hinrichs,andT.Vodola-
sition of hyponyms from large text zova. 2012. WebCAGe – a web-
Harris, Z. S. 1946. From morpheme corpora.COLING.
harvested corpus annotated with
toutterance. Language,22(3):161–
Hearst, M. A. 1997. Texttiling: Seg- GermaNetsenses.EACL.
183.
menting text into multi-paragraph Herdan, G. 1960. Type-token mathe-
Harris,Z.S.1954.Distributionalstruc-
subtopic passages. Computational matics.Mouton.
ture. Word,10:146–162. Reprinted
Linguistics,23:33–64.
inJ.FodorandJ.Katz,TheStructure Hermann,K.M.,T.Kocisky,E.Grefen-
of Language, Prentice Hall, 1964 Hearst,M.A.1998. Automaticdiscov- stette,L.Espeholt,W.Kay,M.Su-
andinZ.S.Harris,PapersinStruc- eryofWordNetrelations.InC.Fell- leyman, and P. Blunsom. 2015.
turalandTransformationalLinguis- baum, editor, WordNet: An Elec- Teachingmachinestoreadandcom-
tics,Reidel,1970,775–794. tronicLexicalDatabase.MITPress. prehend.NeurIPS.
Harris, Z.S.1962. StringAnalysisof Heckerman,D.,E.Horvitz,M.Sahami, Hernault,H.,H.Prendinger,D.A.du-
Sentence Structure. Mouton, The andS.T.Dumais.1998. Abayesian Verle,andM.Ishizuka.2010.Hilda:
Hague. approach to filtering junk e-mail. A discourse parser using support
Hastie, T., R. J. Tibshirani, and J. H. AAAI-98WorkshoponLearningfor vectormachineclassification. Dia-
Friedman. 2001. The Elements of TextCategorization. logue&Discourse,1(3).
StatisticalLearning.Springer. Heim,I.1982.Thesemanticsofdefinite Hidey,C.,E.Musi,A.Hwang,S.Mure-
Hatzivassiloglou,V.andK.McKeown. andindefinitenounphrases. Ph.D. san,andK.McKeown.2017. Ana-
1997.Predictingthesemanticorien- thesis, University of Massachusetts lyzingthesemantictypesofclaims
tationofadjectives.ACL. atAmherst. andpremisesinanonlinepersuasive
forum. 4thWorkshoponArgument
Hatzivassiloglou, V. and J. Wiebe. Heim, I. and A. Kratzer. 1998. Se-
Mining.
2000. Effects of adjective orienta- mantics in a Generative Grammar.
tionandgradabilityonsentencesub- BlackwellPublishers,Malden,MA. Hill,F.,R.Reichart,andA.Korhonen.
jectivity.COLING. Heinz,J.M.andK.N.Stevens.1961. 2015. Simlex-999: Evaluating se-
mantic models with (genuine) sim-
Haviland,S.E.andH.H.Clark.1974. Onthepropertiesofvoicelessfrica-
ilarity estimation. Computational
What’snew? Acquiringnewinfor- tiveconsonants.JASA,33:589–596.
Linguistics,41(4):665–695.
mation as a process in comprehen-
Hellrich, J., S.Buechel, andU.Hahn.
sion.JournalofVerbalLearningand Hinkelman, E. A. and J. Allen. 1989.
2019. Modeling word emotion in
VerbalBehaviour,13:512–521. Twoconstraintsonspeechactambi-
historical language: Quantity beats
guity.ACL.
Hawkins, J. A. 1978. Definiteness supposedstabilityinseedwordse-
andindefiniteness: astudyinrefer- lection. 3rd Joint SIGHUM Work- Hinton, G. E. 1986. Learning dis-
enceandgrammaticalityprediction. shop on Computational Linguistics tributedrepresentationsofconcepts.
CroomHelmLtd. for Cultural Heritage, Social Sci- COGSCI.
Hayashi, T., R. Yamamoto, K. In- ences,HumanitiesandLiterature. Hinton, G.E., S.Osindero, andY.-W.
oue, T. Yoshimura, S. Watanabe, Hellrich, J. and U. Hahn. 2016. Bad Teh.2006.Afastlearningalgorithm
T. Toda, K. Takeda, Y. Zhang, company—Neighborhoodsinneural fordeepbeliefnets. Neuralcompu-
and X. Tan. 2020. ESPnet-TTS: embeddingspacesconsideredharm- tation,18(7):1527–1554.
Unified, reproducible, andintegrat- ful.COLING. Hinton, G. E., N. Srivastava,
ableopensourceend-to-endtext-to- A. Krizhevsky, I. Sutskever, and
Henderson,J.1994. DescriptionBased
speechtoolkit.ICASSP. R.R.Salakhutdinov.2012. Improv-
ParsinginaConnectionistNetwork.
He,K.,X.Zhang,S.Ren,andJ.Sun. ing neural networks by preventing
Ph.D.thesis,UniversityofPennsyl-
2016.Deepresiduallearningforim- co-adaptation of feature detectors.
vania,Philadelphia,PA.
agerecognition.CVPR. ArXivpreprintarXiv:1207.0580.
Henderson, J. 2003. Inducing history
He,L.,K.Lee,M.Lewis,andL.Zettle- Hirschberg, J., D. J. Litman, and
representations for broad coverage
moyer.2017.Deepsemanticrolela- M. Swerts. 2001. Identifying user
statisticalparsing.HLT-NAACL-03.
beling:Whatworksandwhat’snext. correctionsautomaticallyinspoken
ACL. Henderson, J. 2004. Discriminative dialoguesystems.NAACL.
trainingofaneuralnetworkstatisti-
Heafield, K. 2011. KenLM: Faster Hirschman,L.,M.Light,E.Breck,and
calparser.ACL.
andsmallerlanguagemodelqueries. J. D. Burger. 1999. Deep Read:
Workshop on Statistical Machine Henderson,P.,K.Sinha,N.Angelard- A reading comprehension system.
Translation. Gontier, N. R. Ke, G. Fried, ACL.Bibliography 599
Hirschman,L.andC.Pao.1993. The Hovy,E.H.,M.P.Marcus,M.Palmer, Workshop on The Computational
costoferrorsinaspokenlanguage L.A.Ramshaw,andR.Weischedel. TreatmentofAnaphora.
system.EUROSPEECH. 2006. OntoNotes: The 90% solu-
Irsoy, O. and C. Cardie. 2014. Opin-
tion.HLT-NAACL.
Hirst, G. 1981. Anaphora in Natu- ionminingwithdeeprecurrentneu-
ralLanguageUnderstanding:Asur- Hu, M. and B. Liu. 2004a. Mining ralnetworks.EMNLP.
vey.Number119inLecturenotesin andsummarizingcustomerreviews.
Isbell,C.L.,M.Kearns,D.Kormann,
computerscience.Springer-Verlag. KDD.
S.Singh,andP.Stone.2000. Cobot
Hirst, G. 1987. Semantic Interpreta- Hu, M. and B. Liu. 2004b. Mining inLambdaMOO:Asocialstatistics
tionandtheResolutionofAmbigu- andsummarizingcustomerreviews. agent.AAAI/IAAI.
ity.CambridgeUniversityPress. SIGKDD-04.
Ischen, C., T. Araujo, H. Voorveld,
Hirst,G.1988.Resolvinglexicalambi- Huang, E. H., R. Socher, C. D. Man- G. van Noort, and E. Smit. 2019.
guity computationally with spread- ning,andA.Y.Ng.2012.Improving Privacyconcernsinchatbotinterac-
ing activation and polaroid words. wordrepresentationsviaglobalcon- tions. International Workshop on
In S. L. Small, G. W. Cottrell, text and multiple word prototypes. ChatbotResearchandDesign.
andM.K.Tanenhaus,editors,Lexi- ACL.
ISO8601. 2004. Data elements and
calAmbiguityResolution,pages73– Huang, Z., W. Xu, and K. Yu. 2015. interchange formats—information
108.MorganKaufmann. Bidirectional LSTM-CRF models interchange—representation of
Hirst,G.andE.Charniak.1982. Word forsequencetagging.arXivpreprint dates and times. Technical report,
senseandcaseslotdisambiguation. arXiv:1508.01991. InternationalOrganizationforStan-
AAAI. Huffman, S. 1996. Learning infor- dards(ISO).
Hjelmslev, L. 1969. Prologomena to mationextractionpatternsfromex- Itakura,F.1975. Minimumprediction
a Theory of Language. University amples. In S. Wertmer, E. Riloff, residualprincipleappliedtospeech
of Wisconsin Press. Translated by and G. Scheller, editors, Connec- recognition. IEEETransactionson
tionist,Statistical,andSymbolicAp-
FrancisJ.Whitfield;originalDanish Acoustics, Speech, andSignalPro-
proaches to Learning Natural Lan-
edition1943. cessing,ASSP-32:67–72.
guage Processing, pages 246–260.
Hobbs,J.R.1978. Resolvingpronoun Springer. Iter, D., K. Guu, L. Lansing, and
references.Lingua,44:311–338. D. Jurafsky. 2020. Pretraining
Humeau, S., K. Shuster, M.-A.
withcontrastivesentenceobjectives
Hobbs, J. R. 1979. Coherence and Lachaux,andJ.Weston.2020.Poly-
improves discourse performance of
coreference. Cognitive Science, encoders:Transformerarchitectures
languagemodels.ACL.
3:67–90. and pre-training strategies for fast
Hobbs, J. R., D. E. Appelt, J. Bear, andaccuratemulti-sentencescoring. Iter,D.,J.Yoon,andD.Jurafsky.2018.
D. Israel, M. Kameyama, M. E. ICLR. Automatic detection of incoherent
speech for diagnosing schizophre-
Stickel,andM.Tyson.1997. FAS- Hunt, A. J. and A. W. Black. 1996.
nia. Fifth Workshop on Computa-
TUS: A cascaded finite-state trans- Unit selection in a concatenative
tionalLinguisticsandClinicalPsy-
ducer for extracting information speech synthesis system using a
chology.
from natural-language text. In largespeechdatabase.ICASSP.
E. Roche and Y. Schabes, editors, Ito, K. and L. Johnson. 2017.
Hutchins,W.J.1986. MachineTrans-
Finite-State Language Processing, The LJ speech dataset.
lation: Past,Present,Future. Ellis
pages383–406.MITPress. https://keithito.com/
Horwood,Chichester,England.
LJ-Speech-Dataset/.
Hochreiter, S. and J. Schmidhuber.
Hutchins,W.J.1997. Fromfirstcon-
1997. Long short-term memory. Iyer,S.,I.Konstas,A.Cheung,J.Krish-
ception to first demonstration: The
Neural Computation, 9(8):1735– namurthy,andL.Zettlemoyer.2017.
nascent years of machine transla-
1780. tion,1947–1954.Achronology.Ma- Learning a neural semantic parser
Hofmann,T.1999. Probabilisticlatent chineTranslation,12:192–252.
fromuserfeedback.ACL.
semanticindexing.SIGIR-99. Hutchins,W.J.andH.L.Somers.1992. Jackendoff, R. 1983. Semantics and
Hopcroft, J. E. and J. D. Ullman. AnIntroductiontoMachineTransla- Cognition.MITPress.
1979.IntroductiontoAutomataThe- tion.AcademicPress. Jacobs, P. S. and L. F. Rau. 1990.
ory, Languages, and Computation. Hutchinson, B., V. Prabhakaran, SCISOR: A system for extract-
Addison-Wesley. E. Denton, K. Webster, Y. Zhong, inginformationfromon-linenews.
Hou, Y., K. Markert, and M. Strube. and S. Denuyl. 2020. Social bi- CACM,33(11):88–97.
2018. Unrestricted bridging reso- ases in NLP models as barriers for Jaech,A.,G.Mulcaire,S.Hathi,M.Os-
lution. Computational Linguistics, personswithdisabilities.ACL. tendorf, and N. A. Smith. 2016.
44(2):237–284. Hymes, D. 1974. Ways of speaking. Hierarchicalcharacter-wordmodels
Householder, F. W. 1995. Dionysius In R. Bauman and J. Sherzer, ed- for language identification. ACL
Thrax,thetechnai,andSextusEm- itors, Explorations in the ethnog- WorkshoponNLPforSocialMedia.
piricus. In E. F. K. Koerner and raphy of speaking, pages 433–451. Jaitly, N., P. Nguyen, A. Senior, and
R.E.Asher,editors,ConciseHistory CambridgeUniversityPress. V.Vanhoucke.2012. Applicationof
oftheLanguageSciences,pages99– Iacobacci, I., M. T. Pilehvar, and pretrained deep neural networks to
103.ElsevierScience. R. Navigli. 2016. Embeddings largevocabularyspeechrecognition.
forwordsensedisambiguation: An INTERSPEECH.
Hovy, E. H. 1990. Parsimonious
evaluationstudy.ACL.
and profligate approaches to the Jauhiainen, T., M. Lui, M. Zampieri,
questionofdiscoursestructurerela- Iida, R., K. Inui, H. Takamura, and T. Baldwin, and K. Linde´n. 2019.
tions. Proceedingsofthe5thInter- Y.Matsumoto.2003. Incorporating Automaticlanguageidentificationin
nationalWorkshoponNaturalLan- contextualcuesintrainablemodels texts: A survey. JAIR, 65(1):675–
guageGeneration. for coreference resolution. EACL 682.600 Bibliography
Jefferson, G. 1972. Side sequences. Johnson,K.2003. AcousticandAudi- Jurafsky,D.,C.Wooters,G.Tajchman,
In D. Sudnow, editor, Studies in toryPhonetics,2ndedition. Black- J.Segal, A.Stolcke, E.Fosler, and
social interaction, pages 294–333. well. N. Morgan. 1994. The Berkeley
FreePress,NewYork. Johnson,W.E.1932. Probability: de- restaurantproject.ICSLP.
Jeffreys,H.1948. TheoryofProbabil- ductiveandinductiveproblems(ap- Jurgens, D. and I. P. Klapaftis. 2013.
ity, 2nd edition. Clarendon Press. pendixto).Mind,41(164):421–423. SemEval-2013task13: Wordsense
Section3.23. Johnson-Laird, P. N. 1983. Mental inductionforgradedandnon-graded
Models. Harvard University Press, senses.*SEM.
Jelinek,F.1969. Afastsequentialde-
codingalgorithmusingastack.IBM Cambridge,MA. Jurgens, D., S. M. Mohammad,
Journal of Research and Develop- Jones, M. P. and J. H. Martin. 1997. P. Turney, and K. Holyoak. 2012.
ment,13:675–685. Contextualspellingcorrectionusing SemEval-2012 task 2: Measur-
latentsemanticanalysis.ANLP. ing degrees of relational similarity.
Jelinek, F. 1976. Continuous speech
recognition by statistical meth- Jones,R.,A.McCallum,K.Nigam,and *SEM2012.
ods. Proceedings of the IEEE, E. Riloff. 1999. Bootstrapping for Jurgens,D.,Y.Tsvetkov,andD.Juraf-
64(4):532–557. textlearningtasks. IJCAI-99Work- sky. 2017. Incorporating dialectal
shoponTextMining: Foundations, variabilityforsociallyequitablelan-
Jelinek, F. 1990. Self-organized lan- TechniquesandApplications. guageidentification.ACL.
guagemodelingforspeechrecogni-
tion.InA.WaibelandK.-F.Lee,ed- Jones, T. 2015. Toward a descrip- Justeson, J. S. and S. M. Katz. 1991.
itors, Readings in Speech Recogni- tion of African American Vernac- Co-occurrences of antonymous ad-
tion,pages450–506.MorganKauf- ular English dialect regions using jectivesandtheircontexts. Compu-
mann.OriginallydistributedasIBM “BlackTwitter”. AmericanSpeech, tationallinguistics,17(1):1–19.
90(4):403–440.
technicalreportin1985. Kalchbrenner, N. and P. Blunsom.
Joos,M.1950.Descriptionoflanguage
Jelinek, F. and R. L. Mercer. 1980. design.JASA,22:701–708. 2013. Recurrentcontinuoustransla-
Interpolated estimation of Markov tionmodels.EMNLP.
Jordan,M.1986. Serialorder:Aparal-
sourceparametersfromsparsedata. Kameyama, M. 1986. A property-
leldistributedprocessingapproach.
In E. S. Gelsema and L. N. Kanal, sharingconstraintincentering.ACL.
TechnicalReportICSReport8604,
editors, Proceedings, Workshop on
UniversityofCalifornia,SanDiego. Kamp,H.1981. Atheoryoftruthand
Pattern Recognition in Practice,
pages381–397.NorthHolland. Joshi, A. K. and P. Hopely. 1999. A semanticrepresentation. InJ.Groe-
parserfromantiquity. InA.Kornai, nendijk,T.Janssen,andM.Stokhof,
Jelinek, F., R. L. Mercer, and L. R. editor, Extended Finite State Mod- editors,FormalMethodsintheStudy
Bahl. 1975. Design of a linguis- elsofLanguage,pages6–15.Cam- ofLanguage,pages189–222.Math-
ticstatisticaldecoderfortherecog- bridgeUniversityPress. ematicalCentre,Amsterdam.
nitionofcontinuousspeech. IEEE
Transactions on Information The- Joshi,A.K.andS.Kuhn.1979. Cen- Kamphuis, C., A. P. de Vries,
ory,IT-21(3):250–256. teredlogic: Theroleofentitycen- L.Boytsov,andJ.Lin.2020.Which
teredsentencerepresentationinnat- BM25doyoumean? alarge-scale
Ji,H.andR.Grishman.2011. Knowl- urallanguageinferencing.IJCAI-79. reproducibility study of scoring
edge base population: Successful variants. European Conference on
Joshi, A. K. and S. Weinstein. 1981.
approachesandchallenges.ACL. InformationRetrieval.
Controlofinference: Roleofsome
Ji, H., R. Grishman, and H. T. Dang. aspectsofdiscoursestructure–cen- Kane,S.K.,M.R.Morris,A.Paradiso,
2010. Overview of the tac 2011 tering.IJCAI-81. and J. Campbell. 2017. “at times
knowledge base population track. Joshi, M., D. Chen, Y. Liu, D. S. avuncular and cantankerous, with
TAC-11. Weld,L.Zettlemoyer,andO.Levy. the reflexes of a mongoose”: Un-
Ji, Y.andJ.Eisenstein.2014. Repre- 2020. SpanBERT: Improving pre- derstandingself-expressionthrough
sentationlearningfortext-leveldis- trainingbyrepresentingandpredict- augmentative and alternative com-
courseparsing.ACL. ingspans.TACL,8:64–77. municationdevices.CSCW.
Ji,Y.andJ.Eisenstein.2015. Onevec- Joshi, M., E. Choi, D. S. Weld, and Kannan, A. and O. Vinyals. 2016.
torisnotenough:Entity-augmented L.Zettlemoyer.2017. Triviaqa: A Adversarial evaluation of dialogue
distributed semantics for discourse largescaledistantlysupervisedchal- models. NIPS 2016 Workshop on
relations.TACL,3:329–344. lenge dataset for reading compre- AdversarialTraining.
hension.ACL. Kaplan, R. M. 1973. A general syn-
Jia,R.andP.Liang.2016.Datarecom-
Joshi, M., O. Levy, D. S. Weld, and tactic processor. In R. Rustin, ed-
binationforneuralsemanticparsing.
L. Zettlemoyer. 2019. BERT for itor, Natural Language Processing,
ACL.
coreference resolution: Baselines pages193–241.AlgorithmicsPress.
Jia, S., T. Meng, J. Zhao, and K.-W. andanalysis.EMNLP.
Karamanis,N.,M.Poesio,C.Mellish,
Chang.2020.Mitigatinggenderbias
Joty, S., G. Carenini, and R. T. Ng. and J. Oberlander. 2004. Evaluat-
amplificationindistributionbypos-
2015. CODRA: A novel discrimi- ing centering-based metrics of co-
teriorregularization.ACL.
nativeframeworkforrhetoricalanal- herencefortextstructuringusinga
Jiang, K., D.Wu, andH.Jiang.2019. ysis. Computational Linguistics, reliablyannotatedcorpus.ACL.
FreebaseQA:AnewfactoidQAdata 41(3):385–435.
Karita, S., N. Chen, T. Hayashi,
set matching trivia-style question- Jurafsky, D. 2014. The Language of T. Hori, H. Inaguma, Z. Jiang,
answerpairswithFreebase.NAACL Food.W.W.Norton,NewYork. M.Someki,N.E.Y.Soplin,R.Ya-
HLT.
Jurafsky,D.,V.Chahuneau,B.R.Rout- mamoto, X. Wang, S. Watanabe,
Johnson, J., M. Douze, and H. Je´gou. ledge,andN.A.Smith.2014.Narra- T.Yoshimura,andW.Zhang.2019.
2017. Billion-scale similarity tiveframingofconsumersentiment Acomparativestudyontransformer
searchwithGPUs. ArXivpreprint in online restaurant reviews. First vs RNN in speech applications.
arXiv:1702.08734. Monday,19(4). IEEEASRU-19.Bibliography 601
Karlsson, F., A. Voutilainen, Kehler,A.1997b. Probabilisticcoref- Kintsch,W.andT.A.VanDijk.1978.
J. Heikkila¨, and A. Anttila, edi- erence in information extraction. Towardamodeloftextcomprehen-
tors.1995. ConstraintGrammar: A EMNLP. sionandproduction. Psychological
Language-Independent System for review,85(5):363–394.
Kehler,A.2000.Coherence,Reference,
ParsingUnrestrictedText. Mouton
andtheTheoryofGrammar. CSLI Kiperwasser,E.andY.Goldberg.2016.
deGruyter.
Publications. Simple and accurate dependency
Karpukhin, V., B. Og˘uz, S. Min, Kehler,A.,D.E.Appelt,L.Taylor,and parsing using bidirectional LSTM
P. Lewis, L. Wu, S. Edunov, A. Simma. 2004. The (non)utility feature representations. TACL,
D.Chen,andW.-t.Yih.2020.Dense of predicate-argument frequencies 4:313–327.
passage retrieval for open-domain for pronoun interpretation. HLT- Kipper,K.,H.T.Dang,andM.Palmer.
questionanswering.EMNLP. NAACL. 2000. Class-basedconstructionofa
Karttunen, L. 1969. Discourse refer- Kehler,A.andH.Rohde.2013.Aprob- verblexicon.AAAI.
ents.COLING.PreprintNo.70. abilisticreconciliationofcoherence- Kiritchenko,S.andS.M.Mohammad.
Karttunen, L. 1999. Comments on drivenandcentering-driventheories 2017. Best-worst scaling more re-
Joshi.InA.Kornai,editor,Extended ofpronouninterpretation. Theoreti- liable than rating scales: A case
Finite State Models of Language, calLinguistics,39(1-2):1–37. studyonsentimentintensityannota-
pages16–18.CambridgeUniversity Keller,F.andM.Lapata.2003. Using tion.ACL.
Press. thewebtoobtainfrequenciesforun-
Kiritchenko,S.andS.M.Mohammad.
Kasami, T. 1965. An efficient recog- seen bigrams. Computational Lin- 2018. Examining gender and race
nitionandsyntaxanalysisalgorithm guistics,29:459–484. biasintwohundredsentimentanal-
for context-free languages. Tech- Kelly,E.F.andP.J.Stone.1975.Com- ysissystems.*SEM.
nical Report AFCRL-65-758, Air puter Recognition of English Word
Kiss,T.andJ.Strunk.2006. Unsuper-
ForceCambridgeResearchLabora- Senses.North-Holland.
vised multilingual sentence bound-
tory,Bedford,MA.
Kendall, T. and C. Farrington. 2020. ary detection. Computational Lin-
Katz,J.J.andJ.A.Fodor.1963. The The Corpus of Regional African guistics,32(4):485–525.
structureofasemantictheory. Lan- American Language. Version
guage,39:170–210. 2020.05. Eugene, OR: The On- Kitaev, N., S. Cao, and D. Klein.
line Resources for African Amer- 2019. Multilingual constituency
Kawamoto, A. H. 1988. Distributed ican Language Project. http: parsingwithself-attentionandpre-
representationsofambiguouswords //oraal.uoregon.edu/coraal. training.ACL.
and their resolution in connection-
istnetworks. InS.L.Small,G.W. Kennedy,C.andB.K.Boguraev.1996. Kitaev, N. and D. Klein. 2018. Con-
Cottrell,andM.Tanenhaus,editors, Anaphora for everyone: Pronomi- stituency parsing with a self-
LexicalAmbiguityResolution,pages nal anaphora resolution without a attentiveencoder.ACL.
195–228.MorganKaufman. parser.COLING. Klatt, D. H. 1975. Voice onset time,
Kay,M.1967.Experimentswithapow- Kiela,D.andS.Clark.2014.Asystem- friction, and aspiration in word-
erfulparser.COLING. aticstudyofsemanticvectorspace initial consonant clusters. Journal
modelparameters.EACL2ndWork- of Speech and Hearing Research,
Kay,M.1973. TheMINDsystem. In shop on Continuous Vector Space 18:686–706.
R.Rustin,editor,NaturalLanguage
Models and their Compositionality
Processing, pages 155–188. Algo- Klatt,D.H.1977.ReviewoftheARPA
rithmicsPress. (CVSC). speechunderstandingproject.JASA,
Kilgarriff,A.andJ.Rosenzweig.2000. 62(6):1345–1366.
Kay,M.1982.Algorithmschemataand
Framework and results for English
datastructuresinsyntacticprocess- SENSEVAL. Computers and the Klatt,D.H.1982. TheKlattalktext-to-
ing. In S. Alle´n, editor, Text Pro- Humanities,34:15–48. speechconversionsystem.ICASSP.
cessing: TextAnalysisandGenera- Kleene,S.C.1951. Representationof
tion,TextTypologyandAttribution, Kim, E. 2019. Optimize com- events in nerve nets and finite au-
pages327–358.AlmqvistandWik- putational efficiency of skip- tomata. TechnicalReportRM-704,
sell,Stockholm. g hr ta tm ps:/w /i ath egin se 4g 0a 4ti 8v .e gits ham ubp .ling. RAND Corporation. RAND Re-
Kay, M. and M. Ro¨scheisen. 1988. io/optimize_computational_ searchMemorandum.
Text-translationalignment. Techni- efficiency_of_skip-gram_ Kleene,S.C.1956. Representationof
cal Report P90-00143, Xerox Palo with_negative_sampling. events in nerve nets and finite au-
A CAlto . Research Center, Palo Alto, Kim te, rmS. inM in. gan thd eE s. eH nt. imH eo nv ty o. f20 o0 p4 in. ioD ne s- . t Co am rta ht ya ,. eI dn itC or. sS ,h Aa un tn oo mn aa tand SJ tu. dM iec s- ,
Kay, M. and M. Ro¨scheisen. 1993. COLING. pages 3–41. Princeton University
Text-translationalignment. Compu- Press.
King, S. 2020. From African Amer-
tationalLinguistics,19:121–142.
ican Vernacular English to African Klein, S. and R. F. Simmons. 1963.
Kehler, A. 1993. The effect of es- American Language: Rethinking Acomputationalapproachtogram-
tablishingcoherenceinellipsisand the study of race and language in matical coding of English words.
anaphoraresolution.ACL. AfricanAmericans’speech. Annual JournaloftheACM,10(3):334–347.
Kehler, A. 1994. Temporal relations: ReviewofLinguistics,6:285–300. Kneser, R. and H. Ney. 1995. Im-
Reference or discourse coherence? Kingma,D.andJ.Ba.2015. Adam:A provedbacking-offforM-gramlan-
ACL. method for stochastic optimization. guagemodeling.ICASSP,volume1.
ICLR2015.
Kehler, A.1997a. Currenttheoriesof Knott, A. and R. Dale. 1994. Using
centeringforpronouninterpretation: Kintsch,W.1974. TheRepresentation linguistic phenomena to motivate a
Acriticalevaluation.Computational ofMeaninginMemory. Wiley,New setofcoherencerelations.Discourse
Linguistics,23(3):467–475. York. Processes,18(1):35–62.602 Bibliography
Kocijan, V., A.-M. Cretu, O.-M. Kudo,T.2018.Subwordregularization: Ladefoged,P.1996.ElementsofAcous-
Camburu, Y. Yordanov, and Improving neural network transla- ticPhonetics, 2ndedition. Univer-
T. Lukasiewicz. 2019. A surpris- tion models with multiple subword sityofChicago.
inglyrobusttrickfortheWinograd candidates.ACL.
Lafferty, J. D., A. McCallum, and
SchemaChallenge.ACL.
Kudo, T. and Y. Matsumoto. 2002. F.C.N.Pereira.2001. Conditional
Kocmi, T., C. Federmann, R. Grund- Japanesedependencyanalysisusing random fields: Probabilistic mod-
kiewicz, M. Junczys-Dowmunt, cascadedchunking.CoNLL. elsforsegmentingandlabelingse-
H. Matsushita, and A. Menezes. quencedata.ICML.
Kudo, T. and J. Richardson. 2018a.
2021. To ship or not to ship: An
SentencePiece: A simple and lan- Lai, A. and J. Tetreault. 2018. Dis-
extensive evaluation of automatic
guage independent subword tok- course coherence in the wild: A
metrics for machine translation.
enizer and detokenizer for neural dataset, evaluation and methods.
ArXiv.
textprocessing.EMNLP. SIGDIAL.
Koehn, P.2005. Europarl: Aparallel
Kudo, T. and J. Richardson. 2018b. Lake, B.M.andG.L.Murphy.2021.
corpusforstatisticalmachinetrans-
SentencePiece: A simple and lan- Word meaning in minds and ma-
lation.MTsummit,vol.5.
guage independent subword tok- chines. Psychological Review. In
Koehn, P., H. Hoang, A. Birch, enizer and detokenizer for neural press.
C. Callison-Burch, M. Federico, textprocessing.EMNLP.
Lakoff,G.1965.OntheNatureofSyn-
N. Bertoldi, B. Cowan, W. Shen,
Kullback, S. and R. A. Leibler. 1951. tacticIrregularity. Ph.D.thesis,In-
C.Moran,R.Zens,C.Dyer,O.Bo-
On information and sufficiency. dianaUniversity.PublishedasIrreg-
jar, A. Constantin, and E. Herbst.
Annals of Mathematical Statistics, ularityinSyntax.Holt,Rinehart,and
2006. Moses: Opensourcetoolkit
22:79–86. Winston,NewYork,1970.
for statistical machine translation.
ACL. Kulmizev, A., M. de Lhoneux, Lakoff,G.1972a.Linguisticsandnatu-
J. Gontrum, E. Fano, and J. Nivre. rallogic.InD.DavidsonandG.Har-
Koehn, P., F. J. Och, and D. Marcu.
2019. Deep contextualized word man,editors,SemanticsforNatural
2003.Statisticalphrase-basedtrans-
embeddingsintransition-basedand Language, pages 545–665. D. Rei-
lation.HLT-NAACL.
graph-based dependency parsing del.
Koenig, W., H. K. Dunn, and L. Y. - a tale of two parsers revisited.
Lakoff,G.1972b. Structuralcomplex-
Lacy.1946.Thesoundspectrograph. EMNLP. Association for Computa-
ity in fairy tales. In The Study of
JASA,18:19–49. tionalLinguistics.
Man, pages 128–50. School of So-
Kolhatkar, V., A. Roussel, S. Dipper, Kumar, S., S. Jat, K. Saxena, and cialSciences,UniversityofCalifor-
andH.Zinsmeister.2018.Anaphora P. Talukdar. 2019. Zero-shot word nia,Irvine,CA.
with non-nominal antecedents in sense disambiguation using sense
Lakoff, G. and M. Johnson. 1980.
computational linguistics: A sur- definitionembeddings.ACL.
MetaphorsWeLiveBy. University
vey. Computational Linguistics,
Kummerfeld,J.K.andD.Klein.2013. ofChicagoPress,Chicago,IL.
44(3):547–612.
Error-driven analysis of challenges
Lample, G., M.Ballesteros, S.Subra-
Kreutzer, J., I. Caswell, L. Wang, incoreferenceresolution.EMNLP.
manian,K.Kawakami,andC.Dyer.
A. Wahab, D. van Esch, N. Ulzii-
Kuno, S. 1965. The predictive ana- 2016. Neural architectures for
Orshikh, A. Tapo, N. Subra-
lyzer and a path elimination tech- named entity recognition. NAACL
mani, A. Sokolov, C. Sikasote,
nique.CACM,8(7):453–462. HLT.
M. Setyawan, S. Sarin, S. Samb,
B.Sagot,C.Rivera,A.Rios,I.Pa- Kupiec,J.1992. Robustpart-of-speech Lan, Z., M. Chen, S. Goodman,
padimitriou, S. Osei, P. O. Suarez, tagging using a hidden Markov K.Gimpel,P.Sharma,andR.Sori-
I.Orife,K.Ogueji,A.N.Rubungo, model. ComputerSpeechandLan- cut.2020.Albert:Alitebertforself-
T.Q.Nguyen,M.Mu¨ller,A.Mu¨ller, guage,6:225–242. supervisedlearningoflanguagerep-
S. H. Muhammad, N. Muham- resentations.ICLR.
Kurita,K.,N.Vyas,A.Pareek,A.W.
mad,A.Mnyakeni,J.Mirzakhalov,
Black,andY.Tsvetkov.2019.Quan- Landauer,T.K.,editor.1995.TheTrou-
T.Matangira,C.Leong,N.Lawson,
tifying social biases in contextual blewithComputers:Usefulness,Us-
S.Kudugunta,Y.Jernite,M.Jenny,
wordrepresentations.1stACLWork- ability,andProductivity.MITPress.
O.Firat,B.F.P.Dossou,S.Dlamini,
shop on Gender Bias for Natural
N.deSilva, S.C¸abukBallı, S.Bi- Landauer,T.K.andS.T.Dumais.1997.
LanguageProcessing.
derman, A. Battisti, A. Baruwa, AsolutiontoPlato’sproblem: The
A. Bapna, P. Baljekar, I. A. Az- Kucˇera, H. and W. N. Francis. 1967. LatentSemanticAnalysistheoryof
ime, A. Awokoya, D. Ataman, ComputationalAnalysisofPresent- acquisition,induction,andrepresen-
O.Ahia, O.Ahia, S.Agrawal, and DayAmericanEnglish. BrownUni- tationofknowledge. Psychological
M. Adeyemi. 2022. Quality at a versityPress,Providence,RI. Review,104:211–240.
glance: An audit of web-crawled
Kwiatkowski,T.,J.Palomaki,O.Red- Landauer,T.K.,D.Laham,B.Rehder,
multilingualdatasets. TACL,10:50–
field,M.Collins,A.Parikh,C.Al- and M. E. Schreiner. 1997. How
72.
berti, D. Epstein, I. Polosukhin, wellcanpassagemeaningbederived
Krovetz,R.1993.Viewingmorphology J. Devlin, K. Lee, K. Toutanova, withoutusingwordorder? Acom-
asaninferenceprocess.SIGIR-93. L.Jones,M.Kelcey,M.-W.Chang, parisonofLatentSemanticAnalysis
A.M.Dai,J.Uszkoreit,Q.Le,and andhumans.COGSCI.
Kruskal,J.B.1983.Anoverviewofse-
S.Petrov.2019. Naturalquestions:
quencecomparison. InD.Sankoff Landes, S., C. Leacock, and R. I.
A benchmark for question answer-
and J. B. Kruskal, editors, Time Tengi.1998.Buildingsemanticcon-
ingresearch.TACL,7:452–466.
Warps, String Edits, and Macro- cordances. In C. Fellbaum, edi-
molecules:TheTheoryandPractice Ladefoged,P.1993. ACourseinPho- tor, WordNet: An Electronic Lexi-
ofSequenceComparison, pages1– netics. HarcourtBraceJovanovich. calDatabase,pages199–216.MIT
44.Addison-Wesley. (3rded.). Press.Bibliography 603
Lang, J. and M. Lapata. 2014. to coreference resolution integrat- Levin,E.,R.Pieraccini,andW.Eckert.
Similarity-driven semantic role in- ing statistical and rule-based mod- 2000.Astochasticmodelofhuman-
ductionviagraphpartitioning.Com- els. NaturalLanguageEngineering, machineinteractionforlearningdia-
putational Linguistics, 40(3):633– 23(5):733–762. logstrategies.IEEETransactionson
669. SpeechandAudioProcessing,8:11–
Lee, K., M.-W. Chang, and
23.
Lang, K. J., A. H. Waibel, and G. E. K. Toutanova. 2019. Latent re-
Hinton. 1990. A time-delay neu- trieval for weakly supervised open Levine, Y., B. Lenz, O. Dagan,
ralnetworkarchitectureforisolated domainquestionanswering.ACL. O. Ram, D. Padnos, O. Sharir,
wordrecognition. Neuralnetworks, S.Shalev-Shwartz,A.Shashua,and
Lee,K.,L.He,M.Lewis,andL.Zettle-
3(1):23–43. Y. Shoham. 2020. SenseBERT:
moyer. 2017b. End-to-end neural
Driving some sense into BERT.
Lapata, M. 2003. Probabilistic text coreferenceresolution.EMNLP.
ACL.
structuring: Experiments with sen-
Lee, K., L. He, and L. Zettlemoyer.
tenceordering.ACL. Levinson, S.C.1983. Conversational
2018. Higher-order coreference
Analysis,chapter6.CambridgeUni-
Lapesa,G.andS.Evert.2014. Alarge resolutionwithcoarse-to-fineinfer-
versityPress.
scaleevaluationofdistributionalse- ence.NAACLHLT.
manticmodels:Parameters,interac- Levow, G.-A. 1998. Characterizing
Lehiste, I., editor. 1967. Readings in
tions and model selection. TACL, andrecognizingspokencorrections
AcousticPhonetics.MITPress.
2:531–545. inhuman-computerdialogue. COL-
Lehnert, W. G., C. Cardie, D. Fisher, ING/ACL.
Lappin,S.andH.Leass.1994.Analgo-
rithmforpronominalanaphorares- E. Riloff, and R. Williams. 1991. Levy, O. and Y. Goldberg. 2014a.
olution. ComputationalLinguistics, DescriptionoftheCIRCUSsystem Dependency-based word embed-
20(4):535–561. asusedforMUC-3.MUC-3. dings.ACL.
Lascarides, A. and N. Asher. 1993. Lemon,O.,K.Georgila,J.Henderson, Levy,O.andY.Goldberg.2014b. Lin-
Temporal interpretation, discourse and M. Stuttle. 2006. An ISU di- guisticregularitiesinsparseandex-
relations,andcommonsenseentail- aloguesystemexhibitingreinforce- plicitwordrepresentations.CoNLL.
ment learning of dialogue policies:
ment. Linguistics and Philosophy, Levy,O.andY.Goldberg.2014c.Neu-
Genericslot-fillingintheTALKin-
16(5):437–493. ralwordembeddingasimplicitma-
carsystem.EACL.
Lauscher, A., I. Vulic´, E. M. Ponti, trixfactorization.NeurIPS.
Lengerich, B., A.Maas, andC.Potts.
A.Korhonen,andG.Glavasˇ.2019. Levy, O., Y. Goldberg, and I. Da-
2018.Retrofittingdistributionalem-
Informing unsupervised pretraining gan.2015. Improvingdistributional
beddingstoknowledgegraphswith
with external linguistic knowledge. similaritywithlessonslearnedfrom
functionalrelations.COLING.
ArXivpreprintarXiv:1909.02339. word embeddings. TACL, 3:211–
Lesk,M.E.1986.Automaticsensedis- 225.
Lawrence, W.1953. Thesynthesisof
ambiguationusingmachinereadable
speech from signals which have a Li, A., F. Zheng, W. Byrne, P. Fung,
dictionaries:Howtotellapinecone
low information rate. In W. Jack- T.Kamm,L.Yi,Z.Song,U.Ruhi,
fromanicecreamcone.Proceedings
son,editor,CommunicationTheory, V. Venkataramani, and X. Chen.
ofthe5thInternationalConference
pages460–469.Butterworth. 2000. CASS: A phonetically tran-
onSystemsDocumentation.
scribedcorpusofMandarinsponta-
LDC. 1998. LDC Catalog: Hub4
Levenshtein,V.I.1966. Binarycodes neousspeech.ICSLP.
project. University of Penn-
capable of correcting deletions, in-
sylvania. www.ldc.upenn.edu/ Li, B.Z., S.Min, S.Iyer, Y.Mehdad,
sertions,andreversals. Cybernetics
Catalog/LDC98S71.html. andW.-t.Yih.2020. Efficientone-
andControlTheory,10(8):707–710.
pass end-to-end entity linking for
LeCun, Y., B. Boser, J. S. Denker, OriginalinDokladyAkademiiNauk
questions.EMNLP.
D. Henderson, R. E. Howard, SSSR163(4):845–848(1965).
W.Hubbard,andL.D.Jackel.1989. Li,J.,X.Chen,E.H.Hovy,andD.Ju-
Backpropagation applied to hand- Levesque, H. 2011. The Winograd rafsky. 2015. Visualizing and un-
writtenzipcoderecognition.Neural SchemaChallenge.LogicalFormal- derstanding neural models in NLP.
computation,1(4):541–551. izations of Commonsense Reason- NAACLHLT.
ing — Papers from the AAAI 2011
Lee, D. D. and H. S. Seung. 1999. SpringSymposium(SS-11-06). Li, J., M.Galley, C.Brockett, J.Gao,
Learningthepartsofobjectsbynon- and B. Dolan. 2016a. A diversity-
negative matrix factorization. Na- Levesque,H.,E.Davis,andL.Morgen- promoting objective function for
ture,401(6755):788–791. stern.2012. TheWinogradSchema neuralconversationmodels.NAACL
Challenge.KR-12. HLT.
Lee, H., A. Chang, Y. Peirsman,
N. Chambers, M. Surdeanu, and Levesque, H. J., P. R. Cohen, and Li, J. and D. Jurafsky. 2017. Neu-
D. Jurafsky. 2013. Determin- J.H.T.Nunes.1990. Onactingto- ralnetmodelsofopen-domaindis-
istic coreference resolution based gether.AAAI.MorganKaufmann. coursecoherence.EMNLP.
on entity-centric, precision-ranked Levin,B.1977. Mappingsentencesto Li, J., R. Li, and E. H. Hovy. 2014.
rules. Computational Linguistics, caseframes. TechnicalReport167, Recursivedeepmodelsfordiscourse
39(4):885–916. MITAILaboratory.AIWorkingPa- parsing.EMNLP.
per143.
Lee, H., Y. Peirsman, A. Chang, Li, J., W. Monroe, A. Ritter, M. Gal-
N. Chambers, M. Surdeanu, and Levin, B.1993. EnglishVerbClasses ley,J.Gao,andD.Jurafsky.2016b.
D.Jurafsky.2011. Stanford’smulti- andAlternations:APreliminaryIn- Deepreinforcementlearningfordi-
pass sieve coreference resolution vestigation. University of Chicago aloguegeneration.EMNLP.
system at the CoNLL-2011 shared Press.
Li,J.,W.Monroe,A.Ritter,D.Juraf-
task.CoNLL.
Levin, B. and M. Rappaport Hovav. sky, M. Galley, and J. Gao. 2016c.
Lee, H., M. Surdeanu, and D. Juraf- 2005. ArgumentRealization. Cam- Deepreinforcementlearningfordi-
sky.2017a. Ascaffoldingapproach bridgeUniversityPress. aloguegeneration.EMNLP.604 Bibliography
Li, J., W. Monroe, T. Shi, S. Jean, Linzen,T.2016.Issuesinevaluatingse- Louis, A. and A. Nenkova. 2012. A
A. Ritter, and D. Jurafsky. 2017. manticspacesusingwordanalogies. coherencemodelbasedonsyntactic
Adversariallearningforneuraldia- 1stWorkshoponEvaluatingVector- patterns.EMNLP.
loguegeneration.EMNLP. SpaceRepresentationsforNLP. Loureiro, D. and A. Jorge. 2019.
Li,M.,J.Weston,andS.Roller.2019a. Lison, P. and J. Tiedemann. 2016. Language modelling makes sense:
Acute-eval:Improveddialogueeval- Opensubtitles2016:Extractinglarge Propagatingrepresentationsthrough
uationwithoptimizedquestionsand parallelcorporafrommovieandtv WordNet for full-coverage word
multi-turncomparisons. NeurIPS19 subtitles.LREC. sensedisambiguation.ACL.
WorkshoponConversationalAI. Litman,D.J.1985. PlanRecognition Louviere,J.J.,T.N.Flynn,andA.A.J.
Li, Q., T. Li, and B. Chang. 2016d. and Discourse Analysis: An Inte- Marley. 2015. Best-worst scaling:
Discourse parsing with attention- gratedApproachforUnderstanding Theory, methods and applications.
based hierarchical neural networks. Dialogues. Ph.D.thesis,University CambridgeUniversityPress.
EMNLP. ofRochester,Rochester,NY.
Lovins, J. B. 1968. Development of
Li, X., Y. Meng, X. Sun, Q. Han, Litman,D.J.andJ.Allen.1987.Aplan astemmingalgorithm. Mechanical
A. Yuan, and J. Li. 2019b. Is recognitionmodelforsubdialogues TranslationandComputationalLin-
word segmentation necessary for inconversation. CognitiveScience, guistics,11(1–2):9–13.
deeplearningofChineserepresenta- 11:163–200.
Lowerre,B.T.1968.TheHarpySpeech
tions? ACL. Litman,D.J.,M.Swerts,andJ.Hirsch- Recognition System. Ph.D. thesis,
Liberman, A. M., P. C. Delattre, and berg. 2000. Predicting automatic Carnegie Mellon University, Pitts-
F.S.Cooper.1952. Theroleofse- speechrecognitionperformanceus- burgh,PA.
lectedstimulusvariablesintheper- ingprosodiccues.NAACL.
Luhn, H. P. 1957. A statistical ap-
ceptionoftheunvoicedstopconso- Litman, D. J., M. A. Walker, and
proachtothemechanizedencoding
nants.AmericanJournalofPsychol- M.Kearns.1999. Automaticdetec-
and searching of literary informa-
ogy,65:497–516. tion of poor speech recognition at
tion. IBMJournalofResearchand
Lin,D.2003. Dependency-basedeval- thedialoguelevel.ACL. Development,1(4):309–317.
uationofminipar. Workshoponthe Liu, B. and L. Zhang. 2012. A sur-
Lui,M.andT.Baldwin.2011. Cross-
EvaluationofParsingSystems. veyofopinionminingandsentiment
domain feature selection for lan-
Lin, J., R. Nogueira, and A. Yates. analysis. In C. C. Aggarwal and guageidentification.IJCNLP.
2021. Pretrained transformers for C. Zhai, editors, Mining text data,
text ranking: BERT and beyond. pages415–464.Springer. Lui, M. and T. Baldwin. 2012.
langid.py: An off-the-shelf lan-
WSDM. Liu, C.-W., R. T. Lowe, I. V. Ser-
guageidentificationtool.ACL.
ban,M.Noseworthy,L.Charlin,and
Lin,Y.,J.-B.Michel,E.AidenLieber-
J.Pineau.2016a.HowNOTtoeval- Lukasik,M.,B.Dadachev,K.Papineni,
man,J.Orwant,W.Brockman,and
uateyourdialoguesystem: Anem- and G. Simo˜es. 2020. Text seg-
S.Petrov.2012a. Syntacticannota-
piricalstudyofunsupervisedevalu- mentation by cross segment atten-
tions for the Google books NGram
ationmetricsfordialogueresponse tion.EMNLP.
corpus.ACL.
generation.EMNLP. Lukovnikov, D., A. Fischer, and
Lin, Y., J.-B. Michel, E. Lieber-
Liu, H., J. Dacon, W. Fan, H. Liu, J.Lehmann.2019. Pretrainedtrans-
man Aiden, J. Orwant, W. Brock-
Z.Liu,andJ.Tang.2020.Doesgen- formersforsimplequestionanswer-
man,andS.Petrov.2012b. Syntac-
dermatter?Towardsfairnessindia- ingoverknowledgegraphs.Interna-
ticannotationsfortheGoogleBooks
loguesystems.COLING. tionalSemanticWebConference.
NGramcorpus.ACL.
Liu,Y.,C.Sun,L.Lin,andX.Wang. Luo,F.,T.Liu,Z.He,Q.Xia,Z.Sui,
Lin,Z.,A.Madotto,J.Shin,P.Xu,and
2016b. Learning natural language and B. Chang. 2018a. Leverag-
P. Fung. 2019. MoEL: Mixture of
inferenceusingbidirectionalLSTM ingglossknowledgeinneuralword
empatheticlisteners.EMNLP.
modelandinner-attention.ArXiv. sense disambiguation by hierarchi-
Lin,Z.,M.-Y.Kan,andH.T.Ng.2009. Liu, Y., P. Fung, Y. Yang, C. Cieri, calco-attention.EMNLP.
Recognizingimplicitdiscourserela-
S. Huang, and D. Graff. 2006. Luo,F.,T.Liu,Q.Xia,B.Chang,and
tions in the Penn Discourse Tree-
HKUST/MTS: A very large scale Z.Sui.2018b.Incorporatingglosses
bank.EMNLP.
Mandarin telephone speech corpus. intoneuralwordsensedisambigua-
Lin,Z.,H.T.Ng,andM.-Y.Kan.2011. International Conference on Chi- tion.ACL.
Automaticallyevaluatingtextcoher- neseSpokenLanguageProcessing.
Luo,X.2005. Oncoreferenceresolu-
enceusingdiscourserelations.ACL.
Liu, Y., M. Ott, N. Goyal, J. Du, tionperformancemetrics.EMNLP.
Lin,Z.,H.T.Ng,andM.-Y.Kan.2014. M. Joshi, D. Chen, O. Levy,
Luo, X. and S. Pradhan. 2016. Eval-
A pdtb-styled end-to-end discourse M. Lewis, L. Zettlemoyer, and
uation metrics. In M. Poesio,
parser.NaturalLanguageEngineer- V. Stoyanov. 2019. RoBERTa:
R. Stuckardt, and Y. Versley, ed-
ing,20(2):151–184. A robustly optimized BERT pre-
itors, Anaphora resolution: Algo-
Lindsey, R. 1963. Inferential mem- training approach. ArXiv preprint rithms,resources,andapplications,
oryasthebasisofmachineswhich arXiv:1907.11692. pages141–163.Springer.
understand natural language. In Lochbaum, K. E., B. J. Grosz, and
E.FeigenbaumandJ.Feldman,edi- C.L.Sidner.2000. Discoursestruc- Luo,X.,S.Pradhan,M.Recasens,and
tors,ComputersandThought,pages ture and intention recognition. In E.H.Hovy.2014. Anextensionof
217–233.McGrawHill. R.Dale,H.Moisl,andH.L.Somers,
BLANCtosystemmentions.ACL.
Ling, W., C. Dyer, A. W. Black, editors, Handbook of Natural Lan- Lyons,J.1977. Semantics. Cambridge
I.Trancoso,R.Fermandez,S.Amir, guageProcessing.MarcelDekker. UniversityPress.
L.Marujo,andT.Lu´ıs.2015. Find- Logeswaran,L.,H.Lee,andD.Radev. Ma, X. and E. H. Hovy. 2016. End-
ingfunctioninform:Compositional 2018. Sentenceorderingandcoher- to-end sequence labeling via bi-
character models for open vocabu- encemodelingusingrecurrentneu- directional LSTM-CNNs-CRF.
larywordrepresentation.EMNLP. ralnetworks.AAAI. ACL.Bibliography 605
Maas,A.,Z.Xie,D.Jurafsky,andA.Y. Marcus,M.P.1980. ATheoryofSyn- Martschat,S.andM.Strube.2014.Re-
Ng. 2015. Lexicon-free conversa- tacticRecognitionforNaturalLan- call error analysis for coreference
tionalspeechrecognitionwithneu- guage.MITPress. resolution.EMNLP.
ralnetworks.NAACLHLT.
Marcus,M.P.,B.Santorini,andM.A. Martschat,S.andM.Strube.2015. La-
Maas,A.L.,A.Y.Hannun,andA.Y. Marcinkiewicz. 1993. Building a tentstructuresforcoreferencereso-
Ng. 2013. Rectifier nonlineari- large annotated corpus of English: lution.TACL,3:405–418.
tiesimproveneuralnetworkacoustic ThePenntreebank. Computational Masterman,M.1957. Thethesaurusin
models.ICML. Linguistics,19(2):313–330. syntax and semantics. Mechanical
Maas,A.L.,P.Qi,Z.Xie,A.Y.Han- Marie, B., A. Fujita, and R. Rubino. Translation,4(1):1–2.
nun, C. T. Lengerich, D. Jurafsky, 2021. Scientific credibility of ma- Mathis,D.A.andM.C.Mozer.1995.
andA.Y.Ng.2017. Buildingdnn chinetranslationresearch: Ameta- Onthecomputationalutilityofcon-
acoustic models for large vocabu- evaluationof769papers.ACL2021. sciousness.NeurIPS.MITPress.
laryspeechrecognition. Computer
Markov, A. A. 1913. Essai d’une McCallum,A.,D.Freitag,andF.C.N.
Speech&Language,41:195–213.
recherchestatistiquesurletextedu Pereira. 2000. Maximum entropy
Madhu,S.andD.Lytel.1965.Afigure roman“EugeneOnegin”illustrantla Markovmodelsforinformationex-
ofmerittechniquefortheresolution liaison des epreuve en chain (‘Ex- tractionandsegmentation.ICML.
ofnon-grammaticalambiguity. Me- ample of a statistical investigation
McCallum,A.andW.Li.2003. Early
chanicalTranslation,8(2):9–13. of the text of “Eugene Onegin” il-
results for named entity recogni-
Magerman, D. M. 1995. Statisti- lustrating the dependence between tionwithconditionalrandomfields,
caldecision-treemodelsforparsing. samplesinchain’). IzvistiaImpera- featureinductionandweb-enhanced
ACL. torskoiAkademiiNauk(Bulletinde lexicons.CoNLL.
l’Acade´mie Impe´riale des Sciences
Mairesse, F. and M. A. Walker. 2008. deSt.-Pe´tersbourg),7:153–162. McCallum, A. and K. Nigam. 1998.
Trainablegenerationofbig-fiveper- A comparison of event models
sonality styles through data-driven de Marneffe, M.-C., T. Dozat, N. Sil- for naive bayes text classification.
parameterestimation.ACL. veira, K. Haverinen, F. Ginter, AAAI/ICML-98WorkshoponLearn-
J.Nivre,andC.D.Manning.2014.
Manandhar, S., I.P.Klapaftis, D.Dli- UniversalStanforddependencies:A ingforTextCategorization.
gach, and S. Pradhan. 2010. cross-linguistictypology.LREC. McCarthy, J. F. and W. G. Lehnert.
SemEval-2010task14: Wordsense 1995.Usingdecisiontreesforcoref-
induction & disambiguation. Se- de Marneffe, M.-C., B. MacCartney, erenceresolution.IJCAI-95.
mEval. and C. D. Manning. 2006. Gener-
atingtypeddependencyparsesfrom McCawley, J. D. 1968. The role of
Mann, W. C. and S. A. Thompson. phrasestructureparses.LREC. semantics in a grammar. In E. W.
1987.Rhetoricalstructuretheory:A BachandR.T.Harms,editors,Uni-
theoryoftextorganization. Techni- de Marneffe, M.-C. and C. D. Man- versals in Linguistic Theory, pages
cal Report RS-87-190, Information ning.2008. TheStanfordtypedde- 124–169.Holt,Rinehart&Winston.
SciencesInstitute. pendenciesrepresentation.COLING
WorkshoponCross-Frameworkand McCawley,J.D.1993. Everythingthat
Manning, C. D. 2011. Part-of-speech Cross-DomainParserEvaluation. Linguists Have Always Wanted to
tagging from 97% to 100%: Is it KnowaboutLogic,2ndedition.Uni-
timeforsomelinguistics? CICLing de Marneffe, M.-C., C. D. Manning, versity of Chicago Press, Chicago,
2011. J.Nivre,andD.Zeman.2021. Uni- IL.
versal Dependencies. Computa-
Manning, C. D., P. Raghavan, and tionalLinguistics,47(2):255–308. McClelland, J. L. and J. L. Elman.
H.Schu¨tze.2008.IntroductiontoIn- 1986. TheTRACEmodelofspeech
formationRetrieval.Cambridge. deMarneffe,M.-C.,M.Recasens,and perception. Cognitive Psychology,
C.Potts.2015. Modelingthelifes- 18:1–86.
Manning,C.D.,M.Surdeanu,J.Bauer, pan of discourse entities with ap-
J. Finkel, S. Bethard, and D. Mc- plication to coreference resolution. McClelland, J. L. and D. E. Rumel-
Closky. 2014. The Stanford JAIR,52:445–475. hart, editors. 1986. Parallel Dis-
CoreNLPnaturallanguageprocess- tributed Processing: Explorations
ingtoolkit.ACL. Maron,M.E.1961. Automaticindex- in the Microstructure of Cognition,
ing: anexperimentalinquiry. Jour- volume 2: Psychological and Bio-
Marcu,D.1997.Therhetoricalparsing naloftheACM,8(3):404–417. logicalModels.MITPress.
ofnaturallanguagetexts.ACL.
Ma`rquez, L., X. Carreras, K. C. McCulloch,W.S.andW.Pitts.1943.A
Marcu,D.1999. Adecision-basedap- Litkowski, andS.Stevenson.2008. logical calculus of ideas immanent
proachtorhetoricalparsing.ACL.
Semanticrolelabeling:Anintroduc- innervousactivity.BulletinofMath-
Marcu,D.2000a. Therhetoricalpars- tiontothespecialissue. Computa- ematicalBiophysics,5:115–133.
ingofunrestrictedtexts: Asurface- tionallinguistics,34(2):145–159.
McDonald, R., K. Crammer, and
basedapproach.ComputationalLin-
Marshall,I.1983. Choiceofgrammat- F. C. N. Pereira. 2005a. Online
guistics,26(3):395–448.
ical word-class without global syn- large-margintrainingofdependency
Marcu,D.,editor.2000b. TheTheory tacticanalysis:Taggingwordsinthe parsers.ACL.
and Practice of Discourse Parsing LOBcorpus.ComputersandtheHu-
McDonald,R.andJ.Nivre.2011. An-
andSummarization.MITPress. manities,17:139–150.
alyzingandintegratingdependency
Marcu,D.andA.Echihabi.2002. An Marshall,I.1987. Tagselectionusing parsers. ComputationalLinguistics,
unsupervisedapproachtorecogniz- probabilisticmethods.InR.Garside, 37(1):197–230.
ingdiscourserelations.ACL. G.Leech,andG.Sampson,editors,
McDonald,R.,F.C.N.Pereira,K.Rib-
TheComputationalAnalysisofEn-
Marcu, D. and W. Wong. 2002. arov, and J. Hajicˇ. 2005b. Non-
glish,pages42–56.Longman.
A phrase-based, joint probability projective dependency parsing us-
modelforstatisticalmachinetrans- Martin,J.H.1986. Theacquisitionof ingspanningtreealgorithms. HLT-
lation.EMNLP. polysemy.ICML. EMNLP.606 Bibliography
McGuffie, K. and A. Newhouse. Mikolov, T., S. Kombrink, L. Burget, Mitchell, M., S. Wu, A. Zal-
2020. The radicalization risks of J.H.Cˇernocky`, andS.Khudanpur. divar, P. Barnes, L. Vasserman,
GPT-3 and advanced neural lan- 2011.Extensionsofrecurrentneural B.Hutchinson,E.Spitzer,I.D.Raji,
guage models. ArXiv preprint networklanguagemodel.ICASSP. andT.Gebru.2019.Modelcardsfor
arXiv:2009.06807. modelreporting.ACMFAccT.
Mikolov, T., I. Sutskever, K. Chen,
McGuiness,D.L.andF.vanHarmelen. G.S.Corrado,andJ.Dean.2013b. Mitkov,R.2002.AnaphoraResolution.
2004.OWLwebontologyoverview. Distributedrepresentationsofwords Longman.
Technical Report 20040210, World andphrasesandtheircompositional- Mohamed, A., G. E. Dahl, and G. E.
WideWebConsortium. ity.NeurIPS. Hinton.2009.DeepBeliefNetworks
McLuhan, M. 1964. Understanding Mikolov, T., W.-t.Yih, andG.Zweig. forphonerecognition. NIPSWork-
Media:TheExtensionsofMan.New 2013c. Linguistic regularities in shop on Deep Learning for Speech
AmericanLibrary. continuous space word representa- Recognition and Related Applica-
tions.NAACLHLT. tions.
McTear,M.2020. ConversationalAI:
Dialogue Systems, Conversational Miller, G. A. and P. E. Nicely. 1955. Mohammad, S. M. 2018a. Obtaining
Agents,andChatbots.MorganClay- An analysis of perceptual confu- reliable human ratings of valence,
pool. sions among some English conso- arousal, and dominance for 20,000
nants.JASA,27:338–352. Englishwords.ACL.
Melamud,O.,J.Goldberger,andI.Da-
Miller, G. A. and J. G. Beebe-Center. Mohammad,S.M.2018b. Wordaffect
gan. 2016. context2vec: Learn-
1956. Somepsychologicalmethods intensities.LREC.
inggenericcontextembeddingwith
bidirectionalLSTM.CoNLL. for evaluating the quality of trans- Mohammad, S. M. and P. D. Tur-
lations. Mechanical Translation, ney.2013. Crowdsourcingaword-
Merialdo, B. 1994. Tagging En- 3:73–80. emotionassociationlexicon. Com-
glish text with a probabilistic Miller,G.A.andW.G.Charles.1991. putational Intelligence, 29(3):436–
model. Computational Linguistics, Contextual correlates of semantics 465.
20(2):155–172. similarity. LanguageandCognitive Monroe, B. L., M. P. Colaresi, and
Mesgar,M.andM.Strube.2016.Lexi- Processes,6(1):1–28. K.M.Quinn.2008. Fightin’words:
calcoherencegraphmodelingusing Miller, G. A. and N. Chomsky. 1963. Lexicalfeatureselectionandevalu-
wordembeddings.ACL. Finitary models of language users. ation for identifying the content of
Metsis, V., I. Androutsopoulos, and In R. D. Luce, R. R. Bush, and politicalconflict. PoliticalAnalysis,
G. Paliouras. 2006. Spam filter- E. Galanter, editors, Handbook 16(4):372–403.
ing with naive bayes-which naive of Mathematical Psychology, vol- Montague,R.1973. Thepropertreat-
bayes? CEAS. umeII,pages419–491.JohnWiley. ment of quantification in ordinary
Miller,G.A.,C.Leacock,R.I.Tengi, English. In R. Thomason, editor,
Meyers, A., R. Reeves, C. Macleod,
andR.T.Bunker.1993. Asemantic FormalPhilosophy:SelectedPapers
R.Szekely,V.Zielinska,B.Young,
concordance.HLT. of Richard Montague, pages 247–
andR.Grishman.2004. Thenom-
270. Yale University Press, New
bank project: An interim report. Miller, G. A. and J. A. Selfridge.
Haven,CT.
NAACL/HLTWorkshop:Frontiersin 1950. Verbalcontextandtherecall
CorpusAnnotation. of meaningful material. American Moors, A., P. C. Ellsworth, K. R.
JournalofPsychology,63:176–185. Scherer,andN.H.Frijda.2013.Ap-
Mihalcea,R.2007.UsingWikipediafor praisal theories of emotion: State
automatic word sense disambigua- Miller,S.,R.J.Bobrow,R.Ingria,and of the art and future development.
tion.NAACL-HLT. R.Schwartz.1994. Hiddenunder- EmotionReview,5(2):119–124.
standingmodelsofnaturallanguage.
Mihalcea, R. and A. Csomai. 2007. ACL. Moosavi, N. S. and M. Strube. 2016.
Wikify!: Linkingdocumentstoen- Which coreference evaluation met-
Milne, D. and I. H. Witten. 2008.
cyclopedicknowledge.CIKM2007. ric do you trust? A proposal for a
Learning to link with wikipedia.
link-basedentityawaremetric.ACL.
Mihalcea, R. and D. Moldovan. 2001. CIKM2008.
Automatic generation of a coarse Miltsakaki,E.,R.Prasad,A.K.Joshi, Morey, M., P. Muller, and N. Asher.
grainedWordNet.NAACLWorkshop
andB.L.Webber.2004. ThePenn
2017. Howmuchprogresshavewe
on WordNet and Other Lexical Re- DiscourseTreebank.LREC. madeonRSTdiscourseparsing? a
sources. replicationstudyofrecentresultson
Minsky,M.1961. Stepstowardartifi- therst-dt.EMNLP.
Mikheev,A.,M.Moens,andC.Grover. cialintelligence. Proceedingsofthe
1999. Named entity recognition IRE,49(1):8–30. Morgan, A. A., L. Hirschman,
withoutgazetteers.EACL. M.Colosimo, A.S.Yeh, andJ.B.
Minsky,M.1974.Aframeworkforrep- Colombe. 2004. Gene name iden-
Mikolov, T. 2012. Statistical lan- resentingknowledge. TechnicalRe- tificationandnormalizationusinga
guagemodelsbasedonneuralnet- port306,MITAILaboratory.Memo modelorganismdatabase.Journalof
works.Ph.D.thesis,BrnoUniversity 306. BiomedicalInformatics,37(6):396–
ofTechnology. Minsky, M.andS.Papert.1969. Per- 410.
Mikolov, T., K. Chen, G. S. Corrado, ceptrons.MITPress. Morgan, N. and H. Bourlard. 1990.
andJ.Dean.2013a.Efficientestima- Mintz,M.,S.Bills,R.Snow,andD.Ju- Continuous speech recognition us-
tionofwordrepresentationsinvec- rafsky.2009.Distantsupervisionfor ingmultilayerperceptronswithhid-
torspace.ICLR2013. relation extraction without labeled denmarkovmodels.ICASSP.
Mikolov, T., M. Karafia´t, L. Bur- data.ACLIJCNLP. Morgan, N. and H. A. Bourlard.
get, J. Cˇernocky`, and S. Khudan- Mirza, P. and S. Tonelli. 2016. 1995. Neural networks for sta-
pur. 2010. Recurrent neural net- CATENA: CAusal and TEmporal tistical recognition of continuous
work based language model. IN- relation extraction from NAtural speech. Proceedings of the IEEE,
TERSPEECH. languagetexts.COLING. 83(5):742–772.Bibliography 607
Morris,J.andG.Hirst.1991. Lexical Naur, P., J. W. Backus, F. L. Bauer, Ng,V.2017. Machinelearningforen-
cohesioncomputedbythesauralre- J.Green,C.Katz,J.McCarthy,A.J. titycoreferenceresolution: Aretro-
lationsasanindicatorofthestruc- Perlis,H.Rutishauser,K.Samelson, spectivelookattwodecadesofre-
tureoftext.ComputationalLinguis- B.Vauquois,J.H.Wegstein,A.van search.AAAI.
tics,17(1):21–48. Wijnagaarden, and M. Woodger.
Ng, V. and C. Cardie. 2002a. Identi-
1960. Report on the algorith-
Morris, W., editor. 1985. American fying anaphoric and non-anaphoric
miclanguageALGOL60. CACM,
Heritage Dictionary, 2nd college nounphrasestoimprovecoreference
editionedition.HoughtonMifflin. 3(5):299–314. Revised in CACM resolution.COLING.
6:1,1-17,1963.
Mosteller,F.andD.L.Wallace.1963. Navigli,R.2006. Meaningfulcluster- Ng,V.andC.Cardie.2002b. Improv-
Inferenceinanauthorshipproblem: ingofsenseshelpsboostwordsense ingmachinelearningapproachesto
Acomparativestudyofdiscrimina- disambiguationperformance. COL- coreferenceresolution.ACL.
tionmethodsappliedtotheauthor- ING/ACL. Nguyen,D.T.andS.Joty.2017.Aneu-
ship of the disputed federalist pa- rallocalcoherencemodel.ACL.
Navigli, R. 2009. Word sense disam-
pers.JournaloftheAmericanStatis-
biguation:Asurvey. ACMComput- Nguyen, K. A., S. Schulte im Walde,
ticalAssociation,58(302):275–309.
ingSurveys,41(2). andN.T.Vu.2016. Integratingdis-
Mosteller,F.andD.L.Wallace.1964. tributionallexicalcontrastintoword
Navigli,R.2016. Chapter20.ontolo-
InferenceandDisputedAuthorship: embeddings for antonym-synonym
gies. InR.Mitkov,editor,TheOx-
The Federalist. Springer-Verlag. distinction.ACL.
fordhandbookofcomputationallin-
19842ndedition:AppliedBayesian
guistics.OxfordUniversityPress. Nie, A., E. Bennett, and N. Good-
andClassicalInference.
man.2019. DisSent: Learningsen-
Navigli, R. and S. P. Ponzetto. 2012.
Mrksˇic´,N.,D.O´ Se´aghdha,T.-H.Wen, BabelNet: Theautomaticconstruc- tence representations from explicit
B. Thomson, and S. Young. 2017. tion,evaluationandapplicationofa discourserelations.ACL.
Neural belief tracker: Data-driven wide-coverage multilingual seman- Nielsen, J. 1992. The usability engi-
dialoguestatetracking.ACL. tic network. Artificial Intelligence, neeringlifecycle. IEEEComputer,
193:217–250. 25(3):12–22.
Mrksˇic´,N.,D.O´.Se´aghdha,B.Thom-
son, M. Gasˇic´, L. M. Rojas- Navigli, R. and D. Vannella. 2013. Nielsen,M.A.2015. Neuralnetworks
Barahona, P.-H. Su, D. Vandyke, SemEval-2013task11: Wordsense and Deep learning. Determination
T.-H. Wen, and S. Young. 2016. inductionanddisambiguationwithin PressUSA.
Counter-fitting word vectors to lin- anend-userapplication.*SEM. Nigam,K.,J.D.Lafferty,andA.Mc-
guisticconstraints.NAACLHLT. Nayak, N., D. Hakkani-Tu¨r, M. A. Callum.1999. Usingmaximumen-
Muller, P., C. Braud, and M. Morey. Walker, and L. P. Heck. 2017. To tropyfortextclassification. IJCAI-
plan or not to plan? discourse 99 workshop on machine learning
2019. ToNy: Contextual embed-
dingsforaccuratemultilingualdis-
planning in slot-value informed se- forinformationfiltering.
quencetosequencemodelsforlan-
course segmentation of full docu- Nirenburg, S., H. L. Somers, and
guagegeneration.INTERSPEECH.
ments. WorkshoponDiscourseRe- Y.Wilks,editors.2002.Readingsin
lationParsingandTreebanking. Neff, G. and P. Nagy. 2016. Talking MachineTranslation.MITPress.
to bots: Symbiotic agency and the
Murphy,K.P.2012.Machinelearning: case of Tay. International Journal Nissim,M.,S.Dingare,J.Carletta,and
A probabilistic perspective. MIT ofCommunication,10:4915–4931. M.Steedman.2004. Anannotation
Press. schemeforinformationstatusindi-
Ney,H.,U.Essen,andR.Kneser.1994. alogue.LREC.
Musi,E.,M.Stede,L.Kriese,S.Mure- On structuring probabilistic depen-
san, andA.Rocci.2018. Amulti- denciesinstochasticlanguagemod- NIST.1990. TIMITAcoustic-Phonetic
layerannotatedcorpusofargumen- elling. ComputerSpeechandLan- Continuous Speech Corpus. Na-
tativetext: Fromargumentschemes guage,8:1–38. tional Institute of Standards and
todiscourserelations.LREC. Technology Speech Disc 1-1.1.
Ng,A.Y.andM.I.Jordan.2002. On NISTOrderNo.PB91-505065.
Myers, G. 1992. “In this paper we discriminativevs.generativeclassi-
report...”: Speech acts and scien- fiers: A comparison of logistic re- NIST. 2005. Speech recognition
tific facts. Journal of Pragmatics, gressionandnaivebayes.NeurIPS. scoring toolkit (sctk) version 2.1.
http://www.nist.gov/speech/
17(4):295–313. Ng,H.T.,L.H.Teo,andJ.L.P.Kwan. tools/.
Na´das, A. 1984. Estimation of prob- 2000. Amachinelearningapproach
abilities in the language model of to answering questions for reading NIST.2007. MatchedPairsSentence-
the IBM speech recognition sys- comprehensiontests.EMNLP. Segment Word Error (MAPSSWE)
Test.
tem. IEEETransactionsonAcous- Ng, V. 2004. Learning noun phrase
tics, Speech, Signal Processing, anaphoricitytoimprovecoreference Nivre, J. 2007. Incremental non-
32(4):859–861. resolution: Issues in representation projective dependency parsing.
andoptimization.ACL. NAACL-HLT.
Nagata, M. and T. Morimoto. 1994.
Firststepstowardstatisticalmodel- Ng, V. 2005a. Machine learning for Nivre,J.2003. Anefficientalgorithm
ingofdialoguetopredictthespeech coreference resolution: From lo- for projective dependency parsing.
acttypeofthenextutterance.Speech cal classification to global ranking. Proceedingsofthe8thInternational
Communication,15:193–203. ACL. Workshop on Parsing Technologies
(IWPT).
Nash-Webber,B.L.1975. Theroleof Ng, V. 2005b. Supervised ranking
semantics in automatic speech un- forpronounresolution:Somerecent Nivre,J.2006. InductiveDependency
derstanding. InD.G.Bobrowand improvements.AAAI. Parsing.Springer.
A. Collins, editors, Representation Ng, V.2010. Supervisednounphrase Nivre, J. 2009. Non-projective de-
andUnderstanding,pages351–382. coreference research: The first fif- pendencyparsinginexpectedlinear
AcademicPress. teenyears.ACL. time.ACLIJCNLP.608 Bibliography
Nivre, J., J. Hall, S. Ku¨bler, R. Mc- Och,F.J.andH.Ney.2004.Thealign- Palmer, M., P. Kingsbury, and
Donald, J. Nilsson, S. Riedel, and menttemplateapproachtostatistical D. Gildea. 2005. The proposi-
D. Yuret. 2007a. The conll 2007 machinetranslation. Computational tion bank: An annotated corpus
sharedtaskondependencyparsing. Linguistics,30(4):417–449. of semantic roles. Computational
EMNLP/CoNLL. Linguistics,31(1):71–106.
O’Connor,B.,M.Krieger,andD.Ahn.
Nivre,J.,J.Hall,J.Nilsson,A.Chanev, 2010. Tweetmotif: Exploratory Panayotov,V.,G.Chen,D.Povey,and
G. Eryigit, S. Ku¨bler, S. Mari- searchandtopicsummarizationfor S.Khudanpur.2015.Librispeech:an
nov, and E. Marsi. 2007b. Malt- twitter.ICWSM. ASRcorpusbasedonpublicdomain
parser: A language-independent audiobooks.ICASSP.
Olive, J. P. 1977. Rule synthe-
system for data-driven dependency Pang, B. and L. Lee. 2008. Opin-
sis of speech from dyadic units.
parsing. Natural Language Engi- ion mining and sentiment analysis.
ICASSP77.
neering,13(02):95–135. Foundationsandtrendsininforma-
Nivre,J.andJ.Nilsson.2005. Pseudo- Olteanu, A., F. Diaz, and G. Kazai. tionretrieval,2(1-2):1–135.
projectivedependencyparsing.ACL. 2020. Whenaresearchcompletion Pang,B.,L.Lee,andS.Vaithyanathan.
suggestionsproblematic? CSCW. 2002. Thumbs up? Sentiment
Nivre,J.andM.Scholz.2004. Deter-
ministic dependency parsing of en- vandenOord,A.,S.Dieleman,H.Zen, classification using machine learn-
glishtext.COLING. K.Simonyan,O.Vinyals,A.Graves, ingtechniques.EMNLP.
N. Kalchbrenner, A. Senior, and Paolino, J. 2017. Google Home
Niwa, Y. and Y. Nitta. 1994. Co-
K. Kavukcuoglu. 2016. WaveNet: vs Alexa: Two simple user
occurrencevectorsfromcorporavs.
AGenerativeModelforRawAudio. experience design gestures
distance vectors from dictionaries.
ISCAWorkshoponSpeechSynthesis that delighted a female user.
COLING.
Workshop. Medium. Jan 4, 2017. https:
Noreen,E.W.1989. ComputerInten- //medium.com/startup-grind/
Oppenheim,A.V.,R.W.Schafer,and
siveMethodsforTestingHypothesis. google-home-vs-alexa-56e26f69ac77.
T.G.J.Stockham.1968. Nonlinear
Wiley. filteringofmultipliedandconvolved Papineni,K.,S.Roukos,T.Ward,and
Norman,D.A.1988.TheDesignofEv- signals. Proceedings of the IEEE, W.-J. Zhu. 2002. Bleu: A method
erydayThings.BasicBooks. 56(8):1264–1291. forautomaticevaluationofmachine
translation.ACL.
Norman, D. A. and D. E. Rumelhart. Oravecz, C.andP.Dienes.2002. Ef-
1975. Explorations in Cognition. ficientstochasticpart-of-speechtag- Paranjape, A., A. See, K. Kenealy,
Freeman. gingforHungarian.LREC. H. Li, A. Hardy, P. Qi, K. R.
Sadagopan, N. M. Phu, D. Soylu,
Norvig, P. 1991. Techniques for au- Oren,I.,J.Herzig,N.Gupta,M.Gard- and C. D. Manning. 2020. Neural
tomatic memoization with applica- ner,andJ.Berant.2020. Improving generation meets real people: To-
tionstocontext-freeparsing. Com- compositional generalization in se- wardsemotionallyengagingmixed-
putationalLinguistics,17(1):91–98. manticparsing.FindingsofEMNLP. initiative conversations. 3rd Pro-
Nosek, B. A., M. R. Banaji, and Osgood,C.E.,G.J.Suci,andP.H.Tan- ceedingsofAlexaPrize.
A.G.Greenwald.2002a. Harvest-
nenbaum.1957. TheMeasurement Park,J.H.,J.Shin,andP.Fung.2018.
ingimplicitgroupattitudesandbe-
of Meaning. University of Illinois Reducinggenderbiasinabusivelan-
liefsfromademonstrationwebsite.
Press. guagedetection.EMNLP.
GroupDynamics:Theory,Research,
andPractice,6(1):101. Ostendorf, M., P. Price, and Park,J.andC.Cardie.2014. Identify-
S. Shattuck-Hufnagel. 1995. The ingappropriatesupportforproposi-
Nosek,B.A.,M.R.Banaji,andA.G. BostonUniversityRadioNewsCor- tionsinonlineusercomments. First
Greenwald. 2002b. Math=male, pus. TechnicalReportECS-95-001, workshoponargumentationmining.
me=female, therefore math= me.
(cid:54) BostonUniversity. Parsons,T.1990. EventsintheSeman-
Journal of personality and social
ticsofEnglish.MITPress.
psychology,83(1):44. Packard, D. W. 1973. Computer-
assisted morphological analysis of Partee,B.H.,editor.1976. Montague
Ocal, M., A. Perez, A. Radas, and ancientGreek.COLING. Grammar.AcademicPress.
M.Finlayson.2022. Holisticeval-
Paszke, A., S. Gross, S. Chintala,
uationofautomaticTimeMLanno- Palmer,D.2012.Textpreprocessing.In
G. Chanan, E. Yang, Z. DeVito,
tators.LREC. N.IndurkhyaandF.J.Damerau,edi-
Z. Lin, A. Desmaison, L. Antiga,
Och, F. J. 1998. Ein beispiels- tors,HandbookofNaturalLanguage andA.Lerer.2017. Automaticdif-
basierter und statistischer Ansatz Processing,pages9–30.CRCPress. ferentiationinpytorch.NIPS-W.
zum maschinellen Lernen von Palmer, M., O. Babko-Malaya, and Pearl,C.2017. DesigningVoiceUser
natu¨rlichsprachlicher U¨bersetzung. H. T. Dang. 2004. Different sense Interfaces: PrinciplesofConversa-
Ph.D. thesis, Universita¨t Erlangen- granularities for different applica- tionalExperiences.O’Reilly.
Nu¨rnberg, Germany. Diplomarbeit tions. HLT-NAACL Workshop on
Pedersen,T.andR.Bruce.1997. Dis-
(diplomathesis). Scalable Natural Language Under-
tinguishingwordsensesinuntagged
Och, F. J. 2003. Minimum error rate standing. text.EMNLP.
traininginstatisticalmachinetrans- Palmer, M., H. T. Dang, and C. Fell- Peldszus,A.andM.Stede.2013.From
lation.ACL. baum. 2006. Making fine-grained argumentdiagramstoargumentation
Och,F.J.andH.Ney.2002. Discrim- and coarse-grained sense distinc- mining in texts: A survey. In-
inative training and maximum en- tions, both manually and automati- ternationalJournalofCognitiveIn-
tropymodelsforstatisticalmachine cally. NaturalLanguageEngineer- formatics and Natural Intelligence
translation.ACL. ing,13(2):137–163. (IJCINI),7(1):1–31.
Och,F.J.andH.Ney.2003. Asystem- Palmer, M., D. Gildea, and N. Xue. Peldszus, A.andM.Stede.2016. An
aticcomparisonofvariousstatistical 2010. Semanticrolelabeling. Syn- annotated corpus of argumentative
alignment models. Computational thesisLecturesonHumanLanguage microtexts. 1st European Confer-
Linguistics,29(1):19–51. Technologies,3(1):1–103. enceonArgumentation.Bibliography 609
Penn, G. and P. Kiparsky. 2012. On 1966. Language and Machines: Polanyi,L.,C.Culy,M.vandenBerg,
Pa¯n.iniandthegenerativecapacityof Computers in Translation and Lin- G. L. Thione, and D. Ahn. 2004.
contextualizedreplacementsystems. guistics. ALPAC report. National Arulebasedapproachtodiscourse
COLING. AcademyofSciences,NationalRe- parsing.ProceedingsofSIGDIAL.
searchCouncil,Washington,DC.
Pennebaker, J. W., R. J. Booth, and Polifroni, J., L.Hirschman, S.Seneff,
M.E.Francis.2007. LinguisticIn- Pilehvar, M. T. and J. Camacho- andV.W.Zue.1992. Experiments
quiryandWordCount:LIWC2007. Collados. 2019. WiC: the word- inevaluatinginteractivespokenlan-
Austin,TX. in-context dataset for evaluating guagesystems.HLT.
Pennington, J., R. Socher, and C. D. context-sensitivemeaningrepresen- Pollard,C.andI.A.Sag.1994. Head-
Manning. 2014. GloVe: Global tations.NAACLHLT. DrivenPhraseStructureGrammar.
vectors for word representation. Pilehvar,M.T.,D.Jurgens,andR.Nav- UniversityofChicagoPress.
EMNLP. igli.2013. Align,disambiguateand Ponzetto, S. P. and R. Navigli. 2010.
Percival, W.K.1976. Onthehistori- walk: Aunifiedapproachformea- Knowledge-rich word sense disam-
calsourceofimmediateconstituent suringsemanticsimilarity.ACL. biguation rivaling supervised sys-
analysis. In J. D. McCawley, ed- Pitler, E., A. Louis, and A. Nenkova. tems.ACL.
itor, Syntax and Semantics Volume 2009. Automatic sense prediction Ponzetto, S. P. and M. Strube. 2006.
7,NotesfromtheLinguisticUnder- for implicit discourse relations in Exploiting semantic role labeling,
ground, pages 229–242. Academic text.ACLIJCNLP. WordNetandWikipediaforcorefer-
Press.
Pitler, E.andA.Nenkova.2009. Us- enceresolution.HLT-NAACL.
Perrault, C. R. and J. Allen. 1980. ing syntax to disambiguate explicit Ponzetto, S. P. and M. Strube. 2007.
A plan-based analysis of indirect discourseconnectivesintext. ACL KnowledgederivedfromWikipedia
speech acts. American Journal IJCNLP. forcomputingsemanticrelatedness.
of Computational Linguistics, 6(3-
Pitt, M. A., L. Dilley, K. John- JAIR,30:181–212.
4):167–182.
son, S. Kiesling, W. D. Raymond, Popovic´, M. 2015. chrF: charac-
Peters, M., M. Neumann, M. Iyyer, E. Hume, and E. Fosler-Lussier. ter n-gram F-score for automatic
M. Gardner, C. Clark, K. Lee, 2007. Buckeyecorpusofconversa- MTevaluation. Proceedingsofthe
and L. Zettlemoyer. 2018. Deep tionalspeech(2ndrelease). Depart- Tenth Workshop on Statistical Ma-
contextualizedwordrepresentations. mentofPsychology,OhioStateUni- chineTranslation.
NAACLHLT. versity(Distributor).
Popp, D., R. A. Donovan, M. Craw-
Peterson,G.E.andH.L.Barney.1952. Pitt, M. A., K. Johnson, E. Hume, ford, K. L. Marsh, and M. Peele.
Controlmethodsusedinastudyof S. Kiesling, and W. D. Raymond. 2003.Gender,race,andspeechstyle
thevowels.JASA,24:175–184. 2005. Thebuckeyecorpusofcon- stereotypes.SexRoles,48(7-8):317–
Peterson, G. E., W. S.-Y. Wang, and versational speech: Labeling con- 325.
E. Sivertsen. 1958. Segmenta- ventionsandatestoftranscriberre-
Porter, M. F. 1980. An algorithm
tiontechniquesinspeechsynthesis. liability. Speech Communication,
for suffix stripping. Program,
JASA,30(8):739–742. 45:90–95.
14(3):130–137.
Peterson,J.C.,D.Chen,andT.L.Grif- Plutchik,R.1962.Theemotions:Facts,
Post,M.2018. Acallforclarityinre-
fiths.2020.Parallelogramsrevisited: theories,andanewmodel. Random
portingBLEUscores.WMT2018.
Exploring the limitations of vector House.
space models for simple analogies. Plutchik, R.1980. Ageneralpsycho- Potts, C. 2011. On the negativity of
Cognition,205. evolutionarytheoryofemotion. In negation. InN.LiandD.Lutz,edi-
tors, ProceedingsofSemanticsand
Petrov, S., D.Das, andR.McDonald. R. Plutchik and H. Kellerman, ed-
Linguistic Theory 20, pages 636–
2012. A universal part-of-speech itors, Emotion: Theory, Research,
659.CLCPublications,Ithaca,NY.
tagset.LREC. andExperience,Volume1,pages3–
33.AcademicPress. Povey, D., A. Ghoshal, G. Boulianne,
Petrov, S. and R. McDonald. 2012.
L. Burget, O. Glembek, N. Goel,
Overviewofthe2012sharedtaskon Poesio,M.,R.Stevenson,B.DiEuge-
M. Hannemann, P. Motlicek,
parsingtheweb. NotesoftheFirst nio,andJ.Hitzeman.2004. Center-
Y. Qian, P. Schwarz, J. Silovsky´,
Workshop on Syntactic Analysis of ing: Aparametrictheoryanditsin-
G. Stemmer, and K. Vesely´. 2011.
Non-CanonicalLanguage(SANCL), stantiations.ComputationalLinguis-
The Kaldi speech recognition
volume59. tics,30(3):309–363.
toolkit.ASRU.
Phillips, A. V. 1960. A question- Poesio, M., R. Stuckardt, and Y. Ver-
Pradhan, S., E. H. Hovy, M. P. Mar-
answering routine. Technical Re- sley. 2016. Anaphora resolution:
cus, M. Palmer, L. Ramshaw, and
port16,MITAILab. Algorithms,resources,andapplica-
R.Weischedel.2007a. OntoNotes:
tions.Springer.
Picard,R.W.1995. Affectivecomput- Aunifiedrelationalsemanticrepre-
ing.TechnicalReport321,MITMe- Poesio, M., P. Sturt, R. Artstein, and sentation.ProceedingsofICSC.
diaLabPerceputalComputingTech- R. Filik. 2006. Underspecification
Pradhan, S., E. H. Hovy, M. P. Mar-
nicalReport.RevisedNovember26, and anaphora: Theoretical issues
cus, M. Palmer, L. A. Ramshaw,
1995. andpreliminaryevidence.Discourse
and R. M. Weischedel. 2007b.
processes,42(2):157–175.
Pieraccini, R., E. Levin, and C.-H. Ontonotes: a unified relational se-
Lee.1991.Stochasticrepresentation Poesio, M. and R. Vieira. 1998. A manticrepresentation.Int.J.Seman-
ofconceptualstructureintheATIS corpus-based investigation of defi- ticComputing,1(4):405–419.
task. SpeechandNaturalLanguage nitedescriptionuse. Computational
Pradhan, S., X. Luo, M. Recasens,
Workshop. Linguistics,24(2):183–216.
E.H.Hovy, V.Ng, andM.Strube.
Pierce, J. R., J. B. Carroll, E. P. Polanyi, L. 1988. Aformal model of 2014.Scoringcoreferencepartitions
Hamp, D. G. Hays, C. F. Hockett, thestructureofdiscourse. Journal ofpredictedmentions: Areference
A. G. Oettinger, and A. J. Perlis. ofPragmatics,12. implementation.ACL.610 Bibliography
Pradhan,S.,A.Moschitti,N.Xue,H.T. Pundak, G. and T. N. Sainath. 2016. Raghunathan, K., H.Lee, S.Rangara-
Ng, A. Bjo¨rkelund, O. Uryupina, Lower frame rate neural network jan, N. Chambers, M. Surdeanu,
Y.Zhang,andZ.Zhong.2013. To- acousticmodels.INTERSPEECH. D. Jurafsky, and C. D. Manning.
wards robust linguistic analysis us- Purver, M.2004. Thetheoryanduse 2010. Amulti-passsieveforcoref-
ingOntoNotes.CoNLL. ofclarificationrequestsindialogue. erenceresolution.EMNLP.
Pradhan, S., A. Moschitti, N. Xue, Ph.D.thesis,UniversityofLondon. Rahman, A.andV.Ng.2009. Super-
O.Uryupina,andY.Zhang.2012a. Pustejovsky, J. 1991. The generative visedmodelsforcoreferenceresolu-
CoNLL-2012 shared task: Model- lexicon. ComputationalLinguistics, tion.EMNLP.
ing multilingual unrestricted coref- 17(4). Rahman,A.andV.Ng.2012. Resolv-
erenceinOntoNotes.CoNLL. ing complex cases of definite pro-
Pustejovsky, J. 1995. The Generative
Pradhan, S., A. Moschitti, N. Xue, nouns: theWinogradSchemachal-
Lexicon.MITPress.
O.Uryupina,andY.Zhang.2012b. lenge.EMNLP.
Conll-2012 shared task: Model- Pustejovsky,J.andB.K.Boguraev,ed- Rajpurkar, P., R. Jia, and P. Liang.
ing multilingual unrestricted coref- itors.1996. LexicalSemantics: The 2018. Know what you don’t
erenceinOntoNotes.CoNLL. ProblemofPolysemy. OxfordUni- know: Unanswerable questions for
versityPress.
Pradhan, S., L. Ramshaw, M. P. Mar- SQuAD.ACL.
cus,M.Palmer,R.Weischedel,and Pustejovsky, J., P. Hanks, R. Saur´ı, Rajpurkar,P.,J.Zhang,K.Lopyrev,and
N.Xue.2011. CoNLL-2011shared A. See, R. Gaizauskas, A. Setzer, P.Liang.2016. SQuAD:100,000+
task:Modelingunrestrictedcorefer- D.Radev,B.Sundheim,D.S.Day, questions for machine comprehen-
enceinOntoNotes.CoNLL. L.Ferro, andM.Lazo.2003. The sionoftext.EMNLP.
TIMEBANK corpus. Proceedings
Pradhan, S., L. Ramshaw, R. Wei- ofCorpusLinguistics2003Confer- Ram, A., R. Prasad, C. Khatri,
schedel, J. MacBride, and L. Mic- ence.UCRELTechnicalPapernum- A. Venkatesh, R. Gabriel, Q. Liu,
ciulla.2007c. Unrestrictedcorefer- J.Nunn,B.Hedayatnia,M.Cheng,
ber16.
ence:Identifyingentitiesandevents A. Nagar, E. King, K. Bland,
in OntoNotes. Proceedings of Pustejovsky, J., R. Ingria, A. Wartick, Y. Pan, H. Song,
ICSC2007. R. Saur´ı, J. Castan˜o, J. Littman, S.Jayadevan,G.Hwang,andA.Pet-
Pradhan, S., W. Ward, K. Hacioglu, R. Gaizauskas, A. Setzer, G. Katz, tigrue.2017.ConversationalAI:The
J.H.Martin,andD.Jurafsky.2005. and I. Mani. 2005. The Specifica- sciencebehindtheAlexaPrize. 1st
Semanticrolelabelingusingdiffer- tionLanguageTimeML,chapter27. ProceedingsofAlexaPrize.
Oxford.
entsyntacticviews.ACL. Ramshaw, L. A. and M. P. Mar-
Prasad,R.,N.Dinesh,A.Lee,E.Milt- Qin,L.,Z.Zhang,andH.Zhao.2016. cus. 1995. Text chunking using
sakaki,L.Robaldo,A.K.Joshi,and Astackinggatedneuralarchitecture transformation-basedlearning. Pro-
B.L.Webber.2008. ThePennDis- forimplicitdiscourserelationclassi- ceedings of the 3rd Annual Work-
courseTreeBank2.0.LREC. fication.EMNLP. shoponVeryLargeCorpora.
Prasad,R.,B.L.Webber,andA.Joshi. Qin, L., Z. Zhang, H. Zhao, Z. Hu, Raphael, B. 1968. SIR: A com-
2014. ReflectionsonthePennDis- and E. Xing. 2017. Adversarial puterprogramforsemanticinforma-
course Treebank, comparable cor- connective-exploiting networks for tion retrieval. In M. Minsky, edi-
pora, and complementary annota- implicitdiscourserelationclassifica- tor, Semantic Information Process-
tion. Computational Linguistics, tion.ACL. ing,pages33–145.MITPress.
40(4):921–950. Quillian,M.R.1968. Semanticmem- Rashkin, H., E. Bell, Y. Choi, and
Prates,M.O.R.,P.H.Avelar,andL.C. ory. InM.Minsky,editor,Semantic S.Volkova.2017. Multilingualcon-
Lamb.2019. Assessinggenderbias InformationProcessing,pages227– notation frames: A case study on
inmachinetranslation: acasestudy 270.MITPress. social media for targeted sentiment
withGoogleTranslate.NeuralCom- Quillian, M. R. 1969. The teachable analysisandforecast.ACL.
puting and Applications, 32:6363– language comprehender: A simu- Rashkin, H., S. Singh, and Y. Choi.
6381. lation program and theory of lan- 2016. Connotationframes: Adata-
Price, P. J., W. Fisher, J. Bern- guage.CACM,12(8):459–476. driveninvestigation.ACL.
stein, and D. Pallet. 1988. The Radford,A.,J.Wu,R.Child,D.Luan, Rashkin, H., E. M. Smith, M. Li,
DARPA 1000-word resource man- D.Amodei,andI.Sutskever.2019. andY.-L.Boureau.2019. Towards
agement database for continuous Language models are unsupervised empathetic open-domain conversa-
speechrecognition.ICASSP. multitasklearners. OpenAItechre- tionmodels: Anewbenchmarkand
Price,P.J.,M.Ostendorf,S.Shattuck- port. dataset.ACL.
Hufnagel, andC.Fong.1991. The Raffel, C., N. Shazeer, A. Roberts, Ratinov, L. and D. Roth. 2012.
use of prosody in syntactic disam- K. Lee, S. Narang, M. Matena, Learning-based multi-sieve co-
biguation.JASA,90(6). Y. Zhou, W. Li, and P. J. Liu. reference resolution with knowl-
Prince,E.1981.Towardataxonomyof 2020. Exploringthelimitsoftrans- edge.EMNLP.
given-newinformation. InP.Cole, fer learning with a unified text-to- Ratnaparkhi, A. 1996. A maxi-
editor, Radical Pragmatics, pages texttransformer. JMLR,21(140):1– mumentropypart-of-speechtagger.
223–255.AcademicPress. 67. EMNLP.
Propp, V. 1968. Morphology of the Raganato,A.,C.D.Bovi,andR.Nav- Ratnaparkhi, A. 1997. A linear ob-
Folktale,2ndedition. Universityof igli.2017a. Neuralsequencelearn- served time statistical parser based
TexasPress.OriginalRussian1928. ing models for word sense disam- on maximum entropy models.
TranslatedbyLaurenceScott. biguation.EMNLP. EMNLP.
Pu, X., N. Pappas, J. Henderson, and Raganato, A., J. Camacho-Collados, Recasens, M. and E. H. Hovy. 2011.
A. Popescu-Belis. 2018. Integrat- andR.Navigli.2017b. Wordsense BLANC: Implementing the Rand
ing weakly supervised word sense disambiguation: A unified evalua- index for coreference evaluation.
disambiguationintoneuralmachine tionframeworkandempiricalcom- Natural Language Engineering,
translation.TACL,6:635–649. parison.EACL. 17(4):485–510.Bibliography 611
Recasens, M., E.H.Hovy, andM.A. Riloff, E. and J. Shepherd. 1997. A Rosenblatt, F. 1958. The percep-
Mart´ı.2011. Identity, non-identity, corpus-based approach for building tron: A probabilistic model for in-
and near-identity: Addressing the semanticlexicons.EMNLP. formation storage and organization
complexityofcoreference. Lingua, inthebrain. Psychologicalreview,
Riloff,E.andM.Thelen.2000.Arule-
121(6):1138–1152. 65(6):386–408.
based question answering system
Recasens, M. and M. A. Mart´ı. 2010. for reading comprehension tests. Rosenfeld, R.1996. Amaximumen-
AnCora-CO: Coreferentially anno- ANLP/NAACLworkshoponreading tropy approach to adaptive statisti-
tatedcorporaforSpanishandCata- comprehensiontests. cal language modeling. Computer
lan. LanguageResourcesandEval- SpeechandLanguage,10:187–228.
Riloff, E.andJ.Wiebe.2003. Learn-
uation,44(4):315–345. ingextractionpatternsforsubjective Rosenthal,S.andK.McKeown.2017.
Reed,C.,R.MochalesPalau,G.Rowe, expressions.EMNLP. Detectinginfluencersinmultipleon-
and M.-F. Moens. 2008. Lan- line genres. ACM Transactions on
Ritter, A., C. Cherry, and B. Dolan.
guage resources for studying argu- InternetTechnology(TOIT),17(2).
2010a. Unsupervised modeling of
ment.LREC. twitterconversations.NAACLHLT. Rothe, S., S. Ebert, and H. Schu¨tze.
Rehder,B.,M.E.Schreiner,M.B.W. 2016. Ultradense Word Embed-
Ritter, A., C. Cherry, and B. Dolan.
Wolfe, D. Laham, T. K. Landauer, dings by Orthogonal Transforma-
2011. Data-driven response gener-
andW.Kintsch.1998. UsingLatent tion.NAACLHLT.
ationinsocialmedia.EMNLP.
SemanticAnalysistoassessknowl- Roy,N.,J.Pineau,andS.Thrun.2000.
edge: Some technical considera- Ritter, A., O. Etzioni, and Mausam. Spokendialoguemanagementusing
tions. Discourse Processes, 25(2- 2010b. Alatentdirichletallocation probabilisticreasoning.ACL.
3):337–354. method for selectional preferences.
ACL. Rudinger, R., J. Naradowsky,
Rei,R.,C.Stewart,A.C.Farinha,and B. Leonard, and B. Van Durme.
A. Lavie. 2020. COMET: A neu- Ritter,A.,L.Zettlemoyer,Mausam,and 2018. Gender bias in coreference
ral framework for MT evaluation. O. Etzioni. 2013. Modeling miss- resolution.NAACLHLT.
EMNLP. ingdataindistantsupervisionforin-
Rumelhart, D. E., G. E. Hinton, and
formationextraction. TACL,1:367–
Reichenbach, H. 1947. Elements of R.J.Williams.1986. Learningin-
378.
Symbolic Logic. Macmillan, New ternalrepresentationsbyerrorprop-
York. Roberts,A.,C.Raffel,andN.Shazeer. agation. In D. E. Rumelhart and
Reichman,R.1985.GettingComputers 2020. How much knowledge can J. L. McClelland, editors, Parallel
toTalkLikeYouandMe.MITPress. you pack into the parameters of a Distributed Processing, volume 2,
languagemodel? EMNLP. pages318–362.MITPress.
Resnik,P.1993. Semanticclassesand
syntacticambiguity.HLT. Robertson, S., S. Walker, S. Jones, Rumelhart,D.E.andJ.L.McClelland.
M. M. Hancock-Beaulieu, and 1986a. On learning the past tense
Resnik, P. 1996. Selectional con- M.Gatford.1995.OkapiatTREC-3. ofEnglishverbs. InD.E.Rumel-
straints: An information-theoretic OverviewoftheThirdTextREtrieval hart and J. L. McClelland, editors,
modelanditscomputationalrealiza- Conference(TREC-3). ParallelDistributedProcessing,vol-
tion.Cognition,61:127–159. Robins, R. H. 1967. A Short History ume2,pages216–271.MITPress.
Riedel, S., L.Yao, andA.McCallum. of Linguistics. Indiana University Rumelhart,D.E.andJ.L.McClelland,
2010. Modelingrelationsandtheir Press,Bloomington. editors.1986b. ParallelDistributed
mentions without labeled text. In Robinson, T. and F. Fallside. 1991. Processing.MITPress.
Machine Learning and Knowledge
A recurrent error propagation net- Rumelhart,D.E.andA.A.Abraham-
DiscoveryinDatabases,pages148–
work speech recognition system. son. 1973. A model for analogi-
163.Springer.
Computer Speech & Language, cal reasoning. Cognitive Psychol-
Riedel, S., L.Yao, A.McCallum, and 5(3):259–274. ogy,5(1):1–28.
B.M.Marlin.2013.Relationextrac-
Robinson,T.,M.Hochberg,andS.Re- Rumelhart,D.E.andJ.L.McClelland,
tion with matrix factorization and
nals. 1996. The use of recur- editors.1986c. ParallelDistributed
universalschemas.NAACLHLT.
rent neural networks in continu- Processing:ExplorationsintheMi-
Riesbeck, C. K. 1975. Conceptual ous speech recognition. In C.-H. crostructure of Cognition, volume
analysis. In R. C. Schank, editor, Lee, F. K. Soong, and K. K. Pali- 1:Foundations.MITPress.
ConceptualInformationProcessing, wal, editors, Automatic speech and Ruppenhofer,J.,M.Ellsworth,M.R.L.
pages 83–156. American Elsevier, speakerrecognition,pages233–258. Petruck,C.R.Johnson,C.F.Baker,
NewYork. Springer. andJ.Scheffczyk.2016. FrameNet
Riloff, E. 1993. Automatically con- Rohde,D.L.T.,L.M.Gonnerman,and II:Extendedtheoryandpractice.
structing a dictionary for informa- D. C. Plaut. 2006. An improved Ruppenhofer, J., C. Sporleder,
tionextractiontasks.AAAI. model of semantic similarity based R. Morante, C. F. Baker, and
Riloff, E. 1996. Automatically gen- on lexical co-occurrence. CACM, M. Palmer. 2010. Semeval-2010
eratingextractionpatternsfromun- 8:627–633. task 10: Linking events and their
taggedtext.AAAI. Roller, S., E.Dinan, N.Goyal, D.Ju, participants in discourse. 5th In-
Riloff,E.andR.Jones.1999.Learning M. Williamson, Y. Liu, J. Xu, ternational Workshop on Semantic
dictionaries for information extrac- M.Ott,E.M.Smith,Y.-L.Boureau, Evaluation.
tion by multi-level bootstrapping. and J. Weston. 2021. Recipes for Russell, J. A. 1980. A circum-
AAAI. building an open-domain chatbot. plex model of affect. Journal of
Riloff,E.andM.Schmelzenbach.1998. EACL. personality and social psychology,
Anempiricalapproachtoconceptual Rooth, M., S. Riezler, D. Prescher, 39(6):1161–1178.
caseframeacquisition. Proceedings G.Carroll,andF.Beil.1999.Induc- Russell, S. and P. Norvig. 2002. Ar-
oftheSixthWorkshoponVeryLarge ingasemanticallyannotatedlexicon tificial Intelligence: A Modern Ap-
Corpora. viaEM-basedclustering.ACL. proach,2ndedition.PrenticeHall.612 Bibliography
Rutherford,A.andN.Xue.2015. Im- Saur´ı, R., J. Littman, B. Knippen, Schu¨tze, H. 1992a. Context space.
provingtheinferenceofimplicitdis- R. Gaizauskas, A. Setzer, and AAAI Fall Symposium on Proba-
course relations via classifying ex- J.Pustejovsky.2006. TimeMLan- bilisticApproachestoNaturalLan-
plicitdiscourseconnectives.NAACL notation guidelines version 1.2.1. guage.
HLT. Manuscript.
Schu¨tze, H. 1992b. Dimensions of
Sacks,H.,E.A.Schegloff,andG.Jef- Scha, R. and L. Polanyi. 1988. An meaning.ProceedingsofSupercom-
ferson. 1974. A simplest system- augmentedcontextfreegrammarfor puting’92.IEEEPress.
atics for the organization of turn- discourse.COLING.
Schu¨tze,H.1997a. AmbiguityResolu-
takingforconversation. Language, Schank,R.C.1972.Conceptualdepen- tioninLanguageLearning–Com-
50(4):696–735. dency:Atheoryofnaturallanguage putational and Cognitive Models.
Sag, I. A. and M. Y. Liberman. 1975. processing. Cognitive Psychology, CSLI,Stanford,CA.
The intonational disambiguation of 3:552–631.
Schu¨tze,H.1997b. AmbiguityResolu-
indirect speech acts. In CLS- Schank,R.C.andR.P.Abelson.1975. tion in Language Learning: Com-
75, pages 487–498. University of Scripts,plans,andknowledge. Pro- putational and Cognitive Models.
Chicago. ceedingsofIJCAI-75. CSLIPublications,Stanford,CA.
Sagae, K. 2009. Analysis of dis- Schank,R.C.andR.P.Abelson.1977. Schu¨tze, H. 1998. Automatic word
course structure with syntactic de- Scripts, Plans, Goals and Under- sense discrimination. Computa-
pendencies and data-driven shift- standing.LawrenceErlbaum. tionalLinguistics,24(1):97–124.
reduceparsing.IWPT-09.
Schegloff, E.A.1968. Sequencingin Schu¨tze,H.,D.A.Hull,andJ.Peder-
Sagisaka, Y. 1988. Speech synthe- conversationalopenings. American sen. 1995. A comparison of clas-
sis by rule using an optimal selec- Anthropologist,70:1075–1095. sifiersanddocumentrepresentations
tionofnon-uniformsynthesisunits.
Scherer, K. R. 2000. Psychological fortheroutingproblem.SIGIR-95.
ICASSP.
modelsofemotion. InJ.C.Borod, Schu¨tze, H.andJ.Pedersen.1993. A
Sagisaka, Y., N. Kaiki, N. Iwahashi, editor,Theneuropsychologyofemo- vector model for syntagmatic and
andK.Mimura.1992. Atr–ν-talk tion,pages137–162.Oxford. paradigmatic relatedness. 9th An-
speechsynthesissystem.ICSLP.
Schiebinger, L. 2013. Machine nual Conference of the UW Centre
Sahami, M., S. T. Dumais, D. Heck- translation: Analyzing gender. fortheNewOEDandTextResearch.
erman, and E. Horvitz. 1998. A http://genderedinnovations.
Schu¨tze,H.andY.Singer.1994. Part-
Bayesianapproachtofilteringjunk stanford.edu/case-studies/
of-speech tagging using a variable
e-mail.AAAIWorkshoponLearning nlp.html#tabs-2.
memoryMarkovmodel.ACL.
forTextCategorization.
Schiebinger, L. 2014. Scientific re-
Schwartz, H. A., J. C. Eichstaedt,
Sakoe, H. and S. Chiba. 1971. A search must take gender into ac-
M. L. Kern, L. Dziurzynski, S. M.
dynamic programming approach to count.Nature,507(7490):9.
Ramones, M. Agrawal, A. Shah,
continuousspeechrecognition.Pro- Schluter, N.2018. Thewordanalogy M. Kosinski, D. Stillwell, M. E. P.
ceedings of the Seventh Interna- testingcaveat.NAACLHLT. Seligman, and L. H. Ungar. 2013.
tional Congress on Acoustics, vol-
ume3.Akade´miaiKiado´. Schneider, N., J.D.Hwang, V.Sriku- Personality, gender, and age in the
mar, J. Prange, A. Blodgett, S. R. languageofsocialmedia:Theopen-
Sakoe, H. and S. Chiba. 1984. Dy- Moeller, A. Stern, A. Bitan, and vocabulary approach. PloS one,
namicprogrammingalgorithmopti- O.Abend.2018.Comprehensivesu- 8(9):e73791.
mization for spoken word recogni- persensedisambiguationofEnglish Schwenk, H.2007. Continuousspace
tion. IEEETransactionsonAcous- prepositionsandpossessives.ACL. languagemodels. ComputerSpeech
tics,Speech,andSignalProcessing,
Schone, P. and D. Jurafsky. 2000. &Language,21(3):492–518.
ASSP-26(1):43–49.
Knowlege-free induction of mor- Schwenk,H.2018. Filteringandmin-
Salomaa, A. 1969. Probabilistic and phologyusinglatentsemanticanal- ingparalleldatainajointmultilin-
weighted grammars. Information ysis.CoNLL. gualspace.ACL.
andControl,15:529–544.
Schone, P.andD.Jurafsky.2001a. Is Schwenk, H., D.Dechelotte, andJ.-L.
Salton, G. 1971. The SMART Re- knowledge-free induction of multi- Gauvain. 2006. Continuous space
trievalSystem: ExperimentsinAu- word unit dictionary headwords a language models for statistical ma-
tomaticDocumentProcessing.Pren- solvedproblem? EMNLP. chinetranslation.COLING/ACL.
ticeHall.
Schone, P. and D. Jurafsky. 2001b. Schwenk, H., G. Wenzek, S. Edunov,
Sampson, G.1987. Alternativegram- Knowledge-freeinductionofinflec- E. Grave, A. Joulin, and A. Fan.
maticalcodingsystems. InR.Gar- tionalmorphologies.NAACL. 2021. CCMatrix: Mining billions
side,G.Leech,andG.Sampson,ed- Scho¨nfinkel, M. 1924. U¨ber die ofhigh-qualityparallelsentenceson
itors,TheComputationalAnalysisof
Bausteine der mathematischen theweb.ACL2021.
English,pages165–183.Longman.
Logik. Mathematische Annalen, Se´aghdha, D. O. 2010. Latent vari-
Sankoff,D.andW.Labov.1979.Onthe 92:305–316. English translation able models of selectional prefer-
usesofvariablerules. Languagein appearsinFromFregetoGo¨del: A ence.ACL.
society,8(2-3):189–222. SourceBookinMathematicalLogic,
Seddah, D., R. Tsarfaty, S. Ku¨bler,
Sap,M.,D.Card,S.Gabriel,Y.Choi, HarvardUniversityPress,1967.
M. Candito, J. D. Choi, R. Farkas,
andN.A.Smith.2019. Theriskof Schuster, M. and K. Nakajima. 2012. J. Foster, I. Goenaga, K. Gojenola,
racialbiasinhatespeechdetection. Japanese and Korean voice search. Y. Goldberg, S. Green, N. Habash,
ACL. ICASSP. M. Kuhlmann, W. Maier, J. Nivre,
Sap,M.,M.C.Prasettio,A.Holtzman, Schuster, M.andK.K.Paliwal.1997. A. Przepio´rkowski, R. Roth,
H.Rashkin,andY.Choi.2017.Con- Bidirectional recurrent neural net- W. Seeker, Y. Versley, V. Vincze,
notationframesofpowerandagency works.IEEETransactionsonSignal M. Wolin´ski, A. Wro´blewska, and
inmodernfilms.EMNLP. Processing,45:2673–2681. E. Villemonte de la Cle´rgerie.Bibliography 613
2013. Overview of the SPMRL Shoup,J.E.1980.Phonologicalaspects Small,S.L.andC.Rieger.1982. Pars-
2013sharedtask: cross-framework ofspeechrecognition.InW.A.Lea, ing and comprehending with Word
evaluation of parsing morpholog- editor, Trends in Speech Recogni- Experts.InW.G.LehnertandM.H.
ically rich languages. 4th Work- tion,pages125–138.PrenticeHall. Ringle,editors,StrategiesforNatu-
shop on Statistical Parsing of Shriberg, E., R. Bates, P. Taylor, ralLanguageProcessing,pages89–
Morphologically-RichLanguages. A. Stolcke, D. Jurafsky, K. Ries, 147.LawrenceErlbaum.
See, A., S. Roller, D. Kiela, and N.Coccaro, R.Martin, M.Meteer, Smith,V.L.andH.H.Clark.1993.On
J. Weston. 2019. What makes a andC.VanEss-Dykema.1998. Can the course of answering questions.
good conversation? how control- prosodyaidtheautomaticclassifica- Journal of Memory and Language,
lable attributes affect human judg- tionofdialogactsinconversational 32:25–38.
ments.NAACLHLT. speech?LanguageandSpeech(Spe-
Smolensky, P. 1988. On the proper
cialIssueonProsodyandConversa-
Sekine, S. and M. Collins. 1997. treatmentofconnectionism. Behav-
tion),41(3-4):439–487.
The evalb software. http: ioral and brain sciences, 11(1):1–
//cs.nyu.edu/cs/projects/ Sidner,C.L.1979. Towardsacompu- 23.
proteus/evalb. tationaltheoryofdefiniteanaphora
comprehensioninEnglishdiscourse. Smolensky, P. 1990. Tensor product
Sellam,T.,D.Das,andA.Parikh.2020. Technical Report 537, MIT Artifi- variablebindingandtherepresenta-
BLEURT: Learning robust metrics cial Intelligence Laboratory, Cam- tion of symbolic structures in con-
fortextgeneration.ACL. bridge,MA. nectionist systems. Artificial intel-
Seneff,S.andV.W.Zue.1988. Tran- Sidner, C. L. 1983. Focusing in the ligence,46(1-2):159–216.
scription and alignment of the comprehensionofdefiniteanaphora. Snover, M., B. Dorr, R. Schwartz,
TIMIT database. Proceedings of InM.BradyandR.C.Berwick,ed- L.Micciulla,andJ.Makhoul.2006.
theSecondSymposiumonAdvanced itors,ComputationalModelsofDis- Astudyoftranslationeditratewith
Man-Machine Interface through course,pages267–330.MITPress. targetedhumanannotation. AMTA-
SpokenLanguage. Silverman, K., M. E. Beckman, J. F. 2006.
Sennrich,R.,B.Haddow,andA.Birch. Pitrelli,M.Ostendorf,C.W.Wight- Snow, R., D. Jurafsky, and A. Y. Ng.
2016.Neuralmachinetranslationof man, P. J. Price, J. B. Pierrehum- 2005. Learning syntactic patterns
rarewordswithsubwordunits.ACL. bert,andJ.Hirschberg.1992. ToBI: for automatic hypernym discovery.
Seo,M.,A.Kembhavi,A.Farhadi,and A standard for labelling English NeurIPS.
H. Hajishirzi. 2017. Bidirectional prosody.ICSLP.
Snow,R.,S.Prakash,D.Jurafsky,and
attentionflowformachinecompre- Simmons,R.F.1965. AnsweringEn- A.Y.Ng.2007. Learningtomerge
hension.ICLR. glishquestionsbycomputer: Asur- wordsenses.EMNLP/CoNLL.
Serban, I. V., R. Lowe, P. Hender- vey.CACM,8(1):53–70.
Snyder,B.andM.Palmer.2004. The
son,L.Charlin,andJ.Pineau.2018. Simmons, R. F. 1973. Semantic
Englishall-wordstask. SENSEVAL-
A survey of available corpora for networks: Their computation and
3.
building data-driven dialogue sys- use for understanding English sen-
tems:Thejournalversion.Dialogue tences. InR.C.SchankandK.M. Socher, R., J. Bauer, C. D. Man-
&Discourse,9(1):1–49. Colby,editors,ComputerModelsof ning, and A. Y. Ng. 2013. Pars-
Thought and Language, pages 61– ingwithcompositionalvectorgram-
Shang,L.,Z.Lu,andH.Li.2015.Neu-
113.W.H.FreemanandCo. mars.ACL.
ral responding machine for short-
textconversation.ACL. Simmons,R.F.,S.Klein,andK.Mc- Socher,R.,C.C.-Y.Lin,A.Y.Ng,and
Conlogue. 1964. Indexing and de- C.D.Manning.2011. Parsingnatu-
Shannon,C.E.1948. Amathematical
theoryofcommunication. BellSys- pendency logic for answering En- ralscenesandnaturallanguagewith
tem Technical Journal, 27(3):379–
glish questions. American Docu- recursiveneuralnetworks.ICML.
mentation,15(3):196–204.
423.Continuedinthefollowingvol- Soderland, S., D. Fisher, J. Aseltine,
ume. Simons, G. F. and C. D. Fennig. and W. G. Lehnert. 1995. CRYS-
Shannon,C.E.1951.Predictionanden- 2018. Ethnologue: Languages of TAL:Inducingaconceptualdictio-
tropyofprintedEnglish.BellSystem the world, 21st edition. SIL Inter- nary.IJCAI-95.
national.
TechnicalJournal,30:50–64. Søgaard, A. 2010. Simple semi-
Singh,S.P.,D.J.Litman,M.Kearns,
Sheil,B.A.1976.Observationsoncon- andM.A.Walker.2002. Optimiz- supervised training of part-of-
textfreeparsing. SMIL:Statistical ing dialogue management with re- speechtaggers.ACL.
MethodsinLinguistics,1:71–109. inforcement learning: Experiments Søgaard, A. and Y. Goldberg. 2016.
Shen, J., R. Pang, R. J. Weiss, with the NJFun system. JAIR, Deep multi-task learning with low
M. Schuster, N. Jaitly, Z. Yang, 16:105–133. leveltaskssupervisedatlowerlay-
Z. Chen, Y. Zhang, Y. Wang, Sleator, D. and D. Temperley. 1993. ers.ACL.
R. Skerry-Ryan, R. A. Saurous, Parsing English with a link gram- Søgaard, A., A. Johannsen, B. Plank,
Y. Agiomyrgiannakis, and Y. Wu. mar.IWPT-93. D.Hovy, andH.M.Alonso.2014.
2018.NaturalTTSsynthesisbycon- Sloan, M. C. 2010. Aristotle’s Nico- What’sinap-valueinNLP?CoNLL.
ditioning WaveNet on mel spectro-
grampredictions.ICASSP. machean Ethics as the original lo- Solorio, T., E. Blair, S. Maharjan,
cus for the Septem Circumstantiae. S.Bethard, M.Diab, M.Ghoneim,
Sheng, E., K.-W.Chang, P.Natarajan, Classical Philology, 105(3):236– A. Hawwari, F. AlGhamdi,
and N. Peng. 2019. The woman 251. J. Hirschberg, A. Chang, and
workedasababysitter:Onbiasesin
Slobin,D.I.1996. Twowaystotravel. P. Fung. 2014. Overview for the
languagegeneration.EMNLP.
In M. Shibatani and S. A. Thomp- first shared task on language iden-
Shi,P.andJ.Lin.2019. SimpleBERT son,editors,GrammaticalConstruc- tification in code-switched data.
models for relation extraction and tions: Their Form and Meaning, First Workshop on Computational
semanticrolelabeling.ArXiv. pages195–220.ClarendonPress. ApproachestoCodeSwitching.614 Bibliography
Somasundaran, S., J. Burstein, and Stab,C.andI.Gurevych.2017.Parsing Stolz, W. S., P. H. Tannenbaum, and
M.Chodorow.2014. Lexicalchain- argumentation structures in persua- F.V.Carstensen.1965. Astochastic
ing for measuring discourse coher- siveessays.ComputationalLinguis- approachtothegrammaticalcoding
ence quality in test-taker essays. tics,43(3):619–659. ofEnglish.CACM,8(6):399–405.
COLING. Stalnaker, R. C. 1978. Assertion. In Stone, P., D. Dunphry, M. Smith, and
Soon, W. M., H. T. Ng, and D. C. Y. P.Cole,editor,Pragmatics: Syntax D.Ogilvie.1966. TheGeneralIn-
Lim.2001. Amachinelearningap- andSemanticsVolume9,pages315– quirer: A Computer Approach to
proach to coreference resolution of 332.AcademicPress. ContentAnalysis.MITPress.
noun phrases. Computational Lin- Stamatatos,E.2009. Asurveyofmod-
Stoyanchev,S.andM.Johnston.2015.
guistics,27(4):521–544. ern authorship attribution methods.
Localized error detection for tar-
JASIST,60(3):538–556.
Sordoni, A., M. Galley, M. Auli, getedclarificationinavirtualassis-
C. Brockett, Y. Ji, M. Mitchell, Stanovsky, G., N. A. Smith, and tant.ICASSP.
J.-Y. Nie, J. Gao, and B. Dolan. L. Zettlemoyer. 2019. Evaluating
2015.Aneuralnetworkapproachto gender bias in machine translation. Stoyanchev, S., A.Liu, andJ.Hirsch-
context-sensitivegenerationofcon- ACL. berg.2013. Modellinghumanclari-
ficationstrategies.SIGDIAL.
versationalresponses.NAACLHLT. Stede,M.2011. Discourseprocessing.
Soricut,R.andD.Marcu.2003. Sen- Morgan&Claypool. Stoyanchev, S., A.Liu, andJ.Hirsch-
berg. 2014. Towards natural clar-
tence level discourse parsing using Stede,M.andJ.Schneider.2018.Argu-
ification questions in dialogue sys-
syntactic and lexical information. mentationMining. Morgan&Clay-
tems.AISBsymposiumonquestions,
HLT-NAACL. pool.
discourseanddialogue.
Soricut, R. and D. Marcu. 2006. Stern, M., J. Andreas, and D. Klein.
Discourse generation using utility- 2017. Aminimalspan-basedneural Stro¨tgen,J.andM.Gertz.2013. Mul-
trained coherence models. COL- constituencyparser.ACL. tilingualandcross-domaintemporal
ING/ACL. Stevens,K.N.1998. AcousticPhonet- tagging. Language Resources and
Evaluation,47(2):269–298.
ics.MITPress.
Sorokin, D. and I. Gurevych. 2018.
Mixingcontextgranularitiesforim- Stevens, K. N. and A. S. House. Strube,M.andU.Hahn.1996. Func-
proved entity linking on question 1955. Development of a quantita- tionalcentering.ACL.
answering data across entity cate- tive description of vowel articula- Su,Y.,H.Sun,B.Sadler,M.Srivatsa,
gories.*SEM. tion.JASA,27:484–493. I.Gu¨r,Z.Yan,andX.Yan.2016.On
SparckJones,K.1972. Astatisticalin- Stevens,K.N.andA.S.House.1961. generating characteristic-rich ques-
terpretation of term specificity and Anacousticaltheoryofvowelpro- tionsetsforQAevaluation.EMNLP.
itsapplicationinretrieval. Journal ductionandsomeofitsimplications. Subba,R.andB.DiEugenio.2009.An
ofDocumentation,28(1):11–21. JournalofSpeechandHearingRe- effective discourse parser that uses
search,4:303–320.
SparckJones,K.1986. Synonymyand richlinguisticinformation. NAACL
SemanticClassification. Edinburgh Stevens,K.N.,S.Kasowski,andG.M. HLT.
Fant.1953. Anelectricalanalogof
UniversityPress,Edinburgh.Repub- the vocal tract. JASA, 25(4):734– Suendermann, D., K. Evanini, J. Lis-
licationof1964PhDThesis. 742. combe,P.Hunter,K.Dayanidhi,and
Sporleder,C.andA.Lascarides.2005. Stevens, S.S.andJ.Volkmann.1940. R.Pieraccini.2009.Fromrule-based
Exploitinglinguisticcuestoclassify Therelationofpitchtofrequency:A tostatisticalgrammars: Continuous
rhetoricalrelations.RANLP-05. revisedscale.TheAmericanJournal improvement of large-scale spoken
dialogsystems.ICASSP.
Sporleder,C.andM.Lapata.2005.Dis- ofPsychology,53(3):329–353.
coursechunkinganditsapplication Stevens,S.S.,J.Volkmann,andE.B. Sukhbaatar, S., A. Szlam, J. Weston,
tosentencecompression.EMNLP. Newman.1937.Ascaleforthemea- and R. Fergus. 2015. End-to-end
Sproat, R., A. W. Black, S. F. surementofthepsychologicalmag- memorynetworks.NeurIPS.
Chen,S.Kumar,M.Ostendorf,and nitudepitch.JASA,8:185–190. Sundheim, B., editor.1991. Proceed-
C. Richards. 2001. Normalization Stifelman, L. J., B. Arons, ingsofMUC-3.
of non-standard words. Computer C. Schmandt, and E. A. Hulteen. Sundheim, B., editor.1992. Proceed-
Speech & Language, 15(3):287– 1993. VoiceNotes: Aspeechinter- ingsofMUC-4.
333. faceforahand-heldvoicenotetaker.
INTERCHI1993. Sundheim, B., editor.1993. Proceed-
Sproat, R. and K. Gorman. 2018. A
ingsofMUC-5.Baltimore,MD.
brief summary of the Kaggle text Stolcke,A.1998. Entropy-basedprun-
normalizationchallenge. ing of backoff language models. Sundheim, B., editor.1995. Proceed-
Proc.DARPABroadcastNewsTran- ingsofMUC-6.
Srivastava, N., G. E. Hinton, scriptionandUnderstandingWork-
A. Krizhevsky, I. Sutskever, and shop. Surdeanu, M. 2013. Overview of the
R.R.Salakhutdinov.2014.Dropout: TAC2013KnowledgeBasePopula-
asimplewaytopreventneuralnet- Stolcke,A.2002. SRILM–anexten- tion evaluation: English slot filling
works from overfitting. JMLR, siblelanguagemodelingtoolkit.IC- andtemporalslotfilling.TAC-13.
SLP.
15(1):1929–1958.
Surdeanu, M., S. Harabagiu,
Stolcke, A., K. Ries, N. Coccaro,
Stab,C.andI.Gurevych.2014a.Anno- J. Williams, and P. Aarseth. 2003.
E. Shriberg, R. Bates, D. Jurafsky,
tatingargumentcomponentsandre- Usingpredicate-argumentstructures
P. Taylor, R. Martin, M. Meteer,
lationsinpersuasiveessays. COL- forinformationextraction.ACL.
andC.VanEss-Dykema.2000. Di-
ING.
alogue act modeling for automatic Surdeanu, M., T. Hicks, and M. A.
Stab,C.andI.Gurevych.2014b.Identi- taggingandrecognitionofconversa- Valenzuela-Escarcega. 2015. Two
fyingargumentativediscoursestruc- tional speech. Computational Lin- practical rhetorical structure theory
turesinpersuasiveessays.EMNLP. guistics,26(3):339–371. parsers.NAACLHLT.Bibliography 615
Surdeanu, M., R. Johansson, A. Mey- Team,N.,M.R.Costa-jussa`,J.Cross, Toutanova, K., D. Klein, C. D. Man-
ers,L.Ma`rquez,andJ.Nivre.2008. O.C¸elebi,M.Elbayad,K.Heafield, ning,andY.Singer.2003. Feature-
The CoNLL 2008 shared task on K. Heffernan, E. Kalbassi, J. Lam, rich part-of-speech tagging with a
jointparsingofsyntacticandseman- D. Licht, J. Maillard, A. Sun, cyclic dependency network. HLT-
ticdependencies.CoNLL. S. Wang, G. Wenzek, A. Young- NAACL.
Sutskever,I.,O.Vinyals,andQ.V.Le. blood, B. Akula, L. Barrault, Trichelair, P., A. Emami, J. C. K.
2014. Sequencetosequencelearn- G. M. Gonzalez, P. Hansanti, Cheung, A. Trischler, K. Suleman,
ingwithneuralnetworks.NeurIPS. J. Hoffman, S. Jarrett, K. R. and F. Diaz. 2018. On the eval-
Sadagopan, D. Rowe, S. Spruit, uation of common-sense reasoning
Sweet,H.1877. AHandbookofPho- C. Tran, P. Andrews, N. F. Ayan, in natural language understanding.
netics.ClarendonPress. S. Bhosale, S. Edunov, A. Fan, NeurIPS 2018 Workshop on Cri-
Swerts,M.,D.J.Litman,andJ.Hirsch- C. Gao, V. Goswami, F. Guzma´n, tiquing and Correcting Trends in
berg. 2000. Corrections in spoken P. Koehn, A. Mourachko, C. Rop- MachineLearning.
dialoguesystems.ICSLP. ers, S. Saleem, H. Schwenk, and Trnka, K., D. Yarrington, J. McCaw,
J. Wang. 2022. No language left
Swier,R.andS.Stevenson.2004. Un- behind: Scaling human-centered K. F. McCoy, and C. Pennington.
supervised semantic role labelling. machinetranslation. 2007. The effects of word pre-
EMNLP. diction on communication rate for
Teranishi,R.andN.Umeda.1968.Use AAC.NAACL-HLT.
Switzer,P.1965.Vectorimagesindoc- ofpronouncingdictionaryinspeech
Tsvetkov, Y., N. Schneider, D. Hovy,
umentretrieval. StatisticalAssocia- synthesisexperiments. 6thInterna-
A.Bhatia,M.Faruqui,andC.Dyer.
tionMethodsForMechanizedDocu- tionalCongressonAcoustics.
2014.AugmentingEnglishadjective
mentation.SymposiumProceedings. Tesnie`re,L.1959.E´le´mentsdeSyntaxe senseswithsupersenses.LREC.
Washington, D.C., USA,March17,
Structurale. Librairie C. Klinck-
1964. https://nvlpubs.nist. Turian,J.P.,L.Shen,andI.D.Mela-
sieck,Paris.
gov/nistpubs/Legacy/MP/ med.2003. Evaluationofmachine
nbsmiscellaneouspub269.pdf. Tetreault, J.R.2001. Acorpus-based translationanditsevaluation. Pro-
evaluationofcenteringandpronoun ceedingsofMTSummitIX.
Syrdal, A. K., C. W. Wightman, resolution. ComputationalLinguis-
Turian, J., L. Ratinov, and Y. Bengio.
A. Conkie, Y. Stylianou, M. Beut- tics,27(4):507–520.
2010. Wordrepresentations: asim-
nagel, J. Schroeter, V. Strom, and
Teufel, S., J.Carletta, andM.Moens. ple and general method for semi-
K.-S. Lee. 2000. Corpus-based
1999. An annotation scheme for supervisedlearning.ACL.
techniquesintheAT&TNEXTGEN
discourse-levelargumentationinre-
synthesissystem.ICSLP. Turney, P. D. 2002. Thumbs up or
searcharticles.EACL.
thumbs down? Semantic orienta-
Talbot, D. and M. Osborne. 2007. Teufel, S., A. Siddharthan, and tionappliedtounsupervisedclassi-
Smoothed Bloom filter language C. Batchelor. 2009. Towards ficationofreviews.ACL.
models: Tera-scale LMs on the domain-independent argumenta-
Turney, P. D. and M. Littman. 2003.
cheap.EMNLP/CoNLL. tive zoning: Evidence from chem-
Measuringpraiseandcriticism: In-
Talmor, A. and J. Berant. 2018. The istry and computational linguistics. ferenceofsemanticorientationfrom
web as a knowledge-base for an- EMNLP. association. ACM Transactions
sweringcomplexquestions.NAACL Thede,S.M.andM.P.Harper.1999.A on Information Systems (TOIS),
HLT. second-orderhiddenMarkovmodel 21:315–346.
forpart-of-speechtagging.ACL.
Talmy, L. 1985. Lexicalization pat- Turney,P.D.andM.L.Littman.2005.
terns: Semanticstructureinlexical Thompson,B.andP.Koehn.2019. Ve- Corpus-based learning of analogies
forms. In T. Shopen, editor, Lan- calign: Improved sentence align- and semantic relations. Machine
guage Typology and Syntactic De- ment in linear time and space. Learning,60(1-3):251–278.
scription,Volume3.CambridgeUni- EMNLP. Umeda, N.1976. Linguisticrulesfor
versityPress.Originallyappearedas Thompson, K. 1968. Regular ex- text-to-speech synthesis. Proceed-
UCBerkeleyCognitiveSciencePro- pression search algorithm. CACM, ingsoftheIEEE,64(4):443–451.
gramReportNo.30,1980. 11(6):419–422.
Umeda, N., E. Matui, T. Suzuki, and
Talmy,L.1991. Pathtorealization: A Tian, Y., V. Kulkarni, B. Perozzi, H.Omura.1968. Synthesisoffairy
typologyofeventconflation. BLS- and S. Skiena. 2016. On the taleusingananalogvocaltract. 6th
91. convergent properties of word em- International Congress on Acous-
bedding methods. ArXiv preprint tics.
Tan, C., V. Niculae, C. Danescu-
arXiv:1605.03956.
Niculescu-Mizil, andL.Lee.2016. Uryupina, O., R. Artstein, A. Bristot,
Winningarguments: Interactiondy- Tibshirani, R. J. 1996. Regression F. Cavicchio, F. Delogu, K. J. Ro-
namics and persuasion strategies shrinkageandselectionviathelasso. driguez,andM.Poesio.2020. An-
in good-faith online discussions. JournaloftheRoyalStatisticalSo- notatingabroadrangeofanaphoric
WWW-16. ciety. Series B (Methodological), phenomena, in a variety of genres:
58(1):267–288. TheARRAUcorpus. NaturalLan-
Tannen, D.1979. What’sinaframe?
Titov,I.andE.Khoddam.2014. Unsu- guageEngineering,26(1):1–34.
Surfaceevidenceforunderlyingex-
pervisedinductionofsemanticroles van Deemter, K. and R. Kibble.
pectations. In R. Freedle, editor,
within a reconstruction-error mini- 2000. On coreferring: corefer-
New Directions in Discourse Pro-
mizationframework.NAACLHLT. enceinMUCandrelatedannotation
cessing,pages137–181.Ablex.
Titov, I. and A. Klementiev. 2012. A schemes. Computational Linguis-
Taylor,P.2009. Text-to-SpeechSynthe- Bayesian approach to unsupervised tics,26(4):629–637.
sis.CambridgeUniversityPress. semanticroleinduction.EACL. van der Maaten, L. and G. E. Hinton.
Taylor,W.L.1953.Clozeprocedure:A Tomkins, S.S.1962. Affect, imagery, 2008. Visualizinghigh-dimensional
new tool for measuring readability. consciousness: Vol. I. The positive data using t-SNE. JMLR, 9:2579–
JournalismQuarterly,30:415–433. affects.Springer. 2605.616 Bibliography
vanRijsbergen,C.J.1975.Information Voorhees,E.M.1999. TREC-8ques- Wang,A.,A.Singh,J.Michael,F.Hill,
Retrieval.Butterworths. tion answering track report. Pro- O.Levy,andS.R.Bowman.2018a.
ceedings of the 8th Text Retrieval Glue: Amulti-taskbenchmarkand
Vaswani, A., N. Shazeer, N. Parmar,
Conference. analysis platform for natural lan-
J.Uszkoreit,L.Jones,A.N.Gomez,
guageunderstanding.ICLR.
Ł.Kaiser, andI.Polosukhin.2017. Voorhees, E. M. and D. K. Harman.
Attentionisallyouneed.NeurIPS. 2005. TREC: Experiment and Wang, S. and C. D. Manning. 2012.
EvaluationinInformationRetrieval. Baselines and bigrams: Simple,
Vauquois, B. 1968. A survey of for-
MITPress. goodsentimentandtopicclassifica-
mal grammars and algorithms for
tion.ACL.
recognition and transformation in Vossen, P., A. Go¨ro¨g, F. Laan,
machinetranslation. IFIPCongress M.Van Gompel, R.Izquierdo, and Wang,W.andB.Chang.2016. Graph-
1968. A. Van Den Bosch. 2011. Dutch- baseddependencyparsingwithbidi-
semcor: building a semantically rectionalLSTM.ACL.
Velichko,V.M.andN.G.Zagoruyko.
annotated corpus for dutch. Pro-
1970. Automatic recognition of Wang, Y., S. Li, and J. Yang. 2018b.
ceedingsofeLex.
200words. InternationalJournalof Towardfastandaccurateneuraldis-
Man-MachineStudies,2:223–234. Voutilainen, A. 1999. Handcrafted coursesegmentation.EMNLP.
Velikovich, L., S. Blair-Goldensohn, rules. In H. van Halteren, editor, Wang, Y., R. Skerry-Ryan, D. Stan-
K.Hannan,andR.McDonald.2010. SyntacticWordclassTagging,pages ton, Y. Wu, R. J. Weiss, N. Jaitly,
Theviabilityofweb-derivedpolarity 217–246.Kluwer. Z.Yang,Y.Xiao,Z.Chen,S.Ben-
lexicons.NAACLHLT. Vrandecˇic´,D.andM.Kro¨tzsch.2014. gio, Q. Le, Y. Agiomyrgiannakis,
Wikidata: a free collaborative R. Clark, and R. A. Saurous.
Vendler,Z.1967.LinguisticsinPhilos- knowledgebase.CACM,57(10):78– 2017.Tacotron:Towardsend-to-end
ophy.CornellUniversityPress. 85. speechsynthesis.INTERSPEECH.
Verhagen, M., R. Gaizauskas, Wade, E., E.Shriberg, andP.J.Price. Watanabe, S., T. Hori, S. Karita,
F.Schilder,M.Hepple,J.Moszkow- 1992. User behaviors affecting T. Hayashi, J. Nishitoba, Y. Unno,
icz, andJ.Pustejovsky.2009. The speechrecognition.ICSLP. N. E. Y. Soplin, J. Heymann,
TempEval challenge: Identifying M.Wiesner,N.Chen,A.Renduch-
temporal relations in text. Lan- Wagner,R.A.andM.J.Fischer.1974. intala, and T. Ochiai. 2018. ESP-
guage Resources and Evaluation, Thestring-to-stringcorrectionprob- net: End-to-end speech processing
43(2):161–179. lem. JournaloftheACM,21:168– toolkit.INTERSPEECH.
173.
Verhagen, M., I. Mani, R. Sauri, Weaver, W. 1949/1955. Transla-
R.Knippen,S.B.Jang,J.Littman, Waibel, A., T. Hanazawa, G. Hin- tion. In W. N. Locke and A. D.
A. Rumshisky, J. Phillips, and ton, K. Shikano, and K. J. Lang. Boothe, editors, Machine Transla-
J. Pustejovsky. 2005. Automating 1989. Phoneme recognition using tion of Languages, pages 15–23.
temporal annotation with TARSQI. time-delay neural networks. IEEE MITPress. Reprintedfromamem-
ACL. transactions on Acoustics, Speech, orandumwrittenbyWeaverin1949.
and Signal Processing, 37(3):328–
Versley, Y. 2008. Vagueness and ref- 339. Webber, B. L. 1978. A Formal
erential ambiguity in a large-scale Approach to Discourse Anaphora.
annotated corpus. Research on Walker, M. A. 2000. An applica- Ph.D.thesis,HarvardUniversity.
Language and Computation, 6(3- tionofreinforcementlearningtodi-
4):333–353. alogue strategy selection in a spo- Webber, B.L.1983. Sowhatcanwe
kendialoguesystemforemail.JAIR, talkaboutnow? InM.Bradyand
Vieira,R.andM.Poesio.2000.Anem- 12:387–416. R. C. Berwick, editors, Computa-
pirically based system for process- tional Models of Discourse, pages
ingdefinitedescriptions. Computa- Walker,M.A.,J.C.Fromer,andS.S. 331–371.TheMITPress.
tionalLinguistics,26(4):539–593. Narayanan.1998a.Learningoptimal
dialoguestrategies: Acasestudyof Webber,B.L.1991. Structureandos-
Vijayakumar,A.K.,M.Cogswell,R.R. a spoken dialogue agent for email. tension in the interpretation of dis-
Selvaraju,Q.Sun,S.Lee,D.Cran- COLING/ACL. coursedeixis.LanguageandCogni-
dall, and D. Batra. 2018. Diverse tiveProcesses,6(2):107–135.
Walker, M. A., M. Iida, and S. Cote.
beamsearch: Decodingdiverseso- 1994. Japanese discourse and the Webber, B. L. and B. Baldwin. 1992.
lutions from neural sequence mod- processofcentering.Computational Accommodating context change.
els.AAAI. Linguistics,20(2):193–232. ACL.
Vilain, M., J. D. Burger, J. Aberdeen, Webber, B. L., M. Egg, and V. Kor-
Walker, M. A., A. K. Joshi, and
D. Connolly, and L. Hirschman. doni.2012. Discoursestructureand
E. Prince, editors. 1998b. Center-
1995.Amodel-theoreticcoreference languagetechnology. NaturalLan-
inginDiscourse.OxfordUniversity
scoringscheme.MUC-6. guageEngineering,18(4):437–490.
Press.
Vintsyuk,T.K.1968. Speechdiscrim- Webber,B.L.1988. Discoursedeixis:
Walker,M.A.,C.A.Kamm,andD.J.
ination by dynamic programming. Reference to discourse segments.
Litman. 2001. Towards develop-
Cybernetics, 4(1):52–57. Russian ACL.
inggeneralmodelsofusabilitywith
Kibernetika4(1):81-88.1968.
PARADISE. NaturalLanguageEn- Webster, K., M. Recasens, V. Axel-
Vinyals, O., Ł. Kaiser, T. Koo, gineering: Special Issue on Best rod, andJ.Baldridge.2018. Mind
S.Petrov,I.Sutskever,andG.Hin- Practice in Spoken Dialogue Sys- theGAP:Abalancedcorpusofgen-
ton.2015.Grammarasaforeignlan- tems,6(3):363–377. deredambiguouspronouns. TACL,
guage.NeurIPS. 6:605–617.
Walker,M.A.andS.Whittaker.1990.
Vinyals, O. and Q. V. Le. 2015. A Mixedinitiativeindialogue: Anin- Weinschenk,S.andD.T.Barker.2000.
neuralconversationalmodel. ICML vestigationintodiscoursesegmenta- Designing Effective Speech Inter-
DeepLearningWorkshop. tion.ACL. faces.Wiley.Bibliography 617
Weischedel, R., M. Meteer, Wilks, Y. 1973. An artificial intelli- Wittgenstein, L. 1953. Philosoph-
R. Schwartz, L. A. Ramshaw, and genceapproachtomachinetransla- ical Investigations. (Translated by
J.Palmucci.1993. Copingwitham- tion. In R. C. Schank and K. M. Anscombe,G.E.M.).Blackwell.
biguityandunknownwordsthrough Colby,editors,ComputerModelsof Wolf, F. and E. Gibson. 2005. Rep-
probabilisticmodels.Computational ThoughtandLanguage,pages114–
resenting discourse coherence: A
Linguistics,19(2):359–382. 151.W.H.Freeman. corpus-based analysis. Computa-
Weizenbaum, J. 1966. ELIZA – A Wilks,Y.1975a.Anintelligentanalyzer tionalLinguistics,31(2):249–287.
computer program for the study of andunderstanderofEnglish.CACM, Wolf, M. J., K. W. Miller, and F. S.
naturallanguagecommunicationbe- 18(5):264–274. Grodzinsky.2017. Whyweshould
tween man and machine. CACM, Wilks, Y. 1975b. Preference seman- have seen that coming: Comments
9(1):36–45. tics. In E. L. Keenan, editor, The on Microsoft’s Tay “experiment,”
Weizenbaum,J.1976.ComputerPower Formal Semantics of Natural Lan- andwiderimplications. TheORBIT
and Human Reason: From Judge- guage, pages 329–350. Cambridge Journal,1(2):1–12.
menttoCalculation. W.H.Freeman Univ.Press. Wolfson, T., M. Geva, A. Gupta,
andCompany. Wilks, Y. 1975c. A preferential, M. Gardner, Y. Goldberg,
Wells,J.C.1982. AccentsofEnglish. pattern-seeking,semanticsfornatu- D. Deutch, and J. Berant. 2020.
CambridgeUniversityPress. rallanguageinference. ArtificialIn- Break it down: A question under-
Wen, T.-H., M. Gasˇic´, D. Kim, telligence,6(1):53–74. standingbenchmark. TACL,8:183–
N. Mrksˇic´, P.-H. Su, D. Vandyke, 198.
Williams,A.,N.Nangia,andS.Bow-
andS.J.Young.2015a. Stochastic Woods, W. A. 1967. Semantics for a
man.2018. Abroad-coveragechal-
languagegenerationindialogueus- Question-AnsweringSystem. Ph.D.
lenge corpus for sentence under-
ing recurrent neural networks with thesis,HarvardUniversity.
standingthroughinference. NAACL
convolutional sentence reranking. HLT. Woods, W. A. 1973. Progress in nat-
SIGDIAL. ural language understanding. Pro-
Williams, J. D., K. Asadi, and
Wen, T.-H., M. Gasˇic´, N. Mrksˇic´, P.- ceedingsofAFIPSNationalConfer-
G. Zweig. 2017. Hybrid code
H.Su,D.Vandyke,andS.J.Young. ence.
networks: practical and efficient
2015b. Semantically conditioned end-to-end dialog control with su- Woods,W.A.1975. What’sinalink:
LSTM-basednaturallanguagegen- pervisedandreinforcementlearning. Foundationsforsemanticnetworks.
erationforspokendialoguesystems. ACL. InD.G.BobrowandA.M.Collins,
EMNLP. editors, Representation and Under-
Williams,J.D.,A.Raux,andM.Hen-
Werbos, P. 1974. Beyond regression: standing: StudiesinCognitiveSci-
derson.2016.Thedialogstatetrack-
newtoolsforpredictionandanaly- ence,pages35–82.AcademicPress.
ingchallengeseries:Areview. Dia-
sisinthebehavioralsciences.Ph.D.
logue&Discourse,7(3):4–33. Woods, W. A. 1978. Semantics
thesis,HarvardUniversity. and quantification in natural lan-
Werbos, P. J. 1990. Backpropagation Williams,J.D.andS.J.Young.2007. guage question answering. In
throughtime: whatitdoesandhow Partially observable markov deci- M.Yovits,editor,AdvancesinCom-
todoit. ProceedingsoftheIEEE, sionprocessesforspokendialogsys- puters,pages2–64.Academic.
78(10):1550–1560. gte um ags. e,2C 1o (m 1)p :3u 9te 3r –4S 2p 2e .ech and Lan- Woods,W.A.,R.M.Kaplan,andB.L.
Weston, J., S.Chopra, andA.Bordes. Nash-Webber.1972. Thelunarsci-
2015. Memory networks. ICLR Wilson,T.,J.Wiebe,andP.Hoffmann. ences natural language information
2015. 2005.Recognizingcontextualpolar- system: Finalreport. TechnicalRe-
Widrow, B. and M. E. Hoff. 1960. ityinphrase-levelsentimentanaly- port2378,BBN.
sis.EMNLP.
Adaptive switching circuits. IRE Woodsend, K. and M. Lapata. 2015.
WESCON Convention Record, vol- Winograd,T.1972.UnderstandingNat- Distributed representations for un-
ume4. uralLanguage.AcademicPress. supervised semantic role labeling.
Wiebe,J.1994. Trackingpointofview Winston,P.H.1977. ArtificialIntelli- EMNLP.
innarrative.ComputationalLinguis- gence.AddisonWesley. Wu,D.1996. Apolynomial-timealgo-
tics,20(2):233–287. rithmforstatisticalmachinetransla-
Wiseman, S., A. M. Rush, and S. M.
Wiebe,J.2000.Learningsubjectivead- Shieber. 2016. Learning global
tion.ACL.
jectivesfromcorpora.AAAI. features for coreference resolution. Wu, F. and D. S. Weld. 2007. Au-
Wiebe,J.,R.F.Bruce,andT.P.O’Hara. NAACLHLT. tonomously semantifying Wiki-
1999. Development and use of a Wiseman, S., A. M. Rush, S. M.
pedia.CIKM-07.
gold-standarddatasetforsubjectiv- Shieber,andJ.Weston.2015.Learn- Wu, F. and D. S. Weld. 2010. Open
ityclassifications.ACL. ing anaphoricity and antecedent information extraction using Wiki-
Wierzbicka,A.1992. Semantics,Cul- rankingfeaturesforcoreferenceres- pedia.ACL.
ture,andCognition:UniversityHu- olution.ACL. Wu, L., F. Petroni, M. Josifoski,
man Concepts in Culture-Specific Witten, I. H. and T. C. Bell. 1991. S.Riedel,andL.Zettlemoyer.2020.
Configurations. Oxford University The zero-frequency problem: Es- Scalable zero-shot entity linking
Press. timating the probabilities of novel withdenseentityretrieval.
Wierzbicka, A. 1996. Semantics: eventsinadaptivetextcompression. Wu, S. and M. Dredze. 2019. Beto,
PrimesandUniversals.OxfordUni- IEEE Transactions on Information Bentz,Becas: Thesurprisingcross-
versityPress. Theory,37(4):1085–1094. lingual effectiveness of BERT.
Wilensky, R. 1983. Planning and Witten,I.H.andE.Frank.2005. Data EMNLP.
Understanding: A Computational Mining: Practical Machine Learn- Wu, Y., M. Schuster, Z. Chen, Q. V.
Approach to Human Reasoning. ingToolsandTechniques, 2ndedi- Le, M. Norouzi, W. Macherey,
Addison-Wesley. tion.MorganKaufmann. M. Krikun, Y. Cao, Q. Gao,618 Bibliography
K.Macherey,J.Klingner,A.Shah, Yankelovich, N., G.-A. Levow, and Zettlemoyer, L.andM.Collins.2007.
M. Johnson, X. Liu, Ł. Kaiser, M. Marx. 1995. Designing Online learning of relaxed CCG
S. Gouws, Y. Kato, T. Kudo, SpeechActs: Issues in speech user grammars for parsing to logical
H.Kazawa, K.Stevens, G.Kurian, interfaces.CHI-95. form.EMNLP/CoNLL.
N. Patil, W. Wang, C. Young, Yarowsky,D.1995.Unsupervisedword Zhang, H., R. Sproat, A. H. Ng,
J. Smith, J. Riesa, A. Rud- sensedisambiguationrivalingsuper- F. Stahlberg, X. Peng, K. Gorman,
nick, O. Vinyals, G. S. Corrado, visedmethods.ACL. andB.Roark.2019. Neuralmodels
M. Hughes, and J. Dean. 2016. oftextnormalizationforspeechap-
Google’sneuralmachinetranslation Yasseri, T., A.Kornai, andJ.Kerte´sz. plications. ComputationalLinguis-
system: Bridging the gap between 2012. Apracticalapproachtolan- tics,45(2):293–337.
human and machine translation. guagecomplexity:aWikipediacase
Zhang, R., C. N. dos Santos, M. Ya-
ArXivpreprintarXiv:1609.08144. study.PLoSONE,7(11).
sunaga, B. Xiang, and D. Radev.
Wundt, W. 1900. Vo¨lkerpsychologie: Yih, W.-t., M. Richardson, C. Meek, 2018. Neuralcoreferenceresolution
eine Untersuchung der Entwick- M.-W.Chang,andJ.Suh.2016.The withdeepbiaffineattentionbyjoint
lungsgesetze von Sprache, Mythus, valueofsemanticparselabelingfor mentiondetectionandmentionclus-
undSitte. W.Engelmann, Leipzig. knowledgebasequestionanswering. tering.ACL.
BandII:DieSprache,ZweiterTeil. ACL. Zhang, T., V. Kishore, F. Wu, K. Q.
Xu, J., D. Ju, M. Li, Y.-L. Boureau, Young, S. J., M. Gasˇic´, S. Keizer, Weinberger, and Y. Artzi. 2020.
J. Weston, and E. Dinan. 2020. F. Mairesse, J. Schatzmann, BERTscore: Evaluating text gener-
Recipes for safety in open- B.Thomson,andK.Yu.2010. The ationwithBERT.ICLR2020.
domain chatbots. ArXiv preprint Hidden Information State model: Zhang,Y.,V.Zhong,D.Chen,G.An-
arXiv:2010.07079. ApracticalframeworkforPOMDP- geli, and C. D. Manning. 2017.
basedspokendialoguemanagement. Position-aware attention and su-
Xu,P.,H.Saghir,J.S.Kang,T.Long,
Computer Speech & Language, pervised data improve slot filling.
A.J.Bose,Y.Cao,andJ.C.K.Che-
24(2):150–174. EMNLP.
ung.2019.Across-domaintransfer-
ableneuralcoherencemodel.ACL. Younger,D.H.1967. Recognitionand Zhao,H.,W.Chen,C.Kit,andG.Zhou.
parsingofcontext-freelanguagesin 2009. Multilingual dependency
Xu,Y.2005. Speechmelodyasartic-
ulatorily implemented communica- time n3. Information and Control, learning: A huge feature engineer-
tivefunctions. Speechcommunica- 10:189–208. ingmethodtosemanticdependency
parsing.CoNLL.
tion,46(3-4):220–251. Yu,M.andM.Dredze.2014. Improv-
Zhao,J.,T.Wang,M.Yatskar,R.Cot-
Xue, N., H. T. Ng, S. Pradhan, inglexicalembeddingswithseman- terell,V.Ordonez,andK.-W.Chang.
A. Rutherford, B. L. Webber, ticknowledge.ACL. 2019. Genderbiasincontextualized
C. Wang, and H. Wang. 2016. Yu, N., M. Zhang, and G. Fu. 2018. wordembeddings.NAACLHLT.
CoNLL 2016 shared task on mul- Transition-basedneuralRSTparsing Zhao, J., T.Wang, M.Yatskar, V.Or-
tilingual shallow discourse parsing. withimplicitsyntaxfeatures. COL- donez,andK.-W.Chang.2017.Men
CoNLL-16sharedtask. ING. alsolikeshopping:Reducinggender
Xue,N.andM.Palmer.2004.Calibrat- Yu, Y., Y. Zhu, Y. Liu, Y. Liu, biasamplificationusingcorpus-level
ingfeaturesforsemanticrolelabel- S. Peng, M. Gong, and A. Zeldes. constraints.EMNLP.
ing.EMNLP. 2019.GumDropattheDISRPT2019 Zhao, J., T.Wang, M.Yatskar, V.Or-
Yamada, H. and Y. Matsumoto. 2003. shared task: A model stacking ap- donez, and K.-W. Chang. 2018a.
Statisticaldependencyanalysiswith proach to discourse unit segmenta- Gender bias in coreference reso-
supportvectormachines.IWPT-03. tionandconnectivedetection.Work- lution: Evaluation and debiasing
shoponDiscourseRelationParsing methods.NAACLHLT.
Yan,Z.,N.Duan,J.-W.Bao,P.Chen,
andTreebanking2019. Zhao, J., Y. Zhou, Z. Li, W. Wang,
M.Zhou,Z.Li,andJ.Zhou.2016.
DocChat: An information retrieval Zapirain, B., E. Agirre, L. Ma`rquez, and K.-W. Chang. 2018b. Learn-
approach for chatbot engines using andM.Surdeanu.2013. Selectional ing gender-neutral word embed-
unstructureddocuments.ACL. preferencesforsemanticroleclassi- dings.EMNLP.
fication. ComputationalLinguistics, Zheng, J., L. Vilnis, S. Singh, J. D.
Yang,D.,J.Chen,Z.Yang,D.Jurafsky,
39(3):631–663. Choi, and A. McCallum. 2013.
andE.H.Hovy.2019. Let’smake
Dynamicknowledge-basealignment
yourrequestmorepersuasive:Mod- Zelle, J. M. and R. J. Mooney. 1996.
forcoreferenceresolution.CoNLL.
elingpersuasivestrategiesviasemi- Learning to parse database queries
supervised neural nets on crowd- usinginductivelogicprogramming. Zhong,Z.andH.T.Ng.2010.Itmakes
fundingplatforms.NAACLHLT. AAAI. sense: Awide-coveragewordsense
disambiguationsystemforfreetext.
Yang,X.,G.Zhou,J.Su,andC.L.Tan. Zeman,D.2008. Reusabletagsetcon- ACL.
2003. Coreference resolution us- versionusingtagsetdrivers.LREC.
Zhou, D., O. Bousquet, T. N. Lal,
ing competition learning approach.
Zens, R. and H. Ney. 2007. Efficient J.Weston,andB.Scho¨lkopf.2004a.
ACL.
phrase-table representation for ma- Learningwithlocalandglobalcon-
Yang,Y.andJ.Pedersen.1997.Acom- chinetranslationwithapplicationsto sistency.NeurIPS.
parativestudyonfeatureselectionin online MT and speech translation. Zhou, G., J. Su, J. Zhang, and
textcategorization.ICML. NAACL-HLT. M. Zhang. 2005. Exploring var-
Yang, Z., P. Qi, S. Zhang, Y. Ben- Zettlemoyer, L.andM.Collins.2005. ious knowledge in relation extrac-
gio, W. Cohen, R. Salakhutdinov, Learning to map sentences to log- tion.ACL.
and C. D. Manning. 2018. Hot- ical form: Structured classification Zhou, J. and W. Xu. 2015a. End-to-
potQA: A dataset for diverse, ex- with probabilistic categorial gram- endlearningofsemanticrolelabel-
plainable multi-hop question an- mars.UncertaintyinArtificialIntel- ingusingrecurrentneuralnetworks.
swering.EMNLP. ligence,UAI’05. ACL.Bibliography 619
Zhou, J. and W. Xu. 2015b. End-to-
endlearningofsemanticrolelabel-
ingusingrecurrentneuralnetworks.
ACL.
Zhou, L., J. Gao, D. Li, and H.-Y.
Shum.2020. Thedesignandimple-
mentationofXiaoIce,anempathetic
socialchatbot. ComputationalLin-
guistics,46(1):53–93.
Zhou, L., M. Ticrea, and E. H. Hovy.
2004b. Multi-document biography
summarization.EMNLP.
Zhou,Y.andN.Xue.2015. TheChi-
neseDiscourseTreeBank:aChinese
corpusannotatedwithdiscoursere-
lations. Language Resources and
Evaluation,49(2):397–431.
Zhu, X. and Z. Ghahramani. 2002.
Learningfromlabeledandunlabeled
datawithlabelpropagation.Techni-
calReportCMU-CALD-02,CMU.
Zhu, X., Z. Ghahramani, and J. Laf-
ferty.2003. Semi-supervisedlearn-
ing using gaussian fields and har-
monicfunctions.ICML.
Zhu, Y., R. Kiros, R. Zemel,
R. Salakhutdinov, R. Urtasun,
A. Torralba, and S. Fidler. 2015.
Aligning books and movies: To-
wardsstory-likevisualexplanations
by watching movies and reading
books. IEEEInternationalConfer-
enceonComputerVision.
Ziemski, M., M. Junczys-Dowmunt,
andB.Pouliquen.2016.TheUnited
Nationsparallelcorpusv1.0.LREC.
Zue,V.W.,J.Glass,D.Goodine,H.Le-
ung, M. Phillips, J. Polifroni, and
S. Seneff. 1989. Preliminary eval-
uationoftheVOYAGERspokenlan-
guagesystem. SpeechandNatural
LanguageWorkshop.Subject Index
λ-reduction,415 adjacencypairs,299 wordsense,465 BabelNet,471
*?,9 Adjectives,162 AmericanStructuralism, backoff
+?,9 adverb,162 378 insmoothing,46
.wavformat,575 degree,162 amplitude backprop,151
10-foldcross-validation,71 directional,162 ofasignal,573 backpropagationthrough
(derives),359 locative,162 RMS,576 time,188
→
ˆ,60 manner,162 anaphor,517 backtrace
*(REKleene*),7 temporal,162 anaphora,517 inminimumedit
+(REKleene+),7 Adverbs,162 anaphoricitydetector,526 distance,28
.(REanycharacter),7 adversarialevaluation,322 anchortexts,283,292 backtranslation,258
$(REend-of-line),8 AED,336 anchorsinregular Backus-Naurform,358
((REprecedencesymbol), affective,496 expressions,7,29 backwardchaining,417
8 affix,23 antecedent,517 backward-lookingcenter,
[(REcharacter affricatesound,569 antonym,460 553
disjunction),6 agent,asthematicrole,477 AppleAIFF,575 bagofwords,60,61
\B (REnon agglomerativeclustering, approximantsound,569 inIR,270
word-boundary),8 472 approximate bakeoff,352
\b (REword-boundary),8 agglutinative randomization,73 speechrecognition
](REcharacter language,252 Arabic,565 competition,352
disjunction),6 AIFFfile,575 Egyptian,584 barge-in,323
ˆ(REstart-of-line),8 AISHELL-1,332 Aramaic,565 baseline
[ˆ](single-charnegation), aktionsart,449 ARC,293 mostfrequentsense,466
6 ALGOL,379 arceager,392 takethefirstsense,466
(thereexists),413 algorithm arcstandard,386 basicemotions,497
∃
(forall),413 byte-pairencoding,22 argumentationmining,559 batchtraining,96
∀
= (implies),416 CKY,367 argumentationschemes, Bayes’rule,60
⇒
λ-expressions,415 Kneser-Neydiscounting, 560 droppingdenominator,
λ-reduction,415 49 argumentativerelations, 61,170
(and),413 Lesk,469 559 Bayesianinference,60
∧
(not),413 minimumeditdistance, argumentativezoning,561 BDI,327
¬
(or),416 27 Aristotle,160,449 beamsearch,222,394
∨
4-gram,36 naiveBayesclassifier,59 arity,419 beamwidth,222,394
4-tuple,362 pointwisemutual ARPA,352 bearpitchaccent,571
5-gram,36 information,116 ARPAbet,585 BerkeleyRestaurant
semanticrolelabeling, article(part-of-speech),162 Project,35
A-Dconversion,333,574 484 articulatoryphonetics,566, BernoullinaiveBayes,77
AAC,32 SimplifiedLesk,469 566 BERT
AAE,15 TextTiling,556 articulatorysynthesis,354 foraffect,512
ABtest,350 unsupervisedwordsense aspect,449 best-worstscaling,501
abduction,418 disambiguation,472 ASR,329 biasamplification,128
ABox,420 Viterbi,171 confidence,318 biasterm,81,135
ABSITY,473 alignment,25,339 association,105 bidirectionalRNN,196
Absolutediscounting,50 inASR,343 ATIS bigram,33
absolutetemporal minimumcost,27 corpus,360 bilabial,568
expression,452 oftranscript,584 ATN,493 binarybranching,364
abstractword,500 string,25 ATRANS,492 binarynaiveBayes,65
accentedsyllables,571 viaminimumedit attachmentambiguity,366 binarytree,364
accessible,521 distance,27 attention BIO,166
accessingareferent,516 all-wordstaskinWSD,465 cross-attention,257 BIOtagging
accomplishment Allenrelations,447 encoder-decoder,257 forNER,166
expressions,450 allocationalharm,128 historyintransformers, BIOES,166
accuracy,164 alveolarsound,568 227 bitext,254
achievementexpressions, ambiguity
attentionmechanism,206
bitsformeasuringentropy,
Attribution(ascoherence
450 amountofpart-of-speech 52
acknowledgmentspeech inBrowncorpus,
relation),546
blankinCTC,339
augmentative
act,298 164 Bloomfilters,48
activation,135 attachment,366
communication,32
BM25,271,273
activityexpressions,450 coordination,366
authorshipattribution,58
BNF(Backus-Naurform),
autoregressivegeneration,
acute-eval,321 inmeaning 358
adhocretrieval,270 representations,406
194,220
bootstrap,75
Auxiliary,163
addgate,199 ofreferringexpressions, bootstrapalgorithm,75
add-k,46 518 bootstraptest,73
add-onesmoothing,44 part-of-speech,163 B3,535 bootstrapping,73
adequacy,260 resolutionoftag,164 Babbage,C.,330 inIE,435
621622 SubjectIndex
boundpronoun,519 clustering weakandstrong 10-fold,71
boundarytones,573 inwordsense equivalence,364 crowdsourcing,500
BPE,21 disambiguation,474 contextualembeddings,236 CTC,338
BPE,22 CNF,seeChomskynormal continuationrise,573 currying,415
bracketednotation,361 form conversation,296 cyclesinawave,573
bridginginference,521 coarsesenses,474 conversationanalysis,327 cyclespersecond,573
broadcastnews cochlea,581 conversationalagents,296
speechrecognitionof, Cocke-Kasami-Younger conversationalanalysis,299
352 algorithm,seeCKY conversationalimplicature, datasheet,16
Browncorpus,13 coda,syllable,570 300 date
originaltaggingof,182 codeswitching,15 conversationalspeech,331 normalization,311
byte-pairencoding,21 coherence,543 convex,91 dativealternation,479
entity-based,552 coordinationambiguity,366 DBpedia,287
C CA anL dL idH eO ,2M 66E,331 cohre el sa it oio nns,545 c Co Op Rul Aa, A1 L6 ,3
331
d de eb cii sa is oin ng b, o1 u2 n9
dary,82,138
canonicalform,407 lexical,544,556 corefer,516 decisiontree
Cantonese,251 coldlanguages,252 coreferencechain,517 useinWSD,474
capturegroup,12 collectioninIR,270 coreferenceresolution,517 decoding,169
cascade collocation,468 genderagreement,523 Viterbi,169
regularexpressionin commissivespeechact,298 Hobbstreesearch deduction
ELIZA,13 commonground,298,327 algorithm,539 inFOL,417
case Commonnouns,161 numberagreement,522 deep
sensitivityinregular complementizers,162 personagreement,523 neuralnetworks,134
expressionsearch,5 completenessinFOL,418 recencypreferences,523 deeplearning,134
casefolding,23 componentialanalysis,492 selectionalrestrictions, deeprole,477
caseframe,478,493 compression,574 524 definitereference,519
CAT,247 ComputationalGrammar syntactic(“binding”) degreeadverb,162
cataphora,519 Coder(CGC),182 constraints,523 delexicalization,319
CD(conceptual computationalsemantics, verbsemantics,524 denotation,409
dependency),492 405 coronalsound,568 dentalsound,568
CELEX,584 concatenation,5,29 corpora,13 dependency
CenteringTheory,544,552 concepterrorrate,323 corpus,13 grammar,381
centroid,119 conceptualdependency,492 ATIS,360 dependencytree,384
c Ce Fp h Gs it s ,r tu o sem ry e, c3 o5 n2
text-free
c cco oon nnc dco r ie tr itd oea nwn ac loe rr, ad ns ,e d5m o0 ma 0n fiti ec l, d4 ,65 B B Cr r Ao o Swad Snc , pa 1 hs 3t o,n n1e ew t8 i2 cs, o3 f52 d de ep r die iv rn a ed t cie ton n (t i, n3 a8 f2 ormal
grammar Mandarin,584 language),362
c ch ha ai nn neru lsle i, n1 s0 to0 r, e1 d52 confiden1 c7 e4 ,264 fi Ks ih eler o, f3 G52
erman,584
synta 3c 6ti 2c,359,359,362,
chartpaw rsa iv ne gf ,o 3r 6m 7s,575 A inS rR el, a3 ti1 o8
nextraction,437
L reO guB l, ar18 ex2
pression
d De es tc ,r 3ip 5t 8ionlogics,420
chatbots,4,300 confidencevalues,436 searchinginside,5 determiner,162,358
CHiME,331 configuration,387 Switchboard,14,304, Determiners,162
Chinese confusionmatrix,68 331,574,575 developmenttestset,70
asverb-framedlanguage, Conjunctions,162 TimeBank,451 developmenttestset
251 connectionist,159 TIMIT,584 (dev-test),37
characters,565 connotationframe,512 WallStreetJournal,352 devset,seedevelopment
wordsforbrother,250 connotationframes,495 correctionactdetection, testset(dev-test),70
ChirpyCardinal,308 connotations,106,497 316 DFT,334
Chomskynormalform,364 consonant,567 cosine dialogue,296
Chomsky-adjunction,365 constantsinFOL,412 asasimilaritymetric, dialogueact
chrF,261 constativespeechact,298 112 correction,316
CIRCUS,444 constituency,358 costfunction,89 dialogueacts,313
citationform,104 constituent,358 countnouns,161 dialoguemanager
CitizenKane,543 titleswhicharenot,357 counters,29 design,324
CKYalgorithm,357 ConstraintGrammar,403 counts dialoguepolicy,316
claims,559 contentplanning,319 treatinglowaszero,177 dialoguesystems,296
clarificationquestions,320 contextembedding,124 CRF,174 design,324
class-basedn-gram,56 context-freegrammar,358, comparedtoHMM,174 evaluation,321
classifierhead,237 362,377 inference,178 diathesisalternation,479
clefts,522 Chomskynormalform, Viterbiinference,178 diffprogram,30
clitic,19 364 CRFs digitrecognition,330
originofterm,160 inventionof,379 learning,179 digitaldivide,247
closedclass,161 non-terminalsymbol, cross-attention,257 digitization,333,574
closedvocabulary,43 359 cross-brackets,376 dilatedconvolutions,349
closure,stop,568 productions,358 cross-entropy,54 dimension,109
clozetask,232 rules,358 cross-entropyloss,90,150 diphthong,570
cluster,517 terminalsymbol,359 cross-validation,71 originofterm,160SubjectIndex 623
directderivation(ina fordeletedinterpolation, ofn-gramsvia (thereexists),413
∃
formallanguage), 47 perplexity,38 (forall),413
∀
362 embeddinglayer,148 pseudoword,491 = (implies),416
⇒
directionaladverb,162 embeddings,107 relationextraction,440 (and),413,416
∧
directivespeechact,298 cosineforsimilarity,112 testset,37 (not),413,416
¬
disambiguation skip-gram,learning,122 trainingonthetestset,37 (or),416
∨
inparsing,373 sparse,112 trainingset,37 andverifiability,411
syntactic,367 tf-idf,114 TTS,350 constants,412
discount,44,45,47 word2vec,119 unsupervisedWSD,472 expressivenessof,408,
discounting,44 emissionprobabilities,168 WSDsystems,466 411
discourse,543 EmoLex,499 eventcoreference,518 functions,412
segment,546 emotion,497 eventextraction,429,441 inferencein,411
discourseconnectives,547 Encoder-decoder,202 eventvariable,419 terms,412
discoursedeixis,518 encoder-decoderattention, events,449 variables,412
discoursemodel,516 257 representationof,418 fold(incross-validation),
discourseparsing,548 end-to-endtraining,193 Evidence(ascoherence 71
discourse-new,520 endpointing,298 relation),545 forgetgate,199
discourse-old,520 English evokingareferent,516 formallanguage,361
discoveryprocedure,378 lexicaldifferencesfrom expansion,360,361 formant,581
discreteFouriertransform, French,251 expletive,522 formantsynthesis,354
334 simplifiedgrammar explicitconfirmation,317 forwardchaining,417
discriminativemodel,80 rules,360 expressiveness,ofa forwardinference,148
disfluency,14 verb-framed,251 meaning forward-lookingcenters,
disjunction,29 entitydictionary,177 representation,408 553
pipeinregular entitygrid,554 extractiveQA,281 Fosler,E.,see
expressionsas,8 Entitylinking,282 extraposition,522 Fosler-Lussier,E.
squarebracesinregular entitylinking,517 extrinsicevaluation,37 fragmentofword,14
expressionas,6 entity-basedcoherence,552 frame,333
dispreferredresponse,328 entropy,52 F(forF-measure),69 semantic,482
distantsupervision,437 andperplexity,52 F-measure,69 frameelements,482
distributionalhypothesis, cross-entropy,54 F-measure FrameNet,482
103 per-word,53 inNER,179 frames,309
distributionalsimilarity, rate,53 F0,576 freewordorder,381
378 relative,490 factoidquestion,269 Freebase,287,431
divergencesbetween errorbackpropagation,151 Faiss,278 FreebaseQA,287
languagesinMT, ESPnet,353 falsenegatives,10 freeze,156
249 ethos,559 falsepositives,10 French,249
document Euclideandistance Farsi,verb-framed,251 frequency
inIR,270 inL2regularization,97 fastFouriertransform,335, ofasignal,573
documentfrequency,114 EugeneOnegin,55 352 fricativesound,569
documentvector,119 Euler’sformula,335 fasttext,125 Frump,444
domain,409 Europarl,254 FASTUS,443 fully-connected,140
dominationinsyntax,359 evalb,376 featurecutoff,177 functionword,161,181
dotproduct,81,112
evaluatingparsers,375 featureinteractions,84
functionsinFOL,412
dot-productattention,207
evaluation featureselection
fundamentalfrequency,576
DragonSystems,352
10-foldcross-validation, informationgain,77
fusionlanguage,252
dropout,156
71 featuretemplate,391
duration
ABtest,350 featuretemplates,84 Gaussian
temporalexpression,452
comparingmodels,39 part-of-speechtagging, prioronweights,98
dynamicprogramming,25
cross-validation,71 176 gazetteer,177
andparsing,367
developmenttestset,37, featurevectors,332 GeneralInquirer,66,499
Viterbias,171
70 Federalistpapers,77 generalize,97
dynamictimewarping,352
devset,70 feedforwardnetwork,140 generalizedsemanticrole,
devsetordevelopment fenceposts,368 479
edge-factored,395 testset,37 FFT,335,352 generation
editdistance dialoguesystems,321 fileformat,.wav,575 ofsentencestotesta
minimumalgorithm,26 extrinsic,37 filledpause,14 CFGgrammar,360
EDU,546 fluencyinMT,260 filler,14 template-based,311
effectsize,72 Matched-PairSentence finalfall,572 generativegrammar,361
Elaboration(ascoherence SegmentWordError fine-tuning,237 generativelexicon,474
relation),545 (MAPSSWE),344 FirstOrderLogic,seeFOL generativemodel,80
ELIZA,4 meanopinionscore,350 first-orderco-occurrence, generativemodels,61
implementation,13 mostfrequentclass 126 generator,359
sampleconversation,12 baseline,164 flap(phonetic),569 generics,522
ElmanNetworks,185 MT,260 fluency,260 German,249,584
ELMo namedentityrecognition, inMT,260 given-new,521
foraffect,512 179 focus,291 gloss,462
EM ofn-gram,37 FOL,405,411 glosses,458624 SubjectIndex
Glottal,568 simplifyingassumptions Interjections,162 L1regularization,97
glottalstop,568 forPOStagging, intermediatephrase,572 L2regularization,97
glottis,567 170 InternationalPhonetic labeledprecision,375
Godzilla,speakeras,487 states,168 Alphabet,565,585 labeledrecall,375
goldlabels,68 transitionprobabilities, InterpolatedKneser-Ney labialplaceofarticulation,
Good-Turing,48 168 discounting,49,51 568
gradient,92 Hobbsalgorithm,539 interpolatedprecision,276 labiodentalconsonants,568
Grammar Hobbstreesearchalgorithm interpolation lambdanotation,415
Constraint,403 forpronoun insmoothing,46 language
Head-DrivenPhrase resolution,539 interpretable,99 identification,351
Structure(HPSG), holonym,461 interpretation,409 universal,248
376 homonymy,457 intervalalgebra,447 languageid,58
Link,403 hotlanguages,252 intonationphrases,572 languagemodel,32
grammar HotpotQA,279 intrinsicevaluation,37 Laplacesmoothing,44
binarybranching,364 Hungarian inversiontransduction forPMI,118
checking,357 part-of-speechtagging, grammar(ITG),267 larynx,566
equivalence,364 180 invertedindex,274 lassoregression,98
generative,361 hybrid,353 IO,166 latentsemanticanalysis,
inversiontransduction, hyperarticulation,316 IOBtagging 132
267 hypernym,432,460 fortemporalexpressions, lateralsound,569
grammaticalfunction,382 lexico-syntacticpatterns 453 layernorm,217
for,433 IPA,565,585 LDC,19
grammaticalrelation,382
grammaticalsentences,361
hyperparameter,94 IR,270 learningrate,92
greedy,221
hyperparameters,156 idftermweighting,114, lemma,14,104
hyponym,460 271 versuswordform,14
greedyREpatterns,9
Hzasunitofmeasure,573 termweighting,271 lemmatization,5
Greek,565
vectorspacemodel,109 Leskalgorithm,468
G g Gg rr r Uoe ip c u Se, n ,a5 d 3n, i 0n5 m 8g, ,a2 2x9 9im 8s,300 I IB BM MM Th R 5o o 6d em ,e sl e 3as as 5, r 2J2 c.6 hW6 Ca et nso ten r, I I I isR R S --- B aAb ,,a 4,3s 34e 2 26d 6 1QA,278 L lee xS cv i aceim tan els gp h ol ti refi yie n ,d 3d, 5i4 9s6 ta8 nce,25
idf,114 ISO8601,453 cohesion,544,556
H H H H* a a am m np si mi alt rtc io dh nn ,ga , 2,c A 63c 63le en 3xt a, n5 d7 e3 r,77 i id ff thte er nm r2 4ew 7 1a1 7se oig nh inti gng in,1 F1 O4 L, , i i Is S To R Gla Lt (,i in n4g v g9 e r5l ara s mn iog mnu aa t rg r )ae ,n, 2s2 6d5 7u1 ction d g s se taa rmpt ea , sab s2 na ,5 ts 5i1e c 7, s 14 ,6 12 04
immediatelydominates, trigger,inIE,452
hanzi,19
harmonic,583 359 Japanese,249,251,565, lexicalanswertype,291
implicature,300 584 lexicalsampletaskin
harmonicmean,69
implicitargument,495 Jay,John,77 WSD,465
head,376,382
implicitconfirmation,318 jointintention,327 lexico-syntacticpattern,
finding,376
impliedhierarchy 433
Head-DrivenPhrase
indescriptionlogics,423 lexicon,358
StructureGrammar
indefinitereference,519
Kaldi,353
LibriSpeech,331
(HPSG),376
inference,408
Katzbackoff,47
lightverbs,441
Heaps’Law,14
inFOL,417
KBP,445
likelihood,61
Hearstpatterns,432
inference-basedlearning,
KenLM,48,56
linearchainCRF,174,175
Hebrew,565 399 key,214 linearclassifiers,62
held-out,47 infoboxes,431 KLdivergence,490 linearinterpolationfor
Herdan’sLaw,14 information KL-ONE,426 n-grams,47
hertzasunitofmeasure, structure,520 Klattformantsynthesizer, linearlyseparable,138
573 status,520 354 LinguisticData
hidden,168 informationextraction(IE), Kleene*,7 Consortium,19
hiddenlayer,140 429 sneakinessofmatching LinguisticDiscourse
asrepresentationof bootstrapping,435 zerothings,7 model,562
input,141 informationgain,77 Kleene+,7 LinkGrammar,403
hiddenunits,140 forfeatureselection,77 Kneser-Neydiscounting,49 List(ascoherencerelation),
Hindi,249 Informationretrieval,110, knowledgebase,407 546
Hindi,verb-framed,251 270 knowledgeclaim,561 listenattendandspell,336
HKUST,332 initiative,299 knowledgegraphs,429 LIWC,66,500
HMM,168 innerear,581
knowledge-based,468
LM,32
Korean,584
formaldefinitionof,168 innerproduct,112 LOBcorpus,182
KRL,426
historyinspeech instancechecking,423 localization,247
Kullback-Leibler
recognition,352 InstitutionalReviewBoard, location-basedattention,
divergence,490
initialdistribution,168 326 348
observationlikelihood, intensityofsound,577 locative,162
168 intentdetermination,310 L*pitchaccent,573 locativeadverb,162
observations,168 intercept,81 L+H*pitchaccent,573 logSubjectIndex 625
whyusedfor maximumspanningtree, post-editing,247 NextSentencePrediction,
probabilities,36 396 mu-law,575 235
whyusedtocompress Mayan,251 MUC,443,444 NISTforMTevaluation,
speech,575 McNemar’stest,345 MUCF-measure,535 267
loglikelihoodratio,508 mean multi-layerperceptrons, noisy-or,436
logoddsratio,508 element-wise,194 140 NomBank,481
logprobabilities,36,36 meanaverageprecision, multiheadself-attention Nominal,358
logicalconnectives,413 276 layers,217 non-capturinggroup,12
logicalvocabulary,408 meanopinionscore,350 multinomiallogistic non-greedy,9
logisticfunction,81 meanreciprocalrank,293 regression,86 non-logicalvocabulary,408
logisticregression,79 meaningrepresentation, multinomialnaiveBayes, non-standardwords,346
conditionalmaximum 405 59 non-stationaryprocess,333
likelihood assetofsymbols,406 multinomialnaiveBayes non-terminalsymbols,359,
estimation,90 earlyuses,425 classifier,59 360
Gaussianpriors,98 languages,406 multiwordexpressions,132 normalform,364,364
learningin,89 mechanicalindexing,131 MWE,132 normalization
regularization,98 MechanicalTurk,330 dates,311
relationtoneural mel,335 temporal,453
networks,142 scale,577 n-bestlist,337 word,22
logos,559 memorynetworks,227 n-gram,32,34 normalizationof
longshort-termmemory, mentiondetection,525 absolutediscounting,50 probabilities,34
198 mention-pair,528 add-onesmoothing,44 normalize,84
lookaheadinregex,13 mentions,516 asapproximation,34 normalizing,142
loss,89 meronym,461 asgenerators,41 noun
loudness,578 MERT,fortraininginMT, asMarkovchain,167 abstract,161
lowframerate,337 267 equationfor,34 common,161
LPC(LinearPredictive MeSH(MedicalSubject exampleof,35,36 count,161
Coding),352 Headings),59,465 forShakespeare,41 mass,161
LSI,seelatentsemantic MessageUnderstanding historyof,56 proper,161
analysis Conference,443 interpolation,46 nounphrase,358
LSTM,183 METEOR,267 Katzbackoff,47 constituents,358
LUNAR,294 metonymy,461,541 KenLM,48,56 Nouns,161
Lunar,426 Micro-Planner,426 Kneser-Neydiscounting, NP,358,360
microaveraging,70 49 nucleus,545
machinelearning Microsoft.wavformat,575 logprobsin,36 nucleusofsyllable,570
forNER,180 mini-batch,96 normalizing,36 nullhypothesis,72
textbooks,77,102 minimumeditdistance,24, parameterestimation,35 Nyquistfrequency,333,
machinetranslation,247 25,171 sensitivitytocorpus,40 575
macroaveraging,70 exampleof,28 smoothing,43
Madison,James,77 forspeechrecognition SRILM,56 observationlikelihood
MAE,15 evaluation,343 testset,37 roleinViterbi,172
Mandarin,249,584 MINIMUMEDITDISTANCE, trainingset,37 one-hotvector,148
Manhattandistance 27 unknownwords,43 onset,syllable,570
inL1regularization,97 minimumeditdistance naiveBayes ontology,420
manneradverb,162 algorithm,26 multinomial,59 OntoNotes,474
mannerofarticulation,568 MinimumErrorRate simplifyingassumptions, OOV(outofvocabulary)
markerpassingforWSD, Training,267 61 words,43
473 MLE naiveBayesassumption,61 OOVrate,43
Markov,34 forn-grams,34 naiveBayesclassifier openclass,161
assumption,34 forn-grams,intuition,35 useintextcategorization, openinformation
Markovassumption,167 MLM,232 59 extraction,439
Markovchain,55,167 MLP,140 namedentity,160,165 openvocabularysystem
formaldefinitionof,168 modalverb,163 listoftypes,165 unknownwordsin,43
initialdistribution,168 model,408 namedentityrecognition, operationlist,25
n-gramas,167 modelcard,76 165 operatorprecedence,8,9
states,168 modifiedKneser-Ney,51 nasalsound,567,569 optionality
transitionprobabilities, modusponens,417 nasaltract,567 useof?inregular
168 Montaguesemantics,426 naturallanguageinference, expressionsfor,7
Markovmodel,34 morpheme,23 238 oraltract,567
formaldefinitionof,168 MOS(meanopinionscore), NaturalQuestions,280 orthography
history,56 350 negativeloglikelihoodloss, opaque,566
Marx,G.,357 Moses,Michelangelostatue 99,151 transparent,566
MaskedLanguage of,296 neo-Davidsonian,419 outputgate,199
Modeling,232 Moses,MTtoolkit,267 NER,165 overfitting,97
massnouns,161 mostfrequentsense,466 neuralnetworks
maxent,102 MRR,293 relationtologistic p-value,72
maxim,Gricean,300 MT,247 regression,142 Paired,73
maximumentropy,102 divergences,249 newlinecharacter,10 palatalsound,568626 SubjectIndex
palate,568 perplexity,38,54 probabilisticcontext-free Recall,69
palato-alveolarsound,568 asweightedaverage grammars,379 recall
parallelcorpus,254 branchingfactor,38 productions,358 forMTevaluation,267
paralleldistributed definedvia progressiveprompting,318 inNER,179
processing,159 cross-entropy,54 projective,384 recipe
parallelogrammodel,126 personalpronoun,162 Prolog,417 meaningof,405
parsetree,359,361 persuasion,560 prominence,phonetic,572 rectangular,333
PARSEVAL,375 phone,565,585 prominentword,571 reducedvowels,572
parsing phonetics,565 prompts,311 reduction,phonetic,572
ambiguity,365 articulatory,566,566 pronoun,162 reference
CKY,367 phonotactics,570 bound,519 boundpronouns,519
CYK,seeCKY phrasalverb,162 demonstrative,520 cataphora,519
evaluation,375 phrase-basedtranslation, non-binary,523 definite,519
relationtogrammars, 267 personal,162 generics,522
362 phrase-structuregrammar, possessive,162 indefinite,519
syntactic,357 358,379 wh-,162 referencepoint,448
well-formedsubstring PII,305 pronunciationdictionary, referent,516
table,379 pipe,8 584 accessingof,516
partofspeech pitch,577 CELEX,584 evokingof,516
asusedinCFG,359 pitchaccent,571 CMU,584 referentialdensity,252
part-of-speech ToBI,573 PropBank,480 reflexive,523
adjective,162 pitchextraction,578 propernoun,161 regex
adverb,162 pitchtrack,576 propositionalmeaning,105 regularexpression,5
closedclass,161 placeofarticulation,568 prosodicphrasing,572 registerinregex,12
interjection,162 planning Prosody,571 regression
noun,161 andspeechacts,327 prosody lasso,98
openclass,161 sharedplans,327 accentedsyllables,571 ridge,98
particle,162 pleonastic,522 reducedvowels,572 regularexpression,5,29
subtledistinction plosivesound,569 PROTO-AGENT,479 substitutions,12
betweenverband Pointwisemutual PROTO-PATIENT,479 regularization,97
noun,162 information,116 pseudoword,491 rejection
verb,162 polysyntheticlanguage,251 PTRANS,492 conversationact,318
punctuation
part-of-speechtagger pooling,145,193 relatedness,105
fornumbers
PARTS,182 max,194 relationextraction,429
cross-linguistically,
TAGGIT,182 mean,193 relative
18
Part-of-speechtagging,163 Porterstemmer,23 temporalexpression,452
forsentence
part-of-speechtagging POS,160 relativeentropy,490
segmentation,24
ambiguityand,163 positionalembeddings,219 relativefrequency,35
tokenization,18
amountofambiguityin possessivepronoun,162 release,stop,568
treatedaswords,14
Browncorpus,164 post-editing,247 relevance,300
treatedaswordsinLM,
andmorphological postings,274 relexicalize,320
42
analysis,180 postposition,249 ReLU,136
featuretemplates,176 Pottsdiagram,507 reportingevents,441
historyof,182 powerofasignal,577 qualiastructure,474 representationlearning,103
Hungarian,180 PP,360 quantifier representationalharm,129
Turkish,180 PP-attachmentambiguity, semantics,413 representationalharms,75
unknownwords,174 366 quantization,333,575 rescore,337
part-whole,461 PPMI,117 query,214,270 resolutionforinference,
particle,162 praat,578,579,584 inIR,270 418
PARTStagger,182 precedence,8 question resolve,164
partsofspeech,160 precedence,operator,8 factoid,269 ResourceManagement,352
pathos,559 Precision,69 rise,572 responsegeneration,305
pattern,regularexpression, precision questionanswering restrictivegrammar,311
5 forMTevaluation,267 evaluation,293 retrieveandread,278
PCM(PulseCode inNER,179 factoidquestions,269 retrofitting,471
Modulation),575 precision-recallcurve,276 ReVerb,439
PDP,159 preferencesemantics,473 RadioRex,329 reversives,460
PDTB,547 premises,559 range,regularexpression,6 rewrite,359
PennDiscourseTreeBank, prepositionalphrase ranking,260 RhetoricalStructure
547 constituency,360 rapidreprompting,318 Theory,seeRST
PennTreebank,363 prepositions,162 rarefaction,574 rhyme,syllable,570
tagset,163,163 presequences,299 RDF,431 RiauIndonesian,162
PennTreebank pretraining,147,212 RDFtriple,431 ridgeregression,98
tokenization,19 primitivedecomposition, Readspeech,331 rime
per-wordentropy,53 492 readingcomprehension, syllable,570
perceptron,137 principleofcontrast,105 278 RMSamplitude,576
perioddisambiguation,83 priorprobability,61 Reason(ascoherence RNN-T,342
periodofawave,574 pro-droplanguages,252 relation),545 role-fillerextraction,442SubjectIndex 627
Rosebud,slednamed,543 semanticnetworks Speakerdiarization,350 SwitchboardCorpus,14,
roundedvowels,570 origins,426 speakeridentification,351 304,331,574,575
rowvector,110 semanticparsing,405 speakerrecognition,351 syllabification,570
RST,545 semanticrelationsinIE, speakerverification,351 syllable,570
TreeBank,547,562 430 spectrogram,581 accented,571
rules table,430 spectrum,579 coda,570
context-free,358 semanticrole,477,477, speech nucleus,570
context-free,expansion, 479 telephonebandwidth, onset,570
359 Semanticrolelabeling,483 575 prominent,571
context-free,sample,360 semantics speechacts,298 rhyme,570
Russian lexical,104 speechrecognition rime,570
fusionlanguage,252 semivowel,567 architecture,330,336 synchronousgrammar,267
verb-framed,251 sense historyof,351 synonyms,105,460
word,457,458 speechsynthesis,330 synset,462
SasstartsymbolinCFG, sentence spellingcorrection syntacticdisambiguation,
360 errorrate,344 useofn-gramsin,31 367
salience,indiscourse segmentation,24 split-halfreliability,502 syntax,357
model,521 sentenceembedding,237 SQuAD,279 originofterm,160
Sampling,40 sentencerealization,319 SRILM,56
sampling,333 sentencesegmentation,5 SRL,483 TACKBP,432
ofanalogwaveform,574 sentenceseparation,203 StackedRNNs,195 Tacotron2,348
rate,333,574 SentencePiece,254 standardize,84 TACREDdataset,432
usedinclustering,472 sentiment,106 startsymbol,359 TAGGIT,182
satellite,251,545 originofterm,515 state tagset
satellite-framedlanguage, sentimentanalysis,58 semanticrepresentation PennTreebank,163,163
251 sentimentlexicons,66 of,418 tableofPennTreebank
saturated,137 SentiWordNet,505 states,449 tags,163
Scho¨nfinkelization,415 sequencelabeling,160 staticembeddings,120 Tamil,251
schwa,572 SGNS,119 stationaryprocess,333 tanh,136
SCISOR,445 Shakespeare stationarystochastic tap(phonetic),569
sclite,343 n-gramapproximations process,53 targetembedding,124
sclitepackage,30 to,41 statisticalMT,266 Tay,325
script shallowdiscourseparsing, statisticalsignificance TBox,420
Schankian,482 551 MAPSSWEforASR, teacherforcing,191,205,
scripts,442 sharedplans,327 344 220,258
SDRT(Segmented SHRDLU,426 McNemar’stest,345 technai,160
Discourse sibilantsound,569 statisticallysignificant,73 telephone-bandwidth,333
Representation sidesequence,299 telephone-bandwidth
stativeexpressions,449
Theory),562 sigmoid,81,135
stem,23
speech,575
searchengine,270 significancetest telic,450
Stemming,5
searchtree,222 MAPSSWEforASR, templatefilling,429,442
second-order 344
stemming,23
templaterecognition,442
co-occurrence,126 McNemar’s,345
stop(consonant),568
template,inIE,442
seedpatterninIE,435 similarity,105 stoplist,273 template-basedgeneration,
seedtuples,435 cosine,112 stopwords,63 311
segmentation SimpleQuestions,287 streaming,342 temporaladverb,162
sentence,24 SimplifiedLesk,468 stress temporalanchor,454
word,18 singleton,517 lexical,571 temporalexpression
selectionalassociation,490 singularthey,523 stride,333 absolute,452
selectionalpreference skip-gram,119 structuralambiguity,365 metaphorfor,449
strength,490 slotfilling,310,445 structuredpolysemy,461 relative,452
selectionalpreferences slots,309 stupidbackoff,48 temporallogic,446
pseudowordsfor smoothing,43,44 subdialogue,299 temporalnormalization,
evaluation,491 absolutediscounting,50 subjectivity,496,515 453
selectionalrestriction,487 add-one,44 substitutability,378 term
representingwithevents, discounting,44 substitutionoperator clustering,473,474
488 interpolation,46 (regular inFOL,412
violationsinWSD,489 Katzbackoff,47 expressions),12 inIR,270
self-attention,212 Kneser-Neydiscounting, subsumption,420,423 weightinIR,271
self-supervision,120,190 49 subwords,20 termfrequency,114
semanticconcordance,465 Laplace,44 superordinate,461 termweight,271
semanticdriftinIE,436 linearinterpolation,47 supersenses,463 term-documentmatrix,108
semanticfeature,132 softmax,87,142 supervisedmachine term-termmatrix,111
semanticfield,105 source-filtermodel,583 learning,59 terminalsymbol,359
semanticframe,106 SOVlanguage,249 SVD,132 terminology
semanticnetwork spamdetection,58,66 SVOlanguage,249 indescriptionlogics,420
forwordsense span,281,373 Swedish,verb-framed,251 testset,37
disambiguation,473 Spanish,584 Switchboard,331 development,37628 SubjectIndex
howtochoose,37 tune,572 velarsound,568 Wizard-of-Ozsystem,324
textcategorization,58 continuationrise,573 velum,568 word
bag-of-words Turingtest verb boundary,regular
assumption,60 Passedin1972,304 copula,163 expressionnotation,
naiveBayesapproach,59 Turk,Mechanical,330 modal,163 8
unknownwords,63 Turkish phrasal,162 closedclass,161
textnormalization,4 agglutinative,252 verbalternations,479 definitionof,13
text-to-speech,330 part-of-speechtagging, verbphrase,360 errorrate,332,343
TextTiling,556 180 verb-framedlanguage,251 fragment,14
tf-idf,115 turncorrectionratio,323 Verbs,162 function,161,181
thematicgrid,478 turns,297 Verifiability,406 openclass,161
thematicrole,477 TyDiQA,280 Vietnamese,251 punctuationas,14
anddiathesisalternation, typeddependencystructure, Viterbi tokens,14
479 381 andbeamsearch,222 types,14
examplesof,477 types Viterbialgorithm,26,171 wordnormalization,22
problems,479 word,14 inferenceinCRF,178 wordsegmentation,18,20
theme,477 typology,249 VITERBIALGORITHM,171 wordsense,457,458
theme,asthematicrole,477 linguistic,249 vocal wordsensedisambiguation,
thesaurus,473 cords,567 465,seeWSD
time,representationof,446 ungrammaticalsentences, folds,567 wordsenseinduction,471
time-alignedtranscription, 361 tract,567 wordshape,176
584 unigram vocoder,346 wordtokenization,18
TimeBank,451 nameoftokenization vocoding,346 word-in-context,469
TIMIT,584 algorithm,254 voiceuserinterface,324 word-wordmatrix,111
ToBI,573 unitproduction,367 voicedsound,567 word2vec,119
boundarytones,573 unitvector,113 voicelesssound,567 wordform,14
tokenization,4 UniversalDependencies, vowel,567 andlemma,104
sentence,24 382 back,569,570 versuslemma,14
word,18 universal,linguistic,248 front,569 WordNet,462,462
tokens,word,14 Unix,5 height,569,570 wordpiece,253
topicmodels,106 <UNK>,43 high,570 worldknowledge,405
toxicitydetection,75 unknownwords low,570 WSD,465
trachea,566 inn-grams,43 mid,570 AI-orientedefforts,473
trainingoracle,389 inpart-of-speech reduced,572 all-wordstask,465
trainingset,37 tagging,174 rounded,569 bootstrapping,474
cross-validation,71 intextcategorization,63 VSOlanguage,249 decisiontreeapproach,
howtochoose,37 unvoicedsound,567 474
transcription user-centereddesign,324 wakeword,350 evaluationof,466
ofspeech,329 utterance,14 WallStreetJournal history,473
reference,343 WallStreetJournal historyof,474
time-aligned,584 vagueness,407 speechrecognitionof, lexicalsampletask,465
transductiongrammars,267 value,214 352 neuralnetwork
transferlearning,228 valuesensitivedesign,324 warping,352 approaches,473
Transformationsand vanishinggradient,137 wavefileformat,575 robustapproach,473
DiscourseAnalysis vanishinggradients,198 WaveNet,348 supervisedmachine
Project(TDAP), variable Wavenet,348 learning,474
182 existentiallyquantified, WebOntologyLanguage, unsupervisedmachine
transformers,212 414 425 learning,471
transitionprobability universallyquantified, WebQuestions,287 WSI,471
roleinViterbi,172 414 Weighttying,191
transition-based,386 variables,408 well-formedsubstring
YonkersRacetrack,52
translation variablesinFOL,412 table,379
Yupik,251
divergences,249 Vauquoistriangle,266 WFST,379
TREC,295 vector,109,135 wh-pronoun,162
treebank,362 vectorlength,112 WiC,470 z-score,84
trigram,36 Vectorsemantics,107 wikification,282 zeroanaphor,520
truth-conditionalsemantics, vectorsemantics,103 wildcard,regular zero-width,13
410 vectorspace,109 expression,7 zeros,42
TTS,330 vectorspacemodel,109 WinogradSchema,536 zeugma,459