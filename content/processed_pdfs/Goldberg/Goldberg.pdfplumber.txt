Neural Network Methods for
Natural Language ProcessingSynthesis Lectures on Human
Language Technologies
Editor
GraemeHirst,UniversityofToronto
SynthesisLecturesonHumanLanguageTechnologiesiseditedbyGraemeHirstoftheUniversity
ofToronto.eseriesconsistsof50-to150-pagemonographsontopicsrelatingtonaturallanguage
processing,computationallinguistics,informationretrieval,andspokenlanguageunderstanding.
Emphasisisonimportantnewtechniques,onnewapplications,andontopicsthatcombinetwoor
moreHLTsubfields.
NeuralNetworkMethodsforNaturalLanguageProcessing
YoavGoldberg
2017
Syntax-basedStatisticalMachineTranslation
PhilipWilliams,RicoSennrich,MattPost,andPhilippKoehn
2016
Domain-SensitiveTemporalTagging
JannikStrötgenandMichaelGertz
2016
LinkedLexicalKnowledgeBases:FoundationsandApplications
IrynaGurevych,JudithEckle-Kohler,andMichaelMatuschek
2016
BayesianAnalysisinNaturalLanguageProcessing
ShayCohen
2016
Metaphor:AComputationalPerspective
TonyVeale,EkaterinaShutova,andBeataBeigmanKlebanov
2016
GrammaticalInferenceforComputationalLinguistics
JeffreyHeinz,ColindelaHiguera,andMennovanZaanen
2015iii
AutomaticDetectionofVerbalDeception
EileenFitzpatrick,JoanBachenko,andTommasoFornaciari
2015
NaturalLanguageProcessingforSocialMedia
AtefehFarzindarandDianaInkpen
2015
SemanticSimilarityfromNaturalLanguageandOntologyAnalysis
SébastienHarispe,SylvieRanwez,StefanJanaqi,andJackyMontmain
2015
LearningtoRankforInformationRetrievalandNaturalLanguageProcessing,Second
Edition
HangLi
2014
Ontology-BasedInterpretationofNaturalLanguage
PhilippCimiano,ChristinaUnger,andJohnMcCrae
2014
AutomatedGrammaticalErrorDetectionforLanguageLearners,SecondEdition
ClaudiaLeacock,MartinChodorow,MichaelGamon,andJoelTetreault
2014
WebCorpusConstruction
RolandSchäferandFelixBildhauer
2013
RecognizingTextualEntailment:ModelsandApplications
IdoDagan,DanRoth,MarkSammons,andFabioMassimoZanzotto
2013
LinguisticFundamentalsforNaturalLanguageProcessing:100Essentialsfrom
MorphologyandSyntax
EmilyM.Bender
2013
Semi-SupervisedLearningandDomainAdaptationinNaturalLanguageProcessing
AndersSøgaard
2013
SemanticRelationsBetweenNominals
ViviNastase,PreslavNakov,DiarmuidÓSéaghdha,andStanSzpakowicz
2013iv
ComputationalModelingofNarrative
InderjeetMani
2012
NaturalLanguageProcessingforHistoricalTexts
MichaelPiotrowski
2012
SentimentAnalysisandOpinionMining
BingLiu
2012
DiscourseProcessing
ManfredStede
2011
BitextAlignment
JörgTiedemann
2011
LinguisticStructurePrediction
NoahA.Smith
2011
LearningtoRankforInformationRetrievalandNaturalLanguageProcessing
HangLi
2011
ComputationalModelingofHumanLanguageAcquisition
AfraAlishahi
2010
IntroductiontoArabicNaturalLanguageProcessing
NizarY.Habash
2010
Cross-LanguageInformationRetrieval
Jian-YunNie
2010
AutomatedGrammaticalErrorDetectionforLanguageLearners
ClaudiaLeacock,MartinChodorow,MichaelGamon,andJoelTetreault
2010
Data-IntensiveTextProcessingwithMapReduce
JimmyLinandChrisDyer
2010v
SemanticRoleLabeling
MarthaPalmer,DanielGildea,andNianwenXue
2010
SpokenDialogueSystems
KristiinaJokinenandMichaelMcTear
2009
IntroductiontoChineseNaturalLanguageProcessing
Kam-FaiWong,WenjieLi,RuifengXu,andZheng-shengZhang
2009
IntroductiontoLinguisticAnnotationandTextAnalytics
GrahamWilcock
2009
DependencyParsing
SandraKübler,RyanMcDonald,andJoakimNivre
2009
StatisticalLanguageModelsforInformationRetrieval
ChengXiangZhai
2008© Springer Nature Switzerland AG 2022
Reprint of original edition © Morgan & Claypool 2017
Allrightsreserved.Nopartofthispublicationmaybereproduced,storedinaretrievalsystem,ortransmittedin
anyformorbyanymeans—electronic,mechanical,photocopy,recording,oranyotherexceptforbriefquotations
inprintedreviews,withoutthepriorpermissionofthepublisher.
NeuralNetworkMethodsforNaturalLanguageProcessing
YoavGoldberg
ISBN: paperback
978-3-031-01037-8
ISBN: ebook
978-3-031-02165-7
DOI10. /
1007 978-3-031-02165-7
APublicationinthe series
Springer
SYNTHESISLECTURESONHUMANLANGUAGETECHNOLOGIES
Lecture#37
SeriesEditor:GraemeHirst,UniversityofToronto
SeriesISSN
Print1947-4040 Electronic1947-4059Neural Network Methods for
Natural Language Processing
Yoav Goldberg
BarIlanUniversity
SYNTHESISLECTURESONHUMANLANGUAGETECHNOLOGIES#37ABSTRACT
Neural networks are a family of powerful machine learning models. is book focuses on the
applicationofneuralnetworkmodelstonaturallanguagedata.efirsthalfofthebook(PartsI
and II) covers the basics of supervised machine learning and feed-forward neural networks, the
basics of working with machine learning over language data, and the use of vector-based rather
thansymbolicrepresentationsforwords.Italsocoversthecomputation-graphabstraction,which
allows to easily define and train arbitrary neural networks, and is the basis behind the design of
contemporaryneuralnetworksoftwarelibraries.
e second part of the book (Parts III and IV) introduces more specialized neural net-
work architectures, including 1D convolutional neural networks, recurrent neural networks,
conditioned-generationmodels,andattention-basedmodels.esearchitecturesandtechniques
arethedrivingforcebehindstate-of-the-artalgorithmsformachinetranslation,syntacticparsing,
andmanyotherapplications.Finally,wealsodiscusstree-shapednetworks,structuredprediction,
andtheprospectsofmulti-tasklearning.
KEYWORDS
natural language processing, machine learning, supervised learning, deep learning,
neuralnetworks,wordembeddings,recurrentneuralnetworks,sequencetosequence
modelsix
Contents
Preface .......................................................... xvii
Acknowledgments.................................................. xxi
1 Introduction ....................................................... 1
1.1 eChallengesofNaturalLanguageProcessing..........................1
1.2 NeuralNetworksandDeepLearning ..................................2
1.3 DeepLearninginNLP .............................................3
1.3.1 SuccessStories...............................................4
1.4 CoverageandOrganization ..........................................6
1.5 What’snotCovered ................................................8
1.6 ANoteonTerminology.............................................9
1.7 MathematicalNotation .............................................9
PARTI SupervisedClassificationandFeed-forward
NeuralNetworks . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2 LearningBasicsandLinearModels ................................... 13
2.1 SupervisedLearningandParameterizedFunctions ......................13
2.2 Train,Test,andValidationSets......................................14
2.3 LinearModels ...................................................16
2.3.1 BinaryClassification .........................................16
2.3.2 Log-linearBinaryClassification................................20
2.3.3 Multi-classClassification .....................................21
2.4 Representations ..................................................22
2.5 One-HotandDenseVectorRepresentations ...........................23
2.6 Log-linearMulti-classClassification..................................24
2.7 TrainingasOptimization...........................................25
2.7.1 LossFunctions .............................................26
2.7.2 Regularization ..............................................29x
2.8 Gradient-basedOptimization .......................................30
2.8.1 StochasticGradientDescent ..................................31
2.8.2 Worked-outExample ........................................33
2.8.3 BeyondSGD...............................................35
3 FromLinearModelstoMulti-layerPerceptrons ......................... 37
3.1 LimitationsofLinearModels:eXORProblem .......................37
3.2 NonlinearInputTransformations ....................................38
3.3 KernelMethods ..................................................38
3.4 TrainableMappingFunctions .......................................39
4 Feed-forwardNeuralNetworks ....................................... 41
4.1 ABrain-inspiredMetaphor .........................................41
4.2 InMathematicalNotation ..........................................43
4.3 RepresentationPower..............................................44
4.4 CommonNonlinearities............................................45
4.5 LossFunctions ...................................................46
4.6 RegularizationandDropout ........................................47
4.7 SimilarityandDistanceLayers ......................................48
4.8 EmbeddingLayers ................................................49
5 NeuralNetworkTraining ............................................ 51
5.1 eComputationGraphAbstraction .................................51
5.1.1 ForwardComputation .......................................53
5.1.2 BackwardComputation(Derivatives,Backprop)...................54
5.1.3 Software...................................................55
5.1.4 ImplementationRecipe.......................................57
5.1.5 NetworkComposition .......................................58
5.2 Practicalities .....................................................58
5.2.1 ChoiceofOptimizationAlgorithm .............................59
5.2.2 Initialization ...............................................59
5.2.3 RestartsandEnsembles ......................................59
5.2.4 VanishingandExplodingGradients.............................60
5.2.5 SaturationandDeadNeurons .................................60
5.2.6 Shuffling ..................................................61
5.2.7 LearningRate ..............................................61
5.2.8 Minibatches................................................61xi
PARTII WorkingwithNaturalLanguageData . . . . . . . . 63
6 FeaturesforTextualData ............................................ 65
6.1 TypologyofNLPClassificationProblems .............................65
6.2 FeaturesforNLPProblems .........................................67
6.2.1 DirectlyObservableProperties.................................67
6.2.2 InferredLinguisticProperties..................................70
6.2.3 CoreFeaturesvs.CombinationFeatures .........................74
6.2.4 NgramFeatures.............................................75
6.2.5 DistributionalFeatures .......................................76
7 CaseStudiesofNLPFeatures ........................................ 77
7.1 DocumentClassification:LanguageIdentification .......................77
7.2 DocumentClassification:TopicClassification ..........................77
7.3 DocumentClassification:AuthorshipAttribution .......................78
7.4 Word-in-context:PartofSpeechTagging..............................79
7.5 Word-in-context:NamedEntityRecognition ..........................81
7.6 WordinContext,LinguisticFeatures:PrepositionSenseDisambiguation ....82
7.7 RelationBetweenWordsinContext:Arc-FactoredParsing................85
8 FromTextualFeaturestoInputs ...................................... 89
8.1 EncodingCategoricalFeatures ......................................89
8.1.1 One-hotEncodings .........................................89
8.1.2 DenseEncodings(FeatureEmbeddings) .........................90
8.1.3 DenseVectorsvs.One-hotRepresentations ......................90
8.2 CombiningDenseVectors ..........................................92
8.2.1 Window-basedFeatures ......................................93
8.2.2 VariableNumberofFeatures:ContinuousBagofWords ............93
8.3 RelationBetweenOne-hotandDenseVectors..........................94
8.4 OddsandEnds...................................................95
8.4.1 DistanceandPositionFeatures.................................95
8.4.2 Padding,UnknownWords,andWordDropout ...................96
8.4.3 FeatureCombinations........................................98
8.4.4 VectorSharing..............................................98
8.4.5 Dimensionality .............................................99
8.4.6 EmbeddingsVocabulary ......................................99
8.4.7 Network’sOutput ...........................................99xii
8.5 Example:Part-of-SpeechTagging...................................100
8.6 Example:Arc-factoredParsing .....................................101
9 LanguageModeling ............................................... 105
9.1 eLanguageModelingTask ......................................105
9.2 EvaluatingLanguageModels:Perplexity .............................106
9.3 TraditionalApproachestoLanguageModeling ........................107
9.3.1 FurtherReading ...........................................108
9.3.2 LimitationsofTraditionalLanguageModels.....................108
9.4 NeuralLanguageModels..........................................109
9.5 UsingLanguageModelsforGeneration ..............................112
9.6 Byproduct:WordRepresentations...................................113
10 Pre-trainedWordRepresentations ................................... 115
10.1 RandomInitialization ............................................115
10.2 SupervisedTask-specificPre-training ................................115
10.3 UnsupervisedPre-training .........................................116
10.3.1UsingPre-trainedEmbeddings ...............................117
10.4 WordEmbeddingAlgorithms......................................117
10.4.1DistributionalHypothesisandWordRepresentations..............118
10.4.2FromNeuralLanguageModelstoDistributedRepresentations......122
10.4.3ConnectingtheWorlds......................................125
10.4.4OtherAlgorithms ..........................................126
10.5 eChoiceofContexts ...........................................127
10.5.1WindowApproach .........................................127
10.5.2Sentences,Paragraphs,orDocuments ..........................129
10.5.3SyntacticWindow..........................................129
10.5.4Multilingual...............................................130
10.5.5Character-basedandSub-wordRepresentations ..................131
10.6 DealingwithMulti-wordUnitsandWordInflections ...................132
10.7 LimitationsofDistributionalMethods ...............................133
11 UsingWordEmbeddings ........................................... 135
11.1 ObtainingWordVectors ..........................................135
11.2 WordSimilarity .................................................135
11.3 WordClustering.................................................136xiii
11.4 FindingSimilarWords............................................136
11.4.1SimilaritytoaGroupofWords ...............................137
11.5 Odd-oneOut ...................................................137
11.6 ShortDocumentSimilarity ........................................137
11.7 WordAnalogies .................................................138
11.8 RetrofittingandProjections........................................139
11.9 PracticalitiesandPitfalls ..........................................140
12 CaseStudy:AFeed-forwardArchitectureforSentenceMeaningInference .. 141
12.1 NaturalLanguageInferenceandtheSNLIDataset .....................141
12.2 ATextualSimilarityNetwork ......................................142
PARTIII SpecializedArchitectures . . . . . . . . . . . . . . . . . 147
13 NgramDetectors:ConvolutionalNeuralNetworks...................... 151
13.1 BasicConvolution+Pooling .......................................152
13.1.11DConvolutionsOverText ..................................153
13.1.2VectorPooling.............................................155
13.1.3Variations.................................................158
13.2 Alternative:FeatureHashing.......................................158
13.3 HierarchicalConvolutions .........................................159
14 RecurrentNeuralNetworks:ModelingSequencesandStacks ............. 163
14.1 eRNNAbstraction ............................................164
14.2 RNNTraining ..................................................166
14.3 CommonRNNUsage-patterns.....................................167
14.3.1Acceptor .................................................167
14.3.2Encoder ..................................................167
14.3.3Transducer................................................168
14.4 BidirectionalRNNs(biRNN) ......................................169
14.5 Multi-layer(stacked)RNNs .......................................171
14.6 RNNsforRepresentingStacks .....................................172
14.7 ANoteonReadingtheLiterature...................................174xiv
15 ConcreteRecurrentNeuralNetworkArchitectures...................... 177
15.1 CBOWasanRNN ..............................................177
15.2 SimpleRNN....................................................177
15.3 GatedArchitectures ..............................................178
15.3.1LSTM ...................................................179
15.3.2GRU ....................................................181
15.4 OtherVariants ..................................................182
15.5 DropoutinRNNs ...............................................183
16 ModelingwithRecurrentNetworks .................................. 185
16.1 Acceptors ......................................................185
16.1.1SentimentClassification .....................................185
16.1.2Subject-verbAgreementGrammaticalityDetection ...............187
16.2 RNNsasFeatureExtractors........................................189
16.2.1Part-of-speechTagging......................................189
16.2.2RNN–CNNDocumentClassification ..........................191
16.2.3Arc-factoredDependencyParsing .............................192
17 ConditionedGeneration ........................................... 195
17.1 RNNGenerators ................................................195
17.1.1TrainingGenerators ........................................196
17.2 ConditionedGeneration(Encoder-Decoder) ..........................196
17.2.1SequencetoSequenceModels ................................198
17.2.2Applications ..............................................200
17.2.3OtherConditioningContexts.................................202
17.3 UnsupervisedSentenceSimilarity ...................................203
17.4 ConditionedGenerationwithAttention..............................204
17.4.1ComputationalComplexity ..................................208
17.4.2Interpretability ............................................208
17.5 Attention-basedModelsinNLP....................................208
17.5.1MachineTranslation ........................................209
17.5.2MorphologicalInflection ....................................210
17.5.3SyntacticParsing...........................................211xv
PARTIV AdditionalTopics . . . . . . . . . . . . . . . . . . . . . . . 213
18 ModelingTreeswithRecursiveNeuralNetworks ....................... 215
18.1 FormalDefinition................................................215
18.2 ExtensionsandVariations .........................................218
18.3 TrainingRecursiveNeuralNetworks.................................219
18.4 ASimpleAlternative–LinearizedTrees...............................219
18.5 Outlook .......................................................220
19 StructuredOutputPrediction ....................................... 221
19.1 Search-basedStructuredPrediction..................................221
19.1.1StructuredPredictionwithLinearModels.......................221
19.1.2NonlinearStructuredPrediction...............................222
19.1.3ProbabilisticObjective(CRF) ................................224
19.1.4ApproximateSearch ........................................224
19.1.5Reranking ................................................225
19.1.6SeeAlso..................................................225
19.2 GreedyStructuredPrediction ......................................226
19.3 ConditionalGenerationasStructuredOutputPrediction ................227
19.4 Examples ......................................................228
19.4.1Search-basedStructuredPrediction:First-orderDependencyParsing .228
19.4.2Neural-CRFforNamedEntityRecognition .....................229
19.4.3ApproximateNER-CRFWithBeam-Search ....................232
20 Cascaded,Multi-taskandSemi-supervisedLearning .................... 235
20.1 ModelCascading ................................................235
20.2 Multi-taskLearning..............................................238
20.2.1TraininginaMulti-taskSetup ................................241
20.2.2SelectiveSharing...........................................242
20.2.3Word-embeddingsPre-trainingasMulti-taskLearning ............243
20.2.4Multi-taskLearninginConditionedGeneration .................243
20.2.5Multi-taskLearningasRegularization..........................243
20.2.6Caveats ..................................................244
20.3 Semi-supervisedLearning .........................................244
20.4 Examples ......................................................245
20.4.1Gaze-predictionandSentenceCompression .....................245
20.4.2ArcLabelingandSyntacticParsing ............................246xvi
20.4.3PrepositionSenseDisambiguationandPrepositionTranslation
Prediction ................................................ 247
20.4.4ConditionedGeneration:MultilingualMachineTranslation,
Parsing,andImageCaptioning ............................... 249
20.5 Outlook .......................................................250
21 Conclusion ...................................................... 251
21.1 WhatHaveWeSeen? ............................................251
21.2 eChallengesAhead ............................................251
Bibliography ..................................................... 253
Author’sBiography ................................................ 287xvii
Preface
Naturallanguageprocessing(NLP)isacollectivetermreferringtoautomaticcomputationalpro-
cessing of human languages. is includes both algorithms that take human-produced text as
input,andalgorithmsthatproducenaturallookingtextasoutputs.eneedforsuchalgorithms
is ever increasing: human produce ever increasing amounts of text each year, and expect com-
puter interfaces to communicate with them in their own language. Natural language processing
isalsoverychallenging,ashumanlanguageisinherentlyambiguous,everchanging,andnotwell
defined.
Naturallanguageissymbolicinnature,andthefirstattemptsatprocessinglanguagewere
symbolic:basedonlogic,rules,andontologies.However,naturallanguageisalsohighlyambigu-
ousandhighlyvariable,callingforamorestatisticalalgorithmicapproach.Indeed,thecurrent-
daydominantapproachestolanguageprocessingareallbasedonstatisticalmachinelearning.For
overadecade,coreNLPtechniquesweredominatedbylinearmodelingapproachestosupervised
learning,centeredaroundalgorithmssuchasPerceptrons,linearSupportVectorMachines,and
LogisticRegression,trainedoververyhighdimensionalyetverysparsefeaturevectors.
Around2014,thefieldhasstartedtoseesomesuccessinswitchingfromsuchlinearmodels
over sparse inputs to nonlinear neural network models over dense inputs. Some of the neural-
network techniques are simple generalizations of the linear models and can be used as almost
drop-in replacements for the linear classifiers. Others are more advanced, require a change of
mindset, and provide new modeling opportunities. In particular, a family of approaches based
onrecurrentneuralnetworks(RNNs)alleviatestherelianceontheMarkovAssumptionthatwas
prevalent in sequence models, allowing to condition on arbitrarily long sequences and produce
effectivefeatureextractors.eseadvancesledtobreakthroughsinlanguagemodeling,automatic
machinetranslation,andvariousotherapplications.
While powerful, the neural network methods exhibit a rather strong barrier of entry, for
variousreasons.Inthisbook,IattempttoprovideNLPpractitionersaswellasnewcomerswith
the basic background, jargon, tools, and methodologies that will allow them to understand the
principlesbehindneuralnetworkmodelsforlanguage,andapplythemintheirownwork.Ialso
hopetoprovidemachinelearningandneuralnetworkpractitionerswiththebackground,jargon,
tools,andmindsetthatwillallowthemtoeffectivelyworkwithlanguagedata.
Finally,Ihopethisbookcanalsoservearelativelygentle(ifsomewhatincomplete)intro-
ductiontobothNLPandmachinelearningforpeoplewhoarenewcomerstobothfields.xviii PREFACE
INTENDEDREADERSHIP
isbookisaimedatreaderswithatechnicalbackgroundincomputerscienceorarelatedfield,
who want to get up to speed with neural network techniques for natural language processing.
Whiletheprimaryaudienceofthebookisgraduatestudentsinlanguageprocessingandmachine
learning,ImadeanefforttomakeitusefulalsotoestablishedresearchersineitherNLPorma-
chine learning (by including some advanced material), and to people without prior exposure to
eithermachinelearningorNLP(bycoveringthebasicsfromthegroundsup).islastgroupof
peoplewill,obviously,needtoworkharder.
While the book is self contained, I do assume knowledge of mathematics, in particular
undergraduatelevelofprobability,algebra,andcalculus,aswellasbasicknowledgeofalgorithms
anddatastructures.Priorexposuretomachinelearningisveryhelpful,butnotrequired.
isbookevolvedoutofasurveypaper[Goldberg,2016],whichwasgreatlyexpandedand
somewhatre-organizedtoprovideamorecomprehensiveexposition,andmorein-depthcoverage
ofsometopicsthatwereleftoutofthesurveyforvariousreasons.isbookalsocontainsmany
moreconcreteexamplesofapplicationsofneural networkstolanguagedatathatdo notexistin
the survey. While this book is intended to be useful also for people without NLP or machine
learningbackgrounds,thesurveypaperassumesknowledgeinthefield.Indeed,readerswhoare
familiarwithnaturallanguageprocessingaspracticedbetweenroughly2006and2014,withheavy
relianceonmachinelearningandlinearmodels,mayfindthejournalversionquickertoreadand
betterorganizedfortheirneeds.However,suchreadersmayalsoappreciatereadingthechapters
on word embeddings (10 and 11), the chapter on conditioned generation with RNNs (17), and
thechaptersonstructuredpredictionandmulti-tasklearning(MTL)(19and20).
FOCUSOFTHISBOOK
isbookisintendedtobeself-contained,whilepresentingthedifferentapproachesunderauni-
fiednotationandframework.However,themainpurposeofthebookisinintroducingtheneural-
networks (deep-learning) machinery and its application to language data, and not in providing
anin-depthcoverageofthebasicsofmachinelearningtheoryandnaturallanguagetechnology.
Ireferthereadertoexternalsourceswhentheseareneeded.
Likewise, the book is not intended as a comprehensive resource for those who will go on
and develop the next advances in neural network machinery (although it may serve as a good
entry point). Rather, it is aimed at those readers who are interested in taking the existing, use-
ful technology and applying it in useful and creative ways to their favorite language-processing
problems.PREFACE xix
Further reading For in-depth, general discussion of neural networks, the theory behind
them, advanced optimization methods, and other advanced topics, the reader is referred to
other existing resources. In particular, the book by Bengio et al. [2016] is highly recom-
mended.
Forafriendlyyetrigorousintroductiontopracticalmachinelearning,thefreelyavail-
able book of Daumé III [2015] is highly recommended. For more theoretical treatment of
machinelearning,seethefreelyavailabletextbookofShalev-ShwartzandBen-David[2014]
andthetextbookofMohrietal.[2012].
For a strong introduction to NLP, see the book of Jurafsky and Martin [2008]. e
information retrieval book by Manning et al. [2008] also contains relevant information for
workingwithlanguagedata.
Finally,forgettingup-to-speedwithlinguisticbackground,thebookofBender[2013]
in this series provides a concise but comprehensive coverage, directed at computationally
mindedreaders.efirstchaptersoftheintroductorygrammarbookbySagetal.[2003]are
alsoworthreading.
..
Asofthiswriting,theprogressofresearchinneuralnetworksandDeepLearningisvery
fastpaced.estate-of-the-artisamovingtarget,andIcannothopetostayup-to-datewiththe
latest-and-greatest.efocusisthuswithcoveringthemoreestablishedandrobusttechniques,
thatwereproventoworkwellinseveraloccasions,aswellasselectedtechniquesthatarenotyet
fullyfunctionalbutthatIfindtobeestablishedand/orpromisingenoughforinclusion.
YoavGoldberg
March2017xxi
Acknowledgments
is book grew out of a survey paper I’ve written on the topic [Goldberg, 2016], which in turn
grewoutofmyfrustrationwiththelackorganizedandclearmaterialontheintersectionofdeep
learning and natural language processing, as I was trying to learn it and teach it to my students
and collaborators. I am thus indebted to the numerous people who commented on the survey
paper (in its various forms, from initial drafts to post-publication comments), as well as to the
peoplewhocommentedonvariousstagesofthebook’sdraft.Somecommentedinperson,some
overemail,andsomeinrandomconversationsonTwitter.ebookwasalsoinfluencedbypeople
whodidnotcommentonitper-se(indeed,someneverreadit)butdiscussedtopicsrelatedtoit.
Somearedeeplearningexperts,someareNLPexperts,someareboth,andothersweretryingto
learnbothtopics.Some(few)contributedthroughverydetailedcomments,othersbydiscussing
smalldetails,othersinbetween.Buteachoftheminfluencedthefinalformofthebook.eyare,
inalphabeticalorder:YoavArtzi,YonatanAumann,JasonBaldridge,MiguelBallesteros,Mohit
Bansal,MarcoBaroni,TalBaumel,SamBowman,JordanBoyd-Graber,ChrisBrockett,Ming-
Wei Chang, David Chiang, Kyunghyun Cho, Grzegorz Chrupala, Alexander Clark, Raphael
Cohen, Ryan Cotterell, Hal Daumé III, Nicholas Dronen, Chris Dyer, Jacob Eisenstein, Jason
Eisner,MichaelElhadad,YadFaeq,ManaalFaruqui,AmirGloberson,FrédericGodin,Edward
Grefenstette, Matthew Honnibal, Dirk Hovy, Moshe Koppel, Angeliki Lazaridou, Tal Linzen,
angLuong,ChrisManning,StephenMerity,PaulMichel,MargaretMitchell,PieroMolino,
Graham Neubig, Joakim Nivre, Brendan O’Connor, Nikos Pappas, Fernando Pereira, Barbara
Plank, Ana-Maria Popescu, Delip Rao, Tim Rocktäschel, Dan Roth, Alexander Rush, Naomi
Saphra, Djamé Seddah, Erel Segal-Halevi, Avi Shmidman, Shaltiel Shmidman, Noah Smith,
AndersSøgaard,AbeStanway,EmmaStrubell,SandeepSubramanian,LilingTan,ReutTsarfaty,
PeterTurney,TimVieira,OriolVinyals,AndreasVlachos,WenpengYin,andTorstenZesch.
e list excludes, of course, the very many researchers I’ve communicated with through
theiracademicwritingsonthetopic.
ebookalsobenefitedalotfrom—andwasshapedby—myinteractionwiththeNatural
LanguageProcessingGroupatBar-IlanUniversity(anditssoftextensions):YossiAdi,RoeeAha-
roni, Oded Avraham, Ido Dagan, Jessica Ficler, Jacob Goldberger, Hila Gonen, Joseph Keshet,
Eliyahu Kiperwasser, Ron Konigsberg, Omer Levy, Oren Melamud, Gabriel Stanovsky, Ori
Shapira, Micah Shlain, Vered Shwartz, Hillel Taub-Tabib, and Rachel Wities. Most of them
belonginbothlists,butItriedtokeepthingsshort.
eanonymousreviewersofthebookandthesurveypaper—whileunnamed(andsome-
times annoying)—provided a solid set of comments, suggestions, and corrections, which I can
safely say dramatically improved many aspects of the final product. anks, whoever you are!xxii ACKNOWLEDGMENTS
And thanks also to Graeme Hirst, Michael Morgan, Samantha Draper, and C.L. Tondo for
orchestratingtheeffort.
Asusual,allmistakesareofcoursemyown.Doletmeknowifyoufindany,though,and
belistedinthenexteditionifoneisevermade.
Finally,Iwouldliketothankmywife,Noa,whowaspatientandsupportivewhenIdisap-
peared into writing sprees, my parents Esther and Avner and brother Nadav who were in many
casesmoreexcitedabouttheideaofmewritingabookthanIwas,andthestaffateStreetsCafe
(KingGeorgebranch)andShne’orCafewhokeptmewellfedandservedmedrinksthroughout
thewritingprocess,withonlyveryminimaldistractions.
YoavGoldberg
March20171
C H A P T E R 1
Introduction
1.1 THECHALLENGESOFNATURALLANGUAGE
PROCESSING
Natural language processing (NLP) is the field of designing methods and algorithms that take
as input or produce as output unstructured, natural language data. Human language is highly
ambiguous (consider the sentence I ate pizza with friends, and compare it to I ate pizza with
olives),andalsohighlyvariable(thecoremessageofIatepizzawithfriendscanalsobeexpressedas
friendsandIsharedsomepizza).Itisalsoeverchangingandevolving.Peoplearegreatatproducing
languageandunderstandinglanguage,andarecapableofexpressing,perceiving,andinterpreting
very elaborate and nuanced meanings. At the same time, while we humans are great users of
language, we are also very poor at formally understanding and describing the rules that govern
language.
Understandingandproducinglanguageusingcomputersisthushighlychallenging.Indeed,
thebestknownsetofmethodsfordealingwithlanguagedataareusingsupervisedmachinelearn-
ing algorithms,thatattempttoinferusagepatternsandregularitiesfromasetofpre-annotated
inputandoutputpairs.Consider,forexample,thetaskofclassifyingadocumentintooneoffour
categories: S, P, G, and E. Obviously, the words in the documents
provide very stronghints,but which wordsprovide whathints? Writing up rules forthis task is
rather challenging. However, readers can easily categorize a document into its topic, and then,
basedonafewhundredhuman-categorizedexamplesineachcategory,letasupervisedmachine
learningalgorithmcomeupwiththepatternsofwordusagethathelpcategorizethedocuments.
Machine learning methods excel at problem domains where a good set of rules is very hard to
definebutannotatingtheexpectedoutputforagiveninputisrelativelysimple.
Besidesthechallengesofdealingwithambiguousandvariableinputsinasystemwithill-
definedandunspecifiedsetofrules,naturallanguageexhibitsanadditionalsetofpropertiesthat
make it even more challenging for computational approaches, including machine learning: it is
discrete,compositional,andsparse.
Languageissymbolicanddiscrete.ebasicelementsofwrittenlanguagearecharacters.
Characters form words that in turn denote objects, concepts, events, actions, and ideas. Both
charactersandwordsarediscretesymbols:wordssuchas“hamburger”or“pizza”eachevokeinus
acertainmentalrepresentations,buttheyarealsodistinctsymbols,whosemeaningisexternalto
themandlefttobeinterpretedinourheads.ereisnoinherentrelationbetween“hamburger”
and“pizza”thatcanbeinferredfromthesymbolsthemselves,orfromtheindividuallettersthey2 1. INTRODUCTION
are made of. Compare that to concepts such as color, prevalent in machine vision, or acoustic
signals:theseconceptsarecontinuous,allowing,forexample,tomovefromacolorfulimagetoa
gray-scale one using a simple mathematical operation, or to compare two different colors based
on inherent properties such as hue and intensity. is cannot be easily done with words—there
isnosimpleoperationthatwillallowustomovefromtheword“red”totheword“pink”without
usingalargelookuptableoradictionary.
Languageisalsocompositional:lettersformwords,andwordsformphrasesandsentences.
emeaningofaphrasecanbelargerthanthemeaningoftheindividualwordsthatcompriseit,
andfollowsasetofintricaterules.Inordertointerpretatext,wethusneedtoworkbeyondthe
leveloflettersandwords,andlookatlongsequencesofwordssuchassentences,orevencomplete
documents.
ecombinationoftheabovepropertiesleadstodatasparseness.ewayinwhichwords
(discretesymbols)canbecombinedtoformmeaningsispracticallyinfinite.enumberofpossi-
blevalidsentencesistremendous:wecouldneverhopetoenumerateallofthem.Openarandom
book,andthevastmajorityofsentenceswithinityouhavenotseenorheardbefore.Moreover,
itislikelythatmanysequencesoffour-wordsthatappearinthebookarealsonoveltoyou.Ifyou
weretolookatanewspaperfromjust10yearsago,orimagineone10yearsinthefuture,many
ofthewords,inparticularnamesofpersons,brands,andcorporations,butalsoslangwordsand
technicalterms,willbenovelaswell.ereisnoclearwayofgeneralizingfromonesentenceto
another,ordefiningthesimilaritybetweensentences,thatdoesnotdependontheirmeaning—
whichisunobservedtous.isisverychallengingwhenwecometolearnfromexamples:even
with a huge example set we are very likely to observe events that never occurred in the example
set,andthatareverydifferentthanalltheexamplesthatdidoccurinit.
1.2 NEURALNETWORKSANDDEEPLEARNING
Deep learning is a branch of machine learning. It is a re-branded name for neural networks—a
familyoflearningtechniquesthatwashistoricallyinspiredbythewaycomputationworksinthe
brain, and which can be characterized as learning of parameterized differentiable mathematical
functions.¹ ename deep-learning stems fromthe fact that manylayersof these differentiable
functionareoftenchainedtogether.
Whileallofmachinelearningcanbecharacterizedaslearningtomakepredictionsbased
on past observations, deep learning approaches work by learning to not only predict but also to
correctlyrepresentthedata,suchthatitissuitableforprediction.Givenalargesetofdesiredinput-
outputmapping,deeplearningapproachesworkbyfeedingthedataintoanetworkthatproduces
successivetransformationsoftheinputdatauntilafinaltransformationpredictstheoutput.e
transformations produced by the network are learned from the given input-output mappings,
suchthateachtransformationmakesiteasiertorelatethedatatothedesiredlabel.
¹Inthisbookwetakethemathematicalviewratherthanthebrain-inspiredview.1.3. DEEPLEARNINGINNLP 3
Whilethehumandesignerisinchargeofdesigningthenetworkarchitectureandtraining
regime,providingthenetworkwithapropersetofinput-outputexamples,andencodingtheinput
datainasuitableway,alotoftheheavy-liftingoflearningthecorrectrepresentationisperformed
automaticallybythenetwork,supportedbythenetwork’sarchitecture.
1.3 DEEPLEARNINGINNLP
Neuralnetworksprovideapowerfullearningmachinerythatisveryappealingforuseinnatural
languageproblems.Amajorcomponentinneuralnetworksforlanguageistheuseofanembed-
ding layer, a mapping of discrete symbols to continuous vectors in a relatively low dimensional
space.Whenembeddingwords,theytransformfrombeingisolateddistinctsymbolsintomath-
ematical objects that can be operated on. In particular, distance between vectorscan be equated
todistancebetweenwords,makingiteasiertogeneralizethebehaviorfromonewordtoanother.
isrepresentationofwordsasvectorsislearnedbythenetworkaspartofthetrainingprocess.
Goingupthehierarchy,thenetworkalsolearnstocombinewordvectorsinawaythatisusefulfor
prediction.iscapabilityalleviatestosomeextentthediscretenessanddata-sparsityproblems.
erearetwomajorkindsofneuralnetworkarchitectures,thatcanbecombinedinvarious
ways:feed-forwardnetworksandrecurrent/recursivenetworks.
Feed-forwardnetworks,inparticularmulti-layerperceptrons(MLPs),allowtoworkwith
fixedsizedinputs,orwithvariablelengthinputsinwhichwecandisregardtheorderoftheele-
ments.Whenfeedingthenetworkwithasetofinputcomponents,itlearnstocombinethemin
ameaningfulway.MLPscanbeusedwheneveralinearmodelwaspreviouslyused.enonlin-
earityofthenetwork,aswellastheabilitytoeasilyintegratepre-trainedwordembeddings,often
leadtosuperiorclassificationaccuracy.
Convolutionalfeed-forwardnetworksarespecializedarchitecturesthatexcelatextracting
localpatternsinthedata:theyarefedarbitrarilysizedinputs,andarecapableofextractingmean-
ingfullocalpatternsthataresensitivetowordorder,regardlessofwheretheyappearintheinput.
ese work very well for identifying indicative phrases or idioms of up to a fixed length in long
sentencesordocuments.
Recurrent neural networks (RNNs) are specialized models for sequential data. ese are
networkcomponentsthattakeasinputasequenceofitems,andproduceafixedsizevectorthat
summarizesthatsequence.As“summarizingasequence”meansdifferentthingsfordifferenttasks
(i.e.,theinformationneededtoansweraquestionaboutthesentimentofasentenceisdifferent
fromtheinformationneededtoansweraquestionaboutitsgrammaticality),recurrentnetworks
arerarelyusedasstandalonecomponent,andtheirpowerisinbeingtrainablecomponentsthat
can be fed into other network components, and trained to work in tandem with them. For ex-
ample, the output of a recurrent network can be fed into a feed-forward network that will try
to predict some value. e recurrent network is used as an input-transformer that is trained to
produceinformativerepresentationsforthefeed-forwardnetworkthatwilloperateontopofit.
Recurrentnetworksareveryimpressivemodelsforsequences,andarearguablythemostexciting4 1. INTRODUCTION
offerofneuralnetworksforlanguageprocessing.eyallowabandoningthemarkovassumption
that was prevalent in NLP for decades, and designing models that can condition on entire sen-
tences, while taking word order into account when it is needed, and not suffering much from
statistical estimation problems stemming from data sparsity. is capability leads to impressive
gainsinlanguage-modeling,thetaskofpredictingtheprobabilityofthenextwordinasequence
(or,equivalently,theprobabilityofasequence),whichisacornerstoneofmanyNLPapplications.
Recursivenetworksextendrecurrentnetworksfromsequencestotrees.
Manyoftheproblemsinnaturallanguagearestructured,requiringtheproductionofcom-
plex output structures such as sequences or trees, and neural network models can accommodate
that need as well, either by adapting known structured-prediction algorithms for linear models,
or by using novel architectures such as sequence-to-sequence (encoder-decoder) models, which
werefertointhisbookasconditioned-generationmodels.Suchmodelsareattheheartofstate-
of-the-artmachinetranslation.
Finally,manylanguagepredictiontasksarerelatedtoeachother,inthesensethatknowing
to perform one of them will help in learning to perform the others. In addition, while we may
haveashortageofsupervised (labeled)trainingdata,wehaveamplesupplyofrawtext(unlabeled
data).Canwelearnfromrelatedtasksandun-annotateddata?Neuralnetworkapproachespro-
videexcitingopportunitiesforbothMTL(learningfromrelatedproblems)andsemi-supervised
learning(learningfromexternal,unannotateddata).
1.3.1 SUCCESSSTORIES
Fullyconnectedfeed-forwardneuralnetworks(MLPs)can,forthemostpart,beusedasadrop-in
replacementwhereveralinearlearnerisused.isincludesbinaryandmulti-classclassification
problems, as well as more complex structured prediction problems. e nonlinearity of the net-
work,aswellastheabilitytoeasilyintegratepre-trainedwordembeddings,oftenleadtosuperior
classification accuracy. A series of works² managed to obtain improved syntactic parsing results
by simply replacing the linear model of a parser with a fully connected feed-forward network.
Straightforwardapplicationsofafeed-forwardnetworkasaclassifierreplacement(usuallycou-
pledwiththeuseofpre-trainedwordvectors)providebenefitsformanylanguagetasks,including
the very well basic task of language modeling³ CCG supertagging,⁴ dialog state tracking,⁵ and
pre-orderingforstatisticalmachinetranslation.⁶Iyyeretal.[2015]demonstratethatmulti-layer
feed-forward networks can provide competitive results on sentiment classification and factoid
questionanswering.Zhouetal.[2015]andAndoretal.[2016]integratetheminabeam-search
structured-predictionsystem,achievingstellaraccuraciesonsyntacticparsing,sequencetagging
andothertasks.
²[ChenandManning,2014,DurrettandKlein,2015,Peietal.,2015,Weissetal.,2015]
³SeeChapter9,aswellasBengioetal.[2003],Vaswanietal.[2013].
⁴[LewisandSteedman,2014]
⁵[Hendersonetal.,2013]
⁶[deGispertetal.,2015]1.3. DEEPLEARNINGINNLP 5
Networkswithconvolutionalandpoolinglayersareusefulforclassificationtasksinwhich
we expect to find strong local clues regarding class membership, but these clues can appear in
different places in the input. For example, in a document classification task, a single key phrase
(or an ngram) can help in determining the topic of the document [Johnson and Zhang, 2015].
Wewouldliketolearnthatcertainsequencesofwordsaregoodindicatorsofthetopic,anddonot
necessarilycarewheretheyappearinthedocument.Convolutionalandpoolinglayersallowthe
modeltolearntofindsuchlocalindicators,regardlessoftheirposition.Convolutionalandpool-
ingarchitectureshowpromisingresultsonmanytasks,includingdocumentclassification,⁷short-
textcategorization,⁸sentimentclassification,⁹relation-typeclassificationbetweenentities,¹⁰event
detection,¹¹ paraphrase identification,¹² semantic role labeling,¹³ question answering,¹⁴ predict-
ingbox-officerevenuesofmoviesbasedoncriticreviews,¹⁵modelingtextinterestingness,¹⁶and
modelingtherelationbetweencharacter-sequencesandpart-of-speechtags.¹⁷
Innaturallanguageweoftenworkwithstructureddataofarbitrarysizes,suchassequences
andtrees.Wewouldliketobeabletocaptureregularitiesinsuchstructures,ortomodelsimilari-
tiesbetweensuchstructures.Recurrentandrecursivearchitecturesallowworkingwithsequences
andtreeswhilepreservingalotofthestructuralinformation.Recurrentnetworks[Elman,1990]
aredesignedtomodelsequences,whilerecursivenetworks[GollerandKüchler,1996]aregen-
eralizations of recurrent networks that can handle trees. Recurrent models have been shown to
produce very strong results for language modeling,¹⁸ as well as for sequence tagging,¹⁹ machine
translation,²⁰ parsing,²¹ and many other tasks including noisy text normalization,²² dialog state
tracking,²³ response generation,²⁴ and modeling the relation between character sequences and
part-of-speechtags.²⁵
⁷[JohnsonandZhang,2015]
⁸[Wangetal.,2015a]
⁹[Kalchbrenneretal.,2014,Kim,2014]
¹⁰[dosSantosetal.,2015,Zengetal.,2014]
¹¹[Chenetal.,2015,NguyenandGrishman,2015]
¹²[YinandSchütze,2015]
¹³[Collobertetal.,2011]
¹⁴[Dongetal.,2015]
¹⁵[BitvaiandCohn,2015]
¹⁶[Gaoetal.,2014]
¹⁷[dosSantosandZadrozny,2014]
¹⁸SomenotableworksareAdeletal.[2013],AuliandGao[2014],Aulietal.[2013],Duhetal.[2013],Jozefowiczetal.[2016],
Mikolov[2012],Mikolovetal.[2010,2011].
¹⁹[IrsoyandCardie,2014,Lingetal.,2015b,Xuetal.,2015]
²⁰[Choetal.,2014b,Sundermeyeretal.,2014,Sutskeveretal.,2014,Tamuraetal.,2014]
²¹[Dyeretal.,2015,KiperwasserandGoldberg,2016b,WatanabeandSumita,2015]
²²[Chrupala,2014]
²³[Mrkšićetal.,2015]
²⁴[Kannanetal.,2016,Sordonietal.,2015]
²⁵[Lingetal.,2015b]6 1. INTRODUCTION
Recursive models were shown to produce state-of-the-art or near state-of-the-art results
forconstituency²⁶anddependency²⁷parsere-ranking,discourseparsing,²⁸semanticrelationclas-
sification,²⁹politicalideologydetectionbasedonparsetrees,³⁰sentimentclassification,³¹target-
dependentsentimentclassification,³²andquestionanswering.³³
1.4 COVERAGEANDORGANIZATION
e book consists of four parts. Part I introduces the basic learning machinery we’ll be using
throughoutthebook:supervisedlearning,MLPs,gradient-basedtraining,andthecomputation-
graphabstractionforimplementingandtrainingneuralnetworks.PartIIconnectsthemachinery
introducedinthefirstpartwithlanguagedata.Itintroducesthemainsourcesofinformationthat
are available when working with language data, and explains how to integrate them with the
neuralnetworksmachinery.Italsodiscussesword-embeddingalgorithmsandthedistributional
hypothesis, and feed-forward approaches to language modeling. Part III deals with specialized
architectures and their applications to language data: 1D convolutional networks for working
with ngrams, and RNNs for modeling sequences and stacks. RNNs are the main innovation of
the application of neural networks to language data, and most of Part III is devoted to them,
including the powerful conditioned-generation framework they facilitate, and attention-based
models.PartIVisacollectionofvariousadvancedtopics:recursivenetworksformodelingtrees,
structuredpredictionmodels,andmulti-tasklearning.
Part I, covering the basics of neural networks, consists of four chapters. Chapter 2 in-
troducesthebasicconceptsofsupervisedmachinelearning,parameterizedfunctions,linearand
log-linearmodels,regularizationandlossfunctions,trainingasoptimization,andgradient-based
trainingmethods.Itstartsfromthegroundup,andprovidestheneededmaterialforthefollowing
chapters. Readers familiar with basic learning theory and gradient-based learning may consider
skipping this chapter. Chapter 3 spells out the major limitation of linear models, motivates the
needfornonlinearmodels,andlaysthegroundandmotivationformulti-layerneuralnetworks.
Chapter4introducesfeed-forwardneuralnetworksandtheMLPs.Itdiscussesthedefinitionof
multi-layernetworks,theirtheoreticalpower,andcommonsubcomponentssuchasnonlinearities
andlossfunctions.Chapter5dealswithneuralnetworktraining.Itintroducesthecomputation-
graph abstraction that allows for automatic gradient computations for arbitrary networks (the
back-propagationalgorithm),andprovidesseveralimportanttipsandtricksforeffectivenetwork
training.
²⁶[Socheretal.,2013a]
²⁷[LeandZuidema,2014,Zhuetal.,2015a]
²⁸[Lietal.,2014]
²⁹[Hashimotoetal.,2013,Liuetal.,2015]
³⁰[Iyyeretal.,2014b]
³¹[HermannandBlunsom,2013,Socheretal.,2013b]
³²[Dongetal.,2014]
³³[Iyyeretal.,2014a]1.4. COVERAGEANDORGANIZATION 7
Part II introducing language data, consists of seven chapters. Chapter 6 presents a typol-
ogyofcommonlanguage-processingproblems,anddiscussestheavailablesourcesofinformation
(features) available for us when using language data. Chapter 7 provides concrete case studies,
showinghowthefeaturesdescribedinthepreviouschapterareusedforvariousnaturallanguage
tasks.Readersfamiliarwithlanguageprocessingcanskipthesetwochapters.Chapter8connects
thematerialofChapters6and7withneuralnetworks,anddiscussesthevariouswaysofencoding
language-basedfeaturesasinputsforneuralnetworks.Chapter9introducesthelanguagemodel-
ingtask,andthefeed-forwardneurallanguagemodelarchitecture.isalsopavesthewayfordis-
cussingpre-trainedwordembeddingsinthefollowingchapters.Chapter10discussesdistributed
and distributional approaches to word-meaning representations. It introduces the word-context
matrixapproachtodistributionalsemantics,aswellasneurallanguage-modelinginspiredword-
embedding algorithms, such as GV and W2V, and discusses the connection between
themand thedistributionalmethods.Chapter11 dealswith usingwordembeddingsoutside of
thecontextofneuralnetworks.Finally,Chapter12presentsacasestudyofatask-specificfeed-
forwardnetworkthatistailoredfortheNaturalLanguageInferencetask.
Part III introducing the specialized convolutional and recurrent architectures, consists of
five chapters. Chapter 13 deals with convolutional networks, that are specialized at learning in-
formative ngram patterns. e alternative hash-kernel technique is also discussed. e rest of
this part, Chapters 14–17, is devoted to RNNs. Chapter 14 describes the RNN abstraction for
modelingsequencesandstacks.Chapter15describesconcreteinstantiationsofRNNs,including
theSimpleRNN(alsoknownasElmanRNNs)andgatedarchitecturessuchastheLongShort-
term Memory (LSTM) and the Gated Recurrent Unit (GRU). Chapter 16 provides examples
ofmodelingwiththeRNNabstraction,showingtheirusewithinconcreteapplications.Finally,
Chapter17introducestheconditioned-generationframework,whichisthemainmodelingtech-
niquebehindstate-of-the-artmachinetranslation,aswellasunsupervisedsentencemodelingand
manyotherinnovativeapplications.
PartIVisamixofadvancedandnon-coretopics,andconsistsofthreechapters.Chapter18
introducestree-structuredrecursivenetworksformodelingtrees.Whileveryappealing,thisfam-
ilyofmodelsisstillinresearchstage,andisyettoshowaconvincingsuccessstory.Nonetheless,
itisanimportantfamilyofmodelstoknowforresearcherswhoaimtopushmodelingtechniques
beyondthestate-of-the-art.Readerswhoaremostlyinterestedinmatureandrobusttechniques
can safely skip this chapter. Chapter 19 deals with structured prediction. It is a rather techni-
cal chapter. Readers who are particularly interested in structured prediction, or who are already
familiar with structured prediction techniques for linear models or for language processing, will
likelyappreciatethematerial.Othersmayrathersafelyskipit.Finally,Chapter20presentsmulti-
task and semi-supervised learning. Neural networks provide ample opportunities for multi-task
andsemi-supervisedlearning.eseareimportanttechniques,thatarestillattheresearchstage.
However,theexistingtechniquesarerelativelyeasytoimplement,anddoproviderealgains.e
chapterisnottechnicallychallenging,andisrecommendedtoallreaders.8 1. INTRODUCTION
Dependencies Forthemostpart,chapters,dependonthechaptersthatprecedethem.Anex-
ceptionarethefirsttwochaptersofPartII,whichdonotdependonmaterialinpreviouschapters
andcanbereadinanyorder.Somechaptersandsectionscanbeskippedwithoutimpactingthe
understanding of other concepts or material. ese include Section 10.4 and Chapter 11 that
dealwiththedetailsofwordembeddingalgorithmsandtheuseofwordembeddingsoutsideof
neuralnetworks;Chapter12,describingaspecificarchitectureforattackingtheStanfordNatural
LanguageInference(SNLI)dataset;andChapter13describingconvolutionalnetworks.Within
thesequenceonrecurrentnetworks,Chapter15,dealingwiththedetailsofspecificarchitectures,
canalsoberelativelysafelyskipped.echaptersinPartIVareforthemostpartindependentof
eachother,andcanbeeitherskippedorreadinanyorder.
1.5 WHAT’SNOTCOVERED
efocusisonapplicationsofneuralnetworkstolanguageprocessingtasks.However,somesub-
areasoflanguageprocessingwithneuralnetworksweredeliberatelyleftoutofscopeofthisbook.
Specifically, I focus on processingwritten language, and do not cover working with speech data
oracousticsignals.Withinwrittenlanguage,Iremainrelativelyclosetothelowerlevel,relatively
well-defined tasks, and do not cover areas such as dialog systems, document summarization, or
questionanswering,whichIconsidertobevastlyopenproblems.Whilethedescribedtechniques
canbeusedtoachieveprogressonthesetasks,Idonotprovideexamplesorexplicitlydiscussthese
tasks directly. Semantic parsing is similarly out of scope. Multi-modal applications, connecting
languagedatawithothermodalitiessuchasvisionordatabasesareonlyverybrieflymentioned.
Finally, the discussion is mostly English-centric, and languages with richer morphological sys-
temsandfewercomputationalresourcesareonlyverybrieflydiscussed.
Someimportantbasics arealsonotdiscussed.Specifically,twocrucialaspectsofgoodworkin
languageprocessingareproperevaluationanddataannotation.Bothofthesetopicsareleftoutside
thescopeofthisbook,butthereadershouldbeawareoftheirexistence.
Properevaluationincludesthechoiceoftherightmetricsforevaluatingperformanceona
giventask,bestpractices,faircomparisonwithotherwork,performingerroranalysis,andassess-
ingstatisticalsignificance.
Dataannotation is the bread-and-butter of NLP systems. Without data, we cannot train
supervisedmodels.Asresearchers,weveryoftenjustuse“standard”annotateddataproducedby
someoneelse.Itisstillimportanttoknowthesourceofthedata,andconsidertheimplications
resulting from its creation process. Data annotation is a very vast topic, including proper for-
mulation of the annotation task; developing the annotation guidelines; deciding on the source
of annotated data, its coverage and class proportions, good train-test splits; and working with
annotators,consolidatingdecisions,validatingqualityofannotatorsandannotation,andvarious
similartopics.1.6. ANOTEONTERMINOLOGY 9
1.6 ANOTEONTERMINOLOGY
eword“feature”isusedtorefertoaconcrete,linguisticinputsuchasaword,asuffix,orapart-
of-speechtag.Forexample,inafirst-orderpart-of-speechtagger,thefeaturesmightbe“current
word,previousword,nextword,previouspartofspeech.”eterm“inputvector”isusedtorefer
totheactualinputthatisfedtotheneuralnetworkclassifier.Similarly,“inputvectorentry”refers
to a specific value of the input. is is in contrast to a lot of the neural networks literature in
whichtheword“feature”isoverloadedbetweenthetwouses,andisusedprimarilytorefertoan
input-vectorentry.
1.7 MATHEMATICALNOTATION
We use bold uppercase letters to represent matrices (X, Y, Z), and bold lowercase letters to
represent vectors (b). When there are series of related matrices and vectors (for example, where
each matrix corresponds to a different layer in the network), superscript indices are used (W1,
W2). For the rare cases in which we want indicate the power of a matrix or a vector, a pair of
brackets is added around the item to be exponentiated: .W/2;.W3/2. We use (cid:140)(cid:141) as the index
operator of vectors and matrices: b is the ith element of vector b, and W is the element
(cid:140)i(cid:141) (cid:140)i;j(cid:141)
in the ith column and jth row of matrix W. When unambiguous, we sometimes adopt the
more standard mathematical notation and use b to indicate the ith element of vector b, and
i
similarlyw forelementsofamatrixW.Weuse todenotethedot-productoperator:w v
i;j
(cid:1) (cid:1) D
P iw iv i DP iw (cid:140)i(cid:141)v (cid:140)i(cid:141). We use x 1 Wn to indicate a sequence of vectors x 1;:::;x n, and similarly
x isthesequenceofitemsx ;:::;x .Weusex toindicatethereversesequence.x (cid:140)i(cid:141)
1n 1 n n1 1n
W W W D
x ,x (cid:140)i(cid:141) x .Weuse(cid:140)v v (cid:141)todenotevectorconcatenation.
i n1 n i 1 1 2
W D (cid:0) C I
Whilesomewhatunorthodox,unlessotherwisestated,vectorsareassumedtoberowvec-
tors.echoicetouserowvectors,whicharerightmultipliedbymatrices(xW b),issomewhat
C
nonstandard—alotoftheneuralnetworksliteratureusecolumnvectorsthatareleftmultiplied
by matrices (Wx b). We trust the reader to be able to adapt to the column vectors notation
C
whenreadingtheliterature.³⁴
³⁴echoicetousetherowvectorsnotationwasinspiredbythefollowingbenefits:itmatchesthewayinputvectorsandnetwork
diagramsareoftendrawnintheliterature;itmakesthehierarchical/layeredstructureofthenetworkmoretransparentand
putstheinputastheleft-mostvariableratherthanbeingnested;itresultsinfullyconnectedlayerdimensionsbeingdin dout
ratherthandout din;anditmapsbettertothewaynetworksareimplementedincodeusingmatrixlibrariessuchasnu(cid:2)mpy.
(cid:2)PART I
Supervised Classification and
Feed-forward Neural Networks13
C H A P T E R 2
Learning Basics
and Linear Models
Neuralnetworks,thetopicofthisbook,areaclassofsupervisedmachinelearningalgorithms.
ischapterprovidesaquickintroductiontosupervisedmachinelearningterminologyand
practices,andintroduceslinearandlog-linearmodelsforbinaryandmulti-classclassification.
echapteralsosetsthestageandnotationforlaterchapters.Readerswhoarefamiliarwith
linearmodelscanskipaheadtothenextchapters,butmayalsobenefitfromreadingSections2.4
and2.5.
Supervisedmachinelearningtheoryandlinearmodelsareverylargetopics,andthischapter
is far from being comprehensive. For a more complete treatment the reader is referred to texts
suchasDauméIII[2015],Shalev-ShwartzandBen-David[2014],andMohrietal.[2012].
2.1 SUPERVISEDLEARNINGANDPARAMETERIZED
FUNCTIONS
eessenceofsupervisedmachinelearningisthecreationofmechanismsthatcanlookatexam-
plesandproducegeneralizations.Moreconcretely,ratherthandesigninganalgorithmtoperform
a task (“distinguish spam from non-spam email”), we design an algorithm whose input is a set
oflabeledexamples(“ispileofemailsarespam.isotherpileofemailsarenotspam.”),and
itsoutputisafunction(oraprogram)thatreceivesaninstance(anemail)andproducesthede-
siredlabel(spamornot-spam).Itisexpectedthattheresultingfunctionwillproducecorrectlabel
predictionsalsoforinstancesithasnotseenduringtraining.
Assearchingoverthesetofallpossibleprograms(orallpossiblefunctions)isaveryhard
(and rather ill-defined) problem, we often restrict ourselves to search over specific families of
functions,e.g.,thespaceofalllinearfunctionswithd inputsandd outputs,orthespaceofall
in out
decisiontreesoverd variables.Suchfamiliesoffunctionsarecalledhypothesisclasses.Byrestrict-
in
ingourselvestoaspecifichypothesisclass,weareinjectingthelearnerwithinductivebias—aset
ofassumptionsabouttheformofthedesiredsolution,aswellasfacilitatingefficientprocedures
forsearchingforthesolution.Forabroadandreadableoverviewofthemainfamiliesoflearning
algorithmsandtheassumptionsbehindthem,seethebookbyDomingos[2015].
e hypothesis class also determines what can and cannot be represented by the learner.
One common hypothesis class is that of high-dimensional linear function, i.e., functions of the14 2. LEARNINGBASICSANDLINEARMODELS
form:¹
f.x/ x W b (2.1)
D (cid:1) C
x Rdin W Rdin dout b Rdout:
(cid:2)
2 2 2
Here, the vector x is the input to the function, while the matrix W and the vector b are
the parameters. e goal of the learner is to set the values of the parameters W and b such that
thefunctionbehavesasintendedonacollectionofinputvaluesx x ;:::;x andthecorre-
1k 1 k
W D
spondingdesiredoutputsy y ;:::;y .etaskofsearchingoverthespaceoffunctionsis
1k D i k
thusreducedtooneofsearchWingoverthespaceofparameters.Itiscommontorefertoparameters
ofthefunctionas(cid:130).Forthelinearmodelcase,(cid:130) W;b.Insomecaseswewantthenotation
D
tomaketheparameterizationexplicit,inwhichcaseweincludetheparametersinthefunction’s
definition:f.x W;b/ x W b.
I D (cid:1) C
As we will see in the coming chapters, the hypothesis class of linear functions is rather
restricted, and there are many functions that it cannot represent (indeed, it is limited to linear
relations).Incontrast,feed-forwardneuralnetworkswithhiddenlayers,tobediscussedinChap-
ter 4, are also parameterized functions, but constitute a very strong hypothesis class—they are
universalapproximators,capableofrepresentinganyBorel-measurablefunction.²However,while
restricted,linearmodelshaveseveraldesiredproperties:theyareeasyandefficienttotrain,they
often result in convex optimization objectives, the trained models are somewhat interpretable,
and they are often very effective in practice. Linear and log-linear models were the dominant
approachesinstatisticalNLPforoveradecade.Moreover,theyserveasthebasicbuildingblocks
forthemorepowerfulnonlinearfeed-forwardnetworkswhichwillbediscussedinlaterchapters.
2.2 TRAIN,TEST,ANDVALIDATIONSETS
Beforedelvingintothedetailsoflinearmodels,let’sreconsiderthegeneralsetupofthemachine
learningproblem.Wearefacedwithadatasetofk inputexamplesx andtheircorresponding
1k
W
gold labels y , and our goal is to produce a function f.x/ that correctly maps inputs x to
1k
outputsy,aseWvidencedbythetrainingset.Howdoweknowthattheproducedfunctionf./is
O
indeed a good one? One could run the training examples x through f./, record the answers
1k
W
y ,comparethemtotheexpectedlabelsy ,andmeasuretheaccuracy.However,thisprocess
O1k 1k
wilWlnotbeveryinformative—ourmainconceWrnistheabilityoff./togeneralizewelltounseen
examples.Afunctionf./thatisimplementedasalookuptable,thatis,lookingfortheinputx
in its memory and returning the corresponding value y for instances is has seen and a random
valueotherwise,willgetaperfectscoreonthistest,yetisclearlynotagoodclassificationfunction
asithaszerogeneralizationability.Weratherhaveafunctionf./thatgetssomeofthetraining
exampleswrong,providingthatitwillgetunseenexamplescorrectly.
¹AsdiscussedinSection1.7.isbooktakesasomewhatun-orthodoxapproachandassumesvectorsarerowvectorsrather
thancolumnvectors.
²SeefurtherdiscussioninSection4.3.2.2. TRAIN,TEST,ANDVALIDATIONSETS 15
Leave-oneout Wemustassessthetrainedfunction’saccuracyoninstancesithasnotseenduring
training. One solution is to perform leave-one-out cross-validation: train k functions f , each
1k
W
time leaving out a different input example x , and evaluating the resulting function f ./ on its
i i
abilitytopredictx .entrainanotherfunctionf./ontheentiretrainingssetx .Assuming
i 1k
W
thatthetrainingsetisarepresentativesampleofthepopulation,thispercentageoffunctionsf ./
i
thatproducedcorrectpredictionontheleft-outsamplesisagoodapproximationoftheaccuracy
of f./ on new inputs. However, this process is very costly in terms of computation time, and is
usedonlyincaseswherethenumberofannotatedexamplesk isverysmall(lessthanahundred
or so). In language processing tasks, we very often encounter training sets with well over 105
examples.
Held-outset Amoreefficientsolutionintermsofcomputationtimeistosplitthetrainingset
intotwosubsets,sayina80%/20%split,trainamodelonthelargersubset(thetrainingset),and
testitsaccuracyonthesmallersubset(theheld-outset).iswillgiveusareasonableestimateon
theaccuracyofthetrainedfunction,oratleastallowustocomparethequalityofdifferenttrained
models. However, it is somewhat wasteful in terms training samples. One could then re-train a
model on the entire set. However, as the model is trained on substantially more data, the error
estimatesofthemodeltrainedonlessdatamaynotbeaccurate.isisgenerallyagoodproblem
tohave,asmoretrainingdataislikelytoresultinbetterratherthanworsepredictors.³
Some care mustbe taken when performing the split—in general it is better to shuffle the
examplespriortosplittingthem,toensureabalanceddistributionofexamplesbetweenthetrain-
ing and held-out sets (for example, you want the proportion of gold labels in the two sets to be
similar).However,sometimesarandomsplitisnotagoodoption:considerthecasewhereyour
inputarenewsarticlescollectedoverseveralmonths,andyourmodelisexpectedtoprovidepre-
dictionsfornewstories.Here,arandomsplitwillover-estimatethemodel’squality:thetraining
and held-out examples will be from the same time period, and hence on more similar stories,
whichwillnotbethecaseinpractice.Insuchcases,youwanttoensurethatthetrainingsethas
oldernewsstoriesandtheheld-outsetnewerones—tobeassimilaraspossibletohowthetrained
modelwillbeusedinpractice.
A three-way split e split into train and held-out sets works well if you train a single model
andwantstoassessitsquality.However,inpracticeyouoftentrainseveralmodels,comparetheir
quality, and select the best one. Here, the two-way split approach is insufficient—selecting the
bestmodelaccordingtotheheld-outset’saccuracywillresultinanoverlyoptimisticestimateof
themodel’squality.Youdon’tknowifthechosensettingsofthefinalclassifieraregoodingeneral,
orarejustgoodfortheparticularexamplesintheheld-outsets.eproblemwillbeevenworseif
youperformerroranalysisbasedontheheld-outset,andchangethefeaturesorthearchitectureof
themodelbasedontheobservederrors.Youdon’tknowifyourimprovementsbasedontheheld-
³Note,however,thatsomesettinginthetrainingprocedure,inparticularthelearningrateandregularizationweightmaybe
sensitivetothetrainingsetsize,andtuningthembasedonsomedataandthenre-trainingamodelwiththesamesettingson
largerdatamayproducesub-optimalresults.16 2. LEARNINGBASICSANDLINEARMODELS
outsetswillcarryovertonewinstances.eacceptedmethodologyistouseathree-waysplitof
thedataintotrain,validation(alsocalleddevelopment),andtestsets.isgivesyoutwoheld-out
sets:avalidationset (alsocalleddevelopmentset),andatestset.Alltheexperiments,tweaks,error
analysis,andmodelselectionshouldbeperformedbasedonthevalidationset.en,asinglerun
of the final model over the test set will give a good estimate of its expected quality on unseen
examples.Itisimportanttokeepthetestsetaspristineaspossible,runningasfewexperiments
as possible on it. Some even advocate that you should not even look at the examples in the test
set,soastonotbiasthewayyoudesignyourmodel.
2.3 LINEARMODELS
Nowthatwehaveestablishedsomemethodology,wereturntodescribelinearmodelsforbinary
andmulti-classclassification.
2.3.1 BINARYCLASSIFICATION
In binary classification problems we have a single output, and thus use a restricted version of
Equation(2.1)inwhichd 1,makingwavectorandb ascalar.
out
D
f.x/ x w b: (2.2)
D (cid:1) C
e range of the linear function in Equation (2.2) is (cid:140) ; (cid:141). In order to use it for
(cid:0)1 C1
binaryclassification,itiscommontopasstheoutputoff.x/throughthesignfunction,mapping
negativevaluesto 1(thenegativeclass)andnon-negativevaluesto 1(thepositiveclass).
(cid:0) C
Consider the task of predicting which of two neighborhoods an apartment is located at,
basedontheapartment’spriceandsize.Figure2.1showsa2Dplotofsomeapartments,where
the x-axis denotes the monthly rent price in USD, while the y-axis is the size in square feet.
ebluecirclesareforDupontCircle,DCandthegreencrossesareinFairfax,VA.Itisevident
from the plot that we can separate the two neighborhoods using a straight line—apartments in
DupontCircletendtobemoreexpensivethanapartmentsinFairfaxofthesamesize.⁴edataset
islinearlyseparable:thetwoclassescanbeseparatedbyastraightline.
Eachdata-point(anapartment)canberepresentedasa2-dimensional(2D)vectorxwhere
x istheapartment’ssizeandx isitsprice.Wethengetthefollowinglinearmodel:
(cid:140)0(cid:141) (cid:140)1(cid:141)
y sign.f.x// sign.x w b/
O D D (cid:1) C
sign.size w price w b/;
1 2
D (cid:2) C (cid:2) C
where is the dot-product operation, b and w (cid:140)w ;w (cid:141) are free parameters, and we predict
1 2
(cid:1) D
Fairfax if y 0 and Dupont Circle otherwise. e goal of learning is setting the values of w ,
1
O (cid:21)
⁴Notethatlookingateithersizeorpricealonewouldnotallowustocleanlyseparatethetwogroups.2.3. LINEARMODELS 17
2500
2000
1500
1000
500
0
1000 2000 3000 4000 5000
Price
Figure2.1: Housingdata:rentpriceinUSDvs.sizeinsquareft.Datasource:Craigslistads,collected
fromJune7–15,2015.
w , and b such that the predictions are correct for all data-points we observe.⁵ We will discuss
2
learninginSection2.7butfornowconsiderthatweexpectthelearningproceduretosetahigh
valuetow andalowvaluetow .Oncethemodelistrained,wecanclassifynewdata-pointsby
1 2
feedingthemintothisequation.
Itissometimesnotpossibletoseparatethedata-pointsusingastraightline(or,inhigherdi-
mensions,alinearhyperplane)—suchdatasetsaresaidtobenonlinearlyseparable,andarebeyond
thehypothesisclassoflinearclassifiers.esolutionwouldbetoeithermovetoahigherdimen-
sion(addmorefeatures),movetoaricherhypothesisclass,orallowforsomemis-classification.⁶
⁵Geometrically,foragivenwthepointsx w b 0defineahyperplane(whichintwodimensionscorrespondstoaline)
(cid:1) C D
thatseparatesthespaceintotworegions.egoaloflearningisthenfindingahyperplanesuchthattheclassificationinduced
byitiscorrect.
⁶Misclassifyingsomeoftheexamplesissometimesagoodidea.Forexample,ifwehavereasontobelievesomeofthedata-
pointsareoutliers—examplesthatbelongtooneclass,butarelabeledbymistakeasbelongingtotheotherclass.
eziS18 2. LEARNINGBASICSANDLINEARMODELS
FeatureRepresentations Intheexampleabove,eachdata-pointwasapairofsizeandprice
measurements.Eachofthesepropertiesisconsideredafeaturebywhichweclassifythedata-
point.isisveryconvenient,butinmostcasesthedata-pointsarenotgiventousdirectly
as lists of features, but as real-world objects. For example, in the apartments example we
maybegivenalistofapartmentstoclassify.Wethenneedtomakeaconciousdecisionand
select the measurable properties of the apartments that we believe will be useful features
for the classification task at hand. Here, it proved effective to focus on the price and the
size. We could also look at additional properties, such as the number of rooms, the height
of the ceiling, the type of floor, the geo-location coordinates, and so on. After deciding on
a set of features, we create a feature extraction function that maps a real world object (i.e.,
an apartment) to a vector of measurable quantities (price and size) which can be used as
inputstoourmodels.echoiceofthefeaturesiscrucialtothesuccessoftheclassification
accuracy,andisdrivenbytheinformativenessofthefeatures,andtheiravailabilitytous(the
geo-locationcoordinatesaremuchbetterpredictorsoftheneighborhoodthanthepriceand
size,butperhapsweonlyobservelistingsofpasttransactions,anddonothaveaccesstothe
geo-locationinformation).Whenwehavetwofeatures,itiseasytoplotthedataandseethe
underlyingstructures.However,asweseeinthenextexample,weoftenusemanymorethan
justtwofeatures,makingplottingandprecisereasoningimpractical.
Acentralpartinthedesignoflinearmodels,whichwemostlyglossoverinthistext,is
thedesignofthefeaturefunction(socalledfeatureengineering).Oneofthepromisesofdeep
learning is that it vastly simplifies the feature-engineering process by allowing the model
designer to specify a small set of core, basic, or “natural” features, and letting the trainable
neural network architecture combine them into more meaningful higher-level features, or
representations. However, one still needs to specify a suitable set of core features, and tie
themtoasuitablearchitecture.WediscusscommonfeaturesfortextualdatainChapters6
and7.
..
We usually have many more than two features. Moving to a language setup, consider the
taskofdistinguishingdocumentswritteninEnglishfromdocumentswritteninGerman.Itturns
out that letter frequencies make for quite good predictors (features) for this task. Even more
informative are counts of letter bigrams, i.e., pairs of consecutive letters.⁷ Assuming we have an
alphabet of 28 letters (a–z, space, and a special symbol for all other characters including digits,
punctuations, etc.) we represent a document as a 28 28 dimensional vector x R784, where
(cid:2) 2
eachentryx representsacountofaparticularlettercombinationinthedocument,normalized
(cid:140)i(cid:141)
by the document’s length. For example, denoting by x the entry of x corresponding to the
ab
⁷Whileonemaythinkthatwordswillalsobegoodpredictors,letters,orletter-bigramsarefarmorerobust:wearelikelyto
encounteranewdocumentwithoutanyofthewordsweobservedinthetrainingset,whileadocumentwithoutanyofthe
distinctiveletter-bigramsissignificantlylesslikely.2.3. LINEARMODELS 19
letter-bigramab:
#
ab
x ; (2.3)
ab
D D
j j
where # is the number of times the bigram ab appears in the document, and D is the total
ab
j j
numberofbigramsinthedocument(thedocument’slength).
_a_ d_s_td _dee _e neriei nn _o nret _t h _a_ d_s_td _dee _e neriei nn _o nret _t h
_a_ d_s_td _dee _e neriei nn _o nret _t h _a_ d_s_td _dee _e neriei nn _o nret _t h
_a_ d_s_td _dee _e neriei nn _o nret _t h _a_ d_s_td _dee _e neriei nn _o nret _t h
_a_ d_s_td _dee _e neriei nn _o nret _t h _a_ d_s_td _dee _e neriei nn _o nret _t h
_a_ d_s_td _dee _e neriei nn _o nret _t h _a_ d_s_td _dee _e neriei nn _o nret _t h
Figure2.2: Character-bigramhistogramsfordocumentsinEnglish(left,blue)andGerman(right,
green).Underscoresdenotespaces.
Figure 2.2 shows such bigram histograms for several German and English texts. For
readability, we only show the top frequent character-bigrams and not the entire feature vectors.
Ontheleft,weseethebigramsoftheEnglishtexts,andontherightoftheGermanones.ere
areclearpatternsinthedata,and,givenanewitem,suchas:
_a_ d_s_td _dee _e neriei nn _o nret _t h20 2. LEARNINGBASICSANDLINEARMODELS
you could probably tell that it is more similar to the German group than to the English one.
Note,however,thatyoucouldn’tuseasingledefiniterulesuchas“ifithasthitsEnglish”or“if
it has ie its German”: while German texts have considerably less th than English, the th may
and does occur in German texts, and similarly the ie combination does occur in English. e
decisionrequiresweightingdifferentfactorsrelativetoeachother.Let’sformalizetheproblemin
amachine-learningsetup.
Wecanagainusealinearmodel:
y sign.f.x// sign.x w b/
O D D (cid:1) C (2.4)
sign.x w x w x w ::: b/:
aa aa ab ab ac ac
D (cid:2) C (cid:2) C (cid:2) C
AdocumentwillbeconsideredEnglishiff.x/ 0andasGermanotherwise.Intuitively,
(cid:21)
learningshouldassignlargepositivevaluestowentriesassociatedwithletterpairsthataremuch
morecommoninEnglishthaninGerman(i.e.,th)negativevaluestoletterpairsthataremuch
morecommoninGermanthaninEnglish(ie,en),andvaluesaroundzerotoletterpairsthatare
eithercommonorrareinbothlanguages.
Note that unlike the 2D case of the housing data (price vs. size), here we cannot easily
visualize the points and the decision boundary, and the geometric intuition is likely much less
clear. In general, it is difficult for most humans to think of the geometries of spaces with more
thanthreedimensions,anditisadvisabletothinkoflinearmodelsintermsofassigningweights
tofeatures,whichiseasiertoimagineandreasonabout.
2.3.2 LOG-LINEARBINARYCLASSIFICATION
e output f.x/ is in the range (cid:140) ; (cid:141), and we map it to one of two classes 1; 1 using
(cid:0)1 1 f(cid:0) C g
the sign function. is is a good fit if all we care about is the assigned class. However, we may
beinterestedalsointheconfidenceofthedecision,ortheprobabilitythattheclassifierassignsto
the class. An alternative that facilitates this is to map instead to the range (cid:140)0;1(cid:141), by pushing the
outputthroughasquashingfunctionsuchasthesigmoid(cid:27).x/ 1 ,resultingin:
D 1 Ce (cid:0)x
1
y (cid:27).f.x// : (2.5)
O D D 1 e .xw b/
C (cid:0) (cid:1) C
Figure2.3showsaplotofthesigmoidfunction.Itismonotonicallyincreasing,andmapsvalues
totherange(cid:140)0;1(cid:141),with0beingmappedto 1.Whenusedwithasuitablelossfunction(discussed
2
inSection2.7.1)thebinarypredictionsmadethroughthelog-linearmodelcanbeinterpretedas
class membership probability estimates (cid:27).f.x// P.y 1 x/ of x belonging to the positive
D O D j
class. We also get P.y 0 x/ 1 P.y 1 x/ 1 (cid:27).f.x//. e closer the value is to
O D j D (cid:0) O D j D (cid:0)
0 or 1 the more certain the model is in its class membership prediction, with the value of 0.5
indicatingmodeluncertainty.2.3. LINEARMODELS 21
σ(x)
1.0
0.8
0.6
0.4
0.2
0.0
-6 -4 -2 0 2 4 6
Figure2.3: esigmoidfunction(cid:27).x/.
2.3.3 MULTI-CLASSCLASSIFICATION
e previous examples were of binary classification, where we had two possible classes. Binary-
classificationcasesexist,butmostclassificationproblemsareofamulti-classnature,inwhichwe
shouldassignanexampletooneofkdifferentclasses.Forexample,wearegivenadocumentand
asked to classify it into one of six possible languages: English,French,German,Italian,Spanish,
Other.ApossiblesolutionistoconsidersixweightvectorswE;wF;:::andbiases,oneforeach
language,andpredictthelanguageresultinginthehighestscore:⁸
y f.x/ argmax x wL bL: (2.6)
O D D (cid:1) C
L E;F;G;I;S;O
2f g
e six sets of parameters wL R784; bL can be arranged as a matrix W R784 6 and
(cid:2)
2 2
vectorb R6,andtheequationre-writtenas:
2
y f.x/ x W b
O D D (cid:1) C
(2.7)
prediction y argmaxy :
D O D O(cid:140)i(cid:141)
i
Herey R6isavectorofthescoresassignedbythemodeltoeachlanguage,andweagain
O 2
determinethepredictedlanguagebytakingtheargmaxovertheentriesofy.
O
⁸erearemanywaystomodelmulti-classclassification,includingbinary-to-multi-classreductions.esearebeyondthe
scopeofthisbook,butagoodoverviewcanbefoundinAllweinetal.[2000].22 2. LEARNINGBASICSANDLINEARMODELS
2.4 REPRESENTATIONS
Consider the vector y resulting from applying Equation 2.7 of a trained model to a document.
O
evectorcanbeconsideredasarepresentationofthedocument,capturingthepropertiesofthe
document that are important to us, namely the scores of the different languages. e represen-
tation y contains strictly more information than the prediction y argmax y : for example,
O O D i O(cid:140)i(cid:141)
y canbeusedtodistinguishdocumentsinwhichthemainlanguageinGerman,butwhichalso
O
containasizeable amount ofFrench words.By clusteringdocuments basedontheirvectorrep-
resentationsasassignedbythemodel,wecouldperhapsdiscoverdocumentswritteninregional
dialects,orbymultilingualauthors.
e vectors x containing the normalized letter-bigram counts for the documents are also
representations of the documents, arguably containinga similar kind of informationto the vec-
tors y. However, the representations in y is more compact (6 entries instead of 784) and more
O O
specialized for the language prediction objective (clustering by the vectors x would likely reveal
document similarities that are not due to a particular mix of languages, but perhaps due to the
document’stopicorwritingstyles).
e trained matrix W R784 6 can also be considered as containing learned representa-
(cid:2)
2
tions. As demonstrated in Figure 2.4, we can consider two views of W, as rows or as columns.
Eachofthe6columnsofW correspondtoaparticularlanguage,andcanbetakentobea784-
dimensionalvectorrepresentationofthislanguageintermsofitscharacteristicletter-bigrampat-
terns.Wecanthenclusterthe6languagevectorsaccordingtotheirsimilarity.Similarly,eachof
the784rowsofW correspondtoaparticularletter-bigram,andprovidea6-dimensionalvector
representationofthatbigramintermsofthelanguagesitprompts.
Representationsarecentraltodeeplearning.Infact,onecouldarguethatthemainpowerof
deep-learningistheabilitytolearngoodrepresentations.Inthelinearcase,therepresentationsare
interpretable,inthesensethatwecanassignameaningfulinterpretationtoeachdimensioninthe
representationvector(e.g.,eachdimensioncorrespondstoaparticularlanguageorletter-bigram).
isisingeneralnotthecase—deeplearningmodelsoftenlearnacascadeofrepresentationsof
theinputthatbuildontopofeachother,inordertobestmodeltheproblemathand,andthese
representationsareoftennotinterpretable—wedonotknowwhichpropertiesoftheinputthey
capture.However,theyarestillveryusefulformakingpredictions.Moreover,attheboundariesof
themodel,i.e.,attheinputandtheoutput,wegetrepresentationsthatcorrespondtoparticular
aspects of the input (i.e., a vector representation for each letter-bigram) or the output (i.e., a
vectorrepresentationofeachoftheoutputclasses).WewillgetbacktothisinSection8.3after
discussingneuralnetworksandencodingcategoricalfeaturesasdensevectors.Itisrecommended
thatyoureturntothisdiscussiononcemoreafterreadingthatsection.2.5. ONE-HOTANDDENSEVECTORREPRESENTATIONS 23
(a) (b)
aa
ab
ac
ad
ae
af
ag
ah
zy
zz
W W
Figure2.4: Two views of the W matrix. (a) Each column corresponds to a language. (b) Each row
correspondstoaletterbigram.
2.5 ONE-HOTANDDENSEVECTORREPRESENTATIONS
einputvectorxinourlanguageclassificationexamplecontainsthenormalizedbigramcounts
in the document D. is vector can be decomposed into an average of D vectors, each corre-
j j
spondingtoaparticulardocumentpositioni:
D
1 j j
x XxD(cid:140)i(cid:141) (2.8)
D D I
j j i 1
D
here,D isthebigramatdocumentpositioni,andeachvectorxD(cid:140)i(cid:141) R784 isaone-hot vector,
(cid:140)i(cid:141)
2
inwhichallentriesarezeroexceptthesingleentrycorrespondingtotheletterbigramD ,which
(cid:140)i(cid:141)
is1.
e resulting vector x is commonly referred to as an averaged bag of bigrams (more gen-
erallyaveragedbagofwords,orjustbagofwords).Bag-of-words(BOW)representationscontain
informationabouttheidentitiesofallthe“words”(here,bigrams)ofthedocument,withoutcon-
sideringtheirorder.Aone-hotrepresentationcanbeconsideredasabag-of-a-single-word.
eviewoftherowsofthematrixW asrepresentationsoftheletterbigramssuggestsan
alternativewayofcomputingthedocumentrepresentationvectory inEquation(2.7).Denoting
O
En Fr Gr It Sp O24 2. LEARNINGBASICSANDLINEARMODELS
byWD(cid:140)i(cid:141) therowofW correspondingtothebigramD ,wecantaketherepresentationy ofa
(cid:140)i(cid:141)
documentD tobetheaverageoftherepresentationsoftheletter-bigramsinthedocument:
D
1 j j
y XWD(cid:140)i(cid:141): (2.9)
O D D
j j i 1
D
isrepresentationisoftencalledacontinuousbagofwords(CBOW),asitiscomposedofa
sumofwordrepresentations,whereeach“word”representationisalow-dimensional,continuous
vector.
WenotethatEquation(2.9)andthetermx W inEquation(2.7)areequivalent.Tosee
(cid:1)
why,consider:
y x W
D (cid:1)
0 D 1
1 j j
XxD(cid:140)i(cid:141) W
D@ D A(cid:1)
j j i 1
D
D (2.10)
1 j j
X.xD(cid:140)i(cid:141) W/
D D (cid:1)
j j i 1
D
D
1 j j
XWD(cid:140)i(cid:141):
D D
j j i 1
D
In other words, the continuous-bag-of-words (CBOW) representation can be obtained
either by summing word-representation vectors or by multiplying a bag-of-words vector by a
matrix in which each row corresponds to a dense word representation (such matrices are also
called embeddingmatrices). We will return to this point in Chapter 8 (in particular Section 8.3)
whendiscussingfeaturerepresentationsindeeplearningmodelsfortext.
2.6 LOG-LINEARMULTI-CLASSCLASSIFICATION
Inthebinarycase,wetransformedthelinearpredictionintoaprobabilityestimatebypassingit
throughthesigmoidfunction,resultinginalog-linearmodel.eanalogforthemulti-classcase
ispassingthescorevectorthroughthesoftmaxfunction:
x
e (cid:140)i(cid:141)
softmax.x/ : (2.11)
(cid:140)i(cid:141) x
D P e (cid:140)j(cid:141)
j
Resultingin:2.7. TRAININGASOPTIMIZATION 25
y softmax.xW b/
O D C
e.xW b/(cid:140)i(cid:141) (2.12)
y C :
O(cid:140)i(cid:141) D
P
j
e.xW Cb/(cid:140)j(cid:141)
esoftmaxtransformationforcesthevaluesiny tobepositiveandsumto1,makingthem
O
interpretableasaprobabilitydistribution.
2.7 TRAININGASOPTIMIZATION
Recall that the input to a supervised learning algorithm is a trainingset of n training examples
x x ;x ;:::;x togetherwithcorrespondinglabelsy y ;y ;:::;y .Withoutlossof
1n 1 2 n 1n 1 2 n
W D W D
generality,weassumethatthedesiredinputsandoutputsarevectors:x ,y .⁹
1n 1n
egoalofthealgorithmistoreturnafunctionf./thataccurateW lymaWpsinputexamples
to their desired labels, i.e., a function f./ such that the predictions y f.x/ over the training
O D
setareaccurate.Tomakethismoreprecise,weintroducethenotionofalossfunction,quantifying
the loss suffered when predicting y while the true label is y. Formally, a loss function L.y;y/
O O
assignsanumericalscore(ascalar)toapredictedoutputy giventhetrueexpectedoutputy.e
O
loss function should be bounded from below, with the minimum attained only for cases where
thepredictioniscorrect.
eparametersofthelearnedfunction(thematrixW andthebiasesvectorb)arethenset
inordertominimizethelossLoverthetrainingexamples(usually,itisthesumofthelossesover
thedifferenttrainingexamplesthatisbeingminimized).
Concretely, given a labeled training set .x ;y /, a per-instance loss function L and a
1n 1n
parameterizedfunctionf.x (cid:130)/wedefinethecorpW us-wWidelosswithrespecttotheparameters(cid:130)
I
astheaveragelossoveralltrainingexamples:
n
1
L.(cid:130)/ XL.f.x (cid:130)/;y /: (2.13)
D n i I i
i 1
D
In this view, the training examples are fixed, and the values of the parameters determine
theloss.egoalofthetrainingalgorithmisthentosetthevaluesoftheparameters(cid:130)suchthat
L
thevalueof isminimized:
n
1
(cid:130) argminL.(cid:130)/ argmin XL.f.x (cid:130)/;y /: (2.14)
O D D n i I i
(cid:130) (cid:130) i 1
D
Equation(2.14)attemptstominimizethelossatallcosts,whichmayresultinoverfitting
thetrainingdata.Tocounterthat,weoftenposesoftrestrictionsontheformofthesolution.is
⁹Inmanycasesitisnaturaltothinkoftheexpectedoutputasascalar(classassignment)ratherthanavector.Insuchcases,y
issimplythecorrespondingone-hotvector,andargmax iy (cid:140)i(cid:141)isthecorrespondingclassassignment.26 2. LEARNINGBASICSANDLINEARMODELS
is done using a function R.(cid:130)/ taking as input the parameters and returning a scalar that reflect
their “complexity,” which we want to keep low. By adding R to the objective, the optimization
problemneedstobalancebetweenlowlossandlowcomplexity:
0 loss regularization1
(cid:130) n (cid:133)(cid:132) (cid:131) (cid:130)(cid:133)(cid:132)(cid:131)
B1 C
(cid:130) O DargminB BnXL.f.x i I(cid:130)/;y i/ C (cid:21)R.(cid:130)/ C C: (2.15)
(cid:130) B i 1 C
@ D A
efunctionRiscalledaregularizationterm.Differentcombinationsoflossfunctionsand
regularizationcriteriaresultindifferentlearningalgorithms,withdifferentinductivebiases.
Wenowturntodiscusscommonlossfunctions(Section2.7.1),followedbyadiscussionof
regularizationandregularizers(Section2.7.2).en,inSection2.8wepresentanalgorithmfor
solvingtheminimizationproblem(Equation(2.15)).
2.7.1 LOSSFUNCTIONS
e loss can be an arbitrary function mapping two vectors to a scalar. For practical purposes of
optimization,werestrictourselvestofunctionsforwhichwecaneasilycomputegradients(orsub-
gradients).¹⁰Inmostcases,itissufficientandadvisabletorelyonacommonlossfunctionrather
thandefiningyourown.Foradetaileddiscussionandtheoreticaltreatmentoflossfunctionsfor
binaryclassification,seeZhang[2004].Wenowdiscusssomelossfunctionsthatarecommonly
usedwithlinearmodelsandwithneuralnetworksinNLP.
Hinge(binary) Forbinaryclassificationproblems,theclassifier’soutputisasinglescalary and
Q
the intended output y is in 1; 1 . e classification rule is y sign.y/, and a classification
fC (cid:0) g O D Q
isconsideredcorrectify y >0,meaningthaty andy sharethesamesign.ehingeloss,also
(cid:1) Q Q
knownasmarginlossorSVMloss,isdefinedas:
L .y;y/ max.0;1 y y/: (2.16)
hinge(binary)
Q D (cid:0) (cid:1) Q
elossis0wheny andy sharethesamesignand y 1.Otherwise,thelossislinear.
Q jQj(cid:21)
In other words, the binary hinge loss attempts to achieve a correct classification, with a margin
ofatleast1.
Hinge (multi-class) e hinge loss was extended to the multi-class setting by Crammer and
Singer[2002].Lety y ;:::;y betheclassifier’soutputvector,andybetheone-hotvector
O D O(cid:140)1(cid:141) O(cid:140)n(cid:141)
forthecorrectoutputclass.
eclassificationruleisdefinedasselectingtheclasswiththehighestscore:
prediction argmaxy : (2.17)
D O(cid:140)i(cid:141)
i
¹⁰Agradientofafunctionwithkvariablesisacollectionofkpartialderivatives,oneaccordingtoeachofthevariables.Gradients
arediscussedfurtherinSection2.8.2.7. TRAININGASOPTIMIZATION 27
Denotebyt argmax y thecorrectclass,andbyk argmax y thehighestscoring
D i (cid:140)i(cid:141) D i t O(cid:140)i(cid:141)
classsuchthatk t.emulti-classhingelossisdefinedas: ⁄
⁄
L .y;y/ max.0;1 .y y //: (2.18)
hinge(multi-class) O D (cid:0) O(cid:140)t(cid:141)(cid:0) O(cid:140)k(cid:141)
emulti-classhingelossattemptstoscorethecorrectclassaboveallotherclasseswithamargin
ofatleast1.
Both the binary and multi-class hinge losses are intended to be used with linear outputs.
ehingelossesareusefulwheneverwerequireaharddecisionrule,anddonotattempttomodel
classmembershipprobability.
Log loss e log loss is a common variation of the hinge loss, which can be seen as a “soft”
versionofthehingelosswithaninfinitemargin[LeCunetal.,2006]:
L .y;y/ log.1 exp. .y y //: (2.19)
log O D C (cid:0) O(cid:140)t(cid:141)(cid:0) O(cid:140)k(cid:141)
Binarycrossentropy ebinarycross-entropyloss,alsoreferredtoaslogisticlossisusedinbinary
classificationwithconditionalprobabilityoutputs.Weassumeasetoftwotargetclasseslabeled
0and1,withacorrectlabely 0;1 .eclassifier’soutputy istransformedusingthesigmoid
2f g Q
(alsocalledthelogistic)function(cid:27).x/ 1=.1 e x/totherange(cid:140)0;1(cid:141),andisinterpretedasthe
(cid:0)
D C
conditionalprobabilityy (cid:27).y/ P.y 1x/.epredictionruleis:
O D Q D D j
(0 y <0:5
prediction O
D 1 y 0:5:
O (cid:21)
enetworkistrainedtomaximizethelogconditionalprobabilitylogP.y 1x/foreach
D j
trainingexample.x;y/.elogisticlossisdefinedas:
L .y;y/ ylogy .1 y/log.1 y/: (2.20)
logistic
O D(cid:0) O (cid:0) (cid:0) (cid:0) O
elogisticlossisusefulwhenwewantthenetworktoproduceclassconditionalprobability
forabinaryclassificationproblem.Whenusingthelogisticloss,itisassumedthattheoutputlayer
istransformedusingthesigmoidfunction.
Categoricalcross-entropyloss ecategoricalcross-entropyloss(alsoreferredtoasnegativelog
likelihood)isusedwhenaprobabilisticinterpretationofthescoresisdesired.
Lety y ;:::;y beavectorrepresentingthetruemultinomialdistributionoverthe
D (cid:140)1(cid:141) (cid:140)n(cid:141)
labels1;:::;n,¹¹andlety y ;:::;y bethelinearclassifier’soutput,whichwastransformed
O D O(cid:140)1(cid:141) O(cid:140)n(cid:141)
bythesoftmaxfunction(Section2.6),andrepresenttheclassmembershipconditionaldistribu-
tiony P.y i x/.ecategoricalcrossentropylossmeasuresthedissimilaritybetweenthe
O(cid:140)i(cid:141) D D j
truelabeldistributiony andthepredictedlabeldistributiony,andisdefinedascrossentropy:
O
L .y;y/ Xy log.y /: (2.21)
cross-entropy O D(cid:0) (cid:140)i(cid:141) O(cid:140)i(cid:141)
i
¹¹isformulationassumesaninstancecanbelongtoseveralclasseswithsomedegreeofcertainty.28 2. LEARNINGBASICSANDLINEARMODELS
Forhard-classificationproblemsin which each trainingexample has a singlecorrectclass
assignment,y isaone-hotvectorrepresentingthetrueclass.Insuchcases,thecrossentropycan
besimplifiedto:
L .y;y/ log.y /; (2.22)
cross-entropy(hardclassification) O D(cid:0) O(cid:140)t(cid:141)
wheret isthecorrectclassassignment.isattemptstosettheprobabilitymassassignedtothe
correctclasst to1.Becausethescoresy havebeentransformedusingthesoftmaxfunctiontobe
O
non-negativeandsumtoone,increasingthemassassignedtothecorrectclassmeansdecreasing
themassassignedtoalltheotherclasses.
e cross-entropy loss is very common in the log-linear models and the neural networks
literature,andproducesamulti-classclassifierwhichdoesnotonlypredicttheone-bestclasslabel
but also predicts a distribution over the possible labels. When using the cross-entropy loss, it is
assumedthattheclassifier’soutputistransformedusingthesoftmaxtransformation.
Ranking losses In some settings, we are not given supervision in term of labels, but rather as
pairsofcorrectandincorrectitemsxandx ,andourgoalistoscorecorrectitemsaboveincorrect
0
ones.Suchtrainingsituationsarisewhenwehaveonlypositiveexamples,andgeneratenegative
examples by corrupting a positive example. A useful loss in such scenarios is the margin-based
rankingloss,definedforapairofcorrectandincorrectexamples:
L .x;x / max.0;1 .f.x/ f.x ///; (2.23)
ranking(margin) 0 0
D (cid:0) (cid:0)
wheref.x/isthescoreassignedbytheclassifierforinputvectorx.eobjectiveistoscore(rank)
correctinputsoverincorrectoneswithamarginofatleast1.
Acommonvariationistousethelogversionoftherankingloss:
L .x;x / log.1 exp. .f.x/ f.x ////: (2.24)
ranking(log) 0 0
D C (cid:0) (cid:0)
Examplesusingtherankinghingelossinlanguagetasksincludetrainingwiththeauxiliary
tasksusedforderivingpre-trainedwordembeddings(seeSection10.4.2),inwhichwearegivena
correctwordsequenceandacorruptedwordsequence,andourgoalistoscorethecorrectsequence
above the corrupt one [Collobert and Weston, 2008]. Similarly, Van de Cruys [2014] used the
ranking loss in a selectional-preferences task, in which the network was trained to rank correct
verb-object pairs above incorrect, automatically derived ones, and Weston et al. [2013] trained
a model to score correct (head, relation, tail) triplets above corrupted ones in an information-
extractionsetting.AnexampleofusingtherankingloglosscanbefoundinGaoetal.[2014].A
variationoftherankingloglossallowingforadifferentmarginforthenegativeandpositiveclass
isgivenindosSantosetal.[2015].2.7. TRAININGASOPTIMIZATION 29
2.7.2 REGULARIZATION
Consider the optimization problem in Equation (2.14). It may admit multiple solutions, and,
especiallyinhigherdimensions,itcanalsoover-fit.Considerourlanguageidentificationexample,
andasettinginwhichoneofthedocumentsinthetrainingset(callitx )isanoutlier:itisactually
o
in German, but is labeled as French. In order to drive the loss down, the learner can identify
features(letterbigrams)inx thatoccurinonlyfewotherdocuments,andgivethemverystrong
o
weightstowardthe(incorrect)Frenchclass.en,forotherGermandocumentsinwhichthese
features occur, which may now be mistakenly classified as French, the learner will find other
Germanletterbigramsandwillraisetheirweightsinorderforthedocumentstobeclassifiedas
Germanagain.isisabadsolutiontothelearningproblem,asitlearnssomethingincorrect,and
can cause test German documents which share many words with x to be mistakenly classified
o
as French. Intuitively, we would like to control for such cases by driving the learner away from
suchmisguidedsolutionsandtowardmorenaturalones,inwhichitisOKtomis-classifyafew
examplesiftheydon’tfitwellwiththerest.
isisachievedbyaddingaregularizationtermRtotheoptimizationobjective,whosejob
istocontrolthecomplexityoftheparametervalue,andavoidcasesofoverfitting:
(cid:130) argminL.(cid:130)/ (cid:21)R.(cid:130)/
O D C
(cid:130)
n (2.25)
1
argmin XL.f.x (cid:130)/;y / (cid:21)R.(cid:130)/:
D n i I i C
(cid:130) i 1
D
e regularization term considers the parameter values, and scores their complexity. We
thenlookforparametervaluesthathavebothalowlossandlowcomplexity.Ahyperparameter¹²
(cid:21) is used to control the amount of regularization: do we favor simple model over low loss ones,
orviceversa.evalueof(cid:21)hastobesetmanually,basedontheclassificationperformanceona
development set. While Equation (2.25) has a single regularization function and (cid:21) value for all
theparameters,itisofcoursepossibletohaveadifferentregularizerforeachitemin(cid:130).
In practice, the regularizers R equate complexity with large weights, and work to keep
the parameter values low. In particular, the regularizers R measure the norms of the parameter
matrices,anddrivethelearnertowardsolutionswithlownorms.CommonchoicesforRarethe
L norm,theL norm,andtheelastic-net.
2 1
L regularization InL regularization,RtakestheformofthesquaredL normoftheparam-
2 2 2
eters,tryingtokeepthesumofthesquaresoftheparametervalueslow:
R .W/ W 2 X.W /2: (2.26)
L2 Djj jj2 D (cid:140)i;j(cid:141)
i;j
¹²Ahyperparameterisaparameterofthemodelwhichisnotlearnedaspartoftheoptimizationprocess,butneedstobesetby
hand.30 2. LEARNINGBASICSANDLINEARMODELS
eL regularizerisalsocalledagaussianprior orweightdecay.
2
NotethatL regularizedmodelsareseverelypunishedforhighparameterweights,butonce
2
thevalueiscloseenoughtozero,theireffectbecomesnegligible.emodelwillprefertodecrease
the value of one parameter with high weight by 1 than to decrease the value of ten parameters
thatalreadyhaverelativelylowweightsby0.1each.
L regularization In L regularization, R takes the form of the L norm of the parameters,
1 1 1
tryingtokeepthesumoftheabsolutevaluesoftheparameterslow:
R .W/ W XW : (2.27)
L1
Djj
jj1
D j
(cid:140)i;j(cid:141)
j
i;j
In contrast to L , the L regularizer is punished uniformly for low and high values, and
2 1
has an incentive to decrease all the non-zero parameter values toward zero. It thus encourages
a sparse solutions—models with many parameters with a zero value. e L regularizer is also
1
calledasparseprior orlasso[Tibshirani,1994].
Elastic-Net e elastic-net regularization [Zou and Hastie, 2005] combines both L and L
1 2
regularization:
R .W/ (cid:21) R .W/ (cid:21) R .W/: (2.28)
elastic-net
D
1 L1
C
2 L2
Dropout AnotherformofregularizationwhichisveryeffectiveinneuralnetworksisDropout,
whichwediscussinSection4.6.
2.8 GRADIENT-BASEDOPTIMIZATION
In order to train the model, we need to solve the optimization problem in Equation (2.25). A
commonsolutionistouseagradient-basedmethod.Roughlyspeaking,gradient-basedmethods
L
work by repeatedly computing an estimate of the loss over the training set, computing the
gradientsoftheparameters(cid:130)withrespecttothelossestimate,andmovingtheparametersinthe
opposite directions of the gradient. e different optimization methods differ in how the error
estimateiscomputed,andhow“movingintheoppositedirectionofthegradient”isdefined.We
describethebasicalgorithm,stochasticgradientdescent(SGD),andthenbrieflymentiontheother
approacheswithpointersforfurtherreading.
Motivating Gradient-based Optimization Consider the task of finding the scalar value
x that minimizes a function y f.x/. e canonical approach is computing the second
D
derivativef .x/ofthefunction,andsolvingforf .x/ 0togettheextremapoints.Forthe
00 00
D
sakeofexample,assumethisapproachcannotbeused(indeed,itischallengingtousethisap-
proachinfunctionofmultiplevariables).Analternativeapproachisanumericone:compute
.. the first derivative f 0.x/. en, start with an initial guess value x i. Evaluating u f 0.x i/
D2.8. GRADIENT-BASEDOPTIMIZATION 31
will give the direction of change. If u 0, then x is an optimum point. Otherwise, move
i
D
intheoppositedirectionofubysettingx x (cid:17)u,where(cid:17)isarateparameter.With
i 1 i
C (cid:0)
asmallenoughvalueof(cid:17),f.x /willbesmallerthanf.x /.Repeatingthisprocess(with
i 1 i
C
properlydecreasingvaluesof(cid:17))willfindanoptimumpointx .Ifthefunctionf./isconvex,
i
the optimum will be a global one. Otherwise, the process is only guaranteed to find a local
optimum.
Gradient-basedoptimizationsimplygeneralizesthisideaforfunctionswithmultiple
variables.Agradientofafunctionwithk variablesisthecollectionsofk partialderivatives,
oneaccordingtoeachofthevariables.Movingtheinputsinthedirectionofthegradientwill
increasethevalueofthefunction,whilemovingthemintheoppositedirectionwilldecrease
it.WhenoptimizingthelossL.(cid:130) x ;y /,theparameters(cid:130)areconsideredasinputsto
thefunction,whilethetrainingexI am1 pWn lesa1 rWen
treatedasconstants.
..
Convexity Ingradient-basedoptimization,itiscommontodistinguishbetweenconvex(or
concave) functions and non-convex (non-concave) functions. A convex function is a function
whose second-derivative is always non-negative. As a consequence, convex functions have
asingleminimumpoint.Similarly,concavefunctionsarefunctionswhosesecond-derivatives
arealwaysnegativeorzero,andasaconsequencehaveasinglemaximumpoint.Convex(con-
cave)functionshavethepropertythattheyareeasytominimize(maximize)usinggradient-
basedoptimization—simplyfollowthegradientuntilanextremumpointisreached,andonce
itisreachedweknowweobtainedtheglobalextremumpoint.Incontrast,forfunctionsthat
areneitherconvexnorconcave,agradient-basedoptimizationproceduremayconvergetoa
localextremumpoint,missingtheglobaloptimum.
..
2.8.1 STOCHASTICGRADIENTDESCENT
AneffectivemethodfortraininglinearmodelsisusingtheSGDalgorithm[Bottou,2012,LeCun
et al., 1998a] or a variant of it. SGD is a general optimization algorithm. It receives a function
f parameterized by (cid:130), a loss function L, and desired input and output pairs x ;y . It then
1n 1n
attemptstosettheparameters(cid:130)suchthatthecumulativelossoff onthetrainW ingexWamplesis
small.ealgorithmworks,asshowninAlgorithm2.1.
e goal of the algorithm is to set the parameters (cid:130) so as to minimize the total loss
L.(cid:130)/ DPn
i
1L.f.x
i
I(cid:18)/;y i/ over the training set. It works by repeatedly sampling a training
exampleandDcomputingthegradientoftheerrorontheexamplewithrespecttotheparameters
(cid:130) (line 4)—the input and expected output are assumed to be fixed, and the loss is treated as a
functionoftheparameters(cid:130).eparameters(cid:130)arethenupdatedintheoppositedirectionofthe
gradient,scaledbyalearningrate(cid:17) (line5).elearningratecaneitherbefixedthroughoutthe
t32 2. LEARNINGBASICSANDLINEARMODELS
Algorithm2.1Onlinestochasticgradientdescenttraining.
Input:
-Functionf.x (cid:130)/parameterizedwithparameters(cid:130).
I
-Trainingsetofinputsx ;:::;x anddesiredoutputsy ;:::;y .
1 n 1 n
-LossfunctionL.
1: whilestoppingcriterianotmet do
2: Sampleatrainingexamplex i;y i
3: ComputethelossL.f.x i (cid:130)/;y i/
I
4: g gradientsofL.f.x i (cid:130)/;y i/w.r.t(cid:130)
O I
5: (cid:130) (cid:130) (cid:17) tg
(cid:0) O
6: return(cid:130)
trainingprocess,ordecayasafunctionofthetimestept.¹³ Forfurtherdiscussiononsettingthe
learningrate,seeSection5.2.
Note that the error calculated in line 3 is based on a single training example, and is thus
L
just a rough estimate of the corpus-wide loss that we are aiming to minimize. e noise in
thelosscomputationmayresultininaccurategradients.Acommonwayofreducingthisnoiseis
to estimate the error and the gradients based on a sample of m examples. is gives rise to the
minibatchSGDalgorithm(Algorithm2.2).
Inlines3–6,thealgorithmestimatesthegradientofthecorpuslossbasedontheminibatch.
After the loop, g contains the gradient estimate, and the parameters (cid:130) are updated toward g.
O O
eminibatchsizecanvaryinsizefromm 1tom n.Highervaluesprovidebetterestimates
D D
of the corpus-wide gradients, while smaller values allow more updates and in turn faster con-
vergence. Besides the improved accuracy of the gradients estimation, the minibatch algorithm
provides opportunities for improved training efficiency. For modest sizes of m, some comput-
ing architectures (i.e., GPUs) allow an efficient parallel implementation of the computation in
lines 3–6. With a properly decreasing learning rate, SGD is guaranteed to converge to a global
optimum if the function is convex, which is the case for linear and log-linear models coupled
with the loss functions and regularizers discussed in this chapter. However, it can also be used
tooptimizenon-convexfunctionssuchasmulti-layerneuralnetwork.Whiletherearenolonger
guaranteesoffindingaglobaloptimum,thealgorithmprovedtoberobustandperformswellin
practice.¹⁴
¹³LearningratedecayisrequiredinordertoproveconvergenceofSGD.
¹⁴Recentworkfromtheneuralnetworksliteraturearguethatthenon-convexityofthenetworksismanifestedinaproliferation
ofsaddlepointsratherthanlocalminima[Dauphinetal.,2014].ismayexplainsomeofthesuccessintrainingneural
networksdespiteusinglocalsearchtechniques.2.8. GRADIENT-BASEDOPTIMIZATION 33
Algorithm2.2Minibatchstochasticgradientdescenttraining.
Input:
-Functionf.x (cid:130)/parameterizedwithparameters(cid:130).
I
-Trainingsetofinputsx ;:::;x anddesiredoutputsy ;:::;y .
1 n 1 n
-LossfunctionL.
1: whilestoppingcriterianotmet do
2: Sampleaminibatchofmexamples .x 1;y 1/;:::;.x m;y m/
f g
3: g 0
O
4: fori 1tomdo
D
5: ComputethelossL.f.x i (cid:130)/;y i/
I
6: g
O
g
O C
gradientsof m1L.f.x i I(cid:130)/;y i/w.r.t(cid:130)
7: (cid:130) (cid:130) (cid:17) tg
(cid:0) O
8: return(cid:130)
2.8.2 WORKED-OUTEXAMPLE
Asanexample,consideramulti-classlinearclassifierwithhingeloss:
y argmaxy
O D O(cid:140)i(cid:141)
i
y f.x/ xW b
O D D C
L.y;y/ max.0;1 .y y //
O D (cid:0) O(cid:140)t(cid:141)(cid:0) O(cid:140)k(cid:141)
max.0;1 ..xW b/ .xW b/ //
(cid:140)t(cid:141) (cid:140)k(cid:141)
D (cid:0) C (cid:0) C
t argmaxy
D (cid:140)i(cid:141)
i
k argmaxy i t:
D O(cid:140)i(cid:141) ⁄
i
We want to set the parameters W and b such that the loss is minimized. We need to compute
thegradientsofthelosswithrespecttothevaluesW andb.egradientisthecollectionofthe34 2. LEARNINGBASICSANDLINEARMODELS
partialderivativesaccordingtoeachofthevariables:
0@L.y;y/ @L.y;y/ @L.y;y/1
O O O
@W (cid:140)1;1(cid:141) @W (cid:140)1;2(cid:141) (cid:1)(cid:1)(cid:1) @W (cid:140)1;n(cid:141)
B C
B@L.y;y/ @L.y;y/ @L.y;y/C
B O O O C
@L.y;y/ B@W (cid:140)2;1(cid:141) @W (cid:140)2;2(cid:141) (cid:1)(cid:1)(cid:1) @W (cid:140)2;n(cid:141)C
O B C
@W DB
B
: :
:
: :
:
::: : :
:
C
C
B C
B C
B @@L.y O;y/ @L.y O;y/ @L.y O;y/C
A
@W (cid:140)m;1(cid:141) @W (cid:140)m;2(cid:141) (cid:1)(cid:1)(cid:1) @W (cid:140)m;n(cid:141)
@L.y;y/ (cid:18)@L.y;y/ @L.y;y/ @L.y;y/(cid:19)
O O O O :
@b D @b (cid:140)1(cid:141) @b (cid:140)2(cid:141) (cid:1)(cid:1)(cid:1) @b (cid:140)n(cid:141)
Moreconcretely,wewillcomputethederivateofthelossw.r.teachofthevaluesW andb .
(cid:140)i;j(cid:141) (cid:140)j(cid:141)
Webeginbyexpandingthetermsinthelosscalculation:¹⁵
L.y;y/ max.0;1 .y y //
O D (cid:0) O(cid:140)t(cid:141)(cid:0) O(cid:140)k(cid:141)
max.0;1 ..xW b/ .xW b/ //
(cid:140)t(cid:141) (cid:140)k(cid:141)
D (cid:0) C (cid:0) C
! !!!
max 0;1 Xx W b Xx W b
(cid:140)i(cid:141) (cid:140)i;t(cid:141) (cid:140)t(cid:141) (cid:140)i(cid:141) (cid:140)i;k(cid:141) (cid:140)k(cid:141)
D (cid:0) (cid:1) C (cid:0) (cid:1) C
i i
!
max 0;1 Xx W b Xx W b
(cid:140)i(cid:141) (cid:140)i;t(cid:141) (cid:140)t(cid:141) (cid:140)i(cid:141) (cid:140)i;k(cid:141) (cid:140)k(cid:141)
D (cid:0) (cid:1) (cid:0) C (cid:1) C
i i
t argmaxy
D (cid:140)i(cid:141)
i
k argmaxy i t:
D O(cid:140)i(cid:141) ⁄
i
efirstobservationisthatif1 .y y / 0thenthelossis0andsoisthegradient(the
(cid:0) O(cid:140)t(cid:141)(cid:0) O(cid:140)k(cid:141) (cid:20)
derivative of the max operation is the derivative of the maximal value). Otherwise, consider the
derivativeof @L .Forthepartialderivative,b istreatedasavariable,andallothersareconsid-
@b
(cid:140)i(cid:141)
(cid:140)i(cid:141)
eredasconstants.Fori k;t,thetermb doesnotcontributetotheloss,anditsderivativeit
(cid:140)i(cid:141)
⁄
is0.Fori k andi t wetriviallyget:
D D
8 1 i t
@L (cid:136)(cid:0) D
<1 i k
@b D D
(cid:140)i(cid:141)
(cid:136) :0 otherwise:
¹⁵Moreadvancedderivationtechniquesallowworkingwithmatricesandvectorsdirectly.Here,westicktohigh-schoollevel
techniques.2.8. GRADIENT-BASEDOPTIMIZATION 35
Similarly,forW ,onlyj k andj t contributetotheloss.Weget:
(cid:140)i;j(cid:141)
D D
8
(cid:136)
(cid:136)@. (cid:0)x @W(cid:140)i(cid:141) (cid:1)W (cid:140)i;t(cid:141)(cid:140)i;t(cid:141)/ D(cid:0)x
(cid:140)i(cid:141)
j Dt
(cid:136)
@W@L
(cid:140)i;j(cid:141)
D(cid:136) (cid:136) <@.x @(cid:140) Wi(cid:141) (cid:1)W (cid:140)i;k(cid:140)i (cid:141);k(cid:141)/ Dx
(cid:140)i(cid:141)
j Dk
(cid:136)
(cid:136)
(cid:136)
(cid:136)
(cid:136) :0 otherwise:
isconcludesthegradientcalculation.
Asasimpleexercise,thereadershouldtryandcomputethegradientsofamulti-classlinear
modelwithhingelossandL regularization,andthegradientsofmulti-classclassificationwith
2
softmaxoutputtransformationandcross-entropyloss.
2.8.3 BEYONDSGD
WhiletheSGDalgorithmcanandoftendoesproducegoodresults,moreadvancedalgorithms
arealsoavailable.eSGD+Momentum[Polyak,1964]andNesterovMomentum[Nesterov,1983,
2004,Sutskeveretal.,2013]algorithmsarevariantsofSGDinwhichpreviousgradientsareac-
cumulated and affect the current update. Adaptive learning rate algorithms including AdaGrad
[Duchi et al., 2011], AdaDelta [Zeiler, 2012], RMSProp [Tieleman and Hinton, 2012], and
Adam[KingmaandBa,2014]aredesignedtoselectthelearningrateforeachminibatch,some-
times on a per-coordinate basis, potentially alleviating the need of fiddling with learning rate
scheduling. For details of these algorithms, see the original papers or [Bengio et al., 2016, Sec-
tions8.3,8.4].37
C H A P T E R 3
From Linear Models to
Multi-layer Perceptrons
3.1 LIMITATIONSOFLINEARMODELS:THEXOR
PROBLEM
ehypothesisclassoflinear(andlog-linear)modelsisseverelyrestricted.Forexample,itcannot
representtheXORfunction,definedas:
xor.0;0/ 0
D
xor.1;0/ 1
D
xor.0;1/ 1
D
xor.1;1/ 0:
D
atis,thereisnoparameterizationw R2;b Rsuchthat:
2 2
.0;0/ w b <0
(cid:1) C
.0;1/ w b 0
(cid:1) C (cid:21)
.1;0/ w b 0
(cid:1) C (cid:21)
.1;1/ w b <0:
(cid:1) C
To see why, consider the following plot of the XOR function, where blue Os denote the
positiveclassandgreenXsthenegativeclass.
1
0
0 138 3. FROMLINEARMODELSTOMULTI-LAYERPERCEPTRONS
Itisclearthatnostraightlinecanseparatethetwoclasses.
3.2 NONLINEARINPUTTRANSFORMATIONS
However, if we transform the points by feeding each of them through the nonlinear function
(cid:30).x ;x / (cid:140)x x ;x x (cid:141),theXORproblembecomeslinearlyseparable.
1 2 1 2 1 2
D (cid:2) C
2
1
0
0 1 2
efunction(cid:30)mappedthedataintoarepresentationthatissuitableforlinearclassification.
Having(cid:30) atourdisposal,wecannoweasilytrainalinearclassifiertosolvetheXORproblem.
y f.x/ (cid:30).x/W b:
O D D C
Ingeneral,onecansuccessfullytrainalinearclassifieroveradatasetwhichisnotlinearly
separablebydefiningafunctionthatwillmapthedatatoarepresentationinwhichitislinearly
separable,andthentrainalinearclassifierontheresultingrepresentation.IntheXORexample
thetransformeddatahasthesamedimensionsastheoriginalone,butofteninordertomakethe
datalinearlyseparableoneneedstomapittoaspacewithamuchhigherdimension.
issolutionhasoneglaringproblem,however:weneedtomanuallydefinethefunction
(cid:30),aprocesswhichisdependentontheparticulardataset,andrequiresalotofhumanintuition.
3.3 KERNELMETHODS
KernelizedSupportVectorsMachines(SVMs)[Boserandetal.,1992],andKernelMethodsin
general[Shawe-TaylorandCristianini,2004],approachthisproblembydefiningasetofgeneric
mappings, each of them mapping the data into very high dimensional—and sometimes even
infinite—spaces, and then performing linear classification in the transformed space. Working
in very high dimensional spaces significantly increase the probability of finding a suitable linear
separator.
One example mapping is the polynomial mapping, (cid:30).x/ .x/d. For d 2, we get
D D
(cid:30).x ;x / .x x ;x x ;x x ;x x /.isgivesusallcombinationsofthetwovariables,allow-
1 2 1 1 1 2 2 1 2 2
D
ingtosolvetheXORproblemusingalinearclassifier,withapolynomialincreaseinthenumber
ofparameters.IntheXORproblemthemappingincreasedthedimensionalityoftheinput(and3.4. TRAINABLEMAPPINGFUNCTIONS 39
hence the number of parameters) from 2–4. For the language identification example, the input
dimensionalitywouldhaveincreasedfrom784to7842 614,656dimensions.
D
Working in very high dimensional spaces can become computationally prohibitive, and
theingenuityinkernelmethodsistheuseofthekerneltrick[Aizermanetal.,1964,Schölkopf,
2001]thatallowsonetoworkinthetransformedspacewithoutevercomputingthetransformed
representation.egenericmappingsaredesignedtoworkonmanycommoncases,andtheuser
needstoselectthesuitableoneforitstask,oftenbytrialanderror.Adownsideoftheapproach
isthattheapplicationofthekerneltrickmakestheclassificationprocedureforSVMsdependent
linearly on the size of the training set, making it prohibitive for use in setups with reasonably
largetrainingsets.Anotherdownsideofhighdimensionalspacesisthattheyincreasetheriskof
overfitting.
3.4 TRAINABLEMAPPINGFUNCTIONS
A different approach is to define a trainable nonlinear mapping function, and train it in con-
junction with the linear classifier. at is, finding the suitable representation becomes the re-
sponsibilityofthetrainingalgorithm.Forexample,themappingfunctioncantaketheformofa
parameterizedlinearmodel,followedbyanonlinearactivationfunctiong thatisappliedtoeach
oftheoutputdimensions:
y (cid:30).x/W b
O D C (3.1)
(cid:30).x/ g.xW b/:
0 0
D C
Bytakingg.x/ Dmax.0;x/andW
0
D(cid:0)1 11 1(cid:1),b
0
D. (cid:0)10/wegetanequivalentmapping
to.x x ;x x /fortheourpointsofinterest(0,0),(0,1),(1,0),and(1,1),successfullysolv-
1 2 1 2
(cid:2) C
ingtheXORproblem.eentireexpressiong.xW b/W bisdifferentiable(althoughnot
0 0
C C
convex), making it possible to apply gradient-based techniques to the model training, learning
both the representation function and the linear classifier on top of it at the same time. is is
themainideabehinddeeplearningandneuralnetworks.Infact,Equation(3.1)describesavery
common neural network architecture called a multi-layerperceptron (MLP). Having established
themotivation,wenowturntodescribemulti-layerneuralnetworksinmoredetail.41
C H A P T E R 4
Feed-forward Neural Networks
4.1 ABRAIN-INSPIREDMETAPHOR
As the name suggests, neural networks were inspired by the brain’s computation mechanism,
which consists of computation units called neurons. While the connections between artificial
neuralnetworksandthebrainareinfactratherslim,werepeatthemetaphorhereforcomplete-
ness.Inthemetaphor,aneuronisacomputationalunitthathasscalarinputsandoutputs.Each
input has an associated weight. e neuron multiplies each input by its weight, and then sums¹
them,appliesanonlinearfunctiontotheresult,andpassesittoitsoutput.Figure4.1showssuch
aneuron.
y
Output 1
Neuron ∫
x x x x
Input 1 2 3 4
Figure4.1: Asingleneuronwithfourinputs.
eneuronsareconnectedtoeachother,forminganetwork:theoutputofaneuronmay
feedintotheinputsofoneormoreneurons.Suchnetworkswereshowntobeverycapablecom-
putational devices. If the weights are set correctly, a neural network with enough neurons and a
nonlinear activation function can approximate a very wide range of mathematical functions (we
willbemorepreciseaboutthislater).
A typical feed-forward neural network may be drawn as in Figure 4.2. Each circle is a
neuron,withincomingarrowsbeingtheneuron’sinputsandoutgoingarrowsbeingtheneuron’s
outputs.Eacharrowcarriesaweight,reflectingitsimportance(notshown).Neuronsarearranged
in layers, reflecting the flow of information. e bottom layer has no incoming arrows, and is
¹Whilesummingisthemostcommonoperation,otherfunctions,suchasamax,arealsopossible.42 4. FEED-FORWARDNEURALNETWORKS
the input to the network. e top-most layer has no outgoing arrows, and is the output of the
network.eotherlayersareconsidered“hidden.”esigmoidshapeinsidetheneuronsinthe
middlelayersrepresentanonlinearfunction(i.e.,thelogisticfunction1=.1 e x/)thatisapplied
(cid:0)
C
totheneuron’svaluebeforepassingittotheoutput.Inthefigure,eachneuronisconnectedtoall
oftheneuronsinthenextlayer—thisiscalledafullyconnectedlayer oranaffinelayer.
y y y
Output layer 1 2 3
Hidden layer ∫ ∫ ∫ ∫ ∫
Hidden layer ∫ ∫ ∫ ∫ ∫ ∫
x x x x
Input layer 1 2 3 4
Figure4.2: Feed-forwardneuralnetworkwithtwohiddenlayers.
While the brain metaphor is sexy and intriguing, it is also distracting and cumbersome
to manipulate mathematically. We therefore switch back to using more concise mathematical
notation.Aswillsoonbecomeapparent,afeed-forwardnetworkastheoneinFigure4.2issimply
astackoflinearmodelsseparatedbynonlinearfunctions.
evaluesofeachrowofneuronsinthenetworkcanbethoughtofasavector.InFigure4.2
theinputlayerisa4-dimensionalvector(x),andthelayeraboveitisa6-dimensionalvector(h1).
e fully connected layer can be thought of as a linear transformation from 4 dimensions to 6
dimensions.Afullyconnectedlayerimplementsavector-matrixmultiplication,h xW where
D
theweightoftheconnectionfromtheithneuronintheinputrowtothejthneuronintheoutput
rowisW .²evaluesofharethentransformedbyanonlinearfunctiong thatisappliedto
(cid:140)i;j(cid:141)
eachvaluebeforebeingpassedonasinputtothenextlayer.ewholecomputationfrominput
to output can be written as: .g.xW1//W2 where W1 are the weights of the first layer and W2
aretheweightsofthesecondone.Takingthisview,thesingleneuroninFigure4.1isequivalent
toalogistic(log-linear)binaryclassifier(cid:27).xw/withoutabiasterm.
²Toseewhythisisthecase,denotetheweightoftheithinputofthejthneuroninhasW(cid:140)i;j(cid:141).evalueofh(cid:140)j(cid:141)isthen
h(cid:140)j(cid:141) P4
i
1x(cid:140)i(cid:141) W(cid:140)i;j(cid:141).
D D (cid:1)4.2. INMATHEMATICALNOTATION 43
4.2 INMATHEMATICALNOTATION
From this point on, we will abandon the brain metaphor and describe networks exclusively in
termsofvector-matrixoperations.
esimplestneuralnetworkiscalledaperceptron.Itissimplyalinearmodel:
NN .x/ xW b (4.1)
Perceptron
D C
x Rdin; W Rdin dout; b Rdout;
(cid:2)
2 2 2
where W is the weight matrix and b is a bias term.³ In order to go beyond linear functions, we
introduce a nonlinear hidden layer (the network in Figure 4.2 has two such layers), resulting in
theMultiLayerPerceptronwithonehidden-layer(MLP1).Afeed-forwardneuralnetworkwith
onehidden-layerhastheform:
NN .x/ g.xW1 b1/W2 b2 (4.2)
MLP1
D C C
x Rdin; W1 Rdin d1; b1 Rd1; W2 Rd1 d2; b2 Rd2:
(cid:2) (cid:2)
2 2 2 2 2
HereW1andb1areamatrixandabiastermforthefirstlineartransformationoftheinput,
g isanonlinearfunctionthatisappliedelement-wise(alsocalledanonlinearity oranactivation
function),andW2 andb2 arethematrixandbiastermforasecondlineartransform.
Breakingitdown,xW1 b1isalineartransformationoftheinputxfromd dimensions
in
C
to d dimensions. g is then applied to each of the d dimensions, and the matrix W2 together
1 1
with bias vector b2 are then used to transform the result into the d dimensional output vector.
2
enonlinearactivationfunctionghasacrucialroleinthenetwork’sabilitytorepresentcomplex
functions.Withoutthenonlinearitying,theneuralnetworkcanonlyrepresentlineartransfor-
mations of the input.⁴ Taking the view in Chapter 3, the first layer transforms the data into a
goodrepresentation,whilethesecondlayerappliesalinearclassifiertothatrepresentation.
Wecanaddadditionallinear-transformationsandnonlinearities,resultinginanMLPwith
twohidden-layers(thenetworkinFigure4.2isofthisform):
NN .x/ .g2.g1.xW1 b1/W2 b2//W3: (4.3)
MLP2
D C C
Itisperhapsclearertowritedeepernetworkslikethisusingintermediaryvariables:
NN .x/ y
MLP2
D
h1 g1.xW1 b1/
D C (4.4)
h2 g2.h1W2 b2/
D C
y h2W3:
D
³enetworkinFigure4.2doesnotincludebiasterms.Abiastermcanbeaddedtoalayerbyaddingtoitanadditionalneuron
thatdoesnothaveanyincomingconnections,whosevalueisalways1.
⁴Toseewhy,considerthatasequenceoflineartransformationsisstillalineartransformation.44 4. FEED-FORWARDNEURALNETWORKS
e vector resulting from each linear transform is referred to as a layer. e outer-most
lineartransformresultsintheoutputlayer andtheotherlineartransformsresultinhiddenlayers.
Eachhiddenlayerisfollowedbyanonlinearactivation.Insomecases,suchasinthelastlayerof
ourexample,thebiasvectorsareforcedto0(“dropped”).
Layersresultingfromlineartransformationsareoftenreferredtoasfullyconnected,oraffine.
Othertypesofarchitecturesexist.Inparticular,imagerecognitionproblemsbenefitfromconvo-
lutional andpoolinglayers.Suchlayershaveusesalsoinlanguageprocessing,andwillbediscussed
inChapter13.Networkswithseveralhiddenlayersaresaidtobedeepnetworks,hencethename
deeplearning.
Whendescribinganeuralnetwork,oneshouldspecifythedimensionsofthelayersandthe
input.Alayerwillexpectad dimensionalvectorasitsinput,andtransformitintoad dimen-
in out
sionalvector.edimensionalityofthelayeristakentobethedimensionalityofitsoutput.For
afullyconnectedlayerl.x/ xW bwithinputdimensionalityd andoutputdimensionality
in
D C
d ,thedimensionsofx is1 d ,ofW isd d andofbis1 d .
out in in out out
(cid:2) (cid:2) (cid:2)
Likethecasewithlinearmodels,theoutputofaneuralnetworkisad dimensionalvector.
out
In case d 1, the network’s output is a scalar. Such networks can be used for regression (or
out
D
scoring)byconsideringthevalueoftheoutput,orforbinaryclassificationbyconsultingthesign
of the output. Networks with d k >1 can be used for k-class classification, by associating
out
D
eachdimensionwithaclass,andlookingforthedimensionwithmaximalvalue.Similarly,ifthe
outputvectorentriesarepositiveandsumtoone,theoutputcanbeinterpretedasadistribution
over class assignments (such output normalization is typically achieved by applying a softmax
transformationontheoutputlayer,seeSection2.6).
ematricesandthebiastermsthatdefinethelineartransformationsaretheparametersof
thenetwork.Likeinlinearmodels,itiscommontorefertothecollectionofallparametersas(cid:130).
Togetherwiththeinput,theparametersdeterminethenetwork’soutput.etrainingalgorithm
isresponsibleforsettingtheirvaluessuchthatthenetwork’spredictionsarecorrect.Unlikelinear
models, the loss function of multi-layer neural networks with respect to their parameters is not
convex,⁵ making search for the optimal parameter values intractable. Still, the gradient-based
optimizationmethodsdiscussedinSection2.8canbeapplied,andperformverywellinpractice.
TrainingneuralnetworksisdiscussedindetailinChapter5.
4.3 REPRESENTATIONPOWER
Intermsofrepresentationpower,itwasshownbyHorniketal.[1989]andCybenko[1989]that
MLP1isauniversalapproximator—itcanapproximatewithanydesirednon-zeroamountofer-
ror a family of functions that includes all continuous functions on a closed and bounded subset
of Rn, and any function mapping from any finite dimensional discrete space to another.⁶ is
⁵Strictlyconvexfunctionshaveasingleoptimalsolution,makingthemeasytooptimizeusinggradient-basedmethods.
⁶Specifically,afeed-forwardnetworkwithlinearoutputlayerandatleastonehiddenlayerwitha“squashing”activationfunction
canapproximateanyBorelmeasurablefunctionfromonefinitedimensionalspacetoanother.eproofwaslaterextended
byLeshnoetal.[1993]toawiderrangeofactivationfunctions,includingtheReLUfunctiong.x/ max.0;x/.
D4.4. COMMONNONLINEARITIES 45
maysuggestthereisnoreasontogobeyondMLP1tomorecomplexarchitectures.However,the
theoreticalresultdoesnotdiscussthelearnabilityoftheneuralnetwork(itstatesthatarepresen-
tationexists,butdoesnotsayhoweasyorharditistosettheparametersbasedontrainingdata
and a specific learning algorithm). It also does not guarantee that a training algorithm will find
the correct function generating our training data. Finally, it does not state how large the hidden
layershouldbe.Indeed,Telgarsky[2016]showthatthereexistneuralnetworkswithmanylayers
of bounded size that cannot be approximated by networks with fewer layers unless these layers
areexponentiallylarge.
Inpractice,wetrainneuralnetworksonrelativelysmallamountsofdatausinglocalsearch
methodssuchasvariantsofstochasticgradientdescent,andusehiddenlayersofrelativelymod-
est sizes (up to several thousands). As the universal approximation theorem does not give any
guarantees under these non-ideal, real-world conditions, there is definitely benefit to be had in
tryingoutmorecomplexarchitecturesthanMLP1.Inmanycases,however,MLP1doesindeed
providestrongresults.Forfurtherdiscussionontherepresentationpoweroffeed-forwardneural
networks,seeBengioetal.[2016,Section6.5].
4.4 COMMONNONLINEARITIES
e nonlinearity g can take many forms. ere is currently no good theory as to which nonlin-
earity to apply inwhich conditions,and choosing thecorrectnonlinearity for a giventask is for
themostpartanempiricalquestion.Iwillnowgooverthecommonnonlinearitiesfromtheliter-
ature:thesigmoid,tanh,hardtanhandtherectifiedlinearunit(ReLU).SomeNLPresearchers
alsoexperimentedwithotherformsofnonlinearitiessuchascubeandtanh-cube.
Sigmoid esigmoidactivationfunction(cid:27).x/ 1=.1 e x/,alsocalledthelogisticfunction,
(cid:0)
D C
is an S-shaped function, transforming each value x into the range (cid:140)0;1(cid:141). e sigmoid was the
canonicalnonlinearityforneuralnetworkssincetheirinception,butiscurrentlyconsideredtobe
deprecatedforuseininternallayersofneuralnetworks,asthechoiceslistedbelowprovetowork
muchbetterempirically.
Hyperbolic tangent (tanh) e hyperbolic tangent tanh.x/ e2x 1 activation function is an
D
e2x(cid:0)1
S-shapedfunction,transformingthevaluesx intotherange(cid:140) 1;1(cid:141).C
(cid:0)
Hard tanh e hard-tanh activation function is an approximation of the tanh function which
isfastertocomputeandtofindderivativesthereof:
8 1 x < 1
(cid:136)(cid:0) (cid:0)
hardtanh.x/ <1 x >1 (4.5)
D
(cid:136) :x otherwise:
Rectifier(ReLU) erectifieractivationfunction[Glorotetal.,2011],alsoknownastherecti-
fiedlinearunitisaverysimpleactivationfunctionthatiseasytoworkwithandwasshownmany46 4. FEED-FORWARDNEURALNETWORKS
times to produce excellent results.⁷ e ReLU unit clips each value x <0 at 0. Despite its sim-
plicity,itperformswellformanytasks,especiallywhencombinedwiththedropoutregularization
technique(seeSection4.6):
(0 x <0
ReLU.x/ max.0;x/ (4.6)
D D x otherwise:
Asaruleofthumb,bothReLUandtanhunitsworkwell,andsignificantlyoutperformthe
sigmoid. You may want to experiment with both tanh and ReLU activations, as each one may
performbetterindifferentsettings.
Figure4.3showstheshapesofthedifferentactivationsfunctions,togetherwiththeshapes
oftheirderivatives.
sigmoid(x) tanh(x) hardtanh(x) ReLU(x)
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
-0.5 -0.5 -0.5 -0.5
-1.0 -1.0 -1.0 -1.0
-6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6
(cid:127)f (cid:127)f (cid:127)f (cid:127)f
(cid:127)x (cid:127)x (cid:127)x (cid:127)x
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
-0.5 -0.5 -0.5 -0.5
-1.0 -1.0 -1.0 -1.0
-6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6
Figure4.3: Activationfunctions(top)andtheirderivatives(bottom).
4.5 LOSSFUNCTIONS
When training a neural network (more on training in Chapter 5), much like when training a
linear classifier, one defines a loss function L.y;y/, stating the loss of predicting y when the
O O
trueoutputisy.etrainingobjectiveisthentominimizethelossacrossthedifferenttraining
examples. e loss L.y;y/ assigns a numerical score (a scalar) to the network’s output y given
O O
the true expected output y. e loss functions discussed for linear models in Section 2.7.1 are
relevantandwidelyusedalsoforneuralnetworks.Forfurtherdiscussiononlossfunctionsinthe
⁷etechnicaladvantagesoftheReLUoverthesigmoidandtanhactivationfunctionsisthatitdoesnotinvolveexpensive-
to-computefunctions,andmoreimportantlythatitdoesnotsaturate.esigmoidandtanhactivationarecappedat1,and
thegradientsatthisregionofthefunctionsarenearzero,drivingtheentiregradientnearzero.eReLUactivationdoes
nothavethisproblem,makingitespeciallysuitablefornetworkswithmultiplelayers,whicharesusceptibletothevanishing
gradientsproblemwhentrainedwiththesaturatingunits.4.6. REGULARIZATIONANDDROPOUT 47
contextofneuralnetworks,seeLeCunandHuang[2005],LeCunetal.[2006]andBengioetal.
[2016].
4.6 REGULARIZATIONANDDROPOUT
Multi-layer networks can be large and have many parameters, making them especially prone to
overfitting. Model regularization is just as important in deep neural networks as it is in linear
models, and perhaps even more so. e regularizers discussed in Section 2.7.2, namely L , L
2 1
and the elastic-net, are also relevant for neural networks. In particular, L regularization, also
2
calledweightdecayiseffectiveforachievinggoodgeneralizationperformanceinmanycases,and
tuningtheregularizationstrength(cid:21)isadvisable.
Another effective technique for preventing neural networks from overfitting the training
data is dropout training [Hinton et al., 2012, Srivastava et al., 2014]. e dropout method is
designedtopreventthenetworkfromlearningtorelyonspecificweights.Itworksbyrandomly
dropping(settingto0)halfoftheneuronsinthenetwork(orinaspecificlayer)ineachtraining
example in the stochastic-gradient training. For example, consider the multi-layer perceptron
withtwohiddenlayers(MLP2):
NN .x/ y
MLP2
D
h1 g1.xW1 b1/
D C
h2 g2.h1W2 b2/
D C
y h2W3:
D
WhenapplyingdropouttrainingtoMLP2,werandomlysetsomeofthevaluesofh1 and
h2 to0ateachtraininground:
NN .x/ y
MLP2
D
h1 g1.xW1 b1/
D C
m1 Bernouli.r1/
(cid:24)
h1 m1 h1
Q D (cid:12)
(4.7)
h2 g2.h1W2 b2/
D Q C
m2 Bernouli.r2/
(cid:24)
h2 m2 h2
Q D (cid:12)
y h2W3:
DQ
Here, m1 and m2 are random masking vectors with the dimensions of h1 and h2, respectively,
and is the element-wise multiplication operation. e values of the elements in the masking
(cid:12)48 4. FEED-FORWARDNEURALNETWORKS
vectors are either 0 or 1, and are drawn from a Bernouli distribution with parameter r (usually
r 0:5).evaluescorrespondingtozerosinthemaskingvectorsarethenzeroedout,replacing
D
thehiddenlayershwithhbeforepassingthemontothenextlayer.
Q
WorkbyWageretal.[2013]establishesastrongconnectionbetweenthedropoutmethod
andL regularization.Anotherviewlinksdropouttomodelaveragingandensembletechniques
2
[Srivastavaetal.,2014].
edropouttechniqueisoneofthekeyfactorscontributingtoverystrongresultsofneural-
network methods on image classification tasks [Krizhevsky et al., 2012], especially when com-
binedwithReLUactivationunits[Dahletal.,2013].edropouttechniqueiseffectivealsoin
NLPapplicationsofneuralnetworks.
4.7 SIMILARITYANDDISTANCELAYERS
We sometimes wish to calculate a scalar value based on two vectors, such that the value reflects
the similarity, compatibility or distance between the two vectors. For example, vectors v Rd
1
2
andv Rd maybethe outputlayersof twoMLPs,and wewouldliketo trainthenetworkto
2
2
producesimilarvectorsforsometrainingexamples,anddissimilarvectorsforothers.
Inwhatfollowswedescribecommonfunctionsthattaketwovectorsu Rd andv Rd,
2 2
and return a scalar. ese functions can (and often are) integrated in feed-forward neural net-
works.
DotProduct Averycommonoptionsistousethedot-product:
d
sim .u;v/ u v Xu v (4.8)
dot (cid:140)i(cid:141) (cid:140)i(cid:141)
D (cid:1) D
i 1
D
EuclideanDistance AnotherpopularoptionsistheEuclideanDistance:
v d
u
dist .u;v/ uX.u v /2 p.u v/ .u v/ u v (4.9)
euclidean Dt (cid:140)i(cid:141) (cid:0) (cid:140)i(cid:141) D (cid:0) (cid:1) (cid:0) Djj (cid:0) jj2
i 1
D
Note that this is a distance metric and not a similarity: here, small (near zero) values indicate
similarvectorsandlargevaluesdissimilarones.esquare-rootisoftenomitted.
Trainable Forms e dot-product and the euclidean distance above are fixed functions. We
sometimeswanttouseaparameterizedfunction,thatcanbetrainedtoproducedesiredsimilarity
(or dissimilarity) values by focusing on specific dimensions of the vectors. A common trainable
similarityfunctionisthebilinearform:4.8. EMBEDDINGLAYERS 49
sim .u;v/ uMv (4.10)
bilinear
D
M Rd d
(cid:2)
2
wherethematrixM isaparameterthatneedstobetrained.
Similarly,foratrainabledistancefunctionwecanuse:
dist.u;v/ .u v/M.u v/ (4.11)
D (cid:0) (cid:0)
Finally,amulti-layerperceptronwithasingleoutputneuroncanalsobeusedforproducing
ascalarfromtwovectors,byfeedingittheconcatenationofthetwovectors.
4.8 EMBEDDINGLAYERS
As will be further discussed in Chapter 8, when the input to the neural network contains sym-
bolic categorical features (e.g., features that take one of k distinct symbols, such as words from
a closed vocabulary), it is common to associate each possible feature value (i.e., each word in
the vocabulary) with a d-dimensional vector for some d. ese vectors are then considered pa-
rameters of the model, and are trained jointly with the other parameters. e mapping from a
symbolic feature values such as “word number 1249” to d-dimensional vectors is performed by
anembeddinglayer (alsocalleda lookuplayer).eparametersin anembeddinglayeraresimply
a matrix E Rvocab d where each row corresponds to a different word in the vocabulary. e
j j(cid:2)
2
lookupoperationisthensimplyindexing:v E .Ifthesymbolicfeatureisencodedas
1249 (cid:140)1249;(cid:141)
D W
aone-hotvectorx,thelookupoperationcanbeimplementedasthemultiplicationxE.
ewordvectorsareoftenconcatenatedtoeachotherbeforebeingpassedontothenext
layer.EmbeddingsarediscussedinmoredepthinChapter8whendiscussingdenserepresenta-
tionsofcategoricalfeatures,andinChapter10whendiscussingpre-trainedwordrepresentations.51
C H A P T E R 5
Neural Network Training
Similar to linear models, neural network are differentiable parameterized functions, and are
trainedusinggradient-basedoptimization(seeSection2.8).eobjectivefunctionfornonlinear
neuralnetworksisnotconvex,andgradient-basedmethodsmaygetstuckinalocalminima.Still,
gradient-basedmethodsproducegoodresultsinpractice.
Gradientcalculationiscentraltotheapproach.emathematicsofgradientcomputation
for neural networks are the same as those of linear models, simply following the chain-rule of
differentiation. However, for complex networks this process can be laborious and error-prone.
Fortunately, gradients can be efficiently and automatically computed using the backpropagation
algorithm [LeCun et al., 1998b, Rumelhart et al., 1986]. e backpropagation algorithm is a
fancynameformethodicallycomputingthederivativesofacomplexexpressionusingthechain-
rule,whilecachingintermediaryresults.Moregenerally,thebackpropagationalgorithmisaspe-
cial case of the reverse-mode automatic differentiation algorithm [Neidinger, 2010, Section 7],
[Baydinetal.,2015,Bengio,2012].efollowingsectiondescribesreversemodeautomaticdif-
ferentiationinthecontextofthecomputationgraphabstraction.erestofthechapterisdevoted
topracticaltipsfortrainingneuralnetworksinpractice.
5.1 THECOMPUTATIONGRAPHABSTRACTION
While one can compute the gradients of the various parameters of a network by hand and im-
plement them in code, this procedure is cumbersome and error prone. For most purposes, it is
preferable to use automatic tools for gradient computation [Bengio, 2012]. e computation-
graph abstraction allows us to easily construct arbitrary networks, evaluate their predictions for
giveninputs(forwardpass),andcomputegradientsfortheirparameterswithrespecttoarbitrary
scalarlosses(backwardpass).
A computation graph is a representation of an arbitrary mathematical computation as
a graph. It is a directed acyclic graph (DAG) in which nodes correspond to mathematical
operationsor(bound)variablesandedgescorrespondtotheflowofintermediaryvaluesbetween
thenodes.egraphstructuredefinestheorderofthecomputationintermsofthedependencies
between the different components. e graph is a DAG and not a tree, as the result of one
operation can be the input of several continuations. Consider for example a graph for the
computationof.a b 1/ .a b 2/:
(cid:3) C (cid:3) (cid:3) C52 5. NEURALNETWORKTRAINING
*
+ +
*
1 a b 2
ecomputationofa bisshared.Werestrictourselvestothecasewherethecomputationgraph
(cid:3)
isconnected(inadisconnectedgraph,eachconnectedcomponentisanindependentfunctionthat
canbeevaluatedanddifferentiatedindependentlyoftheotherconnectedcomponents).
1 × 1
neg
1 × 1
log
1 × 1
(a) (b) (c)
pick
1 × 17 1 × 17 1 × 17
softmax softmax softmax 5
1 × 17 1 × 17 1 × 17
ADD ADD ADD
1 × 17 1 × 17 1 × 17
MUL MUL MUL
1 × 20 20 × 17 1 × 17 1 × 20 20 × 17 1 × 17 1 × 20 20 × 17 1 × 17
tanh W 2 b2 tanh W 2 b2 tanh W 2 b2
1 × 20 1 × 20 1 × 20
ADD ADD ADD
1 × 20 1 × 20 1 × 20
MUL MUL MUL
150 × 20 1 × 20 1 × 150 150 × 20 1 × 20 1 × 150 150 × 20 1 × 20
1 × x150 W 1 b1 concat W 1 b1 concat W 1 b1
1 × 50 1 × 50 1 × 50 1 × 50 1 × 50 1 × 50
lookup lookup lookup lookup lookup lookup
|V| × 50 |V| × 50
“the” “black” “dog” E “the” “black” “dog” E
Figure5.1: (a)Graphwithunboundinput.(b)Graphwithconcreteinput.(c)Graphwithconcrete
input,expectedoutput,andafinallossnode.
Sinceaneuralnetworkisessentiallyamathematicalexpression,itcanberepresentedasa
computationgraph.Forexample,Figure5.1apresentsthecomputationgraphforanMLPwith
one hidden-layer and a softmax output transformation. In our notation, oval nodes represent5.1. THECOMPUTATIONGRAPHABSTRACTION 53
mathematical operations or functions, and shaded rectangle nodes represent parameters (bound
variables).Networkinputsaretreatedasconstants,anddrawnwithoutasurroundingnode.Input
andparameternodeshavenoincomingarcs,andoutputnodeshavenooutgoingarcs.eoutput
ofeachnodeisamatrix,thedimensionalityofwhichisindicatedabovethenode.
is graph is incomplete: without specifying the inputs, we cannot compute an output.
Figure5.1bshowsacompletegraphforanMLPthattakesthreewordsasinputs,andpredictsthe
distributionoverpart-of-speechtagsforthethirdword.isgraphcanbeusedforprediction,but
notfortraining,astheoutputisavector(notascalar)andthegraphdoesnottakeintoaccount
the correct answer or the loss term. Finally, the graph in Figure 5.1c shows the computation
graph for a specific training example, in which the inputs are the (embeddings of) the words
“the,” “black,” “dog,” and the expected output is “NOUN” (whose index is 5). e pick node
implementsanindexingoperation,receivingavectorandanindex(inthiscase,5)andreturning
thecorrespondingentryinthevector.
Once the graph is built, it is straightforward to run either a forward computation (com-
putetheresultofthecomputation)orabackwardcomputation(computingthegradients),aswe
showbelow.Constructingthegraphsmaylookdaunting,butisactuallyveryeasyusingdedicated
softwarelibrariesandAPIs.
5.1.1 FORWARDCOMPUTATION
eforwardpasscomputestheoutputsofthenodesinthegraph.Sinceeachnode’soutputde-
pends only on itself and on its incoming edges, it is trivial to compute the outputs of all nodes
bytraversingthenodesinatopologicalorderandcomputingtheoutputofeachnodegiventhe
alreadycomputedoutputsofitspredecessors.
More formally, in a graph of N nodes, we associate each node with an index i according
to their topological ordering. Let f be the function computed by node i (e.g., multiplication.
i
addition,etc.).Let(cid:25).i/betheparentnodesofnodei,and(cid:25) 1.i/ j i (cid:25).j/ thechildren
(cid:0)
Df j 2 g
nodesofnodei (thesearetheargumentsoff ).Denotebyv.i/theoutputofnodei,thatis,the
i
application of f to the output values of its arguments (cid:25) 1.i/. For variable and input nodes, f
i (cid:0) i
is a constant function and (cid:25) 1.i/ is empty. e computation-graph forward pass computes the
(cid:0)
valuesv.i/foralli (cid:140)1;N(cid:141).
2
Algorithm5.3Computationgraphforwardpass.
1: fori=1toNdo
2: Leta 1;:::;a m (cid:25) (cid:0)1.i/
D
3: v.i/ f i.v.a 1/;:::;v.a m//54 5. NEURALNETWORKTRAINING
5.1.2 BACKWARDCOMPUTATION(DERIVATIVES,BACKPROP)
e backward pass begins by designating a node N with scalar (1 1) output as a loss-node,
(cid:2)
and running forward computation up to that node. e backward computation computes the
@N
gradients of the parameters with respect to that node’s value. Denote by d.i/ the quantity .
@i
ebackpropagationalgorithmisusedtocomputethevaluesd.i/forallnodesi.
ebackwardpassfillsatableofvaluesd.1/;:::;d.N/asinAlgorithm5.4.
Algorithm5.4Computationgraphbackwardpass(backpropagation).
@N
1: d.N/ 1 1
F @N D
2: fori=N-1to1do
@f @N @N @j
j
3: d.i/ P d.j/ X
j (cid:25).i/ (cid:1) @i F @i D @j @i
2
j (cid:25).i/
2
ebackpropagationalgorithm(Algorithm5.4)isessentiallyfollowingthechain-ruleofdiffer-
@f
entiation.equantity j isthepartialderivativeoff .(cid:25) 1.j//w.r.ttheargumenti (cid:25) 1.j/.
@i j (cid:0) 2 (cid:0)
is value depends on the function f and the values v.a /;:::;v.a / (where a ;:::;a
j 1 m 1 m
D
(cid:25) 1.j/)ofitsarguments,whichwerecomputedintheforwardpass.
(cid:0)
us, in order to define a new kind of node, one needs to define two methods: one for
@f
i
calculatingtheforwardvaluev.i/basedonthenode’sinputs,andtheanotherforcalculating
@x
foreachx (cid:25) 1.i/.
(cid:0)
2
@f
i
Derivativesof“non-mathematical”functions Whiledefining formathematicalfunc-
@x
tions such is as log or is straightforward, some find it challenging to think about the
C
derivativeofoperationsasaspick.x;5/thatselectsthefifthelementofavector.eanswer
istothinkintermsofthecontributiontothecomputation.Afterpickingtheithelementof
avector,onlythatelementparticipatesintheremainderofthecomputation.us,thegradi-
entofpick.x;5/isavectorg withthedimensionalityofx whereg 1andg 0.
(cid:140)5(cid:141) D (cid:140)i 5(cid:141) D
Similarly,forthefunctionmax.0;x/thevalueofthegradientis1forx >0and0ot⁄herwise.
..
Forfurtherinformationonautomaticdifferentiation,seeNeidinger[2010,Section7]and
Baydinetal.[2015].Formoreindepthdiscussionofthebackpropagationalgorithmandcompu-
tationgraphs(alsocalledflowgraphs),seeBengioetal.[2016,Section6.5]andBengio[2012],
LeCun et al. [1998b]. For a popular yet technical presentation, see Chris Olah’s description at
http://colah.github.io/posts/2015-08-Backprop/.5.1. THECOMPUTATIONGRAPHABSTRACTION 55
5.1.3 SOFTWARE
Several software packages implement the computation-graph model, including eano,¹
[Bergstra et al., 2010], TensorFlow² [Abadi et al., 2015], Chainer,³ and DyNet⁴ [Neubig et al.,
2017].Allthesepackagessupportall theessentialcomponents(nodetypes)fordefiningawide
range of neural network architectures, covering the structures described in this book and more.
Graph creation is made almost transparent by use of operator overloading. e framework de-
finesatypeforrepresentinggraphnodes(commonlycalledexpressions),methodsforconstructing
nodes for inputs and parameters, and a set of functions and mathematical operations that take
expressions as input and result in more complex expressions. For example, the python code for
creatingthecomputationgraphfromFigure5.1cusingtheDyNetframeworkis:
import dynet as dy
# model initialization.
model = dy.Model()
mW1= model.add_parameters((20,150))
mb1 = model.add_parameters(20)
mW2= model.add_parameters((17,20))
mb2 = model.add_parameters(17)
lookup = model.add_lookup_parameters((100, 50))
trainer = dy.SimpleSGDTrainer(model)
def get_index(x):
pass # Logic omitted.
Maps words to numeric IDs.
# The following builds and executes the computation graph,
# and updates model parameters.
# Only one data point is shown, in practice the following
# should run in a data-feeding loop.
# Building the computation graph:
dy.renew_cg() # create a new graph.
# Wrap the model parameters as graph-nodes.
W1= dy.parameter(mW1)
b1 = dy.parameter(mb1)
W2= dy.parameter(mW2)
b2 = dy.parameter(mb2)
# Generate the embeddings layer.
vthe = dy.lookup[get_index(”the”)]
vblack = dy.lookup[get_index(”black”)]
vdog = dy.lookup[get_index(”dog”)]
# Connect the leaf nodes into a complete graph.
x = dy.concatenate([vthe, vblack, vdog])
output = dy.softmax(W2*(dy.tanh(W1*x+b1))+b2)
loss = -dy.log(dy.pick(output, 5))
¹http://deeplearning.net/software/theano/
²https://www.tensorflow.org/
³http://chainer.org
⁴https://github.com/clab/dynet56 5. NEURALNETWORKTRAINING
loss_value = loss.forward()
loss.backward() # the gradient is computed
# and stored in the corresponding
# parameters.
trainer.update() # update the parameters according to the gradients.
Mostofthecodeinvolvesvariousinitializations:thefirstblockdefinesmodelparametersthatare
besharedbetweendifferentcomputationgraphs(recallthateachgraphcorrespondstoaspecific
trainingexample).esecondblockturnsthemodelparametersintothegraph-node(Expression)
types. e third block retrieves the Expressions for the embeddings of the input words. Finally,
the fourth block is where the graph is created. Note how transparent the graph creation is—
there is an almost a one-to-one correspondence between creating the graph and describing it
mathematically. e last block shows a forward and backward pass. e equivalent code in the
TensorFlowpackageis:⁵
import tensorflow as tf
W1= tf.get_variable(”W1”, [20, 150])
b1 = tf.get_variable(”b1”, [20])
W2= tf.get_variable(”W2”, [17, 20])
b2 = tf.get_variable(”b2”, [17])
lookup = tf.get_variable(”W”, [100, 50])
def get_index(x):
pass # Logic omitted
p1 = tf.placeholder(tf.int32, [])
p2 = tf.placeholder(tf.int32, [])
p3 = tf.placeholder(tf.int32, [])
target = tf.placeholder(tf.int32, [])
v_w1 = tf.nn.embedding_lookup(lookup, p1)
v_w2 = tf.nn.embedding_lookup(lookup, p2)
v_w3 = tf.nn.embedding_lookup(lookup, p3)
x = tf.concat([v_w1, v_w2, v_w3], 0)
output = tf.nn.softmax(
tf.einsum(”ij ,j->i”, W2, tf.tanh(
tf.einsum(”ij ,j->i”, W1, x) + b1)) + b2)
loss = -tf.log(output[target])
trainer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
# Graph definition done, compile it and feed concrete data.
# Only one data-point is shown, in practice we will use
# a data-feeding loop.
with tf.Session() as sess:
sess.run(tf.global_variables_initializer())
feed_dict = {
p1: get_index(”the”),
p2: get_index(”black”),
p3: get_index(”dog”),
⁵TensorFlowcodeprovidedbyTimRocktäschel.anksTim!5.1. THECOMPUTATIONGRAPHABSTRACTION 57
target: 5
}
loss_value = sess.run(loss , feed_dict)
# update, no call of backward necessary
sess.run(trainer , feed_dict)
emaindifferencebetweenDyNet(andChainer)toTensorFlow(andeano)isthatthe
formers use dynamicgraphconstruction while the latters use staticgraphconstruction. In dynamic
graphconstruction,adifferentcomputationgraphiscreatedfromscratchforeachtrainingsam-
ple,usingcodeinthehostlanguage.Forwardandbackwardpropagationarethenappliedtothis
graph.Incontrast,inthestaticgraphconstructionapproach,theshapeofthecomputationgraph
is defined once in the beginning of the computation, using an API for specifying graph shapes,
withplace-holdervariablesindicatinginputandoutputvalues.en,anoptimizinggraphcom-
pilerproducesanoptimizedcomputationgraph,andeachtrainingexampleisfedintothe(same)
optimized graph. e graph compilation step in the static toolkits (TensorFlow and eano) is
bothablessingandacurse.Ontheonehand,oncecompiled,largegraphscanberunefficiently
oneithertheCPUoraGPU,makingitidealforlargegraphswithafixedstructure,whereonly
the inputs change between instances. However, the compilation step itself can be costly, and it
makes the interface more cumbersome to work with. In contrast, the dynamic packages focus
on building large and dynamic computation graphs and executing them “on the fly” without a
compilationstep.Whiletheexecutionspeedmaysuffercomparedtothestatictoolkits,inprac-
ticethecomputationspeedsofthedynamictoolkitsareverycompetitive.edynamicpackages
are especially convenient when working with the recurrent and recursive networks described in
Chapters 14 and 18 as well as in structured prediction settings as described in Chapter 19, in
which the graphs of different data-points have different shapes. See Neubig et al. [2017] for
furtherdiscussiononthedynamic-vs.-staticapproaches,andspeedbenchmarksforthedifferent
toolkits.Finally,packagessuchasKeras⁶provideahigherlevelinterfaceontopofpackagessuch
aseanoandTensorFlow,allowingthedefinitionandtrainingofcomplexneuralnetworkswith
evenfewerlinesofcode,providedthatthearchitecturesarewellestablished,andhencesupported
inthehigher-levelinterface.
5.1.4 IMPLEMENTATIONRECIPE
Usingthecomputationgraphabstractionanddynamicgraphconstruction,thepseudo-codefor
anetworktrainingalgorithmisgiveninAlgorithm5.5.
Here, build_computation_graph is a user-defined function that builds the computation
graph for the given input, output, and network structure, returning a single loss node. up-
date_parametersisanoptimizerspecificupdaterule.erecipespecifiesthatanewgraphiscre-
ated for each training example. is accommodates cases in which the network structure varies
between training examples, such as recurrent and recursive neural networks, to be discussed in
⁶https://keras.io58 5. NEURALNETWORKTRAINING
Algorithm5.5Neuralnetworktrainingwithcomputationgraphabstraction(usingminibatches
ofsize1).
1: Definenetworkparameters.
2: foriteration=1toTdo
3: forTrainingexamplex i;y i indatasetdo
4: loss_node build_computation_graph(x i,y i,parameters)
5: loss_node.forward()
6: gradients loss_node().backward()
7: parameters update_parameters(parameters,gradients)
8: returnparameters.
Chapters 14–18. For networks with fixed structures, such as an MLPs, it may be more efficient
to create one base computation graph and vary only the inputs and expected outputs between
examples.
5.1.5 NETWORKCOMPOSITION
As long as the network’s output is a vector (1 k matrix), it is trivial to compose networks by
(cid:2)
makingtheoutputofonenetworktheinputofanother,creatingarbitrarynetworks.ecompu-
tation graph abstractions makes this ability explicit: a node in the computation graph can itself
beacomputationgraphwithadesignatedoutputnode.Onecanthendesignarbitrarilydeepand
complexnetworks,andbeabletoeasilyevaluateandtrainthemthankstoautomaticforwardand
gradient computation. is makes it easy to define and train elaborate recurrent and recursive
networks,asdiscussedinChapters14–16and18,aswellasnetworksforstructuredoutputsand
multi-objectivetraining,aswediscussinChapters19and20.
5.2 PRACTICALITIES
Once the gradient computation is taken care of, the network is trained using SGD or another
gradient-based optimization algorithm. e function being optimized is not convex, and for a
long time training of neural networks was considered a “black art” which can only be done by
selectedfew.Indeed, manyparametersaffect the optimizationprocess,and carehas to be taken
totunetheseparameters.Whilethisbookisnotintendedasacomprehensiveguidetosuccessfully
trainingneuralnetworks,wedolisthereafewoftheprominentissues.Forfurtherdiscussionon
optimizationtechniquesandalgorithmsforneuralnetworks,refertoBengioetal.[2016,Chapter
8].Forsometheoreticaldiscussionandanalysis,refertoGlorotandBengio[2010].Forvarious
practicaltipsandrecommendations,seeBottou[2012],LeCunetal.[1998a].5.2. PRACTICALITIES 59
5.2.1 CHOICEOFOPTIMIZATIONALGORITHM
WhiletheSGDalgorithmworkswell,itmaybeslowtoconverge.Section2.8.3listssomealter-
native, more advanced stochastic-gradient algorithms. As most neural network software frame-
works provide implementations of these algorithms, it is easy and often worthwhile to try out
differentvariants.Inmyresearchgroup,wefoundthatwhentraininglargernetworks,usingthe
Adam algorithm [Kingma and Ba, 2014] is very effective and relatively robust to the choice of
thelearningrate.
5.2.2 INITIALIZATION
enon-convexityoftheobjectivefunctionmeanstheoptimizationproceduremaygetstuckin
a local minimum or a saddle point, and that starting fromdifferent initial points (e.g., different
randomvaluesfortheparameters)mayresultindifferentresults.us,itisadvisedtorunseveral
restarts of the training starting at different random initializations, and choosing the best one
basedonadevelopmentset.⁷eamountofvarianceintheresultsduetodifferentrandomseed
selectionsisdifferentfordifferentnetworkformulationsanddatasets,andcannotbepredictedin
advance.
emagnitudeoftherandomvalueshasaveryimportanteffectonthesuccessoftraining.
AneffectiveschemeduetoGlorotandBengio[2010],calledxavierinitializationafterGlorot’s
firstname,suggestsinitializingaweightmatrixW Rdin dout as:
(cid:2)
2
" p6 p6 #
W U ; ; (5.1)
(cid:24) (cid:0)pd d Cpd d
in out in out
C C
where U(cid:140)a;b(cid:141) is a uniformly sampled random value in the range (cid:140)a;b(cid:141). e suggestion is based
onpropertiesofthetanhactivationfunction,workswellinmanysituations,andisthepreferred
defaultinitializationmethodbymany.
Analysis by He et al. [2015] suggests that when using ReLU nonlinearities, the weights
shouldbeinitializedbysamplingfromazero-meanGaussiandistributionwhosestandarddevi-
ationisq 2 .isinitializationwasfoundbyHeetal.[2015]toworkbetterthanxavierinitial-
din
izationinanimageclassificationtask,especiallywhendeepnetworkswereinvolved.
5.2.3 RESTARTSANDENSEMBLES
Whentrainingcomplexnetworks,differentrandominitializationsarelikelytoendupwithdif-
ferentfinalsolutions,exhibitingdifferentaccuracies.us,ifyourcomputationalresourcesallow,
it is advisable to run the training process several times, each with a different random initializa-
tion, and choose the best one on the development set. is technique is called random restarts.
e average model accuracy across random seeds is also interesting, as it gives a hint as to the
stabilityoftheprocess.
⁷Whendebugging,andforreproducibilityofresults,itisadvisedtousedafixedrandomseed.60 5. NEURALNETWORKTRAINING
Whiletheneedto“tune”therandomseedusedtoinitializemodelscanbeannoying,italso
provides a simple way to get different models for performing the same task, facilitating the use
modelensembles.Onceseveralmodelsareavailable,onecanbasethepredictionontheensemble
ofmodelsratherthanonasingleone(forexamplebytakingthemajorityvoteacrossthedifferent
models,orbyaveragingtheiroutputvectorsandconsideringtheresultastheoutputvectorofthe
ensembledmodel).Usingensemblesoftenincreasesthepredictionaccuracy,atthecostofhaving
torunthepredictionstepseveraltimes(onceforeachmodel).
5.2.4 VANISHINGANDEXPLODINGGRADIENTS
Indeepnetworks,itiscommonfortheerrorgradientstoeithervanish(becomeexceedinglyclose
to 0) or explode (become exceedingly high) as they propagate back through the computation
graph.eproblembecomesmoresevereindeepernetworks,andespeciallysoinrecursiveand
recurrent networks [Pascanu et al., 2012]. Dealing with the vanishing gradients problem is still
anopenresearchquestion.Solutionsincludemakingthenetworksshallower,step-wisetraining
(firsttrainthefirstlayersbasedonsomeauxiliaryoutputsignal,thenfixthemandtraintheupper
layers of the complete network based on the real task signal), performing batch-normalization
[Ioffe and Szegedy, 2015] (for every minibatch, normalizing the inputs to each of the network
layerstohavezeromeanandunitvariance)orusingspecializedarchitecturesthataredesignedto
assistingradientflow(e.g.,theLSTMandGRUarchitecturesforrecurrentnetworks,discussed
inChapter15).Dealingwiththeexplodinggradientshasasimplebutveryeffectivesolution:clip-
pingthegradientsiftheirnormexceedsagiventhreshold.Letgbethegradientsofallparameters
O
inthenetwork,and g betheirL norm.Pascanuetal.[2012]suggesttoset:g thresholdg if
kOk 2 O g O
g >threshold.
kOk
kOk
5.2.5 SATURATIONANDDEADNEURONS
Layers with tanh and sigmoid activations can become saturated—resulting in output values for
thatlayerthat areall close to one,theupper-limit of the activationfunction.Saturated neurons
have very small gradients, and should be avoided. Layers with the ReLU activation cannot be
saturated, but can “die”—most or all values are negative and thus clipped at zero for all inputs,
resultinginagradientofzeroforthatlayer.Ifyournetworkdoesnottrainwell,itisadvisableto
monitorthenetworkforlayerswithmanysaturatedordeadneurons.Saturatedneuronsarecaused
by too large values entering the layer. is may be controlled for by changing the initialization,
scalingtherangeoftheinputvalues,orchangingthelearningrate.Deadneuronsarecausedby
all signals entering the layer being negative (for example this can happen after a large gradient
update).Reducingthelearningratewillhelpinthissituation.Forsaturatedlayers,anotheroption
istonormalizethevaluesinthesaturatedlayeraftertheactivation,i.e.,insteadofg.h/ tanh.h/
D
usingg.h/
tanh.h/
.Layernormalizationisaneffectivemeasureforcounteringsaturation,but
D tanh.h/
isalsoexpensivkeintermk sofgradientcomputation.Arelatedtechniqueisbatchnormalization,due5.2. PRACTICALITIES 61
to Ioffe and Szegedy [2015], in which the activations at each layer are normalized so that they
havemean0andvariance1acrosseachmini-batch.ebatch-normalizationtechniquesbecame
akeycomponentforeffectivetrainingofdeepnetworksincomputervision.Asofthiswriting,it
islesspopularinnaturallanguageapplications.
5.2.6 SHUFFLING
e order in which the training examples are presented to the network is important. e SGD
formulationabovespecifiesselectingarandomexampleineachturn.Inpractice,mostimplemen-
tations go over the training example in random order, essentially performing random sampling
withoutreplacement.Itisadvisedtoshufflethetrainingexamplesbeforeeachpassthroughthe
data.
5.2.7 LEARNINGRATE
Selectionofthelearningrateisimportant.Toolargelearningrateswillpreventthenetworkfrom
convergingonaneffectivesolution.Toosmalllearningrateswilltakeaverylongtimetoconverge.
As a rule of thumb, one should experiment with a range of initial learning rates in range (cid:140)0;1(cid:141),
e.g.,0:001,0:01,0:1,1.Monitorthenetwork’slossovertime,anddecreasethelearningrateonce
the loss stops improving on a held-out development set. Learning rate scheduling decreases the
rate as a function of the number of observed minibatches. A common schedule is dividing the
initial learning rate by the iteration number. Léon Bottou [2012] recommends using a learning
rateoftheform(cid:17) (cid:17) .1 (cid:17) (cid:21)t/ 1where(cid:17) istheinitiallearningrate,(cid:17) isthelearningrateto
t 0 0 (cid:0) 0 t
D C
useonthetthtrainingexample,and(cid:21)isanadditionalhyperparameter.Hefurtherrecommends
determiningagoodvalueof(cid:17) basedonasmallsampleofthedatapriortorunningontheentire
0
dataset.
5.2.8 MINIBATCHES
Parameter updates occur either every training example (minibatches of size 1) or every k train-
ingexamples.Someproblemsbenefitfromtrainingwithlargerminibatchsizes.Intermsofthe
computation graph abstraction, one can create a computation graph for each of the k training
examples, and then connecting the k loss nodes under an averaging node, whose output will be
thelossoftheminibatch.Largeminibatchedtrainingcanalsobebeneficialintermsofcomputa-
tionefficiencyonspecializedcomputingarchitecturessuchasGPUs,andreplacingvector-matrix
operationsbymatrix-matrixoperations.isisbeyondthescopeofthisbook.PART II
Working with Natural Language Data65
C H A P T E R 6
Features for Textual Data
Inthepreviouschapterswediscussedthegenerallearningproblem,andsawsomemachinelearn-
ing models and algorithms for training them. All of these models take as input vectors x and
produce predictions. Up until now we assumed the vectors x are given. In language processing,
thevectorsx arederivedfromtextualdata,inordertoreflectvariouslinguisticpropertiesofthe
text. e mapping from textual data to real valued vectors is called feature extraction or feature
representation,andisdonebyafeaturefunction.Decidingontherightfeaturesisanintegralpart
ofasuccessfulmachinelearningproject.Whiledeepneuralnetworksalleviatealotoftheneedin
featureengineering,agoodsetofcorefeaturesstillneedstobedefined.isisespeciallytruefor
language data, which comes in the form of a sequence of discrete symbols. is sequence needs
tobeconvertedsomehowtoanumericalvector,inanon-obviousway.
Wenowdivergefromthetrainingmachineryinordertodiscussthefeaturefunctionsthat
areusedforlanguagedata,whichwillbethetopicofthenextfewchapters.
is chapter provides an overview of the common kinds of information sources that are
availableforuseasfeatureswhendealingwithtextuallanguagedata.Chapter7discussesfeature
choices for some concrete NLP problems. Chapter 8 deals with encoding the features as input
vectorsthatcanbefedtoaneuralnetwork.
6.1 TYPOLOGYOFNLPCLASSIFICATIONPROBLEMS
Generally speaking, classification problems in natural language can be categorized into several
broad categories, depending on the item being classified (some problems in natural language
processingdonotfallneatlyintotheclassificationframework.Forexample,problemsinwhichwe
arerequiredtoproducesentencesorlongertexts—i.e.,indocumentsummarizationandmachine
translation.esewillbediscussedinChapter17).
WordIntheseproblems,wearefacedwithaword,suchas“dog,”“magnificent,”“magnifficant,”
or“parlez”andneedtosaysomethingaboutit:Doesitdenotealivingthing?Whatlanguage
isitin?Howcommonisit?Whatotherwordsaresimilartoit?Isitamis-spellingofanother
word? And so on. ese kind of problems are actually quite rare, as words seldom appear
inisolation,andformanywordstheirinterpretationdependsonthecontextinwhichthey
areused.
TextsIntheseproblemswearefacedwithapieceoftext,beitaphrase,asentence,aparagraph
oradocument,andneedtosaysomethingaboutit.Isitspamornot?Isitaboutpoliticsor66 6. FEATURESFORTEXTUALDATA
sports?Isitsarcastic?Isitpositive,negativeorneutral(towardsomeissue)?Whowroteit?
Isitreliable?Whichofafixedsetofintentsdoesthistextreflect(ornone)?Willthistext
belikedby16–18yearsoldmales?Andsoon.esetypesofproblemsareverycommon,
andwe’llrefertothemcollectivelyasdocumentclassificationproblems.
PairedTexts In these problems we are given a pair of words or longer texts, and need to say
somethingaboutthepair.ArewordsAandBsynonyms?IswordAavalidtranslationfor
wordB?AredocumentsAandBwrittenbythesameauthor?Canthemeaningofsentence
AbeinferredfromsentenceB?
WordinContextHere,wearegivenapieceoftext,andaparticularword(orphrase,orletter,
etc.)withinit,andweneedto classifythewordinthecontextofthetext.Forexample,is
the word book in I want to book a flight a noun, a verb or an adjective? Is the word apple
in a given context referring to a company or a fruit? Is on the right prepositionto use in I
readabookonLondon?Doesagivenperioddenoteasentenceboundaryoranabbreviation?
Is the given word part of a name of a person, location, or organization? And so on. ese
types of questions often arise in the context of larger goals, such as annotating a sentence
forparts-of-speech,splittingadocumentintosentences,findingallthenamedentitiesina
text,findingalldocumentsmentioningagivenentity,andsoon.
Relationbetweentwowords Here we are given two words or phrases within the context of a
largerdocument,andneedtosaysomethingabouttherelationsbetweenthem.IswordA
thesubjectofverbBinagivensentence?Doesthe“purchase”relationholdbetweenwords
AandBinagiventext?Andsoon.
Manyoftheseclassificationcasescanbeextendedtostructuredproblemsinwhichwearein-
terestedinperformingseveralrelatedclassificationdecisions,suchthattheanswertoonedecision
caninfluenceothers.esearediscussedinChapter19.
Whatisaword? Weareusingthetermword ratherloosely.equestion“whatisaword?”
isamatterofdebateamonglinguists,andtheanswerisnotalwaysclear.
Onedefinition(whichistheonebeinglooselyfollowedinthisbook)isthatwordsare
sequencesoflettersthatareseparatedbywhitespace.isdefinitionisverysimplistic.First,
punctuation in English is not separated by whitespace, so according to our definition dog,
dog?,dog.anddog)arealldifferentwords.Ourcorrecteddefinitionisthenwordsseparated
bywhitespaceorpunctuation.Aprocesscalledtokenizationisinchargeofsplittingtextinto
tokens(whatwecallherewords)basedonwhitespaceandpunctuation.InEnglish,thejob
ofthetokenizerisquitesimple,althoughitdoesneedtoconsidercasessuchasabbreviations
(I.B.M) and titles (Mr.) that needn’t be split. In other languages, things can become much
tricker: in Hebrew and Arabic some words attach to the next one without whitespace, and
.. inChinesetherearenowhitespacesatall.esearejustafewexamples.6.2. FEATURESFORNLPPROBLEMS 67
When working in English or a similar language (as this book assumes), tokenizing
onwhitespaceandpunctuation(whilehandling afewcornercases)canprovide agoodap-
proximation of words. However, our definition of word is still quite technical: it is derived
fromthewaythingsarewritten.Anothercommon(andbetter)definitiontakeawordtobe
“thesmallestunitofmeaning.”Byfollowingthisdefinition,weseethatourwhitespace-based
definitionisproblematic.Aftersplittingbywhitespaceandpunctuation,westillremainwith
sequencessuchasdon’t,thatareactuallytwowords,do not,thatgotmergedintoonesym-
bol.ItiscommonforEnglishtokenizerstohandlethesecasesaswell.esymbolscatand
Cathavethesamemeaning,butaretheythesameword?Moreinterestingly,takesomething
likeNew York,isittwowords,orone?Whataboutice cream?Isitthesameasice-cream
oricecream?Andwhataboutidiomssuchaskick the bucket?
Ingeneral,wedistinguishbetweenwordsandtokens.Werefertotheoutputofatok-
enizerasatoken,andtothemeaning-bearingunitsaswords.Atokenmaybecomposedof
multiplewords,multipletokenscanbeasingleword,andsometimesdifferenttokensdenote
thesameunderlyingword.
Having said that, in this book, we use the term word very loosely, and take it to be
interchangeablewithtoken.Itisimportanttokeepinmind,however,thatthestoryismore
complexthanthat.
..
6.2 FEATURESFORNLPPROBLEMS
Inwhatfollows,wedescribethecommonfeaturesthatareusedfortheaboveproblems.Aswords
andlettersarediscreteitems,ourfeaturesoftentaketheformofindicatorsorcounts.Anindicator
feature takes a value of 0 or 1, depending on the existence of a condition (e.g., a feature taking
thevalueof1iftheworddog appearedatleastonceinthedocument,and0otherwise).Acount
takes a value depending on the number of times some event occurred, e.g., a feature indicating
thenumberoftimestheworddog appearsinthetext.
6.2.1 DIRECTLYOBSERVABLEPROPERTIES
FeaturesforSingleWords Whenourfocusentityisawordoutsideofacontext,ourmainsource
of information is the letters comprising the word and their order, as well as properties derived
fromthesesuchasthelengthoftheword,theorthographicshapeoftheword(Isthefirstletter
capitalized?Areallletterscapitalized?Doesthewordincludeahyphen?Doesitincludeadigit?
Andsoon),andprefixesandsuffixesoftheword(Doesitstartwithun?Doesitendwithing?).
Wemayalsolookatthewordwithrelationtoexternalsourcesofinformation:Howmany
timesdoesthewordappearinalargecollectionoftext?Doesthewordappearinalistofcommon
personnamesintheU.S.?Andsoon.
LemmasandStems We often look at the lemma (the dictionary entry) of the word, mapping
forms such as booking, booked, books to their common lemma book. is mapping is usually per-68 6. FEATURESFORTEXTUALDATA
formedusinglemmalexiconsormorphologicalanalyzers,thatareavailableformanylanguages.
elemmaofawordcanbeambiguous,andlemmatizingismoreaccuratewhenthewordisgiven
incontext.Lemmatizationisalinguisticallydefinedprocess,andmaynotworkwellforformsthat
arenotinthelemmatizationlexicon,orformis-spelling.Acoarserprocessthanlemmatization,
thatcanworkonanysequenceofletters,iscalledstemming.Astemmermapssequencesofwords
to shorter sequences, based on some language-specific heuristics, such that different inflections
willmaptothesamesequence.Notethattheresultofstemmingneednotbeavalidword:picture
andpicturesandpicturedwillallbestemmedtopictur.Variousstemmersexist,withdifferent
levelsofaggressiveness.
LexicalResources An additional source of information about word forms are lexical resources.
ese are essentially dictionaries that are meant to be accessed programmatically by machines
rather than read by humans. A lexical resource will typically contain information about words,
linkingthemtootherwordsand/orprovidingadditionalinformation.
Forexample,formanylanguagestherearelexiconsthatmapinflectedwordformstotheir
possiblemorphologicalanalyses(i.e.,tellingyouthatacertainwordmaybeeitherapluralfemi-
ninenounorapast-perfectverb).Suchlexiconswilltypicallyalsoincludelemmainformation.
Averywell-knownlexicalresourceinEnglishisWordNet [Fellbaum,1998].WordNetisa
verylargemanuallycurateddatasetattemptingtocaptureconceptualsemanticknowledgeabout
words.Eachwordbelongstooneorseveralsynsets,whereeachsynsetsdescribesacognitivecon-
cept.Forexample,thewordstarasanounbelongstothesynsetsastronomicalcelestialbody,someone
whoisdazzlinglyskilled,anycelestialbodyvisiblefromearthandanactorwhoplaysaprinciplerole,
among others. e second synset of star contains also the words ace,adept,champion,sensation,
maven,virtuoso, among others. Synsets are linked to each other by means of semantic relations
suchashypernymyandhyponymy(morespecificorlessspecificwords).Forexample,forthefirst
synsetofstarthesewouldincludesunandnova(hyponyms)andcelestialbody(hypernym).Other
semanticrelationsinWordNetcontainantonyms(oppositewords)andholonymsandmeronyms
(part-wholeandwhole-partrelations).WordNetcontainsinformationaboutnouns,verbs,adjec-
tives,andadverbs.
FrameNet[Fillmoreet al., 2004] andVerbNet[Kipperet al., 2000] aremanually curated
lexicalresourcesthatfocusaroundverbs,listingformanyverbsthekindsofargumenttheytake
(i.e.,thatgiving involvesthecoreargumentsD,R,andT(thethingthatis
beinggiven),andmayhavenon-coreargumentssuchasT,P,P,andM,
amongothers.
eParaphraseDatabase(PPDB)[Ganitkevitchetal.,2013,Pavlicketal.,2015]isalarge,
automaticallycreateddatasetofparaphrases.Itlistswordsandphrases,andforeachoneprovides
alistofwordsandphrasesthatcanbeusedtomeanroughlythesamething.
Lexicalresourcessuchasthesecontainalotofinformation,andcanserveagoodsourceof
features. However, the means of using such symbolic information effectively is task dependent,6.2. FEATURESFORNLPPROBLEMS 69
andoftenrequiresnon-trivial engineeringefforts and/oringenuity.eyarecurrentlynot often
usedinneuralnetworkmodels,butthismaychange.
DistributionalInformation Another important source of information about words is distribu-
tional—which other words behave similar to it in the text? ese deserve their own separate
treatment, and are discussed in Section 6.2.5 below. In Section 11.8, we discuss how lexical re-
sources can be used to inject knowledge into distributional word vectors that are derived from
neuralnetworkalgorithms.
Features for Text When we consider a sentence, a paragraph, or a document, the observable
featuresarethecountsandtheorderofthelettersandthewordswithinthetext.
BagofwordsAverycommonfeatureextractionproceduresforsentencesanddocumentsisthe
bag-of-wordsapproach(BOW).Inthisapproach,welookatthehistogramofthewordswithin
thetext,i.e.,consideringeachwordcountasafeature.Bygeneralizingfromwordsto“basicele-
ments,”thebag-of-letter-bigramsweusedinthelanguageidentificationexampleinSection2.3.1
isanexampleofthebag-of-wordsapproach.
Wecanalsocomputequantitiesthataredirectlyderivedfromthewordsandtheletters,such
asthelengthofthesentenceintermsofnumberoflettersornumberofwords.Whenconsidering
individualwords,wemayofcourseusetheword-basedfeaturesfromabove,countingforexample
thenumberofwordsinthedocumentthathaveaspecificprefixorsuffix,orcomputetheratioof
shortwords(withlengthbelowagivenlength)tolongwordsinadocument.
WeightingAsbefore,wecanalsointegratestatisticsbasedonexternalinformation,focusingfor
exampleonwordsthatappearmanytimesinthegivendocument,yetappearrelativelyfewtimesin
anexternalsetofdocuments(thiswilldistinguishwordsthathavehighcountsinthedocuments
because they are generally common, like a and for from words that have a high count because
they relate to the document’s topic). When using the bag-of-words approach, it is common to
use TF-IDF weighting [Manning et al., 2008, Chapter 6]. Consider a document d which is
part of a larger corpus D. Rather than representing each word w in d by its normalized count
inthedocument #d.w/ (theTermFrequency),TF-IDFweightingrepresentitinsteadby
#d.w/
logPw 02d# Dd.w 0/
.esecondtermistheInverseDocumentFrequency:theinverse
oP fw t0h2ed n#d u. mw 0 b/ e(cid:2) rofdijf sd ti2 nDj ctWwj d2 od cg uj
mentsinthecorpusinwhichthiswordoccurred.ishighlights
wordsthataredistinctiveofthecurrenttext.
Besideswords,onemayalsolookatconsecutivepairsortripletsofwords.esearecalled
ngrams.NgramfeaturesarediscussedindepthinSection6.2.4.
FeaturesofWordsinContext Whenconsideringawordwithinasentenceoradocument,the
directlyobservablefeaturesofthewordareitspositionwithinthesentence,aswellasthewordsor
letterssurroundingit.Wordsthatareclosertothetargetwordareoftenmoreinformativeabout
itthanwordsthatarefurtherapart.¹
¹However,notethatthisisagrossgeneralization,andinmanycaseslanguageexhibitalong-rangedependenciesbetween
words:awordattheendofatextmaywellbeinfluencedbyawordatthebeginning.70 6. FEATURESFORTEXTUALDATA
Windows For this reason, it is often common to focus on the immediate context of a word by
considering a window surrounding it (i.e., k words to each side, with typical values of k being
2, 5, and 10), and take the features to be the identities of the words within the window (e.g., a
featurewillbe“wordXappearedwithinawindowoffivewordssurroundingthetargetword”).For
example,considerthesentencethebrownfoxjumpedoverthelazydog,withthetargetwordjumped.
A window of 2 words to each side will produce the set of features { word=brown, word=fox,
word=over, word=the }. e window approach is a version of the bag-of-words approach, but
restrictedtoitemswithinthesmallwindow.
e fixed size of the window gives the opportunity to relax the bag-of-word assumption
thatorderdoesnotmatter,andtaketherelativepositionsofthewordsinthewindowintoaccount.
is results in relative-positionalfeatures such as “word X appeared two words to the left of the
target word.” For example, in the example above the positional window approach will result in
thesetoffeatures{word-2=brown,word-1=fox,word+1=over,word+2=the}.
Encodingofwindow-basedfeaturesasvectorsisdiscussedinSection8.2.1.InChapters14
and16wewillintroducethebiRNNarchitecture,thatgeneralizeswindowfeaturesbyproviding
aflexible,adjustable,andtrainablewindow.
PositionBesidesthecontextoftheword,wemaybeinterestedinitsabsolutepositionwithina
sentence.Wecouldhavefeaturessuchas“thetargetwordisthe5thwordinthesentence,”ora
binnedversionindicatingmorecoarsegrainedcategories:doesitappearwithinthefirst10words,
betweenword10and20,andsoon.
Features for Word Relations When considering two words in context, besides the position of
each one and the words surrounding them, we can also look at the distance between the words
andtheidentitiesofthewordsthatappearbetweenthem.
6.2.2 INFERREDLINGUISTICPROPERTIES
Sentencesinnaturallanguagehavestructuresbeyondthelinearorderoftheirwords.estructure
followsanintricatesetofrulesthatarenotdirectlyobservabletous.eserulesarecollectively
referredtoassyntax,andthestudyofthenatureoftheserulesandregularitiesinnaturallanguage
isthestudy-objectoflinguistics.²Whiletheexactstructureoflanguageisstillamystery,andrules
governingmanyofthemoreintricatepatternsareeitherunexploredorstillopenfordebateamong
linguists,asubsetofphenomenagoverninglanguagearewelldocumentedandwellunderstood.
eseincludeconceptssuchaswordclasses(part-of-speechtags),morphology,syntax,andeven
partsofsemantics.
Whilethelinguisticpropertiesofatextarenotobservabledirectlyfromthesurfaceforms
of words in sentences and their order, they can be inferred from the sentence string with vary-
²islastsentence,is,ofcourse,agrosssimplification.Linguisticshasmuchwiderbreadththansyntax,andthereareother
systemsthatregulatethehumanlinguisticbehaviorbesidesthesyntacticone.Butforthepurposeofthisintroductorybook,
thissimplisticviewwillbesufficient.Foramoreindepthoverview,seethefurtherreadingrecommendationsattheendof
thissection.6.2. FEATURESFORNLPPROBLEMS 71
ingdegreesofaccuracy.Specializedsystemsexistforthepredictionofpartsofspeech,syntactic
trees, semantic roles, discourse relations, and other linguistic properties with various degrees of
accuracy,³andthesepredictionsoftenserveasgoodfeaturesforfurtherclassificationproblems.
Linguistic Annotation Let’s explore some forms of linguistic annotations. Consider the
sentencetheboywiththeblackshirtopenedthedoorwithakey.Onelevelofannotationassigns
toeachworditspartofspeech:
the boy with the black shirt opened the door with a key
D N P D A N V D N P D N
Goingfurtherupthechain,wemarksyntacticchunkboundaries,indicatingthetheboy
isanounphrase.
[ theboy][ with][ theblackshirt][ opened][ thedoor][ with][ akey]
NP PP NP VP NP PP NP
Notethatthewordopened ismarkedasaverbal-chunk(VP).ismaynotseemvery
usefulbecausewealreadyknowitsaverb.However,VPchunksmaycontainmoreelements,
coveringalsocasessuchaswillopened anddidnotopen.
e chunking information is local. A more global syntactic structure is a constituency
tree,alsocalledaphrase-structuretree:
S
NP VP
DT NN PP VBD NP PP
the boy IN NP opened DT NN IN NP
with DT JJ NN the door with DT NN
the black shirt a key
Constituencytreesarenested,labeledbracketingoverthesentence,indicatingthehi-
erarchy of syntactic units: the noun phrase the boy with the black shirt is made of the noun
..
³Indeed,formanyresearchers,improvingthepredictionoftheselinguisticpropertiesisthenaturallanguageprocessingproblem
theyaretryingtosolve.72 6. FEATURESFORTEXTUALDATA
phrasetheboy andtheprepositionphrase(PP)withtheblackshirt.elatteritselfcontains
the noun phrase the black shirt. Having with a key nested under the VP and not under the
NPthedoor signalsthatwithakey modifiestheverbopened (openedwithakey)ratherthan
theNP(adoorwithakey).
A different kind of syntactic annotation is a dependency tree. Under dependency
syntax, each word in the sentence is a modifier of another word, which is called its head.
Each word in the sentence is headed by another sentence word, except for the main word,
usuallyaverb,whichistherootofthesentenceandisheadedbyaspecial“root”node.
root
nsubj
pobj prep
det dobj pobj
det prep amod det det
the boy with the black shirt opened the door with a key
Whileconstituencytreesmakeexplicitthegroupingofwordsintophrases,dependency
treesmakeexplicitthemodificationrelations andconnections betweenwords.Wordsthatare
farapartinthesurfaceformofthesentencemaybecloseinitsdependencytree.Forexample,
boy and opened have four words between them in the surface form, but have a direct nsubj
edgeconnectingtheminthedependencytree.
e dependency relations are syntactic: they are concerned with the structure of the
sentence.Otherkindsofrelationsaremoresemantic.Forexample,considerthemodifiersof
the verb open, also called the arguments of the verb. e syntactic tree clearly marks theboy
(withtheblackshirt), thedoor, and withakey as arguments, and also tells us that withakey
isanargumentofopenratherthanamodifierofdoor.Itdoesnottellus,however,whatare
the semantic-roles of the arguments with respect to the verb, i.e., that theboy is the A
performingtheaction,andthatakeyisanI(comparethattotheboyopenedthe
doorwith a smile. Here, the sentence will have the same syntactic structure, but, unless we
areinamagical-world,asmile isaMratherthananI.esemanticrole
labeling annotationsrevealthesestructures:
..6.2. FEATURESFORNLPPROBLEMS 73
the boy with the black shirt opened the door with a key
Agent Patient
Instrument
the boy with the black shirt opened the door with a smile
Agent Patient
Manner
..
Besidestheobservableproperties(letters,words,counts,lengths,lineardistances,frequen-
cies,etc.),wecanalsolooksuchinferredlinguisticpropertiesofwords,sentences,anddocuments.
Forexample,wecouldlookatthepart-of-speechtag (POS)ofawordwithinadocument(Isita
noun,averb,adjective,oradeterminer?),thesyntacticroleofaword(Doesitserveasasubjector
anobjectofaverb?Isitthemainverbofthesentence?Isitusedasanadverbialmodifier?),orthe
semanticrole of it (e.g., in “the key opened the door,” key acts as an I, while in “the
boy opened the door” boy is an A). When given two words in a sentence, we can consider
thesyntacticdependencytreeofthesentence,andthesubtreeorpathsthatconnectthetwowords
withinthethistree,aswellaspropertiesofthatpath.Wordsthatarefarapartinthesentencein
termsofthenumberofwordsseparatingthemcanbeclosetoeachotherinthesyntacticstructure.
When moving beyond the sentence, we may want to look at the discourse relations that
connectsentencestogether,suchasE,C,CE,andsoon.
eserelationsareoftenexpressedbydiscourse-connectivewordssuchasmoreover,however,and
and,butarealsoexpressedwithlessdirectcues.
Another important phenomena is that of anaphora—consider the sentence sequence the
boyopenedthedoorwithakey.It wasn’tlockedandhe enteredtheroom.He sawaman.He was
1 1 2 3
smiling. Anaphora resolution (also called coreference resolution) will tell us that It refers to the
1
door(andnotthekeyortheboy),he referstotheboyandhe islikelytorefertotheman.
2 3
Partofspeechtags,syntacticroles,discourserelations,anaphora,andsoonareconceptsthat
arebasedonlinguistictheoriesthatweredevelopedbylinguistsoveralongperiodoftime,with
theaimofcapturingtherulesandregularitiesintheverymessysystemofthehumanlanguage.
While many aspects of the rules governing language are still open for debate, and others may
seemoverlyrigidorsimplistic,theconceptsexploredhere(andothers)doindeedcaptureawide
andimportantarrayofgeneralizationsandregularitiesinlanguage.
Are linguistic concepts needed? Some proponents of deep-learning argue that such inferred,
manually designed, linguistic properties are not needed, and that the neural network will learn
theseintermediaterepresentations(orequivalent,orbetterones)onitsown.ejuryisstillouton
this.Mycurrentpersonalbeliefisthatmanyoftheselinguisticconceptscanindeedbeinferredby74 6. FEATURESFORTEXTUALDATA
thenetworkonitsownifgivenenoughdataandperhapsapushintherightdirection.⁴However,
formanyothercaseswedonothaveenoughtrainingdataavailableforthetaskwecareabout,and
inthesecasesprovidingthenetworkwiththemoreexplicitgeneralconceptscanbeveryvaluable.
Evenifwedohaveenoughdata,wemaywanttofocusthenetworkoncertainaspectsofthetext
andhint to it that it should ignoreothers, byproviding the generalized conceptsin additionto,
oreveninsteadof,thesurfaceformsofthewords.Finally,evenifwedonotusetheselinguistic
propertiesasinputfeatures,wemaywanttohelpguidethenetworkbyusingthemasadditional
supervisioninamulti-tasklearningsetup(seeChapter20)orbydesigningnetworkarchitecture
ortrainingparadigmsthataremoresuitableforlearningcertainlinguisticphenomena.Overall,
weseeenoughevidencethattheuseoflinguisticconceptshelpimprovelanguageunderstanding
andproductionsystems.
FurtherReading Whendealingwithnaturallanguagetext,itiswelladvisedtobeawareofthe
linguisticconceptsbeyondlettersandwords,aswellasofthecurrentcomputationaltoolsandre-
sourcesthatareavailable.isbookbarelyscratchesthesurfaceonthistopic.ebookofBender
[2013] provides a good and concise overview of linguistic concepts directed at computational-
minded people. For a discussion on current NLP methods, tools, and resources see the book by
JurafskyandMartin[2008]aswellasthevariousspecializedtitlesinthisseries.⁵
6.2.3 COREFEATURESVS.COMBINATIONFEATURES
In many cases, we are interested in a conjunction of features occurring together. For example,
knowingthatthetwoindicators“thewordbookappearedinawindow”and“thepart-of-speech
V appeared in a window” is strictly less informative than knowing “the word book with the
assignedpartofspeechVappearedinawindow.”Similarly,ifweassignadistinctparameter
weight for each indicator feature (as is the case in linear models), then knowing that the two
distinctfeatures“wordinposition 1islike,”“wordinposition 2isnot”occurisalmostuseless
(cid:0) (cid:0)
compared to the very indicative combined indicator “word in position 1 is like and word in
(cid:0)
position 2isnot.”Similarly,knowingthatadocumentcontainsthewordParisisanindication
(cid:0)
toward the document being in the T category, and the same holds for the word Hilton.
However,ifthedocumentcontainsbothwords,itisanindicationawayfromtheTcategory
andtowardtheCorGcategories.
Linearmodelscannotassignascoretoaconjunctionofevents(XoccurredandYoccurred
and …) that is not a sum of their individual scores, unless the conjunction itself is modeled as
its own feature. us, when designing features for a linear model, we must define not only the
corefeaturesbutalsomanycombinationfeatures.⁶esetofpossiblecombinationisverylarge,and
⁴See,forexample,theexperimentinSection16.1.2inwhichaneuralnetworkslearnstheconceptofsubject-verbagreement
inEnglish,inferringtheconceptsofnouns,verbs,grammaticalnumberandsomehierarchicallinguisticsstructures.
⁵SyntacticdependencystructuresarediscussedinKübleretal.[2008]andsemanticrolesinPalmeretal.[2010].
⁶isisadirectmanifestationoftheXORproblemdiscussedinChapter3,andthemanuallydefinedcombination-featuresare
themappingfunction(cid:30)thatmapsthenonlinearlyseparablevectorsofcore-featurestoahigherdimensionalspaceinwhich
thedataismorelikelytobeseparablebyalinearmodel.6.2. FEATURESFORNLPPROBLEMS 75
human expertise, coupled with trial and error, is needed in order to construct a set of combina-
tionsthatisbothinformativeandrelativelycompact.Indeed,alotofefforthasgoneintodesign
decisionssuchas“includefeaturesoftheformword at position -1 is X and at position +1 is
Ybutdonotincludefeaturesoftheformword at position -3 is X and at position -1 is Y.”
Neural networks provide nonlinear models, and do not suffer from this problem. When
using a neural network such as a multi-layer perceptron (Chapter 4), the model designer can
specify only the set of core features, and rely on the network training procedure to pick up on
the important combinations on its own. is greatly simplifies the work of the model designer.
Inpractice,neuralnetworksindeedmanagetolearngoodclassifiersbasedoncorefeaturesonly,
sometimessurpassingthebestlinearclassifierwithhuman-designedfeaturecombinations.How-
ever, in many other cases a linear classifier with a good hand-crafted feature-set is hard to beat,
withtheneuralnetworkmodelswithcorefeaturesgettingclosetobutnotsurpassingthelinear
models.
6.2.4 NGRAMFEATURES
Aspecialcaseoffeaturecombinationsisthatofngrams—consecutivewordsequencesofagiven
length. We already saw letter-bigram features in the language classification case (Chapter 2).
Word-bigrams, as well as trigrams (sequences of three items) of letters or words are also com-
mon. Beyond that, 4-grams and 5-grams are sometimes used for letters, but rarely for words
duetosparsityissues.Itshouldbeintuitivelyclearwhyword-bigramsaremoreinformativethan
individual words: it captures structures such as New York, not good, and Paris Hilton. Indeed, a
bag-of-bigrams representation is much more powerful than bag-of-words, and in many cases
proves very hard to beat. Of course, not all bigrams are equally informative, bigrams such as of
the,ona,theboy,etc.areverycommonand,formosttasks,notmoreinformativethantheirindi-
vidual components. However, it is very hardto know a-priori which ngrams will be useful for a
giventask.ecommonsolutionistoincludeallngramsuptoagivenlength,andletthemodel
regularizationdiscardofthelessinterestingonesbyassigningthemverylowweights.
Note that vanilla neural network architectures such as the MLP cannot infer ngram fea-
tures from a document on their own in the general case: a multi-layer perceptron fed with a
bag-of-wordsfeaturevectorofadocumentcouldlearncombinationssuchas“wordXappearin
the document and word Y appears in the document” but not “the bigram X Y appears in the
document.”us,ngramfeaturesareusefulalsointhecontextofnonlinearclassification.
Multi-layerperceptronscaninferngramswhenappliedtoafixedsizewindowswithposi-
tionalinformation—thecombinationof“wordatposition 1isX”and“wordatposition 2isY”
(cid:0) (cid:0)
is in effect the bigram XY. More specialized neural network architectures such as convolutional
networks(Chapter13)aredesignedtofindinformativengramfeaturesforagiventaskbasedon
asequenceofwordsofvaryinglengths.BidirectionalRNNs(Chapters14and16)generalizethe
ngram concept even further, and can be sensitive to informative ngrams of varying lengths, as
wellasngramswithgapsinthem.76 6. FEATURESFORTEXTUALDATA
6.2.5 DISTRIBUTIONALFEATURES
Up until now our treatment of words was as discrete and unrelated symbols: the words pizza,
burger,andchair areallequallysimilar(andequallydis-similar)toeachotherasfarasthealgo-
rithmisconcerned.
Wedidachievesomeformofgeneralizationacrosswordtypesbymappingthemtocoarser-
grainedcategoriessuchasparts-of-speechorsyntacticroles(“the,a,an,somearealldeterminers”);
generalizing from inflected words forms to their lemmas (“book, booking, booked all share the
lemmabook”);lookingatmembershipinlistsordictionaries(“John,Jack,andRalphappearinalist
ofcommonU.S.firstnames”);orlookingattheirrelationtootherwordsusinglexicalresources
such as WordNet. However, these solutions are quite limited: they either provide very coarse
grained distinctions, or otherwise rely on specific, manually compiled dictionaries. Unless we
haveaspecializedlistoffoodswewillnotlearnthatpizzaismoresimilartoburger thanitisto
chair,anditwillbeevenhardertolearnthatpizzaismoresimilartoburger thanitistoicecream.
edistributionalhypothesisoflanguage,setforthbyFirth[1957]andHarris[1954],states
thatthemeaningofawordcanbeinferredfromthecontextsinwhichitisused.Byobservingco-
occurrencepatternsofwordsacrossalargebodyoftext,wecandiscoverthatthecontextsinwhich
burgeroccurarequitesimilartothoseinwhichpizzaoccurs,lesssimilartothoseinwhichicecream
occurs,andverydifferentfromthoseinwhichchair occurs.Manyalgorithmswerederivedover
the years to make use of this property, and learn generalizations of words based on the contexts
inwhichtheyoccur.esecanbebroadlycategorizedintoclustering-basedmethods,whichassign
similarwordstothesameclusterandrepresenteachwordbyitsclustermembership[Brownetal.,
1992,Milleretal.,2004],andtoembedding-basedmethodswhichrepresenteachwordasavector
suchthatsimilarwords(wordshavingasimilardistribution)havesimilarvectors[Collobertand
Weston,2008,Mikolovetal.,2013b].Turianetal.[2010]discussandcomparetheseapproaches.
esealgorithmsuncovermanyfacetsofsimilaritybetweenwords,andcanbeusedtoderive
goodwordfeatures:forexample,onecouldreplacewordsbytheirclusterID(e.g.,replacingboth
thewordsJuneandaugbycluster732),replacerareorunseenwordswiththecommonwordmost
similartothem,orjustusethewordvectoritselfastherepresentationoftheword.
However,caremustbetakenwhenusingsuchwordsimilarityinformation,asitcanhave
unintendedconsequences.Forexample,insomeapplicationsitisveryusefultotreatLondonand
Berlinassimilar,whileforothers(forexamplewhenbookingaflightortranslatingadocument)
thedistinctioniscrucial.
We will discuss word embeddings methods and the use of word vectors in more detail in
Chapters10and11.77
C H A P T E R 7
Case Studies of NLP Features
After discussing the different sources of information available for us for deriving features from
natural language text, we will now explore examples of concrete NLP classification tasks, and
suitablefeaturesforthem.Whilethepromiseofneuralnetworksistoalleviatetheneedformanual
feature engineering, we still need to take these sources of information into consideration when
designingourmodels:wewanttomakesurethatthenetworkwedesigncanmakeeffectiveuse
of the available signals, either by giving it direct access to them by use of feature-engineering;
by designing the network architecture to expose the needed signals; or by adding them as an
additionallosssignalswhentrainingthemodels.¹
7.1 DOCUMENTCLASSIFICATION:LANGUAGE
IDENTIFICATION
Inthelanguageidentificationtask,wearegivenadocumentorasentence,andwanttoclassifyit
intooneofafixedsetoflanguages.AswesawinChapter2,abagofletter-bigramsisaverystrong
featurerepresentationforthistask.Concretely,eachpossibleletter-bigram(oreachletterbigram
appearingatleastktimesinatleastonelanguage)isacorefeature,andthevalueofacorefeature
foragivendocumentisthecountofthatfeatureinthedocument.
Asimilartaskistheoneofencodingdetection.Here,agoodfeaturerepresentationisabag-of
byte-bigrams.
7.2 DOCUMENTCLASSIFICATION:TOPIC
CLASSIFICATION
In the Topic Classification task, we are given a document and need to classify it into one of a
predefinedsetoftopics(e.g.,Economy,Politics,Sports,Leisure,Gossip,Lifestyle,Other).
Here,theletterlevelisnotveryinformative,andourbasicunitswillbewords.Wordorder
is not very informative for this task (except maybe for consecutive word pairs such as bigrams).
us,agoodsetoffeatureswillbethebag-of-wordsinthedocument,perhapsaccompaniedbya
bag-of-word-bigrams(eachwordandeachword-bigramisacorefeature).
¹Additionally,linearorlog-linearmodelswithmanuallydesignedfeaturesarestillveryeffectiveformanytasks.eycanbe
verycompetitiveintermsofaccuracy,aswellasbeingveryeasytotrainanddeployatscale,andeasiertoreasonaboutand
debugthanneuralnetworks.Ifnothingelse,suchmodelsshouldbeconsideredasstrongbaselinesforwhatevernetworksyou
aredesigning.78 7. CASESTUDIESOFNLPFEATURES
If we do not have many training examples, we may benefit from pre-processing the doc-
ument by replacing each word with its lemma. We may also replace or supplement words by
distributionalfeaturessuchaswordclustersorword-embeddingvectors.
Whenusingalinearclassifier,wemaywanttoalsoconsiderwordpairs,i.e.,considereach
pairofwords(notnecessarilyconsecutive)thatappearinthesamedocumentasacorefeature.is
willresultinahugenumberofpotentialcorefeatures,andthenumberwillneedtobetrimmed
downbydesigningsomeheuristic,suchasconsideringonlywordpairswhichappearinaspecified
numberofdocuments.Nonlinearclassifiersalleviatethisneed.
Whenusingabag-of-words,itissometimesusefultoweighteachwordwithproportionto
itsinformativeness,forexampleusingTF-IDFweighting(Section6.2.1).However,thelearning
algorithmisoftencapableofcomingupwiththeweightingonitsown.Anotheroptionistouse
wordindicatorsratherthanwordcounts:eachwordinthedocument(oreachwordaboveagiven
count)willberepresentedonce,regardlessofitsnumberofoccurrencesinthedocument.
7.3 DOCUMENTCLASSIFICATION:AUTHORSHIP
ATTRIBUTION
Intheauthorshipattributiontask[Koppeletal.,2009]wearegivenatextandneedtoinferthe
identifyofitsauthor(fromafixedsetofpossibleauthors),orothercharacteristicsoftheauthor
ofthetext,suchastheirgender,theirageortheirnativelanguage.
e kind of information used to solve this task is very different than that of topic
classification—the clues are subtle, and involve stylistic properties of the text rather than con-
tentwords.
us,ourchoiceoffeaturesshouldshyawayfromcontentwordsandfocusonmorestylistic
properties.²Agoodsetforsuchtasksfocusonpartsofspeech(POS)tagsandfunctionwords.ese
arewordslikeon,of,the,and,before andsoonthatdonotcarrymuchcontentontheirownbut
rather serveto connectto content-bearing words and assign meanings to their compositions, as
wellaspronouns(he,she,I,they,etc.)Agoodapproximationoffunctionwordsisthelistoftop-
300 or so most frequent words in a large corpus. By focusing on such features, we can learn to
capturesubtlestylisticvariationsinwriting,thatareuniquetoanauthorandveryhardtofake.
A good feature set for authorship attribution task include a bag-of-function-words-and-
pronouns, bag-of-POS-tags, and bags of POS bigrams, trigrams, and 4grams. Additionally, we
maywanttoconsiderthedensityoffunctionwords(i.e.,theratiobetweenthenumberoffunction
wordsandcontentwordsinawindowoftext),abagofbigramsoffunctionwordsafterremoving
thecontentwords,andthedistributionsofthedistancesbetweenconsecutivefunctionwords.
²Onecouldarguethatforageorgenderidentification,wemayaswellobservealsothecontent-words,astherearestrong
correlationbetweenageandgenderofapersonandthetopicstheywriteaboutandthelanguageregistertheyuse.isis
generallytrue,butifweareinterestedinaforensicoradversarialsettinginwhichtheauthorhasanincentivetohidetheirage
orgender,webetternotrelyoncontent-basedfeatures,asthesearerathereasytofake,comparedtothemoresubtlestylistic
cues.7.4. WORD-IN-CONTEXT:PARTOFSPEECHTAGGING 79
7.4 WORD-IN-CONTEXT:PARTOFSPEECHTAGGING
Intheparts-of-speechtaggingtask,wearegivenasentence,andneedtoassignthecorrectpart-
of-speechtoeachwordinthesentence.eparts-of-speechcomefromapre-definedset,forthis
exampleassumewewillbeusingthetagsetoftheUniversalTreebankProject [McDonaldetal.,
2013,Nivreetal.,2015],containing17tags.³
Part-of-speech tagging is usually modeled as a structured task—the tag of the first word
maydependonthetagofthethirdone—butitcanbeapproximatedquitewellbyclassifyingeach
wordinisolationintoaPOS-tagbasedonawindowoftwowordstoeachsideoftheword.Ifwe
tagthewordsinafixedorder,forexamplefromlefttoright,wecanalsoconditioneachtagging
prediction on tag predictions made on previous tags. Our feature function when classifying a
word w has access to all the words in the sentence (and their letters) as well as all the previous
i
taggingdecisions(i.e.,theassignedtagsforwordsw ;:::;w ).Here,wediscussfeaturesasif
1 i 1
(cid:0)
they are used in an isolated classification task. In Chapter 19 we discuss the structured learning
case—usingthesamesetoffeatures.
e sources of information for the POS-tagging task can be divided into intrinsic cues
(based on the word itself) and extrinsic cues (based on its context). Intrinsic cues include the
identifyoftheword(somewordsaremorelikelythanotherstobenouns,forexample),prefixes,
suffixes,andorthographicshapeoftheword(inEnglish,wordsendingin-edarelikelypast-tense
verbs,wordsstartingwithun-arelikelytobeadjectives,andwordsstartingwithacapitalletter
are likely to be proper names), and the frequency of the word in a large corpus (for example,
rarewordsaremorelikelytobenouns).Extrinsiccuesincludethewordidentities,prefixes,and
suffixes of words surrounding the current word, as well as the part-of-speech prediction for the
previouswords.
Overlapping features If we have the word form as a feature, why do we need the prefixes
andsuffixes?Afteralltheyaredeterministicfunctionsoftheword.ereasonisthatifwe
encounterawordthatwehavenotseenintraining(outofvocabularyorOOV word)oraword
we’veseenonlyahandfuloftimesintraining(arareword),wemaynothaverobustenough
information to base a decision on. In such cases, it is good to back-off to the prefixes and
suffixes, which can provide useful hints. By including the prefix and suffix features also for
words that are observed many times in training, we allow the learning algorithms to better
adjusttheirweights,andhopefullyusethemproperlywhenencounteringOOVwords.
..
³adjective,adposition,adverb,auxiliaryverb,coordinatingconjunction,determiner,interjection,noun,numeral,particle,pro-
noun,propernoun,punctuation,subordinatingconjunction,symbol,verb,other.80 7. CASESTUDIESOFNLPFEATURES
AnexampleofagoodsetofcorefeaturesforPOStaggingis:
• word=X
• 2-letter-suffix=X
• 3-letter-suffix=X
• 2-letter-prefix=X
• 3-letter-prefix=X
• word-is-capitalized
• word-contains-hyphen
• word-contains-digit
• forPin(cid:140) 2; 1; 1; 2(cid:141):
(cid:0) (cid:0) C C
– WordatpositionP=X
– 2-letter-suffixofwordatpositionP=X
– 3-letter-suffixofwordatpositionP=X
– 2-letter-prefixofwordatpositionP=X
– 3-letter-prefixofwordatpositionP=X
– wordatpositionP=Xiscapitalized
– wordatpositionP=Xcontainshyphen
– wordatpositionP=Xcontainsdigit
• PredictedPOSofwordatposition-1=X
• PredictedPOSofwordatposition-2=X
Inadditiontothese,distributionalinformationsuchaswordclustersorword-embeddingvectors
of the word and of surrounding words can also be useful, especially for words not seen in the
training corpus, as words with similar POS-tags tend to occur in more similar contexts to each
otherthanwordsofdifferentPOS-tags.7.5. WORD-IN-CONTEXT:NAMEDENTITYRECOGNITION 81
7.5 WORD-IN-CONTEXT:NAMEDENTITY
RECOGNITION
In the named-entity recognition (NER) task we are given a document and need to find named
entitiessuchasMilan,JohnSmith,McCormikIndustries,andParis,aswellastocategorizethem
intoapre-definedsetofcategoriessuchasL,O,P,orO.Note
thatthistaskiscontextdependent,asMilancanbealocation(thecity)oranorganization(asports
team,“MilanplayedagainstBarsaWednesdayevening”),andPariscanbethenameofacityor
aperson.
Atypicalinputtotheproblemwouldbeasentencesuchas:
John Smith , president of McCormik Industries visited his niece Paris in Milan , reporters
say .
andtheexpectedoutputwouldbe:
[ John Smith ] , president of [ McCormik Industries ] visited his niece [ Paris ]
PER ORG PER
in [ Milan ], reporters say .
LOC
While NER is a sequence segmentation task—it assigns labeled brackets over non-
overlapping sentence spans—it is often modeled as a sequence tagging task, like POS-tagging.
euseoftaggingtosolvesegmentationtasksisperformedusingBIOencodedtags.⁴Eachword
isassignedoneofthefollowingtags,asseeninTable7.1:
Table7.1: BIOtagsfornamedentityrecognition
Tag Meaning
O Not part of a named entity
B-PER First word of a person name
I-PER Continuation of a person name
B-LOC First word of a location name
I-LOC Continuation of a location name
B-ORG First word of an organization name
I-ORG Continuation of an organization name
B-MISC First word of another kind of named entity
I-MISC Continuation of another kind of named entity
⁴VariantsontheBIOtaggingschemeareexploredintheliterature,andsomeperformsomewhatbetterthanit.SeeLample
etal.[2016],RatinovandRoth[2009].82 7. CASESTUDIESOFNLPFEATURES
esentenceabovewouldbetaggedas:
John/B-PER Smith/I-PER ,/O president/O of/O McCormik/B-ORG Industries/I-ORG
visited/O his/O niece/O Paris/B-PER in/O Milan/B-LOC ,/O reporters/O say/O ./O
etranslationfromnon-overlappingsegmentstoBIOtagsandbackisstraightforward.
Like POS-tagging, the NER task is a structured one, as tagging decisions for different
words interact with each other (it is more likely to remain within the same entity type than to
switch,itismorelikelytotag“JohnSmithInc.”asB-ORGI-ORGI-ORGthanasB-PERI-PER
B-ORG).However,weagainassumeitcanbeapproximatedreasonablywellusingindependent
classificationdecisions.
ecorefeaturesetfortheNERtaskissimilartothatofthePOS-taggingtask,andrelies
onwordswithina2-wordswindowtoeachsideofthefocusword.Inadditiontothefeaturesof
thePOS-taggingtaskwhichareusefulforNERaswell(e.g.,-villeisasuffixindicatingalocation,
Mc-isaprefixindicatingaperson),wemaywanttoconsideralsotheidentitiesofthewordsthat
surroundother occurrencesofthesamewordinthetext,aswellasindicatorfunctionsthatcheck
if the word occurs in pre-compiled lists of persons, locations and organizations. Distributional
features such word clusters or word vectors are also extremely useful for the NER task. For a
comprehensivediscussiononfeaturesforNER,seeRatinovandRoth[2009].
7.6 WORDINCONTEXT,LINGUISTICFEATURES:
PREPOSITIONSENSEDISAMBIGUATION
Prepositions, words like on, in, with, and for, serve for connecting predicates with their argu-
mentsandnounswiththeirprepositionalmodifiers.Preposionsareverycommon,andalsovery
ambiguous.Consider,forexample,thewordfor inthefollowingsentences.
(1) a. Wewenttherefor lunch.
b. Hepaidfor me.
c. Weatefor twohours.
d. Hewouldhaveleftfor home,butitstartedraining.
ewordfor playsadifferentroleineachofthem:in(a)itindicatesaPin(b)aB-
,in(c)aDandin(d)aL.
In order to fully understand the meaning of a sentence, one should arguablly know the
correct senses of the prepositions within it. e preposition-sensedisambiguation task deals with
assigningthecorrectsensetoaprepositionincontext,fromafiniteinventoryofsenses.Schneider
et al. [2015, 2016] discuss the task, present a unified sense inventory that covers many preposi-7.6.WORDINCONTEXT,LINGUISTICFEATURES:PREPOSITIONSENSEDISAMBIGUATION 83
tions, and provide a small annotated corpus of sentences from online reviews, covering 4,250
prepositionmentions,eachannotatedwithitssense.⁵
Whichareagoodsetoffeaturesfortheprepositionsensedisambiguationtask?Wefollow
herethefeaturesetinspiredbytheworkofHovyetal.[2010].
Obviously, the preposition itself is a useful feature (the distribution of possible senses for
in is very different from the distribution of senses for with or about, for example). Besides that,
we will look in the context in which the word occurs. A fixed window around the preposition
maynotbeidealintermsofinformationcontent,thought.Consider,forexample,thefollowing
sentences.
(2) a. Helikedtheroundobjectfromtheveryfirsttimehesawit.
b. Hesavedtheroundobjectfromhimtheveryfirsttimetheysawit.
e two instances of from have different senses, but most of the words in a window around the
word are either not informative or even misleading. We need a better mechanism for selecting
informative contexts. One option would be to use a heuristic, such as “the first verb on the
left” and “the first noun on the right.” ese will capture the triplets liked,from,time and
h i
saved,from,him ,whichindeedcontaintheessenceoftheprepositionsense.Inlinguisticterms,
h i
we say that this heuristic helps us capture the governor and the and object of the preposition.
By knowing the identify of the preposition, as well as its governor and objects, humans can in
many cases infer the sense of the preposition, using reasoning processes about the fine-grained
semantics of the words. e heuristic for extracting the object and governor requires the use of
a POS-tagger in order to identify the nouns and verbs. It is also somewhat brittle—it is not
hardtoimaginecasesinwhichitfails.Wecouldrefinetheheuristicwithmorerules,butamore
robust approach would be to use a dependency parser: the governor and object information is
easilyreadablefromthesyntactictree,reducingtheneedforcomplexheuristics:
root
prep pobj
dobj det
det rcmod
nsubj amod amodamod nsubj dobj
he liked the round object from the very first time he saw it
Ofcourse,theparserusedforproducingthetreemaybewrongtoo.Forrobustness,wemaylook
atboththegovernorandobjectextractedfromtheparserand thegovernorandobjectextracted
using the heuristic, and use all four as sources for features (i.e., parse_gov=X, parse_obj=Y,
⁵Earliersenseinventoriesandannotatedcorporaforthetaskarealsoavailable.See,forexample,LitkowskiandHargraves
[2005,2007],SrikumarandRoth[2013a].84 7. CASESTUDIESOFNLPFEATURES
heur_gov=Z, heur_obj=W), letting the learning process decide which of the sources is more
reliableandhowtobalancebetweenthem.
Afterextractingthegovernorandtheobject(andperhapsalsowordsadjacenttothegov-
ernorandtheobject),wecanusethemasthebasisforfurtherfeatureextraction.Foreachofthe
items,wecouldextractthefollowingpiecesofinformation:
• theactualsurfaceformoftheword;
• thelemmaoftheword;
• thepart-of-speechoftheword;
• prefixesandsuffixesoftheword(indicatingadjectivesofdegree,number,order,etcsuchas
ultra-,poly-,post-,aswellassomedistinctionsbetweenagentiveandnon-agentiveverbs);
and
• wordcluster ordistributionalvector oftheword.
Ifweallowtheuseofexternallexicalresourcesanddon’tmindgreatlyenlargingthefeaturespace,
Hovyetal.[2010]foundtheuseofWordNet-basedfeaturestobehelpfulaswell.Foreachofthe
governorandtheobject,wecouldextractmanyWordNetindicators,suchas:
• doesthewordhaveaWordNetentry?;
• hypernymsofthefirstsynsetoftheword;
• hypernymsofallsynsetsoftheword;
• synonymsforfirstsynsetoftheword;
• synonymsforallsynsetsoftheword;
• alltermsinthedefinitionoftheword;
• the super-sense of the word (super-senses, also called lexicographer-files in the WordNet
jargon,arerelativelyhighlevelsintheWordNethierarchy,indicatingconceptssuchasbeing
ananimal,beingabodypart,beinganemotion,beingfood,etc.);and
• variousotherindicators.
is process may result in tens or over a hundred of core features for each preposition
instance, i.e., hyper_1st_syn_gov=a, hyper_all_syn_gov=a, hyper_all_syn_gov=b,
hyper_all_syn_gov=c, ..., hyper_1st_syn_obj=x, hyper_all_syn_obj=y, ...,
term_in_def_gov=q,term_in_def_gov=w,etc.
SeetheworkofHovyetal.[2010]forthefinerdetails.7.7. RELATIONBETWEENWORDSINCONTEXT:ARC-FACTOREDPARSING 85
epreposition-sensedisambiguationtaskisanexampleofahigh-levelsemanticclassifi-
cationproblem,forwhichweneedasetoffeaturesthatcannotbereadilyinferredfromthesurface
forms, and can benefit from linguistic pre-processing (i.e., POS-tagging and syntactic parsing)
aswellasfromselectedpiecesofinformationfrommanuallycuratedsemanticlexicons.
7.7 RELATIONBETWEENWORDSINCONTEXT:
ARC-FACTOREDPARSING
In the dependencyparsing task, we are given a sentence and need to return a syntacticdependency
tree over it, such as the tree in Figure 7.1. Each word is assigned a parent word, except for the
mainwordofthesentencewhoseparentisaspecial*ROOT*symbol.
root
nsubj
pobj prep
det dobj pobj
det prep amod det det
the boy with the black shirt opened the door with a key
Figure7.1: Dependencytree.
For more information on the dependency parsing task, its linguistic foundations and ap-
proachestoitssolution,seethebookbyKübleretal.[2008].
Oneapproachtomodelingthetaskisthearc-factoredapproach[McDonaldetal.,2005],
where each of the possible n2 word-word relations (arcs) is assigned a score independent of the
others,andthenwesearchforthevalidtreewiththemaximaloverallscore.escoreassignment
ismadebyatrainedscoringfunctionAS.h;m;sent/,receivingasentenceaswellasthe
indices h and m of two words within it that are considered as candidates for attachment (h is
theindexofthecandidatehead-wordandmistheindexofthecandidatemodifier).Trainingthe
scoringfunctionsuchthatitworkswellwiththesearchprocedurewillbediscussedinChapter19.
Here,wefocusonthefeaturesusedinthescoringfunction.
Assume a sentence of n words w and their corresponding parts-of-speech p ,
1n 1n
W W
sent .w ;w ;:::;w ;p ;p ;:::;p / When looking at an arc between words w and w ,
1 2 n 1 2 n h m
D
wecanmakeuseofthefollowingpiecesofinformation.
Webeginwiththeusualsuspects:86 7. CASESTUDIESOFNLPFEATURES
• ewordform(andPOS-tag)oftheheadword.
• ewordform(andPOS-tag)ofthemodifierword.(Somewordsarelesslikelytobeheads
ormodifiers,regardlesstowhotheyareconnectedto.Forexample,determiners(“the,”“a”)
areoftenmodifiers,andareneverheads.)
• Words(POS-tags)inawindowoftwowordstoeachsideoftheheadword,includingthe
relativepositions.
• Words(POS-tags)inawindowoftwowordstoeachsideofthemodifierword,including
therelativepositions.(ewindowinformationisneededtogivesomecontexttotheword.
Wordsbehavedifferentlyindifferentcontexts.)
Weusetheparts-of-speechaswellasthewordformsthemselves.Word-formsgiveusveryspecific
information(forexample,thatcakeisagoodcandidateobjectforate),whiletheparts-of-speech
providelowerlevelsyntacticinformationthatismoregeneralizable(forexample,thatdeterminers
andadjectivesaregoodmodifiersfornouns,andthatnounsaregoodmodifiersforverbs).Asthe
training corpora for dependency-trees are usually rather limited in size, it could be a good idea
tosupplementorreplacethewordsusingdistributionalinformation,intheformofwordclusters
or pre-trained word embeddings, that will capture generalizations across similar words, also for
wordsthatmaynothaveagoodcoverageinthetrainingdata.
We do not look at prefixes and suffixes of words, because these are not directly relevant
to the parsing task. While the affixes of words indeed carry important syntactic information (is
thewordlikelytobeanoun?apastverb?),thisinformationisalreadyavailabletousinthrough
thePOS-tags.IfwewereparsingwithoutaccesstoPOS-tagfeatures(forexample,iftheparser
wasinchargeforbothparsingandPOS-tagassignments),itwouldbewisetoincludethesuffix
informationaswell.
Of course, if we use a linear classifier, we need to take care also of feature combinations,
with features such as “head candidate word is X and modifier word candidate is Y and head
part-of-speech is Z and the word before the modifier word is W.” Indeed, it is common for
dependencyparsersbasedonlinearmodelstohavehundredsofsuchfeaturecombinations.
Inadditiontotheseusualsuspects,itisalsoinformativetoconsiderthefollowing.
• edistancebetweenwordsw andw inthesentence,dist h m.Somedistancesare
h m
Dj (cid:0) j
morelikelytostandinadependencyrelationthanothers.
• edirectionbetweenthewords.InEnglish,assumew isadeterminer(“the”)andw is
m h
a noun (“boy”), it is quite likely that there will be an arc between them if m<h and very
unlikelyifm>h.
• All the words (POS-tags) of words that appear between the head and the modifier words
in the sentence. is information is useful as it hints at possible competing attachments.7.7. RELATIONBETWEENWORDSINCONTEXT:ARC-FACTOREDPARSING 87
Forexample,adetermineratw islikelytomodifyanounatw ,butnotifawordw
m h>m k
(m<k <h)betweenthemisalsoadeterminer.Notethatthenumberofwordsbetweenthe
headandthemodifierispotentiallyunbounded(andalsochangesfrominstancetoinstance)
and so we need a way to encode a variable number of features, hinting at a bag-of-words
approach.89
C H A P T E R 8
From Textual Features to Inputs
InChapters2and4wediscussedclassifiersthatacceptfeaturevectorsasinput,withoutgetting
into much details about the contents of these vectors. In Chapters 6 and 7 we discussed the
sourcesofinformationwhichcanserveasthecorefeaturesforvariousnaturallanguagetasks.In
this chapter, we discuss the details of going from a list of core-features to a feature-vector that
canserveasaninputtoaclassifier.
Torecall,inChapters2and4wepresentedmachine-trainablemodels(eitherlinear,log-
linear,ormulti-layerperceptron).emodelsareparameterizedfunctionsf.x/thattakeasinput
a d dimensional vector x and produce a d dimensional output vector. e function is often
in out
usedasaclassifier,assigningtheinputxadegreeofmembershipinoneormoreofd classes.e
out
functioncanbeeithersimple(foralinearmodel)ormorecomplex(forarbitraryneuralnetworks).
Inthischapterwefocusontheinput,x.
8.1 ENCODINGCATEGORICALFEATURES
Whendealingwithnaturallanguage,mostofthefeaturesrepresentdiscrete,categoricalfeatures
suchaswords,letters,andpart-of-speechtags.Howdoweencodesuchcategoricaldatainaway
whichisamenableforusebyastatisticalclassifier?Wediscusstwooptions,one-hotencodingsand
denseembeddingvectors,aswellasthetrade-offsandtherelationsbetweenthem.
8.1.1 ONE-HOTENCODINGS
In linear and log-linear models of the form f.x/ xW b, it is common to think in term of
D C
indicatorfunctions,andassignauniquedimensionforeachpossiblefeature.Forexample,when
consideringabag-of-wordsrepresentationoveravocabularyof40,000items,xwillbea40,000-
dimensional vector, where dimension number 23,227 (say) corresponds to the word dog, and
dimension number 12,425 corresponds to the word cat. A document of 20 words will be repre-
sentedbyaverysparse40,000-dimensionalvectorinwhichatmost20dimensionshavenon-zero
values.Correspondingly,thematrixW willhave40,000rows,eachcorrespondingtoaparticular
vocabulary word. When the core features are the words in a 5 words window surrounding and
including a target word (2 words to each side) with positional information, and a vocabulary of
40,000 words (that is, features of the form word-2=dog or word0=sofa), x will be a 200,000-
dimensional vector with 5 non-zero entries, with dimension number 19,234 corresponding to
(say)word-2=doganddimensionnumber143,167correspondingtoword0=sofa.isiscalled
aone-hot encoding,aseachdimensioncorrespondstoauniquefeature,andtheresultingfeature90 8. FROMTEXTUALFEATURESTOINPUTS
vectorcanbethoughtofasacombinationofhigh-dimensionalindicatorvectorsinwhichasingle
dimensionhasavalueof1andallothershaveavalueof0.
8.1.2 DENSEENCODINGS(FEATUREEMBEDDINGS)
Perhaps the biggest conceptual jump when moving from sparse-input linear models to deeper
nonlinearmodelsistostoprepresentingeachfeatureasauniquedimensioninaone-hotrepre-
sentation,andrepresentingtheminsteadasdensevectors.atis,eachcorefeatureisembedded
into a d dimensional space, and represented as a vector in that space.¹ e dimension d is usu-
ally much smaller than the number of features, i.e., each item in a vocabulary of 40,000 items
(encodedas40,000-dimensionalone-hotvectors)canberepresentedas100or200dimensional
vector.eembeddings(thevectorrepresentationofeachcorefeature)aretreatedasparameters
ofthenetwork,andaretrainedliketheotherparametersofthefunctionf.Figure8.1showsthe
twoapproachestofeaturerepresentation.
egeneralstructureforanNLPclassificationsystembasedonafeed-forwardneuralnet-
workisthus.
1. Extractasetofcorelinguisticfeaturesf ;:::;f thatarerelevantforpredictingtheoutput
1 k
class.
2. Foreachfeaturef ofinterest,retrievethecorrespondingvectorv.f /.
i i
3. Combinethevectors(eitherbyconcatenation,summation,oracombinationofboth)into
aninputvectorx.
4. Feedx intoanonlinearclassifier(feed-forwardneuralnetwork).
e biggest change in the input when moving from linear to deeper classifier is, then,
the move from sparse representations in which each feature is its own dimension, to a dense
representationinwhicheachfeatureismappedtoavector.Anotherdifferenceisthatwemostly
need to extract only corefeatures and not feature combinations. We will elaborate on both these
changesbriefly.
8.1.3 DENSEVECTORSVS.ONE-HOTREPRESENTATIONS
WhatarethebenefitsofrepresentingourfeaturesasvectorsinsteadofasuniqueIDs?Shouldwe
alwaysrepresentfeaturesasdensevectors?Let’sconsiderthetwokindsofrepresentations.
¹Differentfeaturetypes maybe embeddedinto differentspaces.Forexample,onemayrepresentwordfeaturesusing100
dimensions,andpart-of-speechfeaturesusing20dimensions.8.1. ENCODINGCATEGORICALFEATURES 91
(a) pw=the pt=DET w=dog&pw=the
pt=NOUN
w=dog w=dog&pt=DET w=chair&pt=DET
x = (0, ...., 0, 1, 0, ...., 0, 1, 0 ..... 0, 1, 0, .... , 0 , 1, 0 ,0, 1, 0, .... , 0, 0, 0 , .... , 0)
(b)
x = (0.26, 0.25, -0.39, -0.07, 0.13, -0.17) (-0.43, -0.37, -0.12, 0.13, -0.11, 0.34) (-0.04, 0.50, 0.04, 0.44)
NOUN (0.16, 0.03, -0.17, -0.13)
chair (-0.37, -0.23, 0.33, 0.38, -0.02, -0.37)
VERB (0.41, 0.08, 0.44, 0.02)
on (-0.21, -0.11, -0.10, 0.07, 0.37, 0.15)
…
dog (0.26, 0.25, -0.39, -0.07, 0.13, -0.17)
…
…
… DET (-0.04, 0.50, 0.04, 0.44)
the (-0.43, -0.37, -0.12, 0.13, -0.11, 0.34) ADJ (-0.01, -0.35, -0.27, 0.20)
… PREP (-0.26, 0.28, -0.34, -0.02)
… …
mouth (-0.32, 0.43, -0.14, 0.50, -0.13, -0.42) …
… ADV (0.02, -0.17, 0.46, -0.08)
…
…
gone (0.06, -0.21, -0.38, -0.28, -0.16, -0.44)
… POS Embeddings
Word Embeddings
Figure8.1: Sparsevs.densefeaturerepresentations.Twoencodingsoftheinformation:currentword
is “dog;” previous word is “the;” previous pos-tag is “DET.” (a) Sparse feature vector. Each dimension
represents a feature. Feature combinations receive their own dimensions. Feature values are binary.
Dimensionalityisveryhigh.(b)Dense,embeddings-basedfeaturevector.Eachcorefeatureisrepre-
sented as a vector. Each feature corresponds to several input vector entries. No explicit encoding of
featurecombinations.Dimensionalityislow.efeature-to-vectormappingscomefromanembed-
dingtable.
OneHot Eachfeatureisitsowndimension.
• Dimensionalityofone-hotvectorissameasnumberofdistinctfeatures.
• Featuresarecompletelyindependentfromoneanother.efeature“wordis‘dog’”is
asdissimilarto“wordis‘thinking’”thanitisto“wordis‘cat’”.92 8. FROMTEXTUALFEATURESTOINPUTS
Dense Eachfeatureisad-dimensionalvector.
• Dimensionalityofvectorisd.
• Model training will cause similar features to have similar vectors—information is
sharedbetweensimilarfeatures.
Onebenefitofusingdenseandlow-dimensionalvectorsiscomputational:themajorityofneural
network toolkits do not play well with very high-dimensional, sparse vectors. However, this is
justatechnicalobstacle,whichcanberesolvedwithsomeengineeringeffort.
emainbenefitofthedenserepresentationsisingeneralizationpower:ifwebelievesome
featuresmayprovidesimilarclues,itisworthwhiletoprovidearepresentationthatisabletocap-
ture these similarities. For example, assume we have observed the word dog many times during
training, but only observed the word cat a handful of times, or not at all. If each of the words
is associated with its own dimension, occurrences of dog will not tell us anything about the oc-
currencesofcat.However,inthedensevectorsrepresentationthelearnedvectorfordog maybe
similartothelearnedvectorforcat,allowingthemodeltosharestatisticalstrengthbetweenthe
two events. is argument assumes that we saw enough occurrences of the word cat such that
its vector will be similar to that of dog, or otherwise that “good” vectors are somehow given to
us. Such “good” word-vectors (also called pre-trainedembeddings) can be obtained from a large
corpusoftextthroughalgorithmsthatmakeuseofthedistributionalhypothesis.Suchalgorithms
arediscussedinmoredepthinChapter10.
Incaseswherewehaverelativelyfewdistinctfeaturesinthecategory,andwebelievethere
arenocorrelationsbetweenthedifferentfeatures,wemayusetheone-hotrepresentation.How-
ever, if we believe there are going to be correlations between the different features in the group
(for example,for part-of-speech tags, we maybelieve that the differentverb inflectionsVB and
VBZ may behave similarly as far as our task is concerned) it may be worthwhile to let the net-
work figure out the correlations and gain some statistical strength by sharing the parameters. It
may be the case that under some circumstances, when the feature space is relatively small and
the training data is plentiful, or when we do not wish to share statistical information between
distinctwords,therearegainstobemadefromusingtheone-hotrepresentations.However,this
isstillanopenresearchquestion,andthereisnostrongevidencetoeitherside.emajorityof
work (pioneered by Chen and Manning [2014], Collobert and Weston [2008], Collobert et al.
[2011]) advocate the use of dense, trainable embedding vectors for all features. For work using
neuralnetworkarchitecturewithsparsevectorencodings,seeJohnsonandZhang[2015].
8.2 COMBININGDENSEVECTORS
Eachfeaturecorrespondstoadensevector,andthedifferentvectorsneedtobecombinedsome-
how.eprominentoptionsareconcatenation,summation(oraveraging),andcombinationsof
thetwo.8.2. COMBININGDENSEVECTORS 93
8.2.1 WINDOW-BASEDFEATURES
Considerthecaseofencodingawindowofsizek wordstoeachsideofafocuswordatposition
i. Assume k 2; we need to encode the words at positions i 2, i 1, i 1 and i 2. As-
D (cid:0) (cid:0) C C
sume the window items are the words a, b, c, and d, and let a,b,c and d be the corresponding
wordvectors.Ifwedonotcareabouttherelativepositionsofthewordswithinthewindow,we
willencodethewindowasasum:a b c d.Ifwedocareabouttherelativepositions,we
C C C
ratheruseconcatenation:(cid:140)a b c d(cid:141).Here,eventhoughawordwillhavethesamevectorregard-
I I I
less of its position within the window, the word’s position is reflected by its position within the
concatenation.²
We may not care much about the order, but would want to consider words further away
from the context word less than words that are closer to it. is can be encoded as a weighted
sum,i.e., 1a b c 1d.
2 C C C 2
ese encodings can be mixed and matched. Assume we care if the feature occurs before
orafterthefocusword,butdonotcareaboutthedistanceaslongasitiswithinthewindow.is
canbeencodedusingacombinationofsummationandconcatenation:(cid:140).a b/ .c d/(cid:141).
C I C
Anoteonnotation Whendescribingnetworklayersthatgetconcatenatedvectorsx,y,andzas
input,someauthorsuseexplicitconcatenation((cid:140)x y z(cid:141)W b)whileothersuseanaffinetrans-
I I C
formation(xU yV zW b/.IftheweightmatricesU,V,W intheaffinetransformation
C C C
aredifferent³thanoneanother,thetwonotationsareequivalent.
8.2.2 VARIABLENUMBEROFFEATURES:CONTINUOUSBAGOFWORDS
Feed-forwardnetworksassumeafixeddimensionalinput.iscaneasilyaccommodatethecase
ofafeature-extractionfunctionthatextractsafixednumberoffeatures:eachfeatureisrepresented
asa vector,andthe vectorsareconcatenated. isway,each regionof theresultinginput vector
correspondstoadifferentfeature.However,insomecasesthenumberoffeaturesisnotknownin
advance(forexample,indocumentclassificationitiscommonthateachwordinthesentenceis
afeature).Wethusneedtorepresentanunboundednumberoffeaturesusingafixedsizevector.
Onewayofachievingthisisthroughaso-calledcontinuousbagofwords(CBOW)representation
[Mikolovetal.,2013b].eCBOWisverysimilartothetraditionalbag-of-wordsrepresentation
inwhichwediscardorderinformation,andworksbyeithersummingoraveragingtheembedding
²Alternatively,wecouldhaveaseparateembeddingforeachword/positionpair,i.e.,a1 anda 2 willrepresenttheworda
(cid:0)
whenitappearsinrelativepositions 1and 2,respectively.Followingthisapproach,wecouldthenuseasumandstillbe
sensitivetopositioninformation:a C2 b 1(cid:0) c 1 d 2.isapproachwillnotshareinformationbetweeninstances
(cid:0) C (cid:0) C C C C
ofwordswhentheyappearindifferentpositions,andmaybehardertousewithexternallytrainedwordvectors.
³ematricesshouldbedifferentinthesensethatachangetoonewillnotbereflectedintheothers.ItisOKforthematrices
tohappentosharethesamevalues,ofcourse.94 8. FROMTEXTUALFEATURESTOINPUTS
vectorsofthecorrespondingfeatures:⁴
k
1
CBOW.f ;:::;f / Xv.f /: (8.1)
1 k i
D k
i 1
D
AsimplevariationontheCBOWrepresentationisweightedCBOW,inwhichdifferentvectors
receivedifferentweights:
k
1
WCBOW.f ;:::;f / Xa v.f /: (8.2)
1 k D Pk
i
1a
i i 1
i i
D D
Here,eachfeaturef hasanassociatedweighta ,indicatingtherelativeimportanceofthefeature.
i i
For example, in a document classification task, a feature f may correspond to a word in the
i
document,andtheassociatedweighta couldbetheword’sTF-IDFscore.
i
8.3 RELATIONBETWEENONE-HOTANDDENSE
VECTORS
Representingfeaturesasdensevectorsisanintegralpartoftheneuralnetworkframework,and,
consequentially,thedifferencesbetweenusingsparseanddensefeaturerepresentationsaresubtler
thantheymayappearatfirst.Infact,usingsparse,one-hotvectorsasinputwhentraininganeural
networkamountstodedicatingthefirstlayerofthenetworktolearningadenseembeddingvector
foreachfeaturebasedonthetrainingdata.
When using dense vectors, each categorical feature value f is mapped to a dense, d-
i
dimensionalvectorv.f /.ismappingisperformedthroughtheuseofanembeddinglayer ora
i
lookuplayer.Consideravocabularyof V words,eachembeddedasad dimensionalvector.e
j j
collectionofvectorscanthenbethoughtofasa V d embeddingmatrixE inwhicheachrow
j j(cid:2)
correspondstoanembeddedfeature.Letf betheone-hotrepresentationoffeaturef ,thatis,a
i i
V -dimensionalvector,whichisallzerosexceptforoneindex,correspondingtothevalueofthe
j j
ithfeature,inwhichthevalueis1.emultiplicationf E willthen“select”thecorresponding
i
rowofE.us,v.f /canbedefinedintermsofE andf :
i i
v.f / f E: (8.3)
i i
D
And,similarly,
k k !
CBOW.f ;:::;f / X.f E/ Xf E: (8.4)
1 k i i
D D
i 1 i 1
D D
⁴Notethatifthev.fi/swereone-hotvectorsratherthandensefeaturerepresentations,theCBOW(Equation(8.1))and
WCBOW(Equation(8.2))wouldreducetothetraditional(weighted)bag-of-wordsrepresentations,whichisinturnequiv-
alenttoasparsefeature-vectorrepresentationinwhicheachbinaryindicatorfeaturecorrespondstoaunique“word.”8.4. ODDSANDENDS 95
e input to the network is then considered to be a collection of one-hot vectors. While this is
elegant and well-defined mathematically, an efficient implementation typically involves a hash-
baseddatastructuremappingfeaturestotheircorrespondingembeddingvectors,withoutgoing
throughtheone-hotrepresentation.
Consider a network which uses a “traditional” sparse representation for its input vectors,
andnoembeddinglayer.AssumingthesetofallavailablefeaturesisV andwehavek“on”features
f ;:::;f ,f V,thenetwork’sinputis:
1 k i
2
k
x Xf i x N jV j (8.5)
D 2
C
i 1
D
andsothefirstlayer(ignoringthenonlinearactivation)is:
k !
xW b Xf W b (8.6)
i
C D C
i 1
D
W RV d; b Rd:
j j(cid:2)
2 2
islayerselectsrowsofW correspondingtotheinputfeaturesinxandsumsthem,thenadding
a bias term. is is very similar to an embedding layer that produces a CBOW representation
overthefeatures,wherethematrixW actsastheembeddingmatrix.emaindifferenceisthe
introductionofthebiasvectorb,andthefactthattheembeddinglayertypicallydoesnotundergo
anonlinearactivationbutratherispassedondirectlytothefirstlayer.Anotherdifferenceisthat
thisscenarioforceseachfeaturetoreceiveaseparatevector(rowinW)whiletheembeddinglayer
provides more flexibility, allowing for example for the features “next word is dog” and “previous
wordisdog”tosharethesamevector.However,thesedifferencesaresmallandsubtle.Whenit
comes to multi-layer feed-forward networks, the difference between dense and sparse inputs is
smallerthanitmayseematfirstsight.
8.4 ODDSANDENDS
8.4.1 DISTANCEANDPOSITIONFEATURES
elineardistanceinbetweentwowordsinasentencemayserveasaninformativefeature.For
example, in an event extraction task⁵ we may be given a trigger word and a candidate argument
word,andaskedtopredictiftheargumentwordisindeedanargumentofthetrigger.Similarly,
inacoreference-resolutiontask(decidingifwhichofthepreviouslymentionedentities,ifatall,
apronounsuchashe orshe refersto),wemaybegivenapairof(pronoun,candidateword)and
asked to predict if they co-refer or not. e distance (or relative position) between the trigger
⁵eeventextractiontaskinvolvesidentificationofeventsfromapredefinedsetofeventtypes.Forexample,identificationof
“purchase”eventsor“terror-attack”events.Eacheventtypecanbetriggeredbyvarioustriggeringwords(commonlyverbs),
andhasseveralslots(arguments)thatneedstobefilled(i.e.,Whopurchased?Whatwaspurchased?Atwhatamount?).96 8. FROMTEXTUALFEATURESTOINPUTS
and the argument is a strong signal for these prediction tasks. In the “traditional” NLP setup,
distances are usually encoded by binning the distances into several groups (i.e., 1, 2, 3, 4, 5–10,
10+) and associating each bin with a one-hot vector. In a neural architecture, where the input
vectorisnotcomposedofbinaryindicatorfeatures,itmayseemnaturaltoallocateasingleinput
entrytothedistancefeature,wherethenumericvalueofthatentryisthedistance.However,this
approach is not taken in practice. Instead, distance features are encoded similarly to the other
featuretypes:eachbinisassociatedwithad-dimensionalvector,andthesedistance-embedding
vectors are then trained as regular parameters in the network [dos Santos et al., 2015, Nguyen
andGrishman,2015,Zengetal.,2014,Zhuetal.,2015a].
8.4.2 PADDING,UNKNOWNWORDS,ANDWORDDROPOUT
Padding Insomecasesyourfeatureextractorwilllookforthingsthatdonotexist.Forexample,
when working with parse trees, you may have a feature looking for the left-most dependant of
a given word, but the word may not have any dependents to its left. Perhaps you are looking
at the word to positions to the right of the current one, but you are at the end of the sequence
and two positions to the right is past the end. What should be done in such situations? When
usingabag-of-featuresapproach(i.e.,summing)youcouldjustleavethefeatureoutofthesum.
Whenusingaconcatenation,youmayprovideazero-vectorintheplace.esetwoapproaches
work fine technically, but could be sub-optimal for your problem domain. Maybe knowing that
thereisnoleft-modifierisinformative?esuggestedsolutionwouldbetoaddaspecialsymbol
(paddingsymbol) to your embedding vocabulary, and use the associated padding vector in these
cases. Depending on the problem at hand, you may want to use different padding vectors for
differentsituations(i.e.,no-left-modifiermaybeadifferentvectorthanno-right-modifier).Such
paddingsareimportantforgoodpredictionperformance,andarecommonlyused.Unfortunately,
theiruseisnotoftenreported,orquicklyglossedover,inmanyresearchpapers.
UnknownWords Anothercasewherearequestedfeaturevectorwillnotbeavailableisforout-
of-vocabulary(OOV)items.Youarelookingforthewordontheleft,observethevaluevariational,
butthiswordwasnotapartofyourtrainingvocabulary,soyoudon’thaveanembeddingvector
for it. is case is different from the padding case, because the item is there, but you just don’t
knowit.esolutionissimilar,however,reserveaspecialsymbol,U,representinganunknown
token,foruseinsuchcases.Again,youmayormaynotwanttousedifferentunknownsymbols
for different vocabularies. In any case, it is advised to not share the padding and the unknown
vectors,astheyreflecttwoverydifferentconditions.
WordSignatures Anothertechniquefordealingwithunknownwordsisbacking-offfromthe
wordformstowordsignatures.UsingtheUsymbolforunknownwordsisessentiallybacking-
off from all unknown words to the same signature. But, depending on the task one is trying to
solve,onemaycomeupwithmorefine-grainedstrategies.Forexample,wemayreplaceunknown
wordsthatendwithingwithan*__ing*symbol,wordsthatendwithed withan*__ed*sym-8.4. ODDSANDENDS 97
bol,wordsthatstartwithunwithan*un__*symbol,numberswitha*NUM*symbol,andso
on.elistofmappingsishand-craftedtoreflectinformativebacking-offpatterns.isapproach
isoftenusedinpractice,butrarelyreportedindeep-learningpapers.Whilethereareapproaches
thatallowtoautomaticallylearnsuchbacking-offbehavioraspartofthemodeltrainingwithout
needing to manually define the backing-off patterns (see discussion on sub-word units in Sec-
tion 10.5.5), they are in many cases an overkill and hard-coding the patterns is as effective and
morecomputationallyefficient.
WordDropout Reservingaspecialembeddingvectorforunknownwordsisnotenough—ifall
thefeaturesinthetrainingsethavetheirownembeddingvectors,theunknown-wordcondition
willnotbeobservedintraining:theassociatedvectorwillnotreceiveanyupdates,andthemodel
will not be tuned to handle the unknown condition. is is equivalent to just using a random
vectorwhenanunknownwordisencounteredattesttime.emodelneedstobeexposedtothe
unknown-wordconditionduringtraining.Apossiblesolutionwouldbetoreplaceall(orsome)of
thefeatureswithalowfrequencyinthetrainingwiththeunknownsymbol(i.e.,pre-processthe
data,replacingwordswithafrequencybelowathresholdwith*unknown*).issolutionworks,
buthasthedisadvantageoflosingsometrainingdata—theserarewordswillnotreceiveanysignal.
Abettersolutionistheuseofword-dropout:whenextractingfeaturesintraining,randomlyreplace
wordswiththeunknownsymbol.ereplacementshouldbebasedontheword’sfrequency:less
frequent words will be more likely to be replaced by the unknown symbol than frequent ones.
erandomreplacementshouldbedecidedonruntime—awordthatwasdroppedoncemayor
may not be dropped when it is encountered again (say, in different iterations over the training
data).ereisnoestablishedformulafordecidingontheworddropoutrate.Worksinmygroup
use (cid:11) ,where(cid:11)isaparameterforcontrollingtheaggressivenessofthedropout[Kiperwasser
#.w/ (cid:11)
andGolCdberg,2016b].
Word Dropout as Regularization Besides better adaptation to unknown words, word dropout
mayalsobebeneficialforpreventingoverfittingandimprovingrobustnessbynotlettingthemodel
rely too much on any single word being present [Iyyer et al., 2015]. When used this way, word
dropoutshouldbeappliedfrequentlyalsotofrequentwords.Indeed,thesuggestionofIyyeretal.
[2015] is to drop word instances according to a Bernoulli trial with probability p, regardless of
their frequency. When word dropout is a applied as a regularizer, you may not want to replace
the dropped words with the unknown symbol in some circumstances. For example, when the
featurerepresentationisabag-of-wordsoverthedocumentandmorethanaquarterofthewords
are dropped, replacing the dropped words with the unknown word symbol will create a feature
representationthatisnotlikelytooccurattesttime,wheresuchalargeconcentrationofunknown
wordsisunlikely.98 8. FROMTEXTUALFEATURESTOINPUTS
8.4.3 FEATURECOMBINATIONS
Notethatthefeatureextractionstageintheneuralnetworksettingsdealsonlywithextractionof
corefeatures.isisincontrasttothetraditionallinear-model-basedNLPsystemsinwhichthe
featuredesignerhadtomanuallyspecifynotonlythecorefeaturesofinterestbutalsointeractions
betweenthem(e.g.,introducingnotonlyafeaturestating“wordisX”andafeaturestating“tag
is Y” but also combined feature stating “word is X and tag is Y” or sometimes even “word is X,
tagisYandpreviouswordisZ”).ecombinationfeaturesarecrucialinlinearmodelsbecause
theyintroducemoredimensionstotheinput,transformingitintoaspacewherethedata-points
are closer to being linearly separable. On the other hand, the space of possible combinations is
verylarge,andthefeaturedesignerhastospendalotoftimecomingupwithaneffectivesetof
feature combinations. One of the promises of the nonlinear neural network models is that one
needstodefineonlythecorefeatures.enonlinearityoftheclassifier,asdefinedbythenetwork
structure, is expected to take care of finding the indicative feature combinations, alleviating the
needforfeaturecombinationengineering.
AsdiscussedinSection3.3,kernelmethods[Shawe-TaylorandCristianini,2004],andin
particular polynomial kernels [Kudo and Matsumoto, 2003], also allow the feature designer to
specify only core features, leaving the feature combination aspect to the learning algorithm. In
contrasttoneuralnetworkmodels,kernelsmethodsareconvex,admittingexactsolutionstothe
optimizationproblem.However,thecomputationalcomplexityofclassificationinkernelmethods
scaleslinearlywiththesizeofthetrainingdata,makingthemtooslowformostpracticalpurposes,
andnotsuitablefortrainingwithlargedatasets.Ontheotherhand,thecomputationalcomplexity
of classification using neural networks scales linearly with the size of the network, regardless of
thetrainingdatasize.⁶
8.4.4 VECTORSHARING
Consideracasewhereyouhaveafewfeaturesthatsharethesamevocabulary.Forexample,when
assigningapart-of-speechtoagivenword,wemayhaveasetoffeaturesconsideringtheprevious
word,andasetoffeaturesconsideringthenextword.Whenbuildingtheinputtotheclassifier,
we will concatenate the vector representation of the previous word to the vector representation
ofthenext word.eclassifier willthen beableto distinguishthe twodifferentindicators,and
treatthemdifferently.Butshouldthetwofeaturessharethesamevectors?Shouldthevectorfor
“dog:previous-word” be the same as the vector of “dog:next-word”? Or should we assign them
two distinct vectors? is, again, is mostly an empirical question. If you believe words behave
differentlywhentheyappearindifferentpositions(e.g.,wordXbehaveslikewordYwheninthe
previousposition,butXbehaveslikeZwheninthenextposition)thenitmaybeagoodideato
usetwodifferentvocabulariesandassignadifferentsetofvectorsforeachfeaturetype.However,
⁶Ofcourse,onestillneedstogoovertheentiredatasetwhentraining,andsometimesgooverthedatasetseveraltimes.is
makestrainingtimescalelinearlywiththedatasetsize.However,eachexample,ineithertrainingortesttime,isprocessed
inaconstanttime(foragivennetwork).isisincontrasttoakernelclassifier,inwhicheachexampleisprocessedinatime
thatscaleslinearlywiththedatasetsize.8.4. ODDSANDENDS 99
if you believe the words behave similarly in both locations, then something may be gained by
usingasharedvocabularyforbothfeaturetypes.
8.4.5 DIMENSIONALITY
Howmanydimensionsshouldweallocateforeachfeature?Unfortunately,therearenotheoret-
ical bounds or even established best-practices in this space. Clearly, the dimensionality should
growwiththenumberofthemembersintheclass(youprobablywanttoassignmoredimensions
towordembeddingsthantopart-of-speechembeddings)buthowmuchisenough?Incurrentre-
search,thedimensionalityofword-embeddingvectorsrangebetweenabout50toafewhundreds,
and,insomeextremecases,thousands.Sincethedimensionalityofthevectorshasadirecteffect
onmemoryrequirementsandprocessingtime,agoodruleofthumbwouldbetoexperimentwith
afewdifferentsizes,andchooseagoodtrade-offbetweenspeedandtaskaccuracy.
8.4.6 EMBEDDINGSVOCABULARY
Whatdoesitmeantoassociateanembeddingvectorforeveryword?Clearly,wecannotassociate
onewithallpossiblevalues,andneedtorestrictourselvestoeveryvaluefromafinitevocabulary.
isvocabularyisusuallybasedonthetrainingset,or,ifweusepre-trainedembeddings,onthe
trainingonwhichthepre-trainedembeddingsweretrained.Itisrecommendedthatthevocab-
ularywillalsoincludeadesignatedUsymbol,associatingaspecialvectortoallwordsthatare
notinthevocabulary.
8.4.7 NETWORK’SOUTPUT
For multi-class classification problems with k classes, the network’s output is a k-dimensional
vector in which every dimension represents the strength of a particular output class. at is,
the output remains as in the traditional linear models—scalar scores to items in a discrete set.
However, as we saw in Chapter 4, there is a d k matrix associated with the output layer. e
(cid:2)
columnsofthismatrixcanbethoughtofasd dimensionalembeddingsoftheoutputclasses.e
vectorsimilaritiesbetweenthevectorrepresentationsofthekclassesindicatethemodel’slearned
similaritiesbetweentheoutputclasses.
Historical Note Representingwordsas dense vectorsfor input toa neural networkwas popu-
larizedbyBengioetal.[2003]inthecontextofneurallanguagemodeling.Itwasintroducedto
NLPtasksinthepioneeringworkofCollobert,Weston,andcolleagues[CollobertandWeston,
2008, Collobert et al., 2011].⁷ Using embeddings for representing not only words but arbitrary
featureswaspopularizedfollowingChenandManning[2014].
⁷WhiletheworkbyBengio,Collobert,Weston,andcolleaguespopularizedtheapproaches,theywerenotthefirsttousethem.
Earlierauthorsthatusedensecontinuous-spacevectorsforrepresentingwordinputstoneuralnetworksincludeLeeetal.
[1992]andForcadaandÑeco[1997].Similarly,continuous-spacelanguagemodelswereusedformachine-translationalready
bySchwenketal.[2006].100 8. FROMTEXTUALFEATURESTOINPUTS
8.5 EXAMPLE:PART-OF-SPEECHTAGGING
e POS-tagging task (Section 7.4) we are given a sentence of n words w ;w ;:::;w , and a
1 2 n
wordpositioni,andneedtopredictthetagofw .Assumingwetagthewordsfromlefttoright,
i
we can also look at the previous tag predictions, p ;:::;p . A list of concrete core features is
1 i 1
O O (cid:0)
giveninSection7.4,herewediscussencodingthemasaninputvector.Weneedafeaturefunction
x (cid:30).s;i/,gettingasentencescomprisedofwordsandprevioustaggingdecisionsandaninput
D
position i, and returning a feature vector x. We assume a function suf.w;k/ that returns the
k-lettersuffixofwordw,andsimilarlypref.w;k/thatreturnstheprefix.
Webeginwiththethreebooleanquestions:word-is-capitalized,word-contains-hyphenand
word-contains-digit. e most natural way to encode these is to associate each of them with its
own dimension, with a value of 1 if the condition holds for word w and 0 otherwise.⁸ We will
i
puttheseina3-dimensionalvectorassociatedwithwordi,c .
i
Next, we need to encode words, prefixes, suffixes, and part-of-speech tags in various po-
sitions in the window. We associate each word w with an embedding vector v .w / Rdw.
i w i
2
Similarly,weassociateeachtwo-lettersuffixsuf.w ;2/withanembeddingvectorv .suf.w ;2//
i s i
andsimilarlyforthree-lettersuffixesv .suf.w ;3//,v ./ Rds.Prefixesgetthesametreatment,
s i s
(cid:1) 2
withembeddingsv ./ Rdp.Finally,eachPOS-tagreceivesanembeddingv .p / Rdt.Each
p t i
(cid:1) 2 2
positioni canbeassociatedwithavectorv oftherelevantwordinformation(wordform,prefixes,
i
suffixes,Booleanfeatures):
v (cid:140)c v .w / v .suf.w ;2// v .suf.w ;3// v .pref.w ;2// v .pref.w ;3//(cid:141)
i i w i s i s i p i p i
D I I I I I
v R3 dw 2ds 2dp:
i C C C
2
Ourinputvectorx isthenaconcatenationofthefollowingvectors:
x (cid:30).s;i/ (cid:140)v v v v v v .p / v .p /(cid:141)
i 2 i 1 i i 1 i 2 t i 1 t i 2
D D (cid:0) I (cid:0) I I C I C I (cid:0) I (cid:0)
x R5.3 dw 2ds 2dp/ 2dt:
C C C C
2
Discussion Note that the words in each position share the same embedding vectors—when
creating v and v we read from the same embedding tables—and that a vector v does not
i i 1 i
(cid:0)
“know”itsrelativeposition.However,becauseofthevectorconcatenation,thevectorx “knows”
thatwhichrelativepositioneachvisassociatedwithbecauseofitsrelativepositionwithinx.is
allows us to share some information between the words in the different positions (the vector of
theworddog willreceiveupdateswhenthewordisatrelativeposition 2aswellaswhenitisin
(cid:0)
relativeposition 1),butwillalsobetreateddifferentlybythemodelwhenitappearsindifferent
C
relative positions, because it will be multiplied by a different part of the matrix in the first layer
ofthenetwork.
⁸Avalueof-1forthenegativeconditionisalsoapossiblechoice.8.6. EXAMPLE:ARC-FACTOREDPARSING 101
An alternative approach would be to associate each word-and-position pair with its
own embedding, i.e., instead of a single table v we will have five embedding tables
w
v ;v ;v ;v ;v , and use appropriate one for each relative word position. is ap-
w 2 w 1 w0 w 1 w 2
pro(cid:0)ach w(cid:0)ill substantCially inCcrease the number of parameters in the model (we will need to learn
five-timesasmanyembeddingvectors),andwillnotallowsharingbetweenthedifferentwords.
Itwillalsobesomewhatmorewastefulintermsofcomputation,asinthepreviousapproachwe
could compute the vector v for each word in the sentence once, and then re-use them when
i
looking at different positions i, while in the alternative approach the vectors v will need to be
i
re-computed for each position i we are looking at. Finally, it will be harder to use pre-trained
wordvectors,becausethepre-trainedvectorsdonothavelocationinformationattachedtothem.
However,thisalternativeapproachwouldallowustotreateachwordpositioncompletelyinde-
pendentlyfromtheothers,ifwewantedto.⁹
Anotherpointtoconsideriscapitalization.ShouldthewordsDoganddogreceivedifferent
embeddings?Whilecapitalizationisanimportantcluefortagging,inourcasethecapitalization
statusofwordw isalreadyencodedinthebooleanfeaturesc .Itisthusadvisabletolower-case
i i
allwordsinthevocabularybeforecreatingorqueryingtheembeddingtable.
Finally,theprefix-2andprefix-3featuresareredundantwitheachother(onecontainsthe
other)andsimilarlyforthesuffixes.Dowereallyneedboth?Canwemakethemshareinforma-
tion? Indeed, we could use letter embeddings instead of suffix embeddings, and replace the two
suffix embeddings with a vector composed of the concatenation of the three last letters in the
word. In Section 16.2.1, we will see an alternative approach, that uses character-level recurrent
neuralnetworks(RNNs)tocaptureprefix,suffixandvariousotherpropertiesofthewordform.
8.6 EXAMPLE:ARC-FACTOREDPARSING
IntheArc-Factoredparsingtask(Section7.7)wearegivenasentenceofnwordsw andtheir
1n
W
parts-of-speechp ,andneedtopredictaparsetree.Here,weareconcernedwiththefeatures
1n
W
for scoring a single attachment decision between words w and w , where w is the candidate
h m h
head-wordandw isthecandidatemodifierword.
m
AlistofconcretecorefeatureswasgiveninSection7.7,andherewediscussencodingthem
asaninputvector.Wedefineafeaturefunctionx (cid:30).h;m;sent/receivingasentencecomprised
D
ofwordandPOS-tags,andthepositionsofahead-word(h)andamodifier-word(m).
First, we need to consider the head word, its POS-tag, and the words and POS-tags in a
five-wordwindowaroundtheheadword(twowordstoeachside).Weassociateeachwordw in
ourvocabularywithanembeddingvectorv .w/ Rdw andsimilarlyeachpart-of-speechtagp
w
2
with an embedding vector v .p/ Rdt. We then define the vector representation of a word at
t
2
positioni tobev (cid:140)v .w / v .p (cid:141) Rdw dt,theconcatenationofthewordandPOSvector.
i w i t i C
D I 2
⁹Notethat,mathematically,eventhatlastbenefitisnotarealbenefit—whenusedasaninputtoaneuralnetwork,thefirst
layercouldpotentiallylearntolookonlyatcertaindimensionsoftheembeddingswhentheyareusedatposition 1andat
(cid:0)
differentdimensionswhentheyareusedatposition 2,achievingthesameseparationasinthealternativeapproach.us,
C
theoriginalapproachisjustasexpressiveasthealternativeone,atleastintheory.102 8. FROMTEXTUALFEATURESTOINPUTS
en,weassociatetheheadwordwithavectorhrepresentingthewordwithinitscontext,
andassociatethemodifierwordwithasimilarvectorm:
h (cid:140)v v v v v (cid:141)
h 2 h 1 h h 1 h 2
D (cid:0) I (cid:0) I I C I C
m (cid:140)v v v v v (cid:141):
m 2 m 1 m m 1 m 2
D (cid:0) I (cid:0) I I C I C
istakescareoftheelementsinthefirstblockoffeatures.Notethat,likewiththepart-
of-speechtaggingfeatures,thisencodingcaresabouttherelativepositionofeachofthecontext
words. If we didn’t care about the positions, we could have instead represented the head word
as h (cid:140)v .v v v v /(cid:141). is sums the context words into a bag-of-words,
0 h h 2 h 1 h 1 h 2
D I (cid:0) C (cid:0) C C C C
losingtheirpositionalinformation,yet,concatenatesthecontextandthefocuswords,retaining
thedistinctionbetweenthem.
We now turn to the distance and direction features. While we could assign the distance a
singledimensionwiththenumericdistancevalue,itiscommontobinthedistanceintokdiscrete
bins(say1,2,3,4–7,8–10,11+),andassociateeachbinwithad -dimensionalembedding.e
d
directionisaBooleanfeature,andwerepresentitasitsowndimension.¹⁰Wedenotethevector
containingthebinneddistanceembeddingconcatenatedwiththeBooleandirectionfeatureasd.
Finally,weneedtorepresentthewordsandPOS-tagsbetweentheheadandthemodifier.
eirnumberisunboundedandvariesbetweendifferentinstances,sowecannotuseconcatena-
tion.Fortunately,wedonotcareabouttherelativepositionsoftheinterveningitems,sowecan
useabag-of-wordsencoding.Concretely,werepresentthebetween-contextwordsasavectorc
definedastheaverageofthewordsandPOSbetweenvectors:
m
c Xv :
i
D
i h
D
Notethatthissumpotentiallycapturesalsothenumberofelementsbetweentheheadand
modifierwords,makingthedistancefeaturepotentiallyredundant.
Ourfinalrepresentationofanattachmentdecisiontobescored,x isthenencodedasthe
concatenationofthevariouselements:
¹⁰Whilethisencodingofthedimensionisverynatural,Peietal.[2015]followadifferentapproachintheirparser.Perhaps
motivatedbytheimportanceofdistanceinformation,theychosetonotmarkitasaninputfeature,butinsteadtousetwo
differentscoringfunctions,oneforscoringleft-to-rightarcsandanotherforscoringright-to-leftarcs.isgivesalotofpower
tothedirectioninformation,whilesubstantiallyincreasingthenumberofparametersinthemodel.8.6. EXAMPLE:ARC-FACTOREDPARSING 103
x (cid:30).h;m;sent/ (cid:140)h m c d(cid:141);
D D I I I
where:
v (cid:140)v .w / v .p (cid:141)
i w i t i
D I
h (cid:140)v v v v v (cid:141)
h 2 h 1 h h 1 h 2
D (cid:0) I (cid:0) I I C I C
m (cid:140)v v v v v (cid:141)
m 2 m 1 m m 1 m 2
D (cid:0) I (cid:0) I I C I C
m
c Xv
i
D
i h
D
d binneddistanceembeddings;directionindicator.
D
Note how we combine positional window-based features with bag-of-word features by simple
concatenation. e neural network layers on top of x can then infer transformation and feature
combinations between the elements in the different windows, as well as between the different
elements in the bag-of-words representation. e process of creating the representation x—the
embeddingtablesforthewords,POS-tagsandbinneddistances,aswellasthedifferentconcate-
nations and summations, is also part of the neural network. It is reflected in the computation-
graphconstruction,anditsparametersaretrainedjointlywiththenetwork.
efeaturescreationpartofthenetworkcouldbeevenmorecomplex.Forexample,ifwe
hadreasonstobelievethattheinteractionsbetweenawordanditsPOS-tag,andtheinteractions
within a context window, are more important than the interactions across elements of different
entities,wecouldhavereflectedthatintheinputencodingbycreatingfurthernonlineartransfor-
mationsinthefeature-encodingprocess,i.e.,replacingv withv g.v Wv bv/andhwith
i 0i
D
i
C
h g.(cid:140)v v v v v (cid:141)Wh bh/,andsetting:x (cid:140)h m c d(cid:141).
0 D 0h 2I 0h 1I 0hI 0h 1I 0h 2 C D 0 I 0 I I
(cid:0) (cid:0) C C105
C H A P T E R 9
Language Modeling
9.1 THELANGUAGEMODELINGTASK
Languagemodelingisthetaskofassigningaprobabilitytosentencesinalanguage(“whatisthe
probabilityofseeingthesentencethelazydogbarkedloudly?”).Besidesassigningaprobabilityto
each sequence of words, the language models also assigns a probability for the likelihood of a
given word (or a sequence of words) to follow a sequence of words (“what is the probability of
seeingthewordbarked aftertheseeingsequencethelazydog?”).¹
Perfectperformanceatthelanguagemodelingtask,namelypredictingthenextwordina
sequencewithanumberofguessesthatisthesameasorlowerthanthenumberofguessesrequired
byahumanparticipant,isanindicationofhumanlevelintelligence,²andisunlikelytobeachieved
in the near future. Even without achieving human-level performance, language modeling is a
crucialcomponentsinreal-worldapplicationssuchasmachine-translationandautomaticspeech
recognition,wherethesystemproducesseveraltranslationortranscriptionhypotheses,whichare
then scored by a language model. For these reasons, language modeling plays a central role in
natural-languageprocessing,AI,andmachine-learningresearch.
Formally,thetaskoflanguagemodelingistoassignaprobabilitytoanysequenceofwords
w ,i.e.,toestimateP.w /.Usingthechain-ruleofprobability,thiscanberewrittenas:
1n 1n
W W
P.w / P.w /P.w w /P.w w /P.w w /:::P.w w /: (9.1)
1n 1 2 1 3 12 4 13 n 1n 1
W D j j W j W j W (cid:0)
at is, a sequence of word-prediction tasks where each word is predicted conditioned on the
preceding words. While the task of modeling a single word based on its left context seem more
manageablethanassigningaprobabilityscoretoanentiresentence,thelasttermintheequation
¹ aN bo ilt ie tyth toat ast sh ie gnab ai pli rt oy bt ao bia ls its ii eg sn toa ap rr bo ib tra ab ri ylit sy eqf uo er na cew so or fd wf oo rl dlo sw pin .wg 1a ;s weq 2u ;e :n :c :e ;o wf kw /o ar rd es ep qu. iw vai l1 ej nw t,1 a; sw o2 n; e: c: a: n; bw ei d(cid:0)e1 ri/ vea dnd froth me
theother.Assumewecanmodelprobabilitiesofsequences.entheconditionalprobabilityofawordcanbeexpressedasa
fractionoftwosequences:
p.wi jw1;w2;:::;wi (cid:0)1/
D
p. pw .1 w; 1w ;2 w; 2: ;: :: :; :w ;i w(cid:0) i1; 1w /i/ :
(cid:0)
Alternatively,ifwecouldmodeltheconditionalprobabilityofawordfollowingasequenceofwords,wecouldusethechain
ruletoexpresstheprobabilityofsequencesasaproductofconditionalprobabilities:
p.w1;:::;wk/ p.w1<s>/ p.w2<s>;w1/ p.w3<s>;w1;w2/ p.wk <s>;w1;:::;wk 1/;
D j (cid:2) j (cid:2) j (cid:2)(cid:1)(cid:1)(cid:1)(cid:2) j (cid:0)
where<s>isaspecialstart-of-sequencesymbol.
²Indeed,anyquestioncanbeposedasanext-word-guessingtask,e.g.,theanswertoquestionXis___.Evenwithoutsuch
pathologicalcases,predictingthenextwordinthetextrequiresknowledgeofsyntacticandsemanticrulesofthelanguage,as
wellasvastamountsofworldknowledge.106 9. LANGUAGEMODELING
still requires conditioning on n 1 words, which is as hard as modeling an entire sentence. For
(cid:0)
thisreason,languagemodelsmakeuseofthemarkov-assumption,statingthatthefutureisinde-
pendent of the past given the present. More formally, a kth order markov-assumption assumes
thatthenextwordinasequencedependsonlyonthelastk words:
P.w w / P.w w /:
i 1 1i i 1 i ki
C j W (cid:25) C j (cid:0) W
Estimatingtheprobabilityofthesentencethenbecomes
n
P.w / YP.w w /; (9.2)
1n i i ki 1
W (cid:25) j (cid:0) W (cid:0)
i 1
D
wherew ;:::;w aredefinedtobespecialpaddingsymbols.
k 1 0
(cid:0) C
OurtaskisthentoaccuratelyestimateP.w w /givenlargeamountsoftext.
i 1 i ki
C j (cid:0) W
While the kth order markov assumption is clearly wrong for any k (sentences can have
arbitrarily long dependencies, as a simple example consider the strong dependence between the
firstwordofthesentencebeingwhat andthelastonebeing?),itstillproducesstronglanguage
modeling results for relatively small values of k, and was the dominant approach for language
modelingfordecades.ischapterdiscusseskthorderlanguagemodels.InChapter14wediscuss
languagemodelingtechniquesthatdonotmakethemarkovassumption.
9.2 EVALUATINGLANGUAGEMODELS:PERPLEXITY
ereareseveralmetricsforevaluatinglanguagemodeling.eapplication-centriconesevaluate
theminthecontextofperformingahigher-leveltask,forexamplebymeasuringtheimprovement
intranslationqualitywhenswitchingthelanguage-modelingcomponentinatranslationsystem
frommodelAtomodelB.
A more intrinsic evaluation of language models is using perplexity over unseen sentences.
Perplexity is an information theoretic measurement of how well a probability model predicts a
sample. Low perplexity values indicate a better fit. Given a text corpus of n words w ;:::;w
1 n
(n can be in the millions)and a languagemodel functionLM assigning a probability to a word
basedonitshistory,theperplexityofLM withrespecttothecorpusis:
2 (cid:0)n1 Pn i D1log2LM.wi jw1 Wi (cid:0)1/:
Goodlanguagemodels(i.e.,reflectiveofreallanguageusage)willassignhighprobabilities
totheeventsinthecorpus,resultinginlowerperplexityvalues.
eperplexitymeasureisagoodindicatorofthequalityofalanguagemodel.³Perplexities
arecorpusspecific—perplexitiesoftwolanguagemodelsareonlycomparablewithrespecttothe
sameevaluationcorpus.
³Itisimportanttonote,however,thatinmanycasesimprovementinperplexityscoresdonottransfertoimprovementin
extrinsic,task-qualityscores.Inthatsense,theperplexitymeasureisgoodforcomparingdifferentlanguagemodelsinterms
oftheirabilitytopick-upregularitiesinsequences,butisnotagoodmeasureforassessingprogressinlanguageunderstanding
orlanguage-processingtasks.9.3. TRADITIONALAPPROACHESTOLANGUAGEMODELING 107
9.3 TRADITIONALAPPROACHESTOLANGUAGE
MODELING
e traditional approach to language models assumes a k-order markov property, and model
P.w mw / P.w mw /. e role of the language model then is to provide
i 1 1i i 1 i ki
C D j W (cid:25) C D j (cid:0) W
goodestimatesp.w mw /.eestimatesareusuallyderivedfromcorpuscounts.Let
i 1 i ki
O C D j (cid:0) W
#.w /bethecountofthesequenceofwordsw inacorpus.emaximumlikelihoodestimate
i j i j
W W
(MLE)ofp.w mw /isthen:
i 1 i ki
O C D j (cid:0) W
#.w /
i ki 1
p OMLE.w i C1 Dm jw i (cid:0)k Wi/
D
#.w(cid:0)
i
W kC
i/
:
(cid:0) W
While effective, this baseline approach has a big shortcoming: if the event w was never
i ki 1
(cid:0) W C
observedinthecorpus(#.w / 0),theprobabilityassignedtoitwillbe0,andthisinturn
i ki 1
(cid:0) W C D
willresultina0-probabilityassignmenttotheentirecorpusbecauseofthemultiplicativenature
of the sentence probability calculation. A zero probability translates to an infinite perplexity—
which is very bad. Zero events are quite common: consider a trigram language model, which
onlyconditionson2words,andavocabularyof10,000words(whichisrathersmall).ereare
10,0003 1012possiblewordtriplets—itisclearthatmanyofthemwon’tbeobservedintraining
D
corpora of, say, 1010 words. While many of these events don’t occur because they do not make
sense,manyothersjustdidnotoccurinthecorpus.
Onewayofavoidingzero-probabilityeventsisusingsmoothingtechniques,ensuringanal-
location of a (possibly small) probability mass to every possible event. e simplest example is
probablyadditivesmoothing,alsocalledadd-(cid:11)smoothing[ChenandGoodman,1999,Goodman,
2001,Lidstone,1920].Itassumeseacheventoccurredatleast(cid:11)timesinadditiontoitsobserva-
tionsinthecorpus.eestimatethenbecomes
#.w / (cid:11)
i ki 1
p add-(cid:11).w i 1 mw i ki/ (cid:0) W C C ;
O C D j (cid:0) W D #.w i ki/ (cid:11) V
(cid:0) W C j j
where V isthevocabularysizeand0<(cid:11) 1.Manymoreelaboratesmoothingtechniquesexist.
j j (cid:20)
Another popular family of approaches is using back-off: if the kgram was not observed,
compute an estimate based on a .k 1/gram. A representative sample of this family is Jelinek
(cid:0)
Mercerinterpolatedsmoothing [ChenandGoodman,1999,JelinekandMercer,1980]:
#.w /
i ki 1
p Oint.w i C1 Dm jw i (cid:0)k Wi/ D(cid:21) wi (cid:0)k Wi #.w(cid:0)
i
W kC i/ C.1 (cid:0)(cid:21) wi (cid:0)k Wi/p Oint.w i C1 Dm jw i (cid:0).k (cid:0)1/ Wi/:
(cid:0) W
For optimal performance, the values (cid:21) should depend on the content of the condi-
wi ki
tioningcontextw :rarecontextsshouldrece(cid:0)ivWedifferenttreatmentsthanfrequentones.
i ki
(cid:0) W
ecurrentstate-of-the-artnon-neurallanguagemodelingtechniqueusesmodifiedKneser
Ney smoothing [Chen and Goodman, 1996], which is a variant of the technique proposed by
KneserandNey[1995].Fordetails,seeChenandGoodman[1996]andGoodman[2001].108 9. LANGUAGEMODELING
9.3.1 FURTHERREADING
Language modeling is a very vast topic, with decades of research. A good, formal overview of
the task, as well as motivations behind the perplexity measure can be found in the class notes
byMichaelCollins.⁴Agoodoverviewandempiricalevaluationofsmoothingtechniquescanbe
found in the works of Chen and Goodman [1999] and Goodman [2001]. Another review of
traditionallanguagemodelingtechniquescanbefoundinthebackgroundchaptersofthePh.D.
thesis of Mikolov [2012]. For a recent advance in non-neural language modeling, see Pelemans
etal.[2016].
9.3.2 LIMITATIONSOFTRADITIONALLANGUAGEMODELS
Language modeling approaches based on smoothed MLE estimates (“traditional”) are easy to
train,scaletolargecorpora,andworkwellinpractice.eydo,however,haveseveralimportant
shortcomings.
esmoothing techniques areintricate and based onback off to lower-orderevents.is
assumes a fixed backing-up order, that needs to be designed by hand, and makes it hard to add
more“creative”conditioningcontexts(i.e.,ifonewantstoconditiononthekpreviouswordsand
onthegenreofthetext,shouldthebackofffirstdiscardofthekthpreviousword,orofthegenre
variable?).esequentialnatureofthebackoffalsomakesithardtoscaletowardlargerngrams
inordertocapturelong-rangedependencies:inordertocaptureadependencybetweenthenext
word and the word 10 positions in the past, one needs to see a relevant 11-gram in the text. In
practice,thisveryrarelyhappens,andthemodelbacksofffromthelonghistory.Itcouldbethata
betteroptionwouldbetobackofffromtheinterveningwords,i.e.,allowforngramswith“holes”
in them. However, these are tricky to define while retaining a proper generative probabilistic
framework.⁵
ScalingtolargerngramsisaninherentproblemforMLE-basedlanguagemodels.ena-
ture of natural language and the large number of words in the vocabulary means that statistics
forlargerngramswillbesparse.Moreover,scalingtolargerngramsisveryexpensiveintermsof
memory requirements. e number of possible ngrams over a vocabulary V is V n: increasing
j j
the order by one will result in a V -fold increase to that number. While not all of the theoret-
j j
ical ngrams are valid or will occur in the text, the number of observed events does grow at least
multiplicatively when increasing the ngram size by 1. is makes it very taxing to work larger
conditioningcontexts.
Finally, MLE-based language models suffer from lack of generalization across contexts.
Havingobservedblackcar andbluecar doesnotinfluenceourestimatesoftheeventredcar ifwe
haven’tseeitbefore.⁶
⁴http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf
⁵Althoughseelinesofworkonfactoredlanguagemodels(i.e.,A.BilmesandKirchhoff[2003])andonmaximum-entropy
(log-linear)languagemodels,startingwithRosenfeld[1996],aswellasrecentworkbyPelemansetal.[2016].
⁶Class-basedlanguagemodels[Brownetal.,1992]trytotacklethisbyclusteringthewordsusingdistributionalalgorithms,
andconditioningontheinducedword-classesinsteadoforinadditiontothewords.9.4. NEURALLANGUAGEMODELS 109
9.4 NEURALLANGUAGEMODELS
Nonlinear neural network models solve some of the shortcomings of traditional language mod-
els: they allowconditioningon increasingly largecontextsizes with only a linear increase in the
number of parameters, they alleviate the need for manually designing backoff orders, and they
supportgeneralizationacrossdifferentcontexts.
AmodeloftheformpresentedinthischapterwaspopularizedbyBengioetal.[2003].⁷
einputtotheneuralnetworkisakgramofwordsw ,andtheoutputisaprobability
1k
W
distributionoverthenextword.ekcontextwordsw aretreatedasawordwindow:eachword
1k
w is associated with an embedding vector v.w/
RdwW
, and the input vector x a concatenation
2
ofthek words:
x (cid:140)v.w / v.w / ::: v.w /(cid:141):
1 2 k
D I I I
einputx isthenfedtoanMLPwithoneormorehiddenlayers:
y P.w w / LM.w / softmax.hW2 b2/
i 1k 1k
O D j W D W D C
h g.xW1 b1/
D C (9.3)
x (cid:140)v.w / v.w / ::: v.w /(cid:141)
1 2 k
D I I I
v.w/ E
(cid:140)w(cid:141)
D
w i V E R jV j(cid:2)dw W1 Rk (cid:1)dw (cid:2)d hid b1 Rd hid W2 Rd hid (cid:2)jV j b2 R jV j:
2 2 2 2 2 2
V isafinitevocabulary,includingtheuniquesymbolsUforunknownwords,<s>forsentence
initialpadding,and</s>forend-of-sequencemarking.evocabularysize, V ,rangesbetween
j j
10,000–1,000,000words,withthecommonsizesrevolvingaround70,000.
Training etrainingexamplesaresimplywordkgramsfromthecorpus,wheretheidentities
of the first k 1 words are used as features, and the last word is used as the target label for the
(cid:0)
classification. Conceptually, the model is trained using cross-entropy loss. Working with cross
entropylossworksverywell,butrequirestheuseofacostlysoftmaxoperationwhichcanbepro-
hibitiveforverylargevocabularies,promptingtheuseofalternativelossesand/orapproximations
(seebelow).
MemoryandComputationalEfficiency Eachofthek inputwordscontributesd dimensions
w
tox,andmovingfromk tok 1wordswillincreasethedimensionsoftheweightmatrixW1
C
from k d d to .k 1/ d d , a small linear increase in the number of parameters, in
w hid w hid
(cid:1) (cid:2) C (cid:1) (cid:2)
contrast to a polynomial increase in the case of the traditional, count-based language models.
is is possible because the feature combinations are computed in the hidden layer. Increasing
the order k will likely require enlarging the dimension of d as well, but this is still a very
hid
⁷Asimilarmodelwaspresentedasearlyas1988byNakamuraandShikano[1988]intheirworkonwordclasspredictionwith
neuralnetworks.110 9. LANGUAGEMODELING
modestincreaseinthenumberofparameterscomparedtothetraditionalmodelingcase.Adding
additionalnonlinearlayerstocapturemorecomplexinteractionsisalsorelativelycheap.
Each of the vocabulary words is associated with one d dimensional vector (a row in E)
w
andoned dimensionalvector(acolumninW2).us,anewvocabularyitemswillresultina
hid
linearincreaseinthenumberofparameters,againmuchbetterthanthetraditionalcase.However,
whiletheinputvocabulary(thematrixE)requiresonlylookupoperationsandcangrowwithout
affectingthecomputationspeed,thesizeoftheoutputvocabularygreatlyaffectsthecomputation
time: the softmax at the output layer requires an expensive matrix-vector multiplication with
the matrix W2 Rd hid V , followed by V exponentiations. is computation dominates the
(cid:2)j j
2 j j
runtime,andmakeslanguagemodelingwithlargevocabularysizesprohibitive.
Large output spaces Working with neural probabilistic language models with large output
spaces (i.e., efficiently computing the softmax over the vocabulary) can be prohibitive both at
training time and at test time. Dealing with large output spaces efficiently is an active research
question.Someoftheexistingsolutionsareasfollows.
Hierarchicalsoftmax[MorinandBengio,2005]allowstocomputetheprobabilityofasingleword
inO.log V /timeratherthanO. V /.isisachievedbystructuringthesoftmaxcomputation
j j j j
as tree traversal, and the probability of each word as the product of branch selection decisions.
Assumingoneisinterestedintheprobabilityofasingleword(ratherthangettingthedistribution
overallwords)thisapproachprovidesclearbenefitsinbothtrainingandtestingtime.
Self-normalizing aproaches, such as noise-contrastive estimation (NCE) [Mnih and Teh, 2012,
Vaswanietal.,2013]oraddingnormalizingtermtothetrainingobjective[Devlinetal.,2014].
eNCEapproachimprovestrainingtimeperformancebyreplacingthecross-entropyobjective
withacollectionofbinaryclassificationproblems,requiringtheevaluationoftheassignedscores
for k random words rather than the entire vocabulary. It also improves test-time prediction by
pushingthemodeltowardproducing“approximatelynormalized”exponentiatedscores,making
themodelscoreforawordagoodsubstituteforitsprobability.enormalizationtermapproach
of Devlin et al. [2014] similarly improves test time efficiency by adding a term to the training
objective that encourages the exponentiated model scores to sum to one, making the explicit
summationattesttimeunnecessary(theapproachdoesnotimprovetrainingtimeefficiency).
SamplingApproachesapproximatethetraining-timesoftmaxoverasmallersubsetofthevocabu-
lary[Jeanetal.,2015].
Agoodreviewandcomparisonoftheseandothertechniquesfordealingwithlargeoutput
vocabulariesisavailableinChenetal.[2016].
Anorthogonallineofworkisattemptingtosidesteptheproblembyworkingatthechar-
acterslevelratherthanwordslevel.
DesirableProperties Puttingasidetheprohibitivecostofusingthelargeoutputvocabulary,the
model has very appealing properties. It achieves better perplexities than state-of-the-art tradi-
tionallanguagemodelssuchasKneser-Neysmoothedmodels,andcanscaletomuchlargerorders9.4. NEURALLANGUAGEMODELS 111
thanispossiblewiththetraditionalmodels.isisachievablebecauseparametersareassociated
only with individual words, and not with kgrams. Moreover, the words in different positions
shareparameters,makingthemsharestatisticalstrength.ehiddenlayersofthemodelsarein
chargeoffindinginformativewordcombinations,andcan,atleastintheory,learnthatforsome
words only sub-parts of the kgram areinformative: it can learn to back-up to smallerkgrams if
needed,likeatraditionallanguagemodel,anddoitinacontext-dependentway.Itcanalsolearn
skip-grams,i.e.,thatforsomecombinationsitshouldlookatwords1,2,and5,skippingwords
3and4.⁸
Anotherappealingpropertyofthemodel,besidestheflexibilityofthekgramorders,isthe
ability to generalize across contexts. For example, by observing that the words blue, green, red,
black, etc. appear in similar contexts, the model will be able to assign a reasonable score to the
eventgreencareventhoughitneverobservedthiscombinationintraining,becauseitdidobserve
bluecar andredcar.
e combination of these properties—the flexibility of considering only parts of the con-
ditioningcontextandtheabilitytogeneralizetounseencontexts—togetherwiththeonlylinear
dependence on the conditioning context size in terms of both memory and computation, make
it very easy to increase the size of the conditioning context without suffering much from data
sparsityandcomputationefficiency.
eeaseinwhichtheneurallanguagemodelcanincorporatelargeandflexiblecondition-
ingcontextsallowforcreativedefinitionsofcontexts.Forexample,Devlinetal.[2014]propose
amachinetranslationmodelinwhichtheprobabilityofthenextwordisconditionedonthepre-
viousk wordsinthegeneratedtranslation,aswellasonmwordsinthesourcelanguagethatthe
givenpositioninthetranslationisbasedon.isallowsthemodeltobesensitivetotopic-specific
jargonandmulti-wordexpressionsinthesourcelanguage,andindeedresultsinmuchimproved
translationscores.
Limitations Neurallanguagemodelsoftheformpresentedheredohavesomelimitations:pre-
dicting the probability of a word in context is much more expensive than using a traditional
languagemodel,andworkingwithlargevocabularysizesandtrainingcorporacanbecomepro-
hibitive.However,theydomakebetteruseofthedata,andcangetverycompetitiveperplexities
evenwithrelativelysmalltrainingsetsizes.
Whenappliedinthecontextofamachine-translationsystem,neurallanguagemodelsdo
notalwaysimprovethetranslationqualityovertraditionalKneser-Neysmoothedlanguagemod-
els.However,translationqualitydoesimprovewhentheprobabilitiesfromatraditionallanguage
model and a neural one are interpolated. It seems that the models complement each other: the
neurallanguagemodelsgeneralizebettertounseenevents,butsometimesthisgeneralizationcan
hurttheperformance,andtherigidityofthetraditionalmodelsispreferred.Asanexample,con-
sider the opposite of the colors example above: a model is asked to assign a probability to the
sentence redhorse. A traditional model will assign it a very low score, as it likely observed such
⁸Suchskip-gramswereexploredalsofornon-neurallanguagemodels;seePelemansetal.[2016]andthereferencestherein.112 9. LANGUAGEMODELING
an event only a few times, if at all. On the other hand, a neural language model may have seen
brownhorse,blackhorse,andwhitehorse,andalsolearnedindependentlythatblack,white,brown,
andred canappearinsimilarcontexts.Suchamodelwillassignamuchhigherprobabilitytothe
eventredhorse,whichisundesired.
9.5 USINGLANGUAGEMODELSFORGENERATION
Language models can also be used for generating sentences. After training a language model
on a given collection of text, one can generate (“sample”) random sentences from the model
accordingtotheirprobabilityusingthefollowingprocess:predictaprobabilitydistributionover
thefirstwordconditionedonthestartsymbol,anddrawarandomwordaccordingtothepredicted
distribution.en,predictaprobabilitydistributionoverthesecondwordconditionedonthefirst,
andsoon,untilpredictingtheend-of-sequence</s>symbol.Alreadywithk 3thisproduces
D
verypassabletext,andthequalityimproveswithhigherorders.
When decoding (generating a sentence) from a trained language-model in this way, one
can either choose the highest scoring prediction (word) at each step, or sample a random word
according to the predicted distribution. Another option is to use beam search in order to find
a sequence with a globally high probability (following the highest-prediction at each step may
resultinsub-optimaloverallprobability,astheprocessmay“trapitselfintoacorner,”leadingto
prefixesthatarefollowedbylow-probabilityevents.isiscalledthelabel-biasproblem,discussed
indepthbyAndoretal.[2016]andLaffertyetal.[2001].
Sampling from a multinomial distribution A multinomial distribution over V elements
j j
associates a probability value p i (cid:21)0 for each item 0<i (cid:20)jV j, such that P ijV j1p i D1. In
ordertosamplearandomitemfromamultinomialdistributionaccordingtoiDtsprobability,
thefollowingalgorithmcanbeused:
1: i 0
2: s U(cid:140)0;1(cid:141) auniformrandomnumberbetween0and1
(cid:24) F
3: whiles 0do
(cid:21)
4: i i 1
C
5: s s p i
(cid:0)
6: returni
is is a naive algorithm, with a computational complexity linear in the vocabulary
sizeO. V /.iscanbeprohibitivelyslowusinglargevocabularies.Forpeakeddistributions
j j
wherethevaluesaresortedbydecreasingprobability,theaveragetimewouldbemuchfaster.
e alias method [Kronmal and Peterson, Jr., 1979] is an efficient algorithm for sampling
fromarbitrarymultinomialdistributionswithlargevocabularies,allowingsamplinginO.1/
afterlineartimepre-processing.
..9.6. BYPRODUCT:WORDREPRESENTATIONS 113
9.6 BYPRODUCT:WORDREPRESENTATIONS
Languagemodelscanbetrainedonrawtext:fortrainingak-orderlanguagemodelonejustneeds
toextract.k 1/gramsfromrunningtext,andtreatthe.k 1/thwordasthesupervisionsignal.
C C
us,wecangeneratepracticallyunlimitedtrainingdataforthem.
ConsiderthematrixW2 appearingjustbeforethefinalsoftmax.Eachcolumninthisma-
trixisad dimensionalvectorthatisassociatedwithavocabularyitem.Duringthefinalscore
hid
computation,eachcolumninW2ismultipliedbythecontextrepresentationh,andthisproduces
thescoreofthecorrespondingvocabularyitem.Intuitively,thisshouldcausewordsthatarelikely
toappearinsimilarcontextstohavesimilarvectors.Followingthedistributionalhypothesisac-
cordingtowhichwordsthatappearinsimilarcontextshavesimilarmeanings,wordswithsimilar
meanings will have similar vectors. A similar argument can be made about the rows of the em-
bedding matrix E. As a byproduct of the language modeling process, we also learn useful word
representationsintherowsandcolumnsofthematricesE andW2.
In the next chapter we further explore the topic of learning useful word representations
fromrawtext.115
C H A P T E R 10
Pre-trained
Word Representations
Amaincomponentoftheneuralnetworkapproachistheuseofembeddings—representingeach
featureasavectorinalowdimensionalspace.Butwheredothevectorscomefrom?ischapter
surveysthecommonapproaches.
10.1 RANDOMINITIALIZATION
Whenenoughsupervisedtrainingdataisavailable,onecanjusttreatthefeatureembeddingsthe
same as the other model parameters: initialize the embedding vectors to random values, and let
thenetwork-trainingproceduretunetheminto“good”vectors.
Somecarehastobetakeninthewaytherandominitializationisperformed.emethod
usedbytheeffectiveW2Vimplementation[Mikolovetal.,2013b,a]istoinitializetheword
vectorstouniformlysampledrandomnumbersintherange(cid:140) 1 ; 1 (cid:141)whered isthenumberof
(cid:0)2d 2d
dimensions.Anotheroptionistousexavierinitialization(seeSection5.2.2)andinitializewith
uniformlysampledvaluesfromh p6; p6i.
(cid:0)pd pd
Inpractice,onewilloftenusetherandominitializationapproachtoinitializetheembed-
ding vectors of commonly occurring features, such as part-of-speech tags or individual letters,
whileusingsomeformofsupervisedorunsupervisedpre-trainingtoinitializethepotentiallyrare
features,suchasfeaturesforindividualwords.epre-trainedvectorscantheneitherbetreated
asfixedduringthenetworktrainingprocess,or,morecommonly,treatedliketherandomlyini-
tializedvectorsandfurthertunedtothetaskathand.
10.2 SUPERVISEDTASK-SPECIFICPRE-TRAINING
If we are interested in task A, for which we only have a limited amount of labeled data (for
example,syntacticparsing),butthereisanauxiliarytaskB(say,part-of-speechtagging)forwhich
wehavemuchmorelabeleddata,wemaywanttopre-trainourwordvectorssothattheyperform
wellaspredictorsfortaskB,andthenusethetrainedvectorsfortrainingtaskA.Inthisway,we
can utilize the larger amounts of labeled data we have for task B. When training task A we can
eithertreatthepre-trainedvectorsasfixed,ortunethemfurtherfortaskA.Anotheroptionisto
trainjointlyforbothobjectives;seeChapter20formoredetails.116 10. PRE-TRAINEDWORDREPRESENTATIONS
10.3 UNSUPERVISEDPRE-TRAINING
ecommoncaseisthatwedonothaveanauxiliarytaskwithlargeenoughamountsofannotated
data (or maybe we want to help bootstrap the auxiliary task training with better vectors). In
suchcases,weresortto“unsupervised”auxiliarytasks,whichcanbetrainedonhugeamountsof
unannotatedtext.
e techniques for training the word vectors are essentially those of supervised learning,
butinsteadofsupervisionforthetaskthatwecareabout,weinsteadcreatepracticallyunlimited
numberofsupervisedtraininginstancesfromrawtext,hopingthatthetasksthatwecreatedwill
match(orbecloseenoughto)thefinaltaskwecareabout.¹
e key idea behind the unsupervised approaches is that one would like the embedding
vectors of “similar” words to have similar vectors. While word similarity is hard to define and
isusuallyverytask-dependent,thecurrentapproachesderivefromthedistributionalhypothesis
[Harris,1954],statingthatwordsaresimilariftheyappearinsimilarcontexts.edifferentmeth-
odsallcreatesupervisedtraininginstancesinwhichthegoalistoeitherpredictthewordfromits
context,orpredictthecontextfromtheword.
InthefinalsectionofChapter9,wesawhowlanguagemodelingcreateswordvectorsasa
byproductoftraining.Indeed,languagemodelingcanbetreatedasan“unsupervised”approach
in which a word is predicted based on the context of the k preceding words. Historically, the
algorithm of Collobert and Weston [Collobert and Weston, 2008, Collobert et al., 2011] and
theW2Vfamilyofalgorithmsdescribedbelow[Mikolovetal.,2013b,a]wereinspiredby
thispropertyoflanguagemodeling.eW2Valgorithmsaredesignedtoperformthesame
sideeffectsaslanguagemodeling,usingamoreefficientandmoreflexibleframework.eGV
algorithmbyPenningtonetal.[2014]followsasimilarobjective.esealgorithmsarealsodeeply
connectedtoanotherfamilyofalgorithmswhichevolvedintheNLPandIRcommunities,and
thatarebasedonmatrixfactorization[LevyandGoldberg,2014].Wordembeddingsalgorithms
arediscussedinSection10.4.
Animportantbenefitoftrainingwordembeddingsonlargeamountsofunannotateddata
isthatitprovidesvectorrepresentationsforwordsthatdonotappearinthesupervisedtraining
set. Ideally, the representations for these words will be similar to those of related words that do
appear in the training set, allowing the model to generalize better on unseen events. It is thus
desiredthatthesimilaritybetweenwordvectorslearnedbytheunsupervisedalgorithmcaptures
thesameaspectsofsimilaritythatareusefulforperformingtheintendedtaskofthenetwork.
Arguably,thechoiceofauxiliaryproblem(whatisbeingpredicted,basedonwhatkindof
context) affects the resulting vectors much more than the learning method that is being used to
trainthem.Section10.5surveysdifferentchoicesofauxiliaryproblems.
WordembeddingsderivedbyunsupervisedtrainingalgorithmshaveapplicationsinNLP
beyond using them for initializing the word-embeddings layer of neural network model. ese
arediscussedinChapter11.
¹einterpretationofcreatingauxiliaryproblemsfromrawtextisinspiredbyAndoandZhang[2005a,b].10.4. WORDEMBEDDINGALGORITHMS 117
10.3.1 USINGPRE-TRAINEDEMBEDDINGS
Whenusingpre-trainedwordembeddings,therearesomechoicesthatshouldbetaken.efirst
choiceisaboutpre-processing:Shouldthepre-trainedwordvectorsbeusedasis,orshouldeach
vector be normalized to unit length? is is task dependent. For many word embedding algo-
rithms,thenormofthewordvectorcorrelateswiththeword’sfrequency.Normalizingthewords
tounitlengthremovesthefrequencyinformation.iscouldeitherbeadesirableunification,or
anunfortunateinformationloss.
e second choice regards fine-tuning the pre-trained vectors for the task. Consider an
embeddingmatrixE RV d associatingwordsfromvocabularyV withd-dimensionalvectors.
j j(cid:2)
2
AcommonapproachistotreatE asmodelparameters,andchangeitwiththerestofthenetwork.
Whilethisworkswell,ithasthepotentialundesirableeffectofchangingtherepresentationsfor
wordsthatappearinthetrainingdata,butnotforotherwordsthatusedtobeclosetotheminthe
original pre-trained vectors E. is may hurt the generalization properties we aim to get from
thepre-trainingprocedure.Analternativeistoleavethepre-trainedvectorsE fixed.iskeeps
thegeneralization,butpreventsthemodelfromadaptingtherepresentationsforthegiventask.
AmiddlegroundistokeepE fixed,butuseanadditionalmatrixT Rd d.Insteadoflookingat
(cid:2)
2
therowsofE,welookatrowsofatransformedmatrixE ET.etransformationmatrixT is
0
D
tunedaspartofthenetwork,allowingtofine-tunesomeaspectsofthepre-trainedvectorsforthe
task.However,thetask-specificadaptationsareintheformoflineartransformationsthatapplyto
allwords,notjustthoseseenintraining.edownsideofthisapproachistheinabilitytochange
therepresentationsofsomewordsbutnotothers(forexample,ifhotandcold receivedverysimilar
vectors,itcouldbeveryhardforalineartransformationT toseparatethem).Anotheroptionis
tokeepE fixed,butuseanadditionalmatrix(cid:129) RV d andtaketheembeddingmatrixtobe
j j(cid:2)
2
E E (cid:129)orE ET (cid:129).e(cid:129)matrixisinitializedto0andtrainedwiththenetwork,
0 0
D C D C
allowingtolearnadditivechangestospecificwords.Addingastrongregularizationpenaltyover
(cid:129)willencouragethefine-tunedrepresentationstostayclosetotheoriginalones.²
10.4 WORDEMBEDDINGALGORITHMS
eneuralnetworkscommunityhasatraditionofthinkingintermsofdistributedrepresentations
[Hinton et al., 1987]. In contrast to local representations, in which entities are represented as
discrete symbols and the interactions between entities are encoded as a set of discrete relations
betweensymbolsformingagraph,indistributedrepresentationseachentityisinsteadrepresented
asa vectorof value (“apattern of activations”),and the meaning of the entity and its relationto
otherentitiesarecapturedbytheactivationsinthevector,andthesimilaritiesbetweendifferent
vectors. In the context of language processing, it means that words (and sentences) should not
be mapped to discrete dimensions but rather mapped to a shared low dimensional space, where
²Note that all the updates during gradient-based training are additive, and so without regularization, updating E during
trainingandkeepingEfixedbutupdating(cid:129)andlookingatE (cid:129)willresultinthesamefinalembeddings.eapproaches
C
onlydifferwhenregularizationisapplied.118 10. PRE-TRAINEDWORDREPRESENTATIONS
each word will be associated with a d-dimensional vector, and the meaning of the word will be
capturedbyitsrelationtootherwordsandtheactivationvaluesinitsvector.
enaturallanguageprocessingcommunityhasatraditioninthinkingintermsofdistribu-
tionalsemantics,inwhichameaningofawordcouldbederivedfromitsdistributioninacorpus,
i.e., from the aggregate of the contexts in which it is being used. Words that tend to occur in
similarcontextstendtohavesimilarmeanings.
ese two approaches to representing words—in terms of patterns of activations that are
learned in the context of a larger algorithm and in terms of co-occurrence patterns with other
wordsorsyntacticstructures,giverisetoseeminglyverydifferentviewsofwordrepresentations,
leadingtodifferentalgorithmicfamiliesandlinesofthinking.
In Section 10.4.1 we will explore the distributional approach to word representation, and
in Section 10.4.2 we’ll explore the distributed approaches. Section 10.4.3 will connect the two
worlds, and show that for the most part, current state-of-the-art distributed representations of
wordsareusingdistributionalsignalstodomostoftheirheavylifting,andthatthetwoalgorith-
micfamiliesaredeeplyconnected.
10.4.1 DISTRIBUTIONALHYPOTHESISANDWORDREPRESENTATIONS
e DistributionalHypothesis about language and word meaning states that words that occur in
thesamecontextstendtohavesimilarmeanings[Harris,1954].eideawaspopularizedbyFirth
[1957] through the saying “you shall know a word by the company it keeps.” Intuitively, when
people encounter a sentence with an unknown word such as the word wampimuk in Marcosaw
ahairylittlewampinukcrouchingbehindatree, they infer the meaning of the word based on the
contextinwhichitoccurs.isideahasgivenrisetothefieldofdistributionalsemantics:aresearch
area interested in quantifying semantic similarities between linguistic items according to their
distributionalpropertiesinlargetextcorpora.Foradiscussionofthelinguisticandphilosophical
basisofthedistributionalhypothesis,seeSahlgren[2008].
Word-contextMatrices
InNLP,alonglineofresearch³capturesthedistributionalpropertiesofwordsusingword-context
matrices,inwhicheachrowi representsaword,eachcolumnj representsalinguisticcontextin
whichwordscanoccur,andamatrixentryM quantifiesthestrengthofassociationbetween
(cid:140)i;j(cid:141)
awordandacontextinalargecorpus.Inotherwords,eachwordisrepresentedasasparsevector
in high dimensional space, encoding the weighted bag of contexts in which it occurs. Different
definitions of contexts and different ways of measuring the association between a word and a
context give rise to different word representations. Different distance functions can be used to
measurethedistancesbetweenwordvectors,whicharetakentorepresentthesemanticdistances
betweentheassociatedwords.
³SeethesurveyofTurneyandPantel[2010]andBaroniandLenci[2010]foranoverview.10.4. WORDEMBEDDINGALGORITHMS 119
More formally, denote by V the set of words (the words vocabulary) and by V the set
W C
ofpossiblecontexts.Weassumeeachwordandeachcontextareindexed,suchthatw istheith
i
wordinthewordsvocabularyandc isthejthwordinthecontextvocabulary.ematrixMf
j
2
R jVW j(cid:2)jVC
j
istheword-contextmatrix,definedasMf
(cid:140)i;j(cid:141)
Df.w i;c j/,wheref isanassociation
measureofthestrengthbetweenawordandacontext.
SimilarityMeasures
Oncewordsarerepresentedasvectors,onecancomputesimilaritiesbetweenwordsbycomputing
thesimilaritiesbetweenthecorrespondingvectors.Acommonandeffectivemeasureisthecosine
similarity,measuringthecosineoftheanglebetweenthevectors:
sim .u;v/
u (cid:1)v P iu (cid:140)i(cid:141) (cid:1)v (cid:140)i(cid:141)
: (10.1)
cos
D ku k2 kv k2 D pP i.u (cid:140)i(cid:141)/2pP i.v (cid:140)i(cid:141)/2
AnotherpopularmeasureisthegeneralizedJacaardsimilarity,definedas:⁴
sim .u;v/
P imin.u (cid:140)i(cid:141);v (cid:140)i(cid:141)/
: (10.2)
Jacaard
D P imax.u (cid:140)i(cid:141);v (cid:140)i(cid:141)/
Word-contextWeightingandPMI
efunctionf isusuallybasedoncountsfromalargecorpus.Denoteby#.w;c/thenumberof
times word w occurred in the context c in the corpus D, and let D be the corpus size ( D
j j j jD
nP ow rm02 aV lW iz; ec d02 cV oC u# n. tw f0 .; wc 0 ;/ c). /Itis Pin .t wu ;it civ /eto #d .wefi ;cn /e .Hf. ow w; ec v/ ert ,o tb he ist hh ae sc to hu en ut nf d. ew si; rc e/ dD eff# e. cw to; fc/ aso sr igth ne
-
D D D
ing high weights to word-context pairs invjojlving very common contexts (for example, consider
the context of a word to be the previous word. en for a word such as cat the events the cat
and a cat will receive much higher scores than cute cat and small cat even though the later are
muchmoreinformative).Tocounterthiseffect,itisbettertodefinef tofavorinformativecon-
textsforagivenword—contextsthatco-occurmorewiththegivenwordthanwithotherwords.
An effective metric that captures this behavior is the pointwise mutual information (PMI): an
information-theoreticassociationmeasurebetweenapairofdiscreteoutcomesx andy,defined
as:
P.x;y/
PMI.x;y/ log : (10.3)
D P.x/P.y/
In our case, PMI.w;c/ measures the association between a word w and a context c by
calculating the log of the ratio between their joint probability (the frequency in which they co-
occurtogether)andtheirmarginalprobabilities(thefrequenciesinwhichtheyoccurindividually).
PMIcanbeestimatedempiricallybyconsideringtheactualnumberofobservationsinacorpus:
#.w;c/ D
f.w;c/ PMI.w;c/ log (cid:1)j j; (10.4)
D D #.w/ #.c/
(cid:1)
u v
⁴Whenthinkingofuandvassets,theJacaardsimilarityisdefinedas ju\vj.
j [ j120 10. PRE-TRAINEDWORDREPRESENTATIONS
cwh ree sr pe e# c. tw ive/ lD y.P ec 0 u2sV eC o# f. Pw M;c I0/ asan ad m# e. ac s/ uD reoP faw s0 s2oV cW iat# i. ow n0 i; nc N/a Lre Pt whe asco inrp tru os df ur ce eq due bn yc Cie hs uo rf cw haa nn dd
Hanks [1990] and widely adopted for word similarity and distributional semantic tasks [Dagan
etal.,1994,Turney,2001,TurneyandPantel,2010].
WorkingwiththePMImatrixpresentssomecomputationalchallenges.erowsofMPMI
contain many entries of word-context pairs .w;c/ that were never observed in the corpus, for
whichPMI.w;c/ log0 .AcommonsolutionistousethepositivePMI (PPMI)metric,
D D(cid:0)1
inwhichallnegativevaluesarereplacedby0:⁵
PPMI.w;c/ max.PMI.w;c/;0/: (10.5)
D
Systematiccomparisonsofvariousweightingschemesforentriesintheword-contextsim-
ilarity matrix show that the PMI, and more so the positive-PMI (PPMI) metrics provide the
bestresultsforawiderangeofword-similaritytasks[BullinariaandLevy,2007,KielaandClark,
2014].
A deficiency of PMI is that it tends to assign high value to rare events. For example, if
twoeventsoccuronlyonce,butoccurtogether,theywillreceiveahighPMIvalue.Itistherefore
advisabletoapplyacountthresholdbeforeusingthePMImetric,ortootherwisediscountrare
events.
DimensionalityReductionthroughMatrixFactorization
A potential obstacle of representing words as the explicit set of contexts in which they occur is
thatofdatasparsity—someentriesinthematrixM maybeincorrectbecausewedidnotobserve
enough data points. Additionally, the explicit word vectors are of a very high dimensions (de-
pendingonthedefinitionofcontext,thenumberofpossiblecontextscanbeinthehundredsof
thousands,orevenmillions).
Bothissuescanbealleviatedbyconsideringalow-rankrepresentationofthedatausinga
dimensionalityreductiontechniquesuchasthesingularvaluedecomposition(SVD).
SVD works by factorizing the matrix M RVW VC into two narrow matrices: a W
j j(cid:2)j j
2 2
RVW d word matrix and a C RVC d context matrix, such that WC M RVW VC
j j(cid:2) j j(cid:2) > 0 j j(cid:2)j j
2 D 2
isthebestrank-d approximationofM inthesensethatnootherrank-d matrixhasacloserL
2
distancetoM thanM .
0
elow-rankrepresentationM canbeseenasa“smoothed”versionofM:basedonrobust
0
patterns in the data, some of the measurements are “fixed.” is has the effect, for example, of
adding words to contexts that they were not seen with, if other words in this context seem to
⁵Whenrepresentingwords,thereissomeintuitionbehindignoringnegativevalues:humanscaneasilythinkofpositiveassoci-
ations(e.g.,“Canada”and“snow”)butfinditmuchhardertoinventnegativeones(“Canada”and“desert”).issuggeststhat
theperceivedsimilarityoftwowordsisinfluencedmorebythepositivecontexttheysharethanbythenegativecontextthey
share.Itthereforemakessomeintuitivesensetodiscardthenegativelyassociatedcontextsandmarkthemas“uninformative”
(0)instead.Anotableexceptionwouldbeinthecaseofsyntacticsimilarity.Forexample,allverbsshareaverystrongnegative
associationwithbeingprecededbydeterminers,andpasttenseverbshaveaverystrongnegativeassociationtobeprecededby
“be”verbsandmodals.10.4. WORDEMBEDDINGALGORITHMS 121
co-locatewitheachother.Moreover,thematrixW allowstorepresenteachwordasadensed-
dimensional vector instead of a sparse V -dimensional one, where d V (typical choices
C C
j j (cid:28)j j
are50<d <300),suchthatthed-dimensionalvectorscapturesthemostimportantdirections
ofvariationintheoriginalmatrix.Onecanthencomputesimilaritiesbasedonthedensed-dim
vectorsinsteadofthesparsehigh-dimensionalones.
emathematicsofSVD eSingularValueDecomposition(SVD)isanalgebraictech-
niquebywhichanm nrealorcomplexmatrixM isfactorizedintothreematrices:
(cid:2)
M UDV;
D
where U is an m m real or complex matrix, D is an m n real or complex matrix, and
(cid:2) (cid:2)
V isann nmatrix.ematricesU andV areorthonormal,meaningthattheirrowsare
>
(cid:2)
unit-lengthandorthogonaltoeachother.ematrixD isdiagonal,wheretheelementson
thediagonalarethesingularvaluesofM,indecreasingorder.
efactorizationisexact.eSVDhasmanyuses,inmachinelearningandelsewhere.
For our purposes, SVD is used for dimensionalityreduction—finding low-dimensional rep-
resentationsofhigh-dimensionaldatathatpreservemostoftheinformationintheoriginal
data.
ConsiderthemultiplicationUDV whereD isaversionofD inwhichallbutthefirst
Q Q
kelementsonthediagonalarereplacedbyzeros.Wecannowzerooutallbutthefirstkrows
of U and columns of V, as they will be zeroed out by the multiplication anyhow. Deleting
the rows and columns leaves us with three matrices, U (m k), D (k k, diagonal) and
Q (cid:2) (cid:2) (cid:2)
V (k n).eproduct:
(cid:2)
M UDV
0 D Q Q Q
isa.m n/matrixofrankk.
(cid:2)
ematrixM istheproductofthinmatrices(U andV,withkmuchsmallerthanm
0 Q Q
andn),andcanbethoughtofasalowrankapproximationofM.
According to the Eckart-Youngtheorem [Eckart and Young, 1936], the matrix M is
0
thebestrank-kapproximationofM underL loss.atis,M istheminimizerof:
2 0
M argmin X M s:t:X isrank-k:
0 2
D k (cid:0) k
X Rm n
2 (cid:2)
ematrixM canbethoughtofasasmoothedversionofM,inthesensethatitusesonly
0
thek mostinfluentialdirectionsinthedata.
Approximatingrowdistanceselow-dimensionalrowsofE UDarelow-rankapprox-
D Q Q
imationsofthehigh-dimensionalrowsoftheoriginalmatrixM,inthesensethatcomputing
thedotproductbetweenrowsofE isequivalent tocomputingthedot-productbetweenthe
rowsofthereconstructedmatrixM .atis,E E M M .
.. 0 (cid:140)i(cid:141) (cid:140)j(cid:141) 0(cid:140)i(cid:141) 0(cid:140)j(cid:141)
(cid:1) D (cid:1)122 10. PRE-TRAINEDWORDREPRESENTATIONS
Toseewhy,considerthem mmatrixSE EE .Anentry(cid:140)i;j(cid:141)inthismatrixis
>
(cid:2) D
equaltothedotproductbetweenrowsi andj inE:SE E E .Similarlyforthe
(cid:140)i;j(cid:141) (cid:140)i(cid:141) (cid:140)j(cid:141)
D (cid:1)
matrixSM 0 M 0M 0>.
D
WewillshowthatSE SM 0.RecallthatVV> I becauseV isorthonormal.Now:
D Q Q D Q
SM 0 DM 0M 0> D.U QD QV Q/.U QD QV Q/ > D.U QD QV Q/.V Q>D Q>U Q>/
D
D.U QD Q/.V QV Q>/.D Q>U Q>/ D.U QD Q/.U QD Q/
>
DEE
>
DSE:
We can thus use the rows of E instead of the high-dimensional rows of M (and
0
instead of the high-dimensional rows of M. Using a similar argument, we can also use the
rowsof.DV/ insteadofthecolumnsofM ).
Q Q > 0
WhenusingSVDforwordsimilarity,therowsofM correspondtowords,thecolumns
tocontexts,andthevectorscomprisingtherowsofE arelow-dimensionalwordrepresenta-
tions.Inpractice,itisoftenbettertonotuseE UDbutinsteadtousethemore“balanced”
D Q Q
versionE UpD,orevenignoringthesingularvaluesD completelyandtakingE U.
.. D Q Q Q D Q
10.4.2 FROMNEURALLANGUAGEMODELSTODISTRIBUTED
REPRESENTATIONS
Incontrasttotheso-calledcount-basedmethodsdescribedabove,theneuralnetworkscommunity
advocatestheuseofdistributedrepresentationsofwordmeanings.Inadistributedrepresentation,
eachwordisassociatedwithavectorinRd,wherethe“meaning”ofthewordwithrespecttosome
task is captured in the different dimensions of the vector, as well as in the dimensions of other
words.Unliketheexplicitdistributionalrepresentationsinwhicheachdimensioncorrespondsto
a specific context the word occurs in, the dimensions in the distributed representation are not
interpretable, and specific dimensions do not necessarily correspond to specific concepts. e
distributednatureoftherepresentationmeansthatagivenaspectofmeaningmaybecapturedby
(distributedover)acombinationofmanydimensions,andthatagivendimensionmaycontribute
tocapturingseveralaspectsofmeaning.⁶
ConsiderthelanguagemodelingnetworkinEquation(9.3)inChapter9.econtextofa
wordisthekgramofwordsprecedingit.Eachwordisassociatedwithavector,andtheirconcate-
nationisencodedintoad dimensionalvectorhusinganonlineartransformation.evector
hid
histhenmultipliedbyamatrixW2 inwhicheachcolumn correspondstoaword,andinterac-
tionsbetweenhandcolumnsinW2 determinetheprobabilitiesofthedifferentwordsgiventhe
context. e columns of W2 (as well as the rows of the embeddings matrix E) are distributed
⁶Wenotethatinmanywaystheexplicitdistributionalrepresentationsisalso“distributed”:differentaspectsofthemeaning
ofawordarecapturedbygroupsofcontextsthewordoccursin,andagivencontextcancontributetodifferentaspectsof
meaning.Moreover,afterperformingdimensionalityreductionovertheword-contextmatrix,thedimensionsarenolonger
easilyinterpretable.10.4. WORDEMBEDDINGALGORITHMS 123
representations of words: the training process determines good values to the embeddings such
that they produce correct probability estimates for a word in the context of a kgram, capturing
the“meaning”ofthewordsinthecolumnsofW2 associatedwiththem.
CollobertandWeston
edesignofthenetworkinEquation(9.3)isdrivenbythelanguagemodelingtask,whichposes
two important requirements: the need to produce a probabilitydistributions over words, and the
needtoconditiononcontextsthatcanbecombinedusingthechain-ruleofprobabilitytoproduce
sentence-level probability estimates. e need to produce a probability distribution dictates the
needtocomputeanexpensivenormalizationterminvolvingallthewordsintheoutputvocabulary,
while the need to decompose according to the chain-rule restricts the conditioning context to
precedingkgrams.
Ifweonlycareabouttheresultingrepresentations,bothoftheconstraintscanberelaxed,
as was done by Collobert and Weston [2008] in a model which was refined and presented in
greaterdepthbyBengioetal.[2009].efirstchangeintroducedbyCollobertandWestonwas
changing the context of a word from the preceding kgram (the words to its left) to a word-
window surrounding it (i.e., computing P.w
3
w 1w 2(cid:3)w 4w 5/ instead of P.w
5
w 1w 2w 3w 4(cid:3)/).
j j
egeneralizationtootherkindsoffixed-sizedcontextsc isstraightforward.
1k
W
e second change introduced by Collobert and Weston is to abandon the probabilistic
output requirement. Instead of computing a probability distribution over target words given a
context, their model only attempts to assign a score to each word, such that the correct word
scores above incorrect ones. is removes the need to perform the computationally expensive
normalizationovertheoutputvocabulary,makingthecomputationtimeindependentoftheout-
putvocabularysize.isnotonlymakesthenetworkmuchfastertotrainanduse,butalsomakes
it scalable to practically unlimited vocabularies (the only cost of increasing the vocabulary is a
linearincreaseinmemoryusage).
Let w be a target word, c be an ordered list of context items, and v .w/ and v .c/
1k w c
W
embeddingfunctionsmappingwordandcontextindicestod dimensionalvectors(fromnow
emb
onweassumethewordandcontextvectorshavethesamenumberofdimensions).emodelof
Collobert and Weston computes a score s.w;c / of a word-context pair by concatenating the
1k
W
word and the context embeddings into a vector x, which is fed into an MLP with one hidden
layerwhosesingleoutputisthescoreassignedtotheword-contextcombination:
s.w;c / g.xU/ v
1k
W D (cid:1) (10.6)
x (cid:140)v .c / ::: v .c / v .w/(cid:141)
c 1 c k w
D I I I
U R.k 1/d emb d h v Rd h:
C (cid:2)
2 2
e network is trained with a margin-based ranking loss to score correct word-context
pairs.w;c /aboveincorrectword-contextpairs.w ;c /withamarginofatleast1.eloss
1k 0 1k
W W124 10. PRE-TRAINEDWORDREPRESENTATIONS
L.w;c /foragivenword-contextpairisgivenby:
1k
W
L.w;c;w / max.0;1 .s.w;c / s.w ;c /// (10.7)
0 1k 0 1k
D (cid:0) W (cid:0) W
where w is a random word from the vocabulary. e training procedure repeatedly goes over
0
word-contextpairsfromthecorpus,andforeachonesamplesarandomwordw ,computesthe
0
lossL.w;c;w /usingw ,andupdatesparametersU,vandthewordandcontextembeddingsto
0 0
minimizetheloss.
euseofrandomlysampledwordstoproducenegativeexamplesofincorrectword-context
todrivetheoptimizationisalsoatthecoreoftheW2Valgorithm,tobedescribednext.
Word2Vec
e widely popular W2V algorithm was developed by Tomáš Mikolov and colleagues
over a series of papers [Mikolov et al., 2013b,a]. Like the algorithm of Collobert and Weston,
W2V also starts with a neural language model and modifies it to produce faster results.
textscWord2V is not a single algorithm: it is a software package implementing two differ-
entcontextrepresentations(CBOWandSkip-Gram)andtwodifferentoptimizationobjectives
(Negative-Sampling and Hierarchical Softmax). Here, we focus on the Negative-Sampling ob-
jective(NS).
Like Collobert and Weston’s algorithm, the NS variant of W2V works by training
the network to distinguish “good” word-context pairs from “bad” ones. However, W2V
replacesthemargin-basedrankingobjectivewithaprobabilisticone.ConsiderasetD ofcorrect
word-context pairs, and a set D of incorrect word-context pairs. e goal of the algorithm is to
N
estimatetheprobabilityP.D 1w;c/thattheword-contextpaircamefromthecorrectsetD.
D j
is should be high (1) for pairs from D and low (0) for pairs from D. e probability con-
N
straint dictates that P.D 1w;c/ 1 P.D 0w;c/. e probability function is modeled
D j D (cid:0) D j
asasigmoidoverthescores.w;c/:
1
P.D 1w;c/ : (10.8)
D j D 1 e s.w;c/
C (cid:0)
e corpus-wide objective of the algorithm is to maximize the log-likelihood of the data
D D:
[ N
L.(cid:130) D;D/ X logP.D 1w;c/ X logP.D 0 w;c/: (10.9)
I N D D j C D j
.w;c/ D .w;c/ D
2 2N
e positive examples D are generated from a corpus. e negative examples D can be
N
generated in many ways. In W2V, they are generated by the following process: for each
goodpair.w;c/ D,samplek wordsw andaddeachof.w ;c/asanegativeexampletoD.
2 1 Wk i N
isresultsinthenegativesamplesdataD beingk timeslargerthanD.enumberofnegative
N
samplesk isaparameterofthealgorithm.10.4. WORDEMBEDDINGALGORITHMS 125
enegativewordswcanbesampledaccordingtotheircorpus-basedfrequency #.w/ ,
or, as done in the W2V implementation, according to a smoothed version in
wP hw ic0h#.w th0/
e
counts are raised to the power of 3 before normalizing: #.w/0:75 . is second version gives
morerelativeweighttolessfrequen4 twords,andresultsinP bw e0t# te.w r0 w/0 o:7 r5
dsimilaritiesinpractice.
CBOW Other than changing the objective from margin-based to a probabilistic one,
W2V also considerably simplify the definition of the word-context scoring function,
s.w;c/. For a multi-word context c , the CBOW variant of W2V defines the context
1k
vectorc tobeasumoftheembeddinW gvectorsofthecontextcomponents:c DPk
i
1c i.Itthen
definesthescoretobesimplys.w;c/ w c,resultingin: D
D (cid:1)
1
P.D 1w;c / :
D j 1 Wk D 1 Ce (cid:0).w (cid:1)c 1 Cw (cid:1)c 2 C::: Cw (cid:1)c k/
eCBOWvariantlosestheorderinformationbetweenthecontext’selements.Inreturn,
itallowstheuseofvariable-lengthcontexts.However,notethatforcontextswithboundlength,
the CBOW can still retain the order information by including the relative position as part of
the content element itself, i.e., by assigning different embedding vector to context elements in
differentrelativepositions.
Skip-Gram e skip-gram variant of W2V scoring decouples the dependence between
thecontextelementsevenfurther.Forak-elementscontextc ,theskip-gramvariantassumes
1k
W
that the elements c in the context are independent from each other, essentially treating them
i
as k different contexts, i.e., a word-context pair .w;c / will be represented in D as k different
i k
W
contexts: .w;c /;:::;.w;c /. e scoring function s.w;c/ is defined as in the CBOW version,
1 k
butnoweachcontextissingleembeddingvector:
1
P.D 1w;c /
i wc
D j D 1 e i
C (cid:0) (cid:1)
k k
1
P.D 1 w;c / YP.D 1w;c / Y
D j 1 Wk D
i 1
D j i D
1 i
1 Ce (cid:0)w (cid:1)c i (10.10)
D D
k
1
logP.D 1 w;c / Xlog :
1k wc
D j W D i 1 1 Ce (cid:0) (cid:1) i
D
Whileintroducingstrongindependenceassumptionsbetweentheelementsofthecontext,
theskip-gramvariantisveryeffectiveinpractice,andverycommonlyused.
10.4.3 CONNECTINGTHEWORLDS
Boththedistributional“count-based”methodandthedistributed“neural”onesarebasedonthe
distributionalhypothesis,attemptingcapturethesimilaritybetweenwordsbasedonthesimilarity126 10. PRE-TRAINEDWORDREPRESENTATIONS
betweenthecontextsinwhichtheyoccur.Infact,LevyandGoldberg[2014]showthattheties
betweenthetwoworldsaredeeperthanappearatfirstsight.
e training of W2V models result in two embedding matrices, EW RVW d emb
j j(cid:2)
2
andEC RVC d emb representingthewordsandthecontexts,respectively.econtextembed-
j j(cid:2)
2
dingsarediscardedaftertraining,andthewordembeddingsarekept.However,imaginekeeping
the context embedding matrix EC and consider the product EW EC > M 0 R jVW j(cid:2)jVC j.
(cid:2) D 2
Viewed this way, W2V is factorizing an implicit word-context matrix M . What are the
0
elements of matrix M ? Anentry M corresponds to the dot productof the wordand con-
0 0(cid:140)w;c(cid:141)
text embedding vectors w c. Levy and Goldberg show that for the combination of skip-grams
(cid:1)
contextsandthenegativesamplingobjectivewithknegativesamples,theglobalobjectiveismin-
imizedbysettingw c M PMI.w;c/ logk.atis,W2Visimplicitlyfactor-
0(cid:140)w;c(cid:141)
(cid:1) D D (cid:0)
izingamatrixwhichiscloselyrelatedtothewell-knownword-contextPMImatrix!Remarkably,
itdoessowithouteverexplicitlyconstructingthematrixM .⁷
0
e above analysis assumes that the negative samples are sampled according to the cor-
pusfrequencyofthewordsP.w/ #.w/ .RecallthattheW2Vimplementationsam-
plesinsteadfromamodifieddistriD butP iow n0 P#.w 0:0 7/ 5.w/ #.w/0:75 .Underthissamplingscheme,
theoptimalvaluechangestoPMI0:75.w;c/
logkD lP ow g0#.w P0/ .0 w:7 ;5
c/ logk.Indeed,usingthis
(cid:0) D P0:75.w/P.c/ (cid:0)
modified version of PMI when constructing sparse and explicit distributional vectors improves
thesimilarityinthatsetupaswell.
eW2Valgorithmsareveryeffectiveinpractice,andarehighlyscalable,allowingto
trainwordrepresentationswithverylargevocabulariesoverbillionsofwordsoftextinamatter
of hours, with very modest memory requirements. e connection between the SGNS variant
of W2V and word-context matrix-factorization approaches ties the neural methods and
thetraditional“count-based”ones,suggestingthatlessonslearnedinthestudyof“distributional”
representationcantransfertothe“distributed”algorithms,andviceversa,andthatinadeepsense
thetwoalgorithmicfamiliesareequivalent.
10.4.4 OTHERALGORITHMS
Many variants on the W2V algorithms exist, none of which convincingly produce qual-
itatively or quantitatively superior word representations. is sections list a few of the popular
ones.
NCE e noise-contrastive estimation (NCE) approach of Mnih and Kavukcuoglu [2013] is
verysimilartotheSGNSvariantof W2V,butinsteadofmodelingP.D 1 w;c /asin
i
D j
⁷Iftheoptimalassignmentwassatisfiable,theskip-gramswithnegative-sampling(SGNS)solutionisthesameastheSVD
overword-contextmatrixsolution.Ofcourse,thelowdimensionalityd emb ofwandc maymakeitimpossibletosatisfy
w c PMI.w;c/ logkforallwandc pairs,andtheoptimizationprocedurewillattempttofindthebestachievable
(cid:1) D (cid:0)
solution,whilepayingapriceforeachdeviationfromtheoptimalassignment.isiswheretheSGNSandtheSVDobjectives
differ—SVDputsaquadraticpenaltyoneachdeviation,whileSGNSusesamorecomplexpenaltyterm.10.5. THECHOICEOFCONTEXTS 127
Equation(10.10),itismodeledas:
wc
e i
P.D 1 w;c / (cid:0) (cid:1) (10.11)
i wc
D j D e i k q.w/
(cid:0) (cid:1) C (cid:2)
k q.w/
P.D 0 w;c / (cid:2) ; (10.12)
i wc
D j D e i k q.w/
(cid:0) (cid:1) C (cid:2)
whereq.w/ #.w/ istheobservedunigramfrequencyofwinthecorpus.isalgorithmisbased
D D
on the noise-conj trjastive estimation probability modeling technique [Gutmann and Hyvärinen,
2010]. According to Levy and Goldberg [2014], this objective is equivalent to factorizing the
word-contextmatrixwhoseentriesarethelogconditionalprobabilitieslogP.w c/ logk.
j (cid:0)
GloVe e GV algorithm [Pennington et al., 2014] constructs an explicit word-context
matrix,andtrainsthewordandcontextvectorswandc attemptingtosatisfy:
w c b b log#.w;c/ .w;c/ D; (10.13)
(cid:140)w(cid:141) (cid:140)c(cid:141)
(cid:1) C C D 8 2
whereb andb areword-specificandcontext-specifictrainedbiases.eoptimizationpro-
(cid:140)w(cid:141) (cid:140)c(cid:141)
cedurelooksatobservedwordcontextpairswhileskippingzerocountevents.Intermsofmatrix
factorization,ifwefixb log#.w/andb log#.c/we’llgetanobjectivethatisverysim-
(cid:140)w(cid:141) (cid:140)c(cid:141)
D D
ilar to factorizing the word-context PMI matrix, shifted by log.D /. However, in GloVe these
j j
parametersarelearnedandnotfixed,givingitanotherdegreeoffreedom.eoptimizationobjec-
tiveisweightedleast-squaresloss,assigningmoreweighttothecorrectreconstructionoffrequent
items. Finally, when using the same word and context vocabularies, the GloVe model suggests
representingeachwordasthesumofitscorrespondingwordandcontextembeddingvectors.
10.5 THECHOICEOFCONTEXTS
e choice of context by which a word is predicted has a profound effect on the resulting word
vectorsandthesimilaritiestheyencode.
Inmostcases,thecontextsofawordaretakentobeotherwordsthatappearinitssurround-
ing,eitherinashortwindowaroundit,orwithinthesamesentence,paragraphordocument.In
somecasesthetextisautomaticallyparsedbyasyntacticparser,andthecontextsarederivedfrom
the syntactic neighborhood induced by the automatic parse trees. Sometimes, the definitions of
wordsandcontextchangetoincludealsopartsofwords,suchasprefixesorsuffixes.
10.5.1 WINDOWAPPROACH
emostcommonapproachisaslidingwindowapproach,inwhichauxiliarytasksarecreatedby
lookingatasequenceof2m 1words.emiddlewordiscalledthefocusword andthemwords
C
toeachsidearethecontexts.en,eitherasingletaskiscreatedinwhichthegoalistopredictthe
focus word based on all of the context words (represented either using CBOW [Mikolov et al.,128 10. PRE-TRAINEDWORDREPRESENTATIONS
2013b]orvectorconcatenation[CollobertandWeston,2008]),or2mdistincttasksarecreated,
each pairing the focus word with a different context word. e 2m tasks approach, popularized
by Mikolov et al. [2013a] is referred to as a skip-gram model. Skip-gram-based approaches are
shown to be robust and efficient to train [Mikolov et al., 2013a, Pennington et al., 2014], and
oftenproducestateoftheartresults.
EffectofWindowSize esizeoftheslidingwindowhasastrongeffectontheresultingvector
similarities. Larger windows tend to produce more topical similarities (i.e., “dog,” “bark” and
“leash”willbegroupedtogether,aswellas“walked,”“run”and“walking”),whilesmallerwindows
tendtoproducemorefunctionalandsyntacticsimilarities(i.e.,“Poodle,”“Pitbull,”“Rottweiler,”
or“walking,”“running,”“approaching”).
PositionalWindows WhenusingtheCBOWorskip-gramcontextrepresentations,allthedif-
ferentcontextwordswithinthewindowaretreatedequally.ereisnodistinctionbetweencon-
textwordsthatareclosetothefocuswordsandthosethatarefartherfromit,andlikewisethere
isnodistinctionbetweencontextwordsthatappearbeforethefocuswordstocontextwordsthat
appearafterit.Suchinformationcaneasilybefactoredinbyusingpositionalcontexts:indicating
foreachcontextwordalsoitsrelativepositiontothefocuswords(i.e.,insteadofthecontextword
being “the” it becomes “the:+2,” indicating the word appears two positions to the right of the
focus word). e use of positional context together with smaller windows tend to produce sim-
ilarities that are more syntactic, with a strong tendency of grouping together words that share a
partofspeech,aswellasbeingfunctionallysimilarintermsoftheirsemantics.Positionalvectors
wereshownbyLingetal.[2015a]tobemoreeffectivethanwindow-basedvectorswhenusedto
initializenetworksforpart-of-speechtaggingandsyntacticdependencyparsing.
Variants Manyvariantsonthewindowapproacharepossible.Onemaylemmatizewordsbefore
learning,applytextnormalization,filtertooshortortoolongsentences,orremovecapitalization
(see, e.g., the pre-processing steps described by dos Santos and Gatti [2014]). One may sub-
sample part of the corpus, skipping with some probability the creation of tasks from windows
thathavetoocommonortoorarefocuswords.ewindowsizemaybedynamic,usingadiffer-
ent window size at each turn. One may weigh the different positions in the window differently,
focusing more on trying to predict correctly close word-context pairs than further away ones.
Eachofthesechoicesisahyperparametertobemanuallysetbeforetraining,andwilleffectthe
resultingvectors.Ideally,thesewillbetunedforthetaskathand.Muchofthestrongperformance
of the W2V implementation can be attributed to specifying good default values for these
hyperparameters.Someofthesehyperparameters(andothers)arediscussedindetailinLevyetal.
[2015].10.5. THECHOICEOFCONTEXTS 129
10.5.2 SENTENCES,PARAGRAPHS,ORDOCUMENTS
Using a skip-gram (or CBOW) approach, onecan considerthe contexts of a wordto be all the
otherwordsthatappearwithitinthesamesentence,paragraph,ordocument.isisequivalent
to using very large window sizes, and is expected to result in word vectors that capture topical
similarity (words from the same topic, i.e., words that one would expect to appear in the same
document,arelikelytoreceivesimilarvectors).
10.5.3 SYNTACTICWINDOW
Someworkreplacethelinearcontextwithinasentencewithasyntacticone[Bansaletal.,2014,
Levy and Goldberg, 2014]. e text is automatically parsed using a dependency parser, and the
contextofawordistakentobethewordsthatareinitsproximityintheparsetree,togetherwith
the syntactic relation by which they are connected. Such approaches produce highly functional
similarities,groupingtogetherwordsthancanfillthesameroleinasentence(e.g.,colors,names
ofschools,verbsofmovement).egroupingisalsosyntactic,groupingtogetherwordsthatshare
aninflection[LevyandGoldberg,2014].
e effect of context e following table, taken from Levy and Goldberg [2014], shows
thetop-5mostsimilarwordstosomeseedwords,whenusingbag-of-wordswindowsofsize
5and2(BW5andBW2),aswellasdependency-basedcontexts(D),usingthesame
underlyingcorpora(Wikipedia),andthesameembeddingsalgorithm(W2V).
Noticehowforsomewords(e.g.,batman)theinducedwordsimilaritiesaresomewhat
agnostic to the contexts, while for others there is a clear trend: the larger window contexts
resultinmoretopicalsimilarities(hogwarsissimilartoothertermsintheHarryPotteruni-
verse, turing is related to computability, dancing is similar to other inflections of the word)
whilethesyntactic-dependencycontextsresultinmorefunctionalsimilarities(hogwartssim-
ilartootherfictionalornon-fictionalschools,turingissimilartootherscientists,anddancing
toothergerundsofentrainmentactivities).esmallercontextwindowissomewhereinbe-
tweenthetwo.
isre-affirmsthatcontextchoicesstronglyaffectstheresultingwordrepresentations,
andstressestheneedtotakethechoiceofcontextintoconsiderationwhenusing“unsuper-
.. vised”wordembeddings.130 10. PRE-TRAINEDWORDREPRESENTATIONS
Target Word BoW5 BoW2 Deps
nightwing superman superman
aquaman superboy superboy
batman catwoman aquaman supergirl
superman catwoman catwoman
manhunter batgirl aquaman
dumbledore evernight sunnydale
hallows sunnydale collinwood
hogwarts half-blood garderobe calarts
malfoy blandings greendale
snape collinwood millfi eld
nondeterministic non-deterministic pauling
non-deterministic fi nite-state hotelling
turing computability nondeterministic heting
deterministic buchi lessing
fi nite-state primality hamming
gainesville fl a texas
fl a alabama louisiana
fl orida jacksonville gainesville georgia
tampa tallahassee california
lauderdale texas carolina
aspect-oriented aspect-oriented event-driven
smalltalk event-driven domain-specifi c
object-oriented event-driven objective-c rule-based
prolog datafl ow data-driven
domain-specifi c 4gl human-centered
singing singing singing
dance dance rapping
dancing dances dances breakdancing
dancers breakdancing miming
tap-dancing clowning busking
..
10.5.4 MULTILINGUAL
Anotheroptionisusingmultilingual,translation-basedcontexts[FaruquiandDyer,2014,Her-
mannandBlunsom,2014].Forexample,givenalargeamountofsentence-alignedparalleltext,10.5. THECHOICEOFCONTEXTS 131
one can run a bilingual alignment model such as the IBM model 1 or model 2 (i.e., using the
GIZA++ software), and then use the produced alignments to derive word contexts. Here, the
contextof awordinstance isthe foreignlanguagewordsthat arealigned to it.Such alignments
tend to result in synonym words receiving similar vectors. Some authors work instead on the
sentence alignment level, without relying on word alignments [Gouws et al., 2015] or train an
end-to-end machine-translation neural network and use the resulting word embeddings [Hill
etal.,2014].Anappealingmethodistomixamonolingualwindow-basedapproachwithamul-
tilingual approach, creating both kinds of auxiliary tasks. is is likely to produce vectors that
are similar to the window-based approach, but reducing the somewhat undesired effect of the
window-based approach in which antonyms (e.g., hot and cold, high and low) tend to receive
similar vectors [Faruqui and Dyer, 2014]. For further discussion on multilingual word embed-
dingsandacomparisonofdifferentmethodsseeLevyetal.[2017].
10.5.5 CHARACTER-BASEDANDSUB-WORDREPRESENTATIONS
Aninterestinglineofworkattemptstoderivethevectorrepresentationofawordfromthechar-
acters that compose it. Such approaches are likely to be particularly useful for tasks which are
syntactic in nature, as the character patterns within words are strongly related to their syntactic
function. ese approaches also have the benefit of producing very small model sizes (only one
vector for each character in the alphabet together with a handful of small matrices needs to be
stored),andbeingabletoprovideanembeddingvectorforeverywordthatmaybeencountered.
dos Santos and Gatti [2014], dos Santos and Zadrozny [2014], and Kim et al. [2015] model
the embedding of a word using a convolutional network (see Chapter 13) over the characters.
Lingetal.[2015b]modeltheembeddingofawordusingtheconcatenationofthefinalstatesof
twoRNN(LSTM)encoders(Chapter14),onereadingthecharactersfromlefttoright,andthe
otherfromrighttoleft.Bothproduceverystrongresultsforpart-of-speechtagging.eworkof
Ballesterosetal.[2015]showthatthetwo-LSTMsencodingofLingetal.[2015b]isbeneficial
alsoforrepresentingwordsindependencyparsingofmorphologicallyrichlanguages.
Derivingrepresentationsofwordsfromtherepresentationsoftheircharactersismotivated
bytheunknownwordsproblem—whatdoyoudowhenyouencounterawordforwhichyoudonot
have an embedding vector? Working on the level of characters alleviates this problem to a large
extent, as the vocabulary of possible characters is much smaller than the vocabulary of possible
words.However,workingonthecharacterlevelisverychallenging,astherelationshipbetween
form(characters)andfunction(syntax,semantics)inlanguageisquiteloose.Restrictingoneself
tostayonthecharacterlevelmaybeanunnecessarilyhardconstraint.Someresearcherspropose
amiddle-ground,inwhichawordisrepresentedasacombinationofavectorfortheworditself
withvectorsofsub-wordunitsthatcompriseit.esub-wordembeddingsthenhelpinsharing
informationbetweendifferentwordswithsimilarforms,aswellasallowingback-offtothesub-
word level when the word is not observed. At the same time, the models are not forced to rely
solelyonformwhenenoughobservationsofthewordareavailable.BothaandBlunsom[2014]132 10. PRE-TRAINEDWORDREPRESENTATIONS
suggest to model the embedding vector of a word as a sum of the word-specific vector if such
vectorisavailable,withvectorsforthedifferentmorphologicalcomponentsthatcompriseit(the
componentsarederivedusingMorfessor[CreutzandLagus,2007],anunsupervisedmorphologi-
calsegmentationmethod).Gaoetal.[2014]suggestusingascorefeaturesnotonlythewordform
itselfbutalsoauniquefeature(henceauniqueembeddingvector)foreachoftheletter-trigrams
intheword.
Another middle ground between characters and words is breaking up words into “mean-
ingfulunits”whicharelargerthancharactersandareautomaticallyderivedfromthecorpus.One
suchapproachistouseByte-PairEncoding(B)[Gage,1994],whichwasintroducedbySen-
nrichetal.[2016a]inthecontextofMachineTranslationandprovedtobeveryeffective.Inthe
Bapproach,onedecidesonavocabularysize(say10,000),andthenlooksfor10,000unitsthat
canrepresentallthewordsinthecorpusvocabularyaccordingtothefollowingalgorithm,taken
fromSennrichetal.[2016a].
Weinitializethesymbolvocabularywiththecharactervocabulary,andrepresenteach
wordasasequenceofcharacters,plusaspecialend-of-wordsymbol‘’,whichallows
(cid:1)
ustorestoretheoriginaltokenizationaftertranslation.Weiterativelycountallsymbol
pairs and replace each occurrence of the most frequent pair (A, B) with a new sym-
bol AB. Each merge operation produces a new symbol which represents a character
n-gram. Frequent character n-grams (or whole words) are eventually merged into a
singlesymbol,thusBrequiresnoshortlist.efinalsymbolvocabularysizeisequal
tothesizeoftheinitialvocabulary,plusthenumberofmergeoperations—thelatter
istheonlyhyperparameterofthealgorithm.Forefficiency,wedonotconsiderpairs
thatcrosswordboundaries.ealgorithmcanthusberunonthedictionaryextracted
fromatext,witheachwordbeingweightedbyitsfrequency.
10.6 DEALINGWITHMULTI-WORDUNITSANDWORD
INFLECTIONS
Twoissuesthatarestillunder-exploredwithrespecttowordrepresentationshavetodowiththe
definition of a word. e unsupervised word embedding algorithms assume words correspond
totokens(consecutivecharacterswithoutwhitespaceorpunctuation,seethe“Whatisaword?”
discussioninSection6.1).isdefinitionoftenbreaks.
InEnglish,wehavemanymulti-tokenunitssuchasNewYorkandicecream,aswellaslooser
casessuchasBostonUniversityorVolgaRiver,thatwemaywanttoassigntosinglevectors.
InmanylanguagesotherthanEnglish,richmorphologicalinflectionsystemsmakeforms
thatrelatetothesameunderlyingconceptlookdifferently.Forexample,inmanylanguagesadjec-
tivesareinflectedfornumberandgender,causingthewordyellowdescribingaplural,masculine
noun to have a different form from the word yellow describing a singular, feminine noun. Even
worse,astheinflectionsystemalsodictatestheformsoftheneighboringwords(nounsnearthe10.7. LIMITATIONSOFDISTRIBUTIONALMETHODS 133
singularfeminineformofyellowarethemselvesinasingularfeminineform),differentinflections
ofthesamewordoftendonotendupsimilartoeachother.
Whiletherearenogoodsolutionstoeitheroftheseproblems,theycanbothbeaddressedto
areasonabledegreebydeterministicallypre-processingthetextsuchthatitbetterfitsthedesired
definitionsofwords.
Inthemulti-tokenunitscase,onecanderivealistofsuchmulti-tokenitems,andreplace
them in the text with single entities (i.e., replacing occurrences of New York with New_York.
Mikolov et al. [2013a] proposes a PMI-based method for automatically creating such a list, by
consideringthePMIofawordpair,andmergingpairswithPMIscoresthatpasssomepredefined
thresholds. e process then iteratively repeats to merge pairs + words into triplets, and so on.
en, the embedding algorithm is run over the pre-processed corpus. is coarse but effective
heuristicisimplementedaspartoftheW2Vpackage,allowingtoderiveembeddingsalso
forsomeprominentmulti-tokenitems.⁸
Intheinflectionscase,onecanmitigatetheproblemtoalargeextentbypre-processingthe
corpusbylemmatizingsomeorallofthewords,embeddingthelemmasinsteadoftheinflected
forms.
Arelatedpre-processingisPOS-taggingthecorpus,andreplacingwordswith(word,POS)
pairs,creating,forexample,thetwodifferenttokentypesbook andbook ,thatwilleach
NOUN VERB
receive a different embedding vector. For further discussion on the interplay of morphological
inflections and word embeddings algorithms see Avraham and Goldberg [2017], Cotterell and
Schutze[2015].
10.7 LIMITATIONSOFDISTRIBUTIONALMETHODS
edistributionalhypothesisoffersanappealingplatformforderivingwordsimilaritiesbyrepre-
sentingwordsaccordingtothecontextsinwhichtheyoccur.Itdoes,however,havesomeinherent
limitationsthatshouldbeconsideredwhenusingthederivedrepresentations.
Definitionofsimilarity e definition of similarity in distributional approaches is completely
operational:wordsaresimilarifusedinsimilarcontexts.Butinpractice,therearemanyfacetsof
similarity.Forexample,considerthewordsdog,cat,andtiger.Ontheonehand,catismoresimilar
todogthantotiger,asbotharepets.Ontheotherhand,catcanbeconsideredmoresimilartotiger
thantodogastheyarebothfelines.Somefacetsmaybepreferredoverothersincertainusecases,
andsomemaynotbeattestedbythetextasstronglyasothers.edistributionalmethodsprovide
verylittlecontroloverthekindofsimilaritiestheyinduce.iscouldbecontrolledtosomeextent
bythechoiceofconditioningcontexts(Section10.5),butitisfarfrombeingacompletesolution.
BlackSheepsWhenusingtextsastheconditioningcontexts,manyofthemore“trivial”properties
of the word may not be reflected in the text, and thus not captured in the representation. is
happensbecauseofawell-documentedbiasinpeople’suseoflanguage,stemmingfromefficiency
constraintsoncommunication:peoplearelesslikelytomentionknowninformationthantheyare
⁸Forin-depthdiscussionofheuristicsforfindinginformativewordcollocations,seeManningandSchütze[1999,Chapter5].134 10. PRE-TRAINEDWORDREPRESENTATIONS
tomentionnovelone.us,whenpeopletalkofwhitesheep,theywilllikelyrefertothemassheep,
whileforblacksheeptheyaremuchmorelikelytoretainthecolorinformationandsayblacksheep.
Amodeltrainedontextdataonlycanbegreatlymisledbythis.
AntonymsWordsthataretheoppositeofeachother(good vs.bad,buyvs.sell,hotvscold)tendto
appearinsimilarcontexts(thingsthatcanbehotcanalsobecold,thingsthatareboughtareoften
sold). As a consequence, models based on the distributional hypothesis tend to judge antonyms
asverysimilartoeachother.
CorpusBiases For better or worse, the distributional methods reflect the usage patterns in the
corporaonwhichtheyarebased,andthecorporainturnreflecthuman biasesintherealworld
(culturalorotherwise).Indeed,Caliskan-Islametal.[2016]foundthatdistributionalwordvec-
torsencode“everylinguisticbiasdocumentedinpsychologythatwehavelookedfor,”includingracial
andgenderstereotypes(i.e.,EuropeanAmericannamesareclosertopleasanttermswhileAfrican
American names are closer to unpleasant terms; female names are more associated with family
terms than with career terms; it is possible to predict the percentage of women in an occupa-
tion according to U.S. census based on the vector representation of the occupation name). Like
with the antonyms case, this behavior may or may not be desired, depending on the use case:
if our task is to guess the gender of a character, knowing that nurses are stereotypically females
whiledoctorsarestereotypicallymalesmaybeadesiredpropertyofthealgorithm.Inmanyother
cases,however,wewouldliketoignoresuchbiases.Inanycase,thesetendenciesoftheinduced
word similarities should be taken into consideration when using distributional representations.
Forfurtherdiscussion,seeCaliskan-Islametal.[2016]andBolukbasietal.[2016].
LackofContextedistributionalapproachesaggregatethecontextsinwhichatermoccursin
a large corpus. e result is a word representation which is contextindependent. In reality, there
is no such thing as a context-independent meaning for a word. As argued by Firth [1935], “the
completemeaningofawordisalwayscontextual,andnostudyofmeaningapartfromcontextcanbe
takenseriously”.Anobviousmanifestationofthisisthecaseofpolysemy:somewordshaveobvious
multiplesenses:abankmayrefertoafinancialinstitutionortothesideofariver,astar mayan
abstract shape, a celebrity, an astronomical entity, and so on. Using a single vector for all forms
is problematic. In addition to the multiple senses problem, there are also much subtler context-
dependentvariationsinwordmeaning.135
C H A P T E R 11
Using Word Embeddings
InChapter10wediscussedalgorithmsforderivingwordvectorsfromlargequantitiesofunan-
notated text. Such vectors can be very useful as initialization for the word embedding matrices
in dedicated neural networks. ey also have practical uses on their own, outside the context of
neuralnetworks.ischapterdiscussessomeoftheseuses.
Notation In this chapter, we assume each word is assigned an integer index, and use symbols
such as w or w to refer to both a word and its index. E is then the row in E corresponding
i (cid:140)w(cid:141)
towordw.Wesometimesusew,w todenotethevectorscorrespondingtow andw .
i i
11.1 OBTAININGWORDVECTORS
Word-embeddingvectorsareeasytotrainfromacorpus,andefficientimplementationsoftrain-
ingalgorithmsareavailable.Moreover,onecanalsodownloadpre-trainedwordvectorsthatwere
trainedonverylargequantitiesoftext(bearinginmindthatdifferencesintrainingregimesand
underlyingcorporahaveastronginfluenceontheresultingrepresentations,andthattheavailable
pre-trainedrepresentationsmaynotbethebestchoicefortheparticularusecase).
As the time of this writing, efficient implementations of the W2V algorithms are
availableasastand-alonebinary¹aswellasin the GSpythonpackage.² Amodificationof
theW2Vbinarythatallowsusingarbitrarycontextsisalsoavailable.³Anefficientimple-
mentationoftheGloVemodelisavailableaswell.⁴Pre-trainedwordvectorsforEnglishcanbe
obtained from Google⁵ and Stanford⁶ as well as other sources. Pre-trained vectors in languages
otherthanEnglishcanbeobtainfromthePolyglotproject.⁷
11.2 WORDSIMILARITY
Givenpre-trainedwordembeddingvectors,themajoruseasidefromfeedingthemintoaneural
networkistocomputethesimilaritybetweentwowordsusingasimilarityfunctionovervectors
sim.u;v/.Acommonandeffectivechoiceforsimilaritybetweenvectorsisthecosinesimilarity,
¹https://code.google.com/archive/p/word2vec/
²https://radimrehurek.com/gensim/
³https://bitbucket.org/yoavgo/word2vecf
⁴http://nlp.stanford.edu/projects/glove/
⁵https://code.google.com/archive/p/word2vec/
⁶http://nlp.stanford.edu/projects/glove/
⁷http://polyglot.readthedocs.org136 11. USINGWORDEMBEDDINGS
correspondingtothecosineoftheanglebetweenthevectors:
u v
sim .u;v/ (cid:1) : (11.1)
cos
D u v
2 2
k k k k
Whenthe vectorsuandv areofunit-length( u v 1)thecosinesimilarityre-
2 2
k k Dk k D
ducestoadot-productsim cos.u;v/ Du (cid:1)v DP iu (cid:140)i(cid:141)v (cid:140)i(cid:141).Workingwithdot-productsisverycon-
venient computationally, and it is common to normalize the embeddings matrix such that each
row has unit length. From now on, we assume the embeddings matrix E is normalized in this
way.
11.3 WORDCLUSTERING
e word vectors can be easily clustered using clustering algorithms such as KMeans that are
defined over Euclidean spaces. e clusters can then be used as features in learning algorithms
that work with discrete features, or in other systems that require discrete symbols such as IR
indexingsystems.
11.4 FINDINGSIMILARWORDS
Withrow-normalizedembeddingsmatrixasdescribedabove,thecosinesimilaritybetweentwo
wordsw andw isgivenby:
1 2
sim .w ;w / E E : (11.2)
cos 1 2
D
(cid:140)w1(cid:141)
(cid:1)
(cid:140)w2(cid:141)
Weareofteninterestedinthek mostsimilarwordstoagivenword.Letw E bethe
(cid:140)w(cid:141)
D
vectorcorrespondingtowordw.esimilaritytoallotherwordscanbecomputedbythematrix-
vectormultiplications Ew.eresultsisavectorofsimilarities,wheres isthesimilarityof
(cid:140)i(cid:141)
D
wtotheithwordinthevocabulary(theithrowinE).ekmostsimilarwordscanbeextracted
byfindingtheindicescorrespondingtothek highestvaluesins.
In a optimized modern scientific computing library such as numpy,⁸ such matrix-vector
multiplicationisexecutedinmillisecondsforembeddingmatriceswithhundredsofthousandsof
vectors,allowingratherrapidcalculationofsimilarities.
Word similarities that result from distributional measures can be combined with other
formsofsimilarity.Forexample,wecandefineasimilaritymeasurethatisbasedonorthographic
similarity(wordsthatsharethesameletters).Byfilteringthelistoftop-kdistributional-similar
words to contain words that are also orthographically similar to the target word, we can find
spellingvariantsandcommontyposofthetargetword.
⁸http://www.numpy.org/11.5. ODD-ONEOUT 137
11.4.1 SIMILARITYTOAGROUPOFWORDS
Wemaybeinterestedinfindingthemostsimilarwordtoagroupofwords.isneedariseswhen
wehavealistofrelatedwords,andwanttoexpandit(forexample,wehavealistoffourcountries
and want to extend it with names of more countries, or we have a list of gene names, and want
findnamesofadditionalgenes).Anotherusecaseiswhenwewanttodirectthesimilaritytobe
toagivensenseofaword.Bycreatingalistofwordsthatarerelatedtothatsense,wecandirect
thesimilarityquerytowardthatsense.
erearemanywayofdefiningsimilarityofanitemtoagroup,herewetakethedefinition
tobetheaveragesimilaritytotheitemsinthegroup,i.e.,givenagroupofwordsw wedefine
1k
itssim ilar ai nty ksto tow lo inrd eaw rita ys ,: cs oi mm p.w ut; iw ng1 Wk th/ eD avk1 erP agk i eDc1 os si im necos s. imw; ilw ari it/ y.
from a group
ofW
words to all
otherwordscanbeagaindoneusingasinglematrix-vectormultiplication,thistimebetweenthe
embeddingmatrixandtheaveragewordvectorofthewordsinthegroup.evectorsinwhich
s sim.w;w /iscomputedby:
(cid:140)w(cid:141) 1k
D W
s E.w w ::: w /=k: (11.3)
1 2 k
D C C C
11.5 ODD-ONEOUT
Consideranodd-one-outquestioninwhichwearegivenalistofwordsandneedtofindtheone
that does not belong. is can be done by computing the similarity between each word to the
averagewordvectorofthegroup,andreturningtheleastsimilarword.
11.6 SHORTDOCUMENTSIMILARITY
Sometimesweareinterestedincomputingasimilaritybetweentwodocuments.Whilethebest
resultsarelikelytobeachievedusingdedicatedmodelssolutionsbasedonpre-trainedwordem-
beddingsareoftenverycompetitive,especiallywhendealingwithshortdocumentsassuchweb
queries, newspaper headlines, or tweets. e idea is to represent each document as a bag-of-
words,anddefinethesimilaritybetweenthedocumentstobethesumofthepairwisesimilarities
betweenthewordsinthedocuments.Formally,considertwodocumentsD w1;w1;:::;w1
1 D 1 2 m
andD w2;w2;:::;w2,anddefinethedocumentsimilarityas:
2 D 1 2 n
m n
sim .D ;D / XXcos.w1;w2/:
doc 1 2 D i j
i 1j 1
D D
Using basic linear algebra, it is straightforward to show that for normalized word vectors
thissimilarityfunctioncanbecomputedasthedotproductbetweenthecontinuous-bag-of-words138 11. USINGWORDEMBEDDINGS
representationsofthedocuments:
m ! 0 n 1
sim .D ;D / Xw1 Xw2 :
doc 1 2 D i (cid:1)@ jA
i 1 j 1
D D
Consider a document collection D , and let D be a matrix in which each row i is the
1k
W
continuous bag-of-words representation of document D . en the similarity between a new
i
documentD w andeachofthedocumentsinthecollectioncanbecomputedusingasingle
matrix-vector0 D prod10 uWn ct:s DD (cid:1)(cid:0)Pn
i
1w 0i(cid:1).
D
11.7 WORDANALOGIES
An interesting observation by Mikolov and colleagues [Mikolov et al., 2013a, Mikolov et al.,
2013] that greatly contributed to the popularity of word embeddings is that one can perform
“algebra” on the word vectors and get meaningful results. For example, for word embeddings
trainedusingW2V,onecouldtakethevectorofthewordking,subtractthewordman,add
thewordwomanandgetthattheclosestvectortotheresult(whenexcludingthewordsking,man,
andwoman)belongstothewordqueen.atis,invectorspacew w w w .
king man woman queen
(cid:0) C (cid:25)
Similar results are obtained for various other semantic relations, for example w w
France Paris
(cid:0) C
w w ,andthesameholdsformanyothercitiesandcountries.
London England
(cid:25)
ishasgivenrisetotheanalogysolvingtaskinwhichdifferentwordembeddingsareeval-
uatedontheirabilitytoansweranalogyquestionsoftheformman:woman king:? bysolving:
!
analogy.m w k (cid:139)/ argmax cos.v;k m w/: (11.4)
W ! W D (cid:0) C
v V m;w;k
2 nf g
LevyandGoldberg[2014]observethatfornormalizedvectors,solvingthemaximization
in Equation (11.4) is equivalent to solving Equation (11.5), that is, searching for a word that is
similartoking,similartoman,anddissimilartowoman:
analogy.m w k (cid:139)/ argmax cos.v;k/ cos.v;m/ cos.v;w/: (11.5)
W ! W D (cid:0) C
v V m;w;k
2 nf g
LevyandGoldbergrefertothismethodas3CA.emovefromarithmeticsbetween
wordsinvectorspacetoarithmeticsbetweenwordsimilaritieshelpstoexplaintosomeextentthe
ability of the word embeddings to “solve” analogies, as well as suggest which kinds of analogies
canberecoveredbythismethod.Italsohighlightsapossibledeficiencyofthe3CAanalogy
recoverymethod:becauseoftheadditivenatureoftheobjective,oneterminthesummationmay
dominatetheexpression,effectivelyignoringtheothers.AssuggestedbyLevyandGoldberg,this
canbealleviatedbychangingtoamultiplicativeobjective(3CM):
cos.v;k/cos.v;w/
analogy.m w k (cid:139)/ argmax : (11.6)
W ! W D cos.v;m/ (cid:15)
v V m;w;k C
2 nf g11.8. RETROFITTINGANDPROJECTIONS 139
While the analogy-recovery task is somewhat popular for evaluating word embeddings,
it is not clear what success on a benchmark of analogy tasks says about the quality of the word
embeddingsbeyondtheirsuitabilityforsolvingthisspecifictask.
11.8 RETROFITTINGANDPROJECTIONS
Moreoftenthannot,theresultingsimilaritiesdonotfullyreflectthesimilaritiesonehasinmind
fortheirapplication.Often,onecancomeupwithorhaveaccesstoarepresentativeandrelatively
largelistofwordpairsthatreflectsthedesiredsimilaritybetterthanthewordembeddings,buthas
worsecoverage.eretrofittingmethodofFaruquietal.[2015]allowstousesuchdatainorderto
improvethequalityofthewordembeddingsmatrix.Faruquietal.[2015]showtheeffectivenessof
theapproachbyusinginformationderivedfromWordNetandPPDB(Section6.2.1)toimprove
pre-trainedembeddingvectors.
emethodassumespre-trainedwordembeddingmatrixE aswellasagraphG thaten-
codes binary word to word similarities—nodes in the graph are words, and words are similar if
they are connected by an edge. Note that the graph representation is very general, and a list of
wordpairsthatareconsideredsimilareasilyfitswithintheframework.emethodworksbysolv-
inganoptimizationproblemsthatsearchesforanewwordembeddingsmatrixE whoserowsare
O
closebothtothecorrespondingrowsinE butalsototherowscorrespondingtotheirneighbors
G
inthegraph .Concretely,theoptimizationobjectiveis:
n 0 1
argminX (cid:11) E E 2 X (cid:12) E E 2 ; (11.7)
@
i
k
O(cid:140)wi(cid:141)
(cid:0)
(cid:140)wi(cid:141)
k C
ij
k
O(cid:140)wi(cid:141)
(cid:0)
O(cid:140)wj(cid:141)
k A
E
O
i D1 .wi;wj/ 2G
where (cid:11) and (cid:12) reflect the importance of a word being similar to itself or to another word. In
i ij
practice,(cid:11) aretypicallysetuniformlyto1,while(cid:12) issettotheinverseofthedegreeofw in
i ij i
thegraph(ifawordhasmanyneighbors,ithassmallerinfluenceoneachofthem).eapproach
worksquitewellinpractice.
Arelatedproblemiswhenonehastwoembeddingmatrices:onewithasmallvocabulary
ES R jV Sj(cid:2)d emb andanotheronewithalargevocabularyEL R jV Lj(cid:2)d emb thatweretrainedsep-
2 2
arately, and are hence incompatible. Perhaps the smaller vocabulary matrix was trained using a
moreexpensivealgorithm(possiblyaspartofalargerandmorecomplexnetwork),andthelarger
one was downloaded from the web. ere is some overlap in the vocabularies, and one is inter-
estedinusingwordvectorsfromthelargermatrixELforrepresentingwordsthatarenotavailable
inthesmalleroneES.Onecanthenbridgethegapbetweenthetwoembeddingspacesusinga
linearprojection⁹[Kirosetal.,2015,Mikolovetal.,2013].etrainingobjectiveissearchingfor
a good projection matrix M Rd emb d emb that will map rows in EL such that they are close to
(cid:2)
2
⁹Ofcourse,forthistoworkoneneedstoassumealinearrelationbetweenthetwospaces.elinearprojectionmethodoften
workswellinpractice.140 11. USINGWORDEMBEDDINGS
correspondingrowsinES bysolvingthefollowingoptimizationproblem:
argmin X EL M ES : (11.8)
k (cid:140)w(cid:141)(cid:1) (cid:0) (cid:140)w(cid:141)k
M
w V V
2 S\ L
elearnedmatrixcanthenbeusedtoprojectalsotherowsofELthatdonothavecorresponding
rowsinES.isapproachwassuccessfullyusedbyKirosetal.[2015]toincreasethevocabulary
sizeofanLSTM-basedsentenceencoder(thesentenceencodingmodelofKirosetal.[2015]is
discussedinSection17.3).
Another cute (if somewhat less robust) application of the projection approach was taken
byMikolovetal.[2013]wholearnedmatricestoprojectbetweenembeddingvectorstrainedon
language A (say English) to embedding vectors trained on language B (say Spanish) based on a
seedlistofknownword-wordtranslationbetweenthelanguages.
11.9 PRACTICALITIESANDPITFALLS
Whileoff-the-shelf,pre-trainedwordembeddingscanbedownloadedandused,itisadvisedto
not just blindly download word embeddings and treat them as a black box. Choices such as the
sourceofthetrainingcorpus(butnotnecessarilyitssize:largerisnotalwaysbetter,andasmaller
butcleaner,orsmallerbutmoredomain-focusedcorpora,areoftenmoreeffectiveforagivenuse
case), the contexts that were used for defining the distributional similarities, and many hyper-
parameters of the learning can greatly influence the results. In presence of an annotated test set
forthesimilaritytaskonecaresabout,itisbesttoexperimentwithseveralsettingandchoosethe
setupthatworksbestonadevelopmentset.Fordiscussiononthepossiblehyper-parametersand
howtheymayaffecttheresultingsimilarities,seetheworkofLevyetal.[2015].
Whenusingoff-the-shelfembeddingvectors,itisbettertousethesametokenizationand
textnormalizationschemesthatwereusedwhenderivingthecorpus.
Finally, the similarities induced by word vectors are based on distributional signals, and
thereforesusceptibletoallthelimitationsofdistributionalsimilaritymethodsdescribedinSec-
tion10.7.Oneshouldbeawareoftheselimitationswhenusingwordvectors.141
C H A P T E R 12
Case Study: A Feed-forward
Architecture for Sentence
Meaning Inference
InSection 11.6 weintroducedthe sum of pairwise wordsimilarities as a strongbaseline for the
short document similarity task. Given two sentences, the first one with words w1;:::;w1 and
1 ‘1
thesecondonewithwordsw2;:::;w2 ,eachwordisassociatedwithacorrespondingpre-trained
1 ‘2
wordvectorw1 ,w2 ,andthesimilaritybetweenthedocumentsisgivenby:
1 W‘1 1 W‘2
‘1 ‘2
XXsim(cid:16)w1;w2(cid:17):
i j
i 1j 1
D D
While this is a strong baseline, it is also completely unsupervised. Here, we show how a
document similarity score can be greatly improved if we have a source of training data. We will
followthenetworkpresentedbyParikhetal.[2016]fortheStanfordNaturalLanguageInference
(SNLI) semantic inference task. Other than providing a strong model for the SNLI task, this
modelalsodemonstrateshowthebasicnetworkcomponentsdescribedsofarcanbecombinedin
variouslayers,resultinginacomplexandpowerfulnetworkthatistrainedjointlyforatask.
12.1 NATURALLANGUAGEINFERENCEANDTHESNLI
DATASET
In the natural language inference task, also called recognizing textual entailment (RTE), you are
given two texts, s and s , and need to decide if s entails s (that is, can you infer s from s ),
1 2 1 2 2 1
contradictsit(theycannotbothbetrue),orifthetextsareneutral(thesecondoneneitherentails
norcontradictsthefirst).ExamplesentencesforthedifferentconditionsaregiveninTable12.1.
eentailmenttaskwasintroducedbyDaganandGlickman[2004],andsubsequentlyes-
tablishedthroughaseriesofbenchmarksknownasthePASCALRTEChallenges[Daganetal.,
2005].etaskisverychallenging,¹andsolvingitperfectlyentailshumanlevelunderstandingof
¹eSNLIdatasetdescribedherefocusesondescriptionsofscenesthatappearinimages,andiseasierthanthegeneraland
un-restrictedRTEtask,whichmayrequirerathercomplexinferencestepsinordertosolve.Anexampleofanentailingpairin
theun-restrictedRTEtaskisAbouttwoweeksbeforethetrialstarted,IwasinShapiro’sofficeinCenturyCity Shapiroworked
)
inCenturyCity.142 12. CASESTUDY:AFEED-FORWARDARCHITECTUREFORSENTENCE
Table12.1: eNaturalLanguageInference(TextualEntailment)Task.eexamplesaretakenfrom
thedevelopmentsetoftheSNLIdataset.
Two men on bicycles competing in a race.
Entail People are riding bikes.
Neutral Men are riding bicycles on the street.
Contradict A few people are catching fi sh.
Two doctors perform surgery on patient.
Entail Doctors are performing surgery.
Neutral Two doctors are performing surgery on a man.
Contradict Two surgeons are having lunch.
language.Forin-depthdiscussiononthetaskandapproachestoitssolutionthatdonotinvolve
neural networks, see the book by Dagan, Roth, Sammons, and Zanzotto in this series [Dagan
etal.,2013].
SNLIisalargedatasetcreatedbyBowmanetal.[2015],containing570khuman-written
sentence pairs, each pair manually categorized as entailing, contradicting, or neutral. e sen-
tenceswerecreatedbypresentingimagecaptionstoannotators,andaskingthem,withoutseeing
theimage,towriteacaptionthatisdefinitelyatruedescriptionoftheimage(entail),acaption
that is might be a true description of the image (neutral), and a caption that is a definitely false
descriptionoftheimage(contradict).Aftercollecting570ksentencepairsthisway,10%ofthem
were further validated by presenting sentence pairs to different annotators and asking them to
categorizethepairintoentailing,neutral,orcontradicting.evalidatedsentencesarethenused
forthetestandvalidationsets.eexamplesinTable12.1arefromtheSNLIdataset.
WhilesimplerthanthepreviousRTEchallengedatasets,itisalsomuchlarger,andstillnot
trivialtosolve(inparticularfordistinguishingtheentailingfromtheneutralevents).eSNLI
dataset is a popular dataset for assessing meaning inference models. Notice that the task goes
beyondmerepairwisewordsimilarity:forexample,considerthesecondsentenceinTable12.1:
the neutral sentence is much more similar (in terms of average word similarity) to the original
one than the entailed sentence. We need the ability to highlight some similarities, degrade the
strengthofothers,andalsotounderstandwhichkindofsimilaritiesaremeaningpreserving(i.e.,
goingfrommantopatientinthecontextofasurgery),andwhichaddnewinformation(i.e.,going
frompatient toman).enetworkarchitectureisdesignedtofacilitatethiskindofreasoning.
12.2 ATEXTUALSIMILARITYNETWORK
e network will work in several stages. In the first stage,our goal is to compute pairwise word
similarities that are more suitable for the task. e similarity function for two word vectors is12.2. ATEXTUALSIMILARITYNETWORK 143
definedtobe:
sim.w ;w / MLPtransform.w / MLPtransform.w / (12.1)
1 2 1 2
D (cid:1)
MLPtransform.x/ Rds w 1;w
2
Rd emb:
2 2
at is, we first transform each word vector by use of a trained nonlinear transformation,
andthentakethedot-productofthetransformedvectors.
Eachwordinsentenceacanbesimilartoseveralwordsinsentenceb,andviceversa.For
each word wa in sentence a we compute a ‘ -dimensional vector of its similarities to words in
i b
sentence b, normalized via softmax so that all similarities are positive and sum to one. is is
calledthealignmentvector fortheword:
(cid:11)a softmax.sim.wa;wb/;:::;sim.wa;wb //: (12.2)
i D i 1 i ‘b
Wesimilarlycomputeanalignmentvectorforeachwordinb
(cid:11)b softmax.sim.wa;wb/;:::;sim.wa ;wb//
i D 1 i ‘a i
(cid:11)a N ‘b (cid:11)b N ‘a:
i 2 C i 2 C
Foreverywordwa wecomputeavectorwb composedofaweighted-sumofthewordsin
i Ni
bthatarealignedtowa,andsimilarlyforeverywordwb:
i j
‘b
wb X(cid:11)a wb
Ni D i(cid:140)j(cid:141) j
j 1
D (12.3)
‘a
wa X(cid:11)b wa:
Nj D i(cid:140)j(cid:141) i
i 1
D
A vector wb captures the weighted mixture of words in sentence b that are triggered by the ith
Ni
wordinsentencea.
Such weighted sum representations of a sequence of vectors, where the weights are com-
puted by a softmax over scores such as the one in Equation (12.2), are often referred to as an
attention mechanism. e name comes from the fact that the weights reflect how important is
eachiteminthetargetsequencetothegivensourceitem—howmuchattentionshouldbegiven
toeachoftheitemsinthetargetsequencewithrespecttothesourceitem.Wewilldiscussatten-
tioninmoredetailsinChapter17,whendiscussingconditioned-generationmodels.
esimilaritybetweenwaandthecorrespondingtriggeredmixturewbinsentencebisnot
i Ni
necessarilyrelevantfortheNLItask.Weattempttotransformeachsuchpairintoarepresentation144 12. CASESTUDY:AFEED-FORWARDARCHITECTUREFORSENTENCE
vectorvathatfocusesontheimportantinformationforthetask.isisdoneusinganotherfeed-
i
forwardnetwork:
va MLPpair.(cid:140)wa wb(cid:141)/
i D iI Ni
(12.4)
vb MLPpair.(cid:140)wb wa(cid:141)/:
j D jI Nj
NotethatunlikethesimilarityfunctioninEquation(12.1)thatconsideredeachtermindividually,
herethefunctioncanhandlebothtermsdifferently.
Finally,wesumtheresultingvectorsandpassthemintoafinalMLPclassifierforpredicting
therelationbetweenthetwosentences(entail,contradict,orneutral):
va Xva
D i
i
vb Xvb (12.5)
D j
j
y MLPdecide.(cid:140)va vb(cid:141)/:
O D I
IntheworkofParikhetal.[2016],alltheMLPshavetwohiddenlayersofsize200,anda
ReLUactivationfunction.eentireprocessiscapturedinthesamecomputationgraph,andthe
network is trained end-to-end using the cross-entropy loss. e pre-trained word embeddings
themselves were not changed with the rest of the network, relying on MLPtransform to do the
neededadaptation.Asofthetimeofthiswriting,thisarchitectureisthebestperformingnetwork
ontheSNLIdataset.
Tosummarizethearchitecture,thetransformnetworklearnsasimilarityfunctionforword-
level alignment. It transforms each word into a space that preserves important word-level sim-
ilarities. After the transform network, each word vector is similar to other words that are likely
torefertothesameentityorthesameevent.egoalofthisnetworkistofindwordsthatmay
contributetotheentailment.Wegetalignmentsinbothdirections:fromeachwordinatomul-
tiplewordsinb,andfromeachwordinbtomultiplewordsina.ealignmentsaresoft,andare
manifestedbyweightedgroupmembershipinsteadofbyharddecisions,soawordcanparticipate
inmanypairsofsimilarities.isnetworkislikelytoputmenandpeoplenexttoeachother,men
and two next to each other and man and patient next to each other, and likewise for inflected
formsperformandperforming.
epairnetworkthenlooksateachalignedpair(word+group)usingaweighted-CBOW
representation,andextractsinformationrelevanttothepair.Isthispairusefulfortheentailment
predictiontask?It alsolooksat sentenceeach componentofthe paircame from,and willlikely
learnthatpatient andmanareentailinginonedirectionandnottheother.
Finally,thedecidenetworklooksattheaggregateddatafromthewordpairs,andcomesup
withadecisionbasedonalltheevidence.Wehavethreestagesofreasoning:firstonerecoversweak
localevidenceintermsofsimilarityalignment;thesecondonelooksatweightedmulti-wordunits
andalsoaddsdirectionality;andthethirdintegratesallthelocalevidenceintoaglobaldecision.12.2. ATEXTUALSIMILARITYNETWORK 145
edetailsofthenetworkaretunedforthisparticulartaskanddataset,anditisnotclear
if they will generalize to other settings. e idea of this chapter was not to introduce a specific
networkarchitecture,butrathertodemonstratethatcomplexarchitecturescanbedesigned,and
thatitissometimesworththeefforttodoso.Anewcomponentintroducedinthischapterthat
is worth noting is the use of the softalignment weights (cid:11)a (also sometimes called attention), in
i
ordertocomputeaweightedsumofelementswb [Equation(12.3)].Wewillencounterthisidea
Ni
againwhendiscussingattention-basedconditionedgenerationwithRNNsinChapter17.PART III
Specialized Architectures149
Inthepreviouschapters,we’vediscussedsupervisedlearningandfeed-forwardneuralnet-
works,andhowtheycanbeappliedtolanguagetasks.efeed-forwardneuralnetworksarefor
the most part general-purpose classification architectures—nothing in them is tailored specifi-
cally for language data or sequences. Indeed, we mostly structured the language tasks to fit into
theMLPframework.
Inthefollowingchapters,wewillexploresomeneuralarchitecturesthataremorespecial-
izedfordealingwithlanguagedata.Inparticular,wewilldiscuss1Dconvolutional-and-pooling
architectures (CNNs), and recurrent neural networks (RNNs). CNNs are neural architectures
that are specialized at identifying informative ngrams and gappy-ngrams in a sequence of text,
regardlessoftheirposition,butwhiletakinglocalorderingpatternsintoaccount.RNNsareneu-
ral architectures that are designed to capture subtle patterns and regularities in sequences, and
that allow modeling non-markovian dependencies looking at “infinite windows” around a fo-
cus word, while zooming-in on informative sequential patterns in that window. Finally, we will
discusssequence-generationmodelsandconditionedgeneration.
FeatureExtraction eCNNandRNNarchitecturesexploredinthispartofthebookarepri-
marily used as featureextractors. A CNN or an RNN network are not a standalone component,
but rather each such network produces a vector (or a sequence of vectors) that are then fed into
furtherpartsofthenetworkthatwilleventuallyleadtopredictions.enetworkistrainedend-
to-end(thepredictingpartandtheconvolutional/recurrentarchitecturesaretrainedjointly)such
thatthevectorsresultingfromtheconvolutionalorrecurrentpartofthenetworkwillcapturethe
aspects of the input that are useful for the given prediction task. In the following chapters, we
introducefeatureextractorsthatarebasedontheCNNandtheRNNarchitectures.Asthetime
ofthiswriting,RNN-basedfeatureextractorsaremoreestablishedthanCNNsasfeatureextrac-
torsfortext-basedapplications.However,thedifferentarchitectureshavedifferentstrengthsand
weaknesses,andthebalancebetweenthemmayshiftinthefuture.Bothareworthknowing,and
hybridapproachesarealsolikelytobecomepopular.Chapters16and17discusstheintegration
ofRNN-basedfeatureextractorsindifferentNLPpredictionandgenerationarchitectures.Large
partsofthediscussioninthesechaptersareapplicablealsotoconvolutionalnetworks.
CNNsandRNNsasLegoBricks WhenlearningabouttheCNNandRNNarchitectures,itis
usefultothinkaboutthemas“LegoBricks,”thatcanbemixedandmatchedtocreateadesired
structureandtoachieveadesiredbehavior.
isLego-bricks-likemixing-and-matchingisfacilitatedbythecomputation-graphmech-
anism and gradient-based optimization. It allows treating network architectures such as MLPs,
CNNs and RNNs as components, or blocks, that can be mixed and matched to create larger
andlargerstructures—onejustneedstomakesurethatthatinputandoutputdimensionsofthe
differentcomponentsmatch—andthecomputationgraphandgradient-basedtrainingwilltake
careoftherest.150
is allows us to create large and elaborate network structures, with multiple layers of
MLPs, CNNs and RNNs feeding into each other, and training the entire network in an end-
to-endfashion.Severalexamplesareexploredinlaterchapters,butmanyothersarepossible,and
differenttasksmaybenefitfromdifferentarchitectures.Whenlearningaboutanewarchitecture,
don’tthink“whichexistingcomponentdoesitreplace?”or“howdoIuseittosolveatask?”but
rather “howcan I integrate it into my arsenal of building blocks, and combineit with the other
componentsinordertoachieveadesiredresult?”.151
C H A P T E R 13
Ngram Detectors:
Convolutional Neural Networks
Sometimes we are interested in making predictions based on ordered sets of items (e.g., the se-
quence of words in a sentence, the sequence of sentences in a document, and so on). Consider,
for example, predicting the sentiment (positive, negative, or neutral) of sentences such as the
following.
• PartofthecharmofSatinRougeisthatitavoidstheobviouswithhumorandlightness.
• Still,thisflickisfunandhosttosometrulyexcellentsequences.
Some of the sentence words are very informative of the sentiment (charm, fun, excellent) other
wordsarelessinformative(Still,host,flick,lightness,obvious,avoids)andtoagoodapproximation,
aninformativeclueisinformativeregardlessofitspositioninthesentence.Wewouldliketofeed
all of the sentence words into a learner, and let the training process figure out the important
clues. One possible solution is feeding a CBOW representation into a fully connected network
such as an MLP. However, a downside of the CBOW approach is that it ignores the ordering
informationcompletely,assigningthesentences“itwasnotgood,itwasactuallyquitebad”and“it
wasnotbad,itwasactuallyquitegood”theexactsamerepresentation.Whiletheglobalpositionsof
theindicators“notgood”and“notbad”donotmatterfortheclassificationtask,thelocalordering
ofthewords(thattheword“not”appearsrightbeforetheword“bad”)isveryimportant.Similarly,
inthecorpus-basedexampleMontiaspumpsalotofenergyintohisnuancednarative,andsurrounds
himselfwithacastofquirky—butnotstereotyped—streetcharacters,thereisabigdifferencebetween
“notstereotyped”(positiveindicator)and“notnuanced”(negativeindicator).Whiletheexamples
abovearesimplecasesofnegation,somepatternsarenotasobvious,e.g.,“avoidstheobvious”vs.
“obvious”orvs.“avoidsthecharm”inthefirstexample.Inshort,lookingatngramsismuchmore
informativethanlookingatabag-of-words.
A naive approach would suggest embedding word-pairs (bi-grams) or word-triplets (tri-
grams) rather than words, and building a CBOW over the embedded ngrams. While such an
architecture is indeed quite effective, it will result huge embedding matrices, will not scale for
longerngrams,andwillsufferfromdatasparsityproblemsasitdoesnotsharestatisticalstrength
betweendifferentngrams(theembeddingof“quitegood”and“verygood”arecompletelyinde-
pendentofoneanother,soifthelearnersawonlyoneofthemduringtraining,itwillnotbeable
todeduceanythingabouttheotherbasedonitscomponentwords).152 13. NGRAMDETECTORS:CONVOLUTIONALNEURALNETWORKS
e CNN architecture is chapter introduces the convolution-and-pooling (also called con-
volutional neural networks, or CNNs) architecture, which is tailored to this modeling problem.
Aconvolutionalneuralnetworkisdesignedtoidentifyindicativelocalpredictorsinalargestruc-
ture,andtocombinethemtoproduceafixedsizevectorrepresentationofthestructure,capturing
thelocalaspectsthataremostinformativeforthepredictiontaskathand.I.e.,theconvolutional
architecture will identify ngrams that are predictive for the task at hand, without the need to
pre-specifyanembedding vectorforeach possiblengram. (InSection13.2,wediscussan alter-
nativemethodthatallowsworkingwithunboundedngramvocabularieswhilekeepingabounded
size embedding matrix). e convolutional architecture also allows to share predictive behavior
between ngrams that share similar components, even if the exact ngram was never seen at test
time.
e convolutional architecture could be expanded into a hierarchy of convolution layers,
each one effectively looking at a longer range of ngrams in the sentence. is also allows the
modeltobesensitivetosomenon-contiguousngrams.isisdiscussedinSection13.3.
As discussed in the opening section of this part of the book, the CNN is in essence a
feature-extracting architecture. It does not constitute a standalone, useful network on its own,
but rather is meant to be integrated into a larger network, and to be trained to work in tandem
withitinordertoproduceanendresult.eCNNlayer’sresponsibilityistoextractmeaningful
sub-structuresthatareusefulfortheoverallpredictiontaskathand.
HistoryandTerminology Convolution-and-poolingarchitectures[LeCunandBengio,1995]
evolved in the neural networks vision community, where they showed great success as object
detectors—recognizing an object from a predefined category (“cat,” “bicycles”) regardless of its
positionintheimage[Krizhevskyetal.,2012].Whenappliedtoimages,thearchitectureisus-
ing2D(grid)convolutions.Whenappliedtotext,wearemainlyconcernedwith1D(sequence)
convolutions.ConvolutionalnetworkswereintroducedtotheNLPcommunityinthepioneering
workofCollobertetal.[2011]whousedthemforsemantic-rolelabeling,andlaterbyKalchbren-
neretal.[2014]andKim[2014]whousedthemforsentimentandquestion-typeclassification.
Becauseoftheiroriginsinthecomputer-visioncommunity,alotoftheterminologyaround
convolutionalneuralnetworksisborrowedfromcomputervisionandsignalprocessing,including
terms such as filter, channel, and receptive-field which are often used also in the context of text
processing.Wewillmentionthesetermswhenintroducingthecorrespondingconcepts.
13.1 BASICCONVOLUTION+POOLING
emainideabehindaconvolutionandpoolingarchitectureforlanguagetasksistoapplyanon-
linear(learned)functionovereachinstantiationofak-wordslidingwindowoverthesentence.¹
isfunction(alsocalled“filter”)transformsawindowofkwordsintoascalarvalue.Severalsuch
filterscanbeapplied,resultingin‘dimensionalvector(eachdimensioncorrespondingtoafilter)
¹ewindow-size,k,issometimesreferredtoasthereceptivefieldoftheconvolution.13.1. BASICCONVOLUTION+POOLING 153
thatcapturesimportantpropertiesofthewordsinthewindow.en,a“pooling”operationisused
to combine the vectors resulting from the different windows into a single ‘-dimensional vector,
by taking the max or the average value observed in each of the ‘ dimensions over the different
windows.eintentionistofocusonthemostimportant“features”inthesentence,regardlessof
theirlocation—eachfilterextractsadifferentindicatorfromthewindow,andthepoolingopera-
tionzoomsinontheimportantindicators.eresulting‘-dimensionalvectoristhenfedfurther
intoanetworkthatisusedforprediction.egradientsthatarepropagatedbackfromthenet-
work’s loss during the training process are used to tune the parameters of the filter function to
highlight the aspects of the data that are important for the task the network is trained for. In-
tuitively, when the sliding window of size k is run over a sequence, the filter function learns to
identifyinformativekgrams.Figure13.2illustratesanapplicationofaconvolution-and-pooling
networkoverasentence.
13.1.1 1DCONVOLUTIONSOVERTEXT
Webeginbyfocusingontheone-dimensionalconvolutionoperation.²enextsectionwillfocus
onpooling.
Consider a sequence of words w w ;:::;w , each with their corresponding d
1n 1 n emb
W D
dimensional word embedding E w . A 1D convolution of width-k works by moving a
(cid:140)wi(cid:141)
D
i
sliding-window of size k over the sentence, and applying the same “filter” to each window
in the sequence, where a filter is a dot-product with a weight vector u, which is often fol-
lowed by a nonlinear activation function. Define the operator .w / to be the con-
i i k 1
(cid:8) W C (cid:0)
catenation of the vectors w ;:::;w . e concatenated vector of the ith window is then
i i k 1
x
i
.w
i i k
1/ (cid:140)w
i
w
i 1
:::C w(cid:0)
i k
1(cid:141),x
i
Rk (cid:1)d emb.
D(cid:8) W C (cid:0) D I C I I C (cid:0) 2
Wethenapplythefiltertoeachwindow-vector,resultingscalarvaluesp :
i
p g.x u/ (13.1)
i i
D (cid:1)
x .w / (13.2)
i i i k 1
D(cid:8) W C (cid:0)
p i R x i Rk (cid:1)d emb u Rk (cid:1)d emb;
2 2 2
whereg isanonlinearactivation.
Itiscustomarytouse‘differentfilters,u ;:::;u ,whichcanbearrangedintoamatrixU,
1 ‘
andabiasvectorbisoftenadded:
p g.x U b/ (13.3)
i i
D (cid:1) C
p i 2R‘ x i 2Rk (cid:1)d emb U 2Rk (cid:1)d emb (cid:2)‘ b 2R‘:
Eachvectorp isacollectionof‘valuesthatrepresent(orsummarize)theithwindow.Ideally,
i
eachdimensioncapturesadifferentkindofindicativeinformation.
²1Dherereferstoaconvolutionoperatingover1-dimensionalinputssuchassequences,asopposedto2Dconvolutionswhich
areappliedtoimages.154 13. NGRAMDETECTORS:CONVOLUTIONALNEURALNETWORKS
Narrowvs.WideConvolutions Howmanyvectorsp dowehave?Forasentenceoflengthn
i
with a window of size k, there are n k 1 positions in which to start the sequence, and we
(cid:0) C
getn k 1vectorsp .isiscalledanarrowconvolution.Analternativeistopadthe
1n k 1
(cid:0) C W (cid:0) C
sentencewithk 1padding-wordstoeachside,resultinginn k 1vectorsp .isis
1n k 1
(cid:0) C C W C C
calledawideconvolution[Kalchbrenneretal.,2014].Weusemtodenotethenumberofresulting
vectors.
AnAlternativeFormulationofConvolutions Inourdescriptionofconvolutionsoverasequence
ofnitemsw eachitemisassociatedwithad-dimensionalvector,andthevectorareconcate-
1n
W
nated into a large 1 d n sentence vector. e convolution network with a window of size k
(cid:2) (cid:1)
and‘outputvaluesisthenbasedonak d ‘matrix.ismatrixisappliedtosegmentsofthe
(cid:1) (cid:2)
1 d n sentence matrix that correspond to k-word windows. Each such multiplication results
(cid:2) (cid:1)
in ‘ values. Each of these k values can be thought of as the result of a dot product between a
k d 1vector(arowinthematrix)andasentencesegment.
(cid:1) (cid:2)
Another(equivalent)formulationthatisoftenusedintheliteratureisoneinwhichthen
vectorsarestackedontopofeachother,resultinginann d sentencematrix.econvolution
(cid:2)
operation is then performed by sliding ‘ different k d matrices (called “kernels” or “filters”)
(cid:2)
overthesentencematrix,andperformingamatrixconvolutionbetweeneachkernelandthecor-
respondingsentence-matrixsegment.ematrixconvolutionoperationbetweentwomatricesis
definedasperformingelement-wisemultiplicationofthetwomatrices,andsummingtheresults.
Each of the ‘ sentence-kernel convolution operations produces a single value, for a total of ‘
values.Itiseasytoconvinceoneselfthatthetwoapproachesareindeedequivalent,byobserving
thateachkernelcorrespondstoarowinthek d ‘matrix,andtheconvolutionwithakernel
(cid:1) (cid:2)
correspondstoadot-productwithamatrixrow.
Figure13.1shownarrowandwideconvolutionsinthetwonotations.
the
actual
actual
service
service
was
was
not
not
very
very
good
*PAD th*
e
*PAD* the
the actual
actual
actual service
service
service was
was
was not
not
not very
very
very good
the actual service was not very good good good *PAD*
*PAD*
(a) (b)
Figure13.1: einputsandoutputsofanarrowandawideconvolutioninthevector-concatenation
and the vector-stacking notations. (a) A narrow convolution with a window of size k 2 and 3-
D
dimensionaloutput(‘ 3),inthevector-concatenationnotation.(b)Awideconvolutionwithawin-
D
dowofsizek 2,a3-dimensionaloutput(‘ 3),inthevector-stackingnotation.
D D13.1. BASICCONVOLUTION+POOLING 155
Channels Incomputervision,apictureisrepresentedasacollectionofpixels,eachrepresent-
ing the color intensity of a particular point. When using an RGB color scheme, each pixel is a
combination of three intensity values—one for each of the Red, Green, and Blue components.
ese are then stored in three different matrices. Each matrix provides a different “view” of the
image, and is referred to as a Channel. When applying a convolution to an image in computer
vision,itiscommontoapplyadifferentsetoffilterstoeachchannel,andthencombinethethree
resulting vectors into a single vector. Taking the different-views-of-the-data metaphor, we can
havemultiplechannelsintextprocessingaswell.Forexample,onechannelwillbethesequence
ofwords,whileanotherchannelisthesequenceofcorrespondingPOStags.Applyingtheconvo-
lutionoverthewordswillresultinmvectorspw ,andapplyingitoverthePOS-tagswillresult
1m
in m vectors pt . ese twoviews can then beWcombined either by summationp pw pt
1m i D i C i
orbyconcatenatWionp (cid:140)pw pt(cid:141).
i D i I i
Tosummarize emainideabehindtheconvolutionlayeristoapplythesameparameterized
functionoverallk-gramsinthesequence.iscreatesasequenceofmvectors,eachrepresenting
a particular k-gram in the sequence. e representation is sensitive to the identity and order of
thewordswithinak-gram,butthesamerepresentationwillbeextractedforak-gramregardless
ofitspositionwithinthesequence.
13.1.2 VECTORPOOLING
Applyingtheconvolutionoverthetextresultsinmvectorsp ,eachp R‘.esevectorsare
1m i
thencombined(pooled)intoasinglevectorc R‘ representiW ngtheentire2 sequence.Ideally,the
2
vectorc willcapturetheessenceoftheimportantinformationinthesequence.enatureofthe
importantinformationthatneedstobeencodedinthevectorc istaskdependent.Forexample,
if we are performing sentiment classification, the essence are informative ngrams that indicate
sentiment,andifweareperformingtopic-classification,theessenceareinformativengramsthat
indicateaparticulartopic.
During training, the vector c is fed into downstream network layers (i.e., an MLP), cul-
minatinginanoutputlayerwhichisusedforprediction.³etrainingprocedureofthenetwork
calculates the loss with respect to the prediction task, and the error gradients are propagated all
the way back through the pooling and convolution layers, as well as the embedding layers. e
trainingprocesstunestheconvolutionmatrixU,thebiasvectorb,thedownstreamnetwork,and
potentiallyalsotheembeddingsmatrixE suchthatthevectorc resultingfromtheconvolution
andpoolingprocessindeedencodesinformationrelevanttothetaskathand.⁴
³einputtothedownstreamnetworkcanbeeithercitself,oracombinationofcandothervectors.
⁴Besidesbeingusefulforprediction,aby-productofthetrainingprocedureisasetofparametersW,B,andembeddingsE
thatcanbeusedinaconvolutionandpoolingarchitecturetoencodearbitrarylengthsentencesintofixed-sizevectors,such
thatsentencesthatsharethesamekindofpredictiveinformationwillbeclosetoeachother.156 13. NGRAMDETECTORS:CONVOLUTIONALNEURALNETWORKS
Max-pooling emostcommonpoolingoperationismaxpooling,takingthemaximumvalue
acrosseachdimension.
c max p j (cid:140)1;‘(cid:141); (13.4)
(cid:140)j(cid:141)
D1<i m
i(cid:140)j(cid:141)
8 2
(cid:20)
p denotes the jth component of p . e effect of the max-pooling operation is to get the
i(cid:140)j(cid:141) i
most salient information across window positions. Ideally, each dimension will “specialize” in a
particularsortofpredictors,andmaxoperationwillpickonthemostimportantpredictorofeach
type.
Figure 13.2 provides an illustration of the convolution and pooling process with a max-
poolingoperation.
6 × 3
W
max
the quick brown fox jumped over the lazy dog
the quick brown MUL+tanh
quick brown fox MUL+tanh
brown fox jumped MUL+tanh
fox jumped over MUL+tanh
jumped over the MUL+tanh
over the lazy MUL+tanh
the lazy dog MUL+tanh
convolution pooling
Figure13.2: 1D convolution+pooling over the sentence “the quick brown fox jumped over the lazy
dog.” is is a narrow convolution (no padding is added to the sentence) with a window size of 3.
Each word is translated to a 2-dim embedding vector(not shown).e embedding vectorsarethen
concatenated, resulting in 6-dim window representations. Each of the seven windows is transfered
through a 6 3 filter (linear transformation followed by element-wise tanh), resulting in seven 3-
(cid:2)
dimensional filtered representations. en, a max-pooling operation is applied, taking the max over
eachdimension,resultinginafinal3-dimensionalpooledvector.
Average Pooling e second most common pooling type being average-pooling—taking the
averagevalueofeachindexinsteadofthemax:
m
1
c Xp : (13.5)
i
D m
i 1
D13.1. BASICCONVOLUTION+POOLING 157
Oneviewofaverage-poolingisthatoftakingacontinuousbag-of-words(CBOW)ofthek-gram
representationsresultingfromtheconvolutionsratherthanfromthesentencewords.
K-max Pooling Another variation, introduced by Kalchbrenner et al. [2014] is k-max pooling
operation,inwhichthetopk valuesineachdimensionareretainedinsteadofonlythebestone,
whilepreservingtheorderinwhichtheyappearedinthetext.⁵Forexample,considerthefollowing
matrix:
1 2 3
2 3
9 6 5
6 7
62 3 17:
6 7
67 8 17
4 5
3 4 1
A 1-max pooling over the column vectors will result in (cid:2)9 8 5(cid:3), while a 2-max pool-
(cid:20)9 6 3(cid:21)
ing will result in the following matrix: whose rows will then be concatenated to
7 8 5
(cid:2)9 6 3 7 8 5(cid:3).
e k-max pooling operation makes it possible to pool the k most active indicators that
may be a number of positions apart; it preserves the order of the features, but is insensitive to
theirspecificpositions.Itcanalsodiscernmorefinelythenumberoftimesthefeatureishighly
activated[Kalchbrenneretal.,2014].
DynamicPooling Ratherthanperformingasinglepoolingoperationovertheentiresequence,
we may want to retain some positional information based on our domain understanding of the
predictionproblemathand.Tothisend,wecansplitthevectorsp intor distinctgroups,apply
i
thepoolingseparatelyoneachgroup,andthenconcatenatether resulting‘-dimensionalvectors
c ;:::;c . e division of the p s into groups is performed based on domain knowledge. For
1 r i
example,wemayconjecturethatwordsappearingearlyinthesentencearemoreindicativethan
words appearing late. We can then split the sequence into r equally sized regions, applying a
separatemax-poolingtoeachregion.Forexample,JohnsonandZhang[2015]foundthatwhen
classifyingdocumentsintotopics,itisusefultohave20average-poolingregions,clearlyseparating
theinitialsentences(wherethetopicisusuallyintroduced)fromlaterones,whileforasentiment
classificationtaskasinglemax-poolingoperationovertheentiresentencewasoptimal(suggesting
that one or two very strong signals are enough to determine the sentiment, regardless of the
positioninthesentence).
Similarly, in a relation extraction kind of task we may be given two words and asked to
determine the relation between them. We could argue that the words before the first word, the
wordsafterthesecondword,andthewordsbetweenthemprovidethreedifferentkindsofinfor-
⁵Inthischapter,weusektodenotethewindow-sizeoftheconvolution.ekink-maxpoolingisadifferent,andunrelated,
value.Weusetheletterkforconsistencywiththeliterature.158 13. NGRAMDETECTORS:CONVOLUTIONALNEURALNETWORKS
mation[Chenetal.,2015].Wecanthussplitthep vectorsaccordingly,poolingseparatelythe
i
windowsresultingfromeachgroup.
13.1.3 VARIATIONS
Rather than a single convolutional layer, several convolutional layers may be applied in parallel.
Forexample,wemayhavefourdifferentconvolutionallayers,eachwithadifferentwindowsizein
the range 2–5, capturing k-gram sequences of varying lengths. e result of each convolutional
layer will then be pooled, and the resulting vectors concatenated and fed to further processing
[Kim,2014].
e convolutional architecture need not be restricted into the linear ordering of a sen-
tence.Forexample,Maetal.[2015]generalizetheconvolutionoperationtoworkoversyntactic
dependencytrees.ere,eachwindowisaroundanodeinthesyntactictree,andthepoolingis
performedoverthedifferentnodes.Similarly,Liuetal.[2015]applyaconvolutionalarchitecture
on top of dependency paths extracted from dependency trees. Le and Zuidema [2015] propose
performing max pooling over vectors representing the different derivations leading to the same
chartiteminachartparser.
13.2 ALTERNATIVE:FEATUREHASHING
Convolutionalnetworksfortextworkasveryeffectivefeaturedetectorsforconsecutivek-grams.
However,theyrequiremanymatrixmultiplications,resultinginnon-negligiblecomputation.A
more time-efficient alternative would be to just use k-gram embeddings directly, and then pool
thek-gramsusingaveragepooling(resultinginacontinuous-bag-of-ngramsrepresentations)or
max pooling. A downside of the approach is that it requires allocating a dedicated embedding
vectorforeachpossiblek-gram,whichcanbeprohibitiveintermsofmemoryasthenumberof
k-gramsinthetrainingcorpuscanbeverylarge.
A solution to the problems is the use of the feature hashing technique that originated in
linearmodels[GanchevandDredze,2008,Shietal.,2009,Weinbergeretal.,2009]andrecently
adoptedtoneuralnetworks[Joulinetal.,2016].eideabehindfeaturehashingisthatwedon’t
pre-computevocabulary-to-indexmapping.Instead,weallocateanembeddingmatrixE withN
rows(N shouldbesufficientlylarge,butnotprohibitive,sayinthemillionsortensofmillions).
Whenak-gramisseenintraining,weassignittoarowinE byapplyingahashfunctionhthat
willdeterministicallymapitintoanumberintherange(cid:140)1;N(cid:141),i h.k-gram/ (cid:140)1;N(cid:141).Wethen
D 2
usethecorrespondingrowE astheembeddingvector.Everyk-gramwillbedynamically
(cid:140)h.k-gram/(cid:141)
assignedarowindexthisway,withouttheneedtostoreanexplicitkgram-to-indexmappingor
todedicateanembeddingvectortoeachk-gram.Somek-gramsmaysharethesameembedding
vectorduetohashcollisions(indeed,withthespaceofpossiblek-gramsbeingmuchlargerthan
thenumberofallocatedembeddingvectorssuchcollisionsareboundtohappen),butasmostk-
gramsarenotinformativeforthetaskthecollisionswillbesmoothedoutbythetrainingprocess.
Ifonewantstobemorecareful,severaldistincthashfunctionsh ;:::;h canbeused,andeach
1 r13.3. HIERARCHICALCONVOLUTIONS 159
k-gramrepresentedasthesumoftherowscorrespondingtoitshashes(Pr
i
1E (cid:140)hi.k-gram/(cid:141)).is
way, if an informative k-gram happens to collide with another informativDe k-gram using one
hash,itstilllikelytohaveanon-collidingrepresentationfromoneoftheotherhashes.
is hashing trick (also called hash kernel) works very well in practice, resulting in very
efficientbag-of-ngramsmodels.Itisrecommendedasago-tobaselinebeforeconsideringmore
complexapproachesorarchitectures.
13.3 HIERARCHICALCONVOLUTIONS
e1Dconvolutionapproachdescribedsofarcanbethoughtofasanngramdetector.Aconvo-
lutionlayerwithawindowofsizek islearningtoidentifyindicativek-gramsintheinput.
e approach can be extended into a hierarchyofconvolutionallayers, in which a sequence
ofconvolutionlayersareappliedoneaftertheother.LetCONVk.w /betheresultofapplying
(cid:130) 1n
W
aconvolutionwithwindowsizekandparameters(cid:130)toeachk-sizewindowinthesequencew :
1n
W
p CONVk .w /
1 Wm
D
U;b 1 Wn
p g. .w / U b/
i i i k 1
D (cid:8) W C (cid:0) (cid:1) C (13.6)
(n k 1 narrowconvolution
m (cid:0) C
D n k 1 wideconvolution:
C C
Wecannowhaveasuccessionofr convolutionallayersthatfeedintoeachotherasfollows:
p1 CONVk1 .w /
1 Wm1 D U1 ;b1 1 Wn
p2 CONVk2 .p1 /
1 Wm2 D U2 ;b2 1 Wm1
(13.7)
(cid:1)(cid:1)(cid:1)
pr CONVkr .pr 1 /:
1 Wmr D Ur ;br 1 W(cid:0)mr (cid:0)1
eresultingvectorspr captureincreasinglylargereffectivewindows(“receptive-fields”)ofthe
1mr
sentence. For r layers wWith a window of size k, each vector pr will be sensitive to a window
i
of r.k 1/ 1 words.⁶ Moreover, the vector pr can be sensitive to gappy-ngrams of k r 1
(cid:0) C i C (cid:0)
words, potentially capturing patterns such as “not good” or “obvious predictable plot”
where standsforashortsequenceofwords,aswellmorespecializedpatternswherethegaps
⁶Toseewhy,considerthatthefirstconvolutionlayertransformseachsequenceofk neighboringword-vectorsintovectors
representingk-grams.en,thesecondconvolutionlayerwillcombineeachkconsecutivek-gram-vectorsintovectorsthat
captureawindowofk .k 1/words,andsoon,untiltherthconvolutionwillcapturek .r 1/.k 1/ r.k
1/ 1words. C (cid:0) C (cid:0) (cid:0) D (cid:0)
C160 13. NGRAMDETECTORS:CONVOLUTIONALNEURALNETWORKS
can be further specialized (i.e., “a sequence of words that do not contain not” or “a sequence of
wordsthatareadverb-like”).⁷Figure13.3showsatwo-layerhierarchicalconvolutionwithk 2.
D
the
actual service actual service w as service w as n ot
w as
n ot very
n ot
very g o o d
the actual actual
service
service w as w as n ot n ot very very g o o d
the actual service was not very good
Figure13.3: Two-layerhierarchicalconvolutionwithk=2.
Strides,DilationandPooling Sofar,theconvolutionoperationisappliedtoeachk-wordwin-
dow in the sequence, i.e., windows starting at indices 1;2;3;:::. is is said to have a stride of
size1.Largerstridesarealsopossible,i.e.,withastrideofsize2theconvolutionoperationwill
beappliedtowindowsstartingatindices1;3;5;:::.Moregenerally,wedefineCONVk;s
as:
p CONVk;s .w /
1 Wm D U;b 1 Wn
(13.8)
p g. .w / U b/;
i 1 .i 1/s.s k/i
D (cid:8) C (cid:0) W C (cid:1) C
where s is the stride size. e result will be a shorter output sequence from the convolutional
layer.
In a dilated convolution architecture [Strubell et al., 2017, Yu and Koltun, 2016] the hi-
erarchy of convolution layers each has a stride size of k 1 (i.e., CONVk;k 1 ). is allows an
(cid:0)
(cid:0)
exponentialgrowthintheeffectivewindowsizeasafunctionofthenumberoflayers.Figure13.4
shows convolution layers with different stride lengths. Figure 13.5 shows a dilated convolution
architecture.
Analternativetothedilationapproachistokeepthestride-sizefixedat1,butshortenthe
sequencelengthbetweeneachlayerbyapplyinglocalpooling,i.e,consecutivek -gramofvectors
0
⁷Toseewhy,considerasequenceoftwoconvolutionlayereachwithawindowofsize2overthesequencefunnyandappealing.
efirstconvolutionlayerwillencodefunnyand andandappealingasvectors,andmaychoosetoretaintheequivalentof
“funny ”and“ appealing”intheresultingvectors.esecondconvolutionlayercanthencombinetheseinto“funny
appealing,”“funny ”or“ appealing.”13.3. HIERARCHICALCONVOLUTIONS 161
(a)
k = 3, s = 1
(b)
k = 3, s = 2
(c)
k = 3, s = 3
Figure13.4: Strides.(a–c)Convolutionlayerwithk=3andstridesizes1,2,3.
can be converted into a single vector using max pooling or averaged pooling. Even if we pool
just every two neighboring vectors, each convolutional-and-pooling layer in the hierarchy will
halvethelengthofthesequence.Similartothedilationapproach,weagaingainanexponential
decreaseinsequencelengthasafunctionofthenumberoflayers.
ParameterTyingandSkip-connections Anothervariationthatcanbeappliedtothehierarchical
convolution architecture is performing parameter-tying, using the same set of parameters U;b
in all the parameter layers. is results in more parameter sharing, as well as allowing to use an
unboundednumberofconvolutionlayers(asalltheconvolutionlayerssharethesameparameters,
the number of convolution layers need not be set in advance), which in turn allows to reduce
arbitrarylengthsequencesintoasinglevectorbyusingasequenceofnarrowconvolutions,each
resultinginashortersequenceofvectors.
Whenusingdeeparchitectures,skip-connectionsaresometimesuseful:theseworkbyfeed-
ingintotheithlayernotonlythevectorsresultingfromthei 1thlayer,butalsovectorsfrom
(cid:0)162 13. NGRAMDETECTORS:CONVOLUTIONALNEURALNETWORKS
Figure13.5: ree-layerdilatedhierarchicalconvolutionwithk=3.
previouslayerswhicharecombinedtothevectorsofthei 1thlayerusingeitherconcatenation,
(cid:0)
averaging,orsummation.
Further Reading e use of hierarchical and dilated convolution and pooling architectures is
verycommoninthecomputer-visioncommunity,wherevariousdeeparchitectures—comprising
ofarrangementsofmanyconvolutionsandpoolinglayerswithdifferentstrides—havebeenpro-
posed,resultinginverystrongimageclassificationandobjectrecognitionresults[Heetal.,2016,
Krizhevskyetal.,2012,SimonyanandZisserman,2015].euseofsuchdeeparchitecturesfor
NLP is still more preliminary. Zhang et al. [2015] provide initial experiments with text classi-
fication with hierarchical convolutions over characters, and Conneau et al. [2016] provide fur-
therresults,thistimewithverydeepconvolutionalnetworks.eworkofStrubelletal.[2017]
provides a good overview of hierarchical and dilated architectures for a sequence labeling task.
Kalchbrenner et al. [2016] use dilated convolutions as encoders in an encoder-decoder archi-
tecture(Section17.2)formachinetranslation.ehierarchyofconvolutionswithlocalpooling
approachisusedbyXiaoandCho[2016],whoapplyittoasequenceofcharacterinadocument-
classificationtask,andthenfeedtheresultingvectorsintoarecurrentneuralnetwork.Wereturn
tothisexampleinSection16.2.2,afterdiscussingrecurrent-neural-networks.163
C H A P T E R 14
Recurrent Neural Networks:
Modeling Sequences and Stacks
Whendealingwithlanguagedata,itisverycommontoworkwithsequences,suchaswords(se-
quences of letters), sentences (sequences of words), and documents. We saw how feed-forward
networkscanaccommodatearbitraryfeaturefunctionsoversequencesthroughtheuseofvector
concatenationandvectoraddition(CBOW).Inparticular,theCBOWrepresentationsallowsto
encodearbitrarylengthsequencesasfixedsizedvectors.However,theCBOWrepresentationis
quitelimited,andforcesonetodisregardtheorderoffeatures.econvolutionalnetworksalso
allowencodingasequenceintoafixedsizevector.Whilerepresentationsderivedfromconvolu-
tionalnetworksareanimprovementovertheCBOWrepresentationastheyoffersomesensitivity
towordorder,theirordersensitivityisrestrictedtomostlylocalpatterns,anddisregardstheorder
ofpatternsthatarefarapartinthesequence.¹
Recurrentneuralnetworks(RNNs)[Elman,1990]allowrepresentingarbitrarilysizedse-
quentialinputsinfixed-sizevectors,whilepayingattentiontothestructuredpropertiesofthein-
puts.RNNs,particularlyoneswithgatedarchitecturessuchastheLSTMandtheGRU,arevery
powerfulatcapturingstatisticalregularitiesinsequentialinputs.eyarearguablythestrongest
contributionofdeep-learningtothestatisticalnatural-languageprocessingtool-set.
is chapter describes RNNs as an abstraction: an interface for translating a sequence of
inputs into a fixed sized output, that can then be plugged as components in larger networks.
VariousarchitecturesthatuseRNNsasacomponentarediscussed.Inthenextchapter,wedeal
withconcreteinstantiationsof theRNNabstraction,and describe theElman RNN(also called
SimpleRNN),theLong-short-termMemory(LSTM),andtheGatedRecurrentUnit(GRU).
en,inChapter16weconsiderexamplesofmodelingNLPproblemsusingwithRNNs.
InChapter9,wediscussedlanguagemodelingandtheMarkovassumption.RNNsallow
forlanguagemodelsthatdonotmaketheMarkovassumption,andconditionthenextwordon
theentiresentencehistory(allthewordsprecedingit).isabilityopensthewaytoconditioned
generation models, where a language model that is used as a generator is conditioned on some
othersignal,suchasasentenceinanotherlanguage.Suchmodelsaredescribedinmoredepthin
Chapter17.
¹However,asdiscussedinSection13.3,hierarchicalanddilatedconvolutionalarchitecturesdohavethepotentialofcapturing
relativelylong-rangedependencieswithinasequence.164 14. RECURRENTNEURALNETWORKS:MODELINGSEQUENCESANDSTACKS
14.1 THERNNABSTRACTION
Weusex todenotethesequenceofvectorsx ;:::;x .Onahigh-level,theRNNisafunction
i j i j
W
that takes as input an arbitrary length ordered sequence of n d -dimensional vectors x
in 1n
x 1;x 2;:::;x n,(x
i
Rdin)andreturnsasoutputasingled
out
dimensionalvectory
n
RdoutW : D
2 2
y RNN.x / (14.1)
n 1n
D W
x Rdin y Rdout:
i n
2 2
is implicitly defines an output vector y for each prefix x of the sequence x . We
i 1i 1n
denotebyRNN? thefunctionreturningthissequence: W W
y RNN?.x /
1n 1n
W D W (14.2)
y RNN.x /
i 1i
D W
x Rdin y Rdout:
i i
2 2
eoutputvectory isthenusedforfurtherprediction.Forexample,amodelforpredict-
n
ing the conditional probability of an event e given the sequence x can be defined as p.e
1n
W D
j x / softmax.RNN.x / W b/ ,thejthelementintheoutputvectorresultingfrom
1n 1n (cid:140)j(cid:141)
j W D W (cid:1) C
thesoftmaxoperationoveralineartransformationoftheRNNencodingy RNN.x /.e
n 1n
D W
RNN function provides a framework for conditioning on the entire history x ;:::;x without
1 i
resortingtotheMarkovassumptionwhichistraditionallyusedformodelingsequences,described
in Chapter 9. Indeed, RNN-based language models result in very good perplexity scores when
comparedtongram-basedmodels.
Looking in a bit more detail, the RNN is defined recursively, by means of a function R
taking as input a state vector s and an input vector x and returning a new state vector s .
i 1 i i
(cid:0)
estatevectors isthenmappedtoanoutputvectory usingasimpledeterministicfunction
i i
O./.²ebaseoftherecursionisaninitialstatevector,s ,whichisalsoaninputtotheRNN.
0
(cid:1)
Forbrevity,weoftenomittheinitialvectors ,orassumeitisthezerovector.
0
When constructing an RNN, much like when constructing a feed-forward network, one
has to specify the dimension of the inputs x as well as the dimensions of the outputs y . e
i i
dimensionsofthestatess areafunctionoftheoutputdimension.³
i
²UsingtheO functionissomewhatnon-standard,andisintroducedinordertounifythedifferentRNNmodelstotobe
presentedinthenextchapter.FortheSimpleRNN(ElmanRNN)andtheGRUarchitectures,Oistheidentitymapping,
andfortheLSTMarchitectureOselectsafixedsubsetofthestate.
³WhileRNNarchitecturesinwhichthestatedimensionisindependentoftheoutputdimensionarepossible,thecurrent
populararchitectures,includingtheSimpleRNN,theLSTM,andtheGRUdonotfollowthisflexibility.14.1. THERNNABSTRACTION 165
RNN?.x s / y
1n 0 1n
W I D W
y O.s / (14.3)
i i
D
s R.s ;x /
i i 1 i
D (cid:0)
x Rdin; y Rdout; s Rf.dout/:
i i i
2 2 2
e functions R and O are the same across the sequence positions, but the RNN keeps
trackofthestatesofcomputationthroughthestatevectors thatiskeptandbeingpassedacross
i
invocationsofR.
Graphically,theRNNhasbeentraditionallypresentedasinFigure14.1.
y
i
s R, O s
i-1 i
θ x i
Figure14.1: GraphicalrepresentationofanRNN(recursive).
is presentation follows the recursive definition, and is correct for arbitrarily long sequences.
However, for a finite sized input sequence (and all input sequences we deal with are finite) one
canunroll therecursion,resultinginthestructureinFigure14.2.
Whilenotusuallyshowninthevisualization,weincludeheretheparameters(cid:18) inordertohigh-
lightthefactthatthesameparametersaresharedacrossalltimesteps.Differentinstantiationsof
RandO willresultindifferentnetworkstructures,andwillexhibitdifferentpropertiesinterms
of their running times and their ability to be trained effectively using gradient-based methods.
However, they all adhere to the same abstract interface. We will provide details of concrete in-
stantiationsofR andO—theSimpleRNN,theLSTM,andtheGRU—inChapter15.Before
that,let’sconsiderworkingwiththeRNNabstraction.166 14. RECURRENTNEURALNETWORKS:MODELINGSEQUENCESANDSTACKS
y y y y y
1 2 3 4 5
s s s s
s 0 R, O 1 R, O 2 R, O 3 R, O 4 R, O s 5
x x x x x
1 2 3 4 5
θ
Figure14.2: GraphicalrepresentationofanRNN(unrolled).
First,wenotethatthevalueofs (andhencey )isbasedontheentireinputx ;:::;x .
i i 1 i
Forexample,byexpandingtherecursionfori 4weget:
D
s R.s ;x /
4 3 4
D
s
3
R.(cid:130)R.s(cid:133)(cid:132);x(cid:131)/;x /
2 3 4
D
s (14.4)
2
R.R.(cid:130)R.s(cid:133)(cid:132);x(cid:131)/;x /;x /
1 2 3 4
D
s
1
R.R.R.(cid:130)R.s(cid:133)(cid:132);x(cid:131)/;x /;x /;x /:
0 1 2 3 4
D
us,s andy canbethoughtofasencoding theentireinputsequence.⁴Istheencoding
n n
useful?isdependsonourdefinitionofusefulness.ejobofthenetworktrainingistosetthe
parametersofRandO suchthatthestateconveysusefulinformationforthetaskwearetyingto
solve.
14.2 RNNTRAINING
ViewedasinFigure14.2itiseasytoseethatanunrolledRNNisjustaverydeepneuralnetwork
(orrather,averylargecomputationgraphwithsomewhatcomplexnodes),inwhichthesamepa-
rametersaresharedacrossmanypartsofthecomputation,andadditionalinputisaddedatvarious
layers. To train an RNN network, then, all we need to do is to create the unrolled computation
graphforagiveninputsequence,addalossnodetotheunrolledgraph,andthenusethebackward
⁴Notethat,unlessRisspecificallydesignedagainstthis,itislikelythatthelaterelementsoftheinputsequencehavestronger
effectonsnthanearlierones.14.3. COMMONRNNUSAGE-PATTERNS 167
(backpropagation) algorithm to compute the gradients with respect to that loss. is procedure
isreferredtointheRNNliteratureasbackpropagationthroughtime(BPTT)[Werbos,1990].⁵
Whatistheobjectiveofthetraining?ItisimportanttounderstandthattheRNNdoesnot
domuchonitsown,butservesasatrainablecomponentinalargernetwork.efinalprediction
and loss computation are performed by that larger network, and the error is back-propagated
throughtheRNN.isway,theRNNlearnstoencodepropertiesoftheinputsequencesthatare
usefulforthefurtherpredictiontask.esupervisionsignalisnotappliedtotheRNNdirectly,
butthroughthelargernetwork.
SomecommonarchitecturesofintegratingtheRNNwithinlargernetworksaregivenbe-
low.
14.3 COMMONRNNUSAGE-PATTERNS
14.3.1 ACCEPTOR
Oneoptionistobasethesupervisionsignalonlyatthefinaloutputvector,y .Viewedthisway,
n
theRNNistrainedasanacceptor.Weobservethefinalstate,andthendecideonanoutcome.⁶For
example,considertraininganRNNtoreadthecharactersofawordonebyoneandthenusethe
finalstatetopredictthepart-of-speechofthatword(thisisinspiredbyLingetal.[2015b]),an
RNNthatreadsinasentenceand,basedonthefinalstatedecidesifitconveyspositiveornegative
sentiment(thisisinspiredbyWangetal.[2015b])oranRNNthatreadsinasequenceofwords
and decides whether it is a valid noun-phrase. e loss in such cases is defined in terms of a
functionofy O.s /.Typically,theRNN’soutputvectory isfedintoafullyconnectedlayer
n n n
D
oranMLP,whichproduceaprediction.eerrorgradientsarethenbackpropagatedthroughthe
restofthesequence(seeFigure14.3).⁷elosscantakeanyfamiliarform:crossentropy,hinge,
margin,etc.
14.3.2 ENCODER
Similartotheacceptorcase,anencodersupervisionusesonlythefinaloutputvector,y .However,
n
unlike the acceptor, where a prediction is made solely on the basis of the final vector, here the
⁵VariantsoftheBPTTalgorithmincludeunrollingtheRNNonlyforafixednumberofinputsymbolsateachtime:first
unrolltheRNNforinputsx1k,resultingins1k.Computealoss,andbackpropagatetheerrorthroughthenetwork(k
stepsback).en,unrolltheinWputsxk 12k,thiWstimeusingskastheinitialstate,andagainbackpropagatetheerrorfork
steps,andsoon.isstrategyisbasedoCntWheobservationsthatfortheSimpleRNNvariant,thegradientsafterkstepstend
tovanish(forlargeenoughk),andsoomittingthemisnegligible.isprocedureallowstrainingofarbitrarilylongsequences.
ForRNNvariantssuchastheLSTMortheGRUthataredesignedspecificallytomitigatethevanishinggradientsproblem,
thisfixedsizeunrollingislessmotivated,yetitisstillbeingused,forexamplewhendoinglanguagemodelingoverabook
withoutbreakingitintosentences.Asimilarvariantunrollsthenetworkfortheentiresequenceintheforwardstep,butonly
propagatesthegradientsbackforkstepsfromeachposition.
⁶eterminologyisborrowedfromFinite-StateAcceptors.However,theRNNhasapotentiallyinfinitenumberofstates,
makingitnecessarytorelyonafunctionotherthanalookuptableformappingstatestodecisions.
⁷iskindofsupervisionsignalmaybehardtotrainforlongsequences,especiallysowiththeSimpleRNN,becauseofthe
vanishinggradientsproblem.Itisalsoagenerallyhardlearningtask,aswedonottelltheprocessonwhichpartsoftheinput
tofocus.Yet,itdoesworkverywellinmanycases.168 14. RECURRENTNEURALNETWORKS:MODELINGSEQUENCESANDSTACKS
loss
predict and
calculate loss
y
5
s s s s
s R, O 1 R, O 2 R, O 3 R, O 4 R, O
0
x x x x x
1 2 3 4 5
Figure14.3: AcceptorRNNtraininggraph.
finalvectoristreatedasanencodingoftheinformationinthesequence,andisusedasadditional
information together with other signals. For example, an extractive document summarization
system may first run over the document with an RNN, resulting in a vector y summarizing
n
the entire document. en, y will be used together with other features in order to select the
n
sentencestobeincludedinthesummarization.
14.3.3 TRANSDUCER
Another option is to treat the RNN as a transducer, producing an output t for each input it
Oi
reads in. Modeled this way, we can compute a local loss signal L .t ;t / for each of the out-
local Oi i
puts t based on a true label t . e loss for unrolled sequence will then be: L.t ;t /
Pn
i
1Oi L local.t Oi;t i/, or using ani other combination rather than a sum such as an aO v1 eWn rag1 eWn orD a
weiDghted average (see Figure 14.4). One example for such a transducer is a sequence tagger, in
whichwetakex tobefeaturerepresentationsforthenwordsofasentence,andt asaninput
i n i
W
for predicting the tag assignment of word i based on words 1:i. A CCG super-tagger based on
suchanarchitectureprovidesverystrongCCGsuper-taggingresults[Xuetal.,2015],although
inmanycasesatransducerbasedonabi-directionalRNN(biRNN,seeSection14.4below)isa
betterfitforsuchtaggingproblems.
A very natural use-case of the transduction setup is for language modeling, in which the
sequence of words x is used to predict a distribution over the .i 1/th word. RNN-based
1i
W C
languagemodelsareshowntoprovidevastlybetterperplexitiesthantraditionallanguagemodels
[Jozefowiczetal.,2016,Mikolov,2012,Mikolovetal.,2010,Sundermeyeretal.,2012].
UsingRNNsastransducersallowsustorelaxtheMarkovassumptionthatistraditionally
takeninlanguagemodelsandHMMtaggers,andconditionontheentirepredictionhistory.14.4. BIDIRECTIONALRNNS(BIRNN) 169
loss
sum
predict and predict and predict and predict and predict and
calculate loss calculate loss calculate loss calculate loss calculate loss
y y y y y
1 2 3 4 5
s s s s
s R, O 1 R, O 2 R, O 3 R, O 4 R, O
0
x x x x x
1 2 3 4 5
Figure14.4: TransducerRNNtraininggraph.
Special cases of the RNN transducer is the RNN generator, and the related conditioned-
generation(alsocalledencoder-decoder)andtheconditioned-generationwithattentionarchitectures.
esewillbediscussedinChapter17.
14.4 BIDIRECTIONALRNNS(BIRNN)
A useful elaboration of an RNN is a bidirectional-RNN (also commonly referred to as biRNN)
[Graves, 2008, Schuster and Paliwal,1997].⁸ Considerthe task ofsequence tagging overa sen-
tence x ;:::;x . An RNN allows us to compute a function of the ith word x based on the
1 n i
past—the words x up to and including it. However, the following words x may also be
1i i 1n
W C W
useful for prediction, as is evident by the common sliding-window approach in which the focus
wordiscategorizedbasedonawindowofkwordssurroundingit.MuchliketheRNNrelaxesthe
Markovassumptionandallowslookingarbitrarilybackintothepast,thebiRNNrelaxesthefixed
window size assumption, allowing to look arbitrarily far at both the past and the future within
thesequence.
Consider an input sequence x . e biRNN works by maintaining two separate states,
1n
sf and sb for each input position i. W e forwardstate sf is based on x ;x ;:::;x , while the
i i i 1 2 i
backwardstate sb is based on x ;x ;:::;x . e forward and backward states are generated
i n n (cid:0)1 i
by two different RNNs. e first RNN (Rf, Of) is fed the input sequence x as is, while
1n
the second RNN (Rb, Ob) is fed the input sequence in reverse. e state represW entation s is
i
⁸WhenusedwithaspecificRNNarchitecturesuchasanLSTM,themodeliscalledbiLSTM.170 14. RECURRENTNEURALNETWORKS:MODELINGSEQUENCESANDSTACKS
thencomposedofboththeforwardandbackwardstates.eoutputatpositioni isbasedonthe
concatenationofthetwooutputvectorsy (cid:140)yf yb(cid:141) (cid:140)Of.sf/ Ob.sb/(cid:141),takingintoaccount
i D i I i D i I i
both the past and the future. In other words, y , the biRNN encoding of the ith word in a
i
sequenceistheconcatenationoftwoRNNs,onereadingthesequencefromthebeginning,and
theotherreadingitfromtheend.
WedefinebiRNN.x ;i/tobetheoutputvectorcorrespondingtotheithsequencepo-
1n
W
sition:⁹
biRNN.x ;i/ y (cid:140)RNNf.x / RNNb.x /(cid:141): (14.6)
1n i 1i ni
W D D W I W
evectory canthenbeuseddirectlyforprediction,orfedaspartoftheinputtoamore
i
complexnetwork.WhilethetwoRNNsarerunindependentlyofeachother,theerrorgradients
atpositioni willflowbothforwardandbackwardthroughthetwoRNNs.Feedingthevectory
i
through an MLP prior to prediction will further mix the forward and backward signals. Visual
representationofthebiRNNarchitectureisgiveninFigure14.5.
concat
yf yr
4 4
s R f,O f R f,O f R f,O f R f,O f Rb,Ob Rb,Ob Rb,Ob Rb,Ob s
0 0
x x x x x x x x
the brown fox jumped jumped over the dog
1 2 3 4 5 6 7 8
Figure14.5: ComputingthebiRNNrepresentationofthewordjumped inthesentence“thebrown
foxjumpedoverthedog.”
Note how the vector y , corresponding to the word jumped, encodes an infinite window
4
around(andincluding)thefocusvectorx .
jumped
SimilarlytotheRNNcase,wealsodefinebiRNN?.x /asthesequenceofvectorsy :
1n 1n
W W
biRNN?.x / y biRNN.x ;1/;:::;biRNN.x ;n/: (14.7)
1n i n 1n 1n
W D W D W W
⁹ebiRNNvectorcaneitherasimpleconcatenationofthetwoRNNvectorsasinEquation(14.6),orfollowedbyanother
linear-transformationtoreduceitsdimension,oftenbacktothedimensionofthesingleRNNinput:
biRNN.x1n;i/ y
i
(cid:140)RNNf.x 1i/ RNNb.x ni/(cid:141)W: (14.5)
W D D W I W
isisvariantisoftenusedwhenstackingseveralbiRNNsontopofeachotherasdiscussedinSection14.5.14.5. MULTI-LAYER(STACKED)RNNS 171
e n output vectors y can be efficiently computed in linear time by first running the
i n
W
forward and backward RNNs, and then concatenating the relevant outputs. is architecture is
depictedinFigure14.6.
y y y y y
the brown fox jumped *
concat concat concat concat concat
yb yb yb yb yb
5 4 3 2 1
sb
5 Rb,Ob
sb
4 Rb,Ob
sb
3 Rb,Ob
sb
2 Rb,Ob
sb
1 Rb,Ob
sb
0
yf yf yf yf yf
1 2 3 4 5
s 0f R f,O f s 1f R f,O f s 2f R f,O f s 3f R f,O f s 4f R f,O f s 5f
x x x x x
the brown fox jumped *
Figure14.6: ComputingthebiRNN? forthesentence“thebrownfoxjumped.”
e biRNN is very effective for tagging tasks, in which each input vector corresponds to
one output vector. It is also useful as a general-purpose trainable feature-extracting component,
thatcanbeusedwheneverawindowaroundagivenwordisrequired.Concreteusageexamples
aregiveninChapter16.
euseofbiRNNsforsequencetaggingwasintroducedtotheNLPcommunitybyIrsoy
andCardie[2014].
14.5 MULTI-LAYER(STACKED)RNNS
RNNs can be stacked in layers, forming a grid [Hihi and Bengio, 1996]. Consider k RNNs,
RNN ;:::;RNN ,wherethejthRNNhasstatessj andoutputsyj .einputforthefirst
1 k 1n 1n
RNN are x , while the input of the jth RNN (jW 2) are the outWputs of the RNN below
1n
it, yj (cid:0)1 . eW output of the entire formation is the ou(cid:21) tput of the last RNN, yk . Such layered
1n 1n
architWectures are often called deepRNNs. A visual representation of a three-layeWr RNN is given
inFigure14.7.biRNNscanbestackedinasimilarfashion.¹⁰
¹⁰etermdeep-biRNN isusedintheliteraturetodescribetodifferentarchitecture:inthefirst,thebiRNNstateisaconcate-
nationoftwodeepRNNs.Inthesecond,theoutputsequenceofonbiRNNisfedasinputtoanother.Myresearchgroup
foundthesecondvarianttooftenperformsbetter.172 14. RECURRENTNEURALNETWORKS:MODELINGSEQUENCESANDSTACKS
y y y y y
1 2 3 4 5
y3 y3 y3 y3 y3
1 2 3 4 5
s3 s3 s3 s3 s3 s3
0 R 3,O
3
1 R 3,O
3
2 R 3,O
3
3 R 3,O
3
4 R 3,O
3
5
y2 y2 y2 y2 y2
1 2 3 4 5
s2 s2 s2 s2 s2 s2
0 R 2,O
2
1 R 2,O
2
2 R 2,O
2
3 R 2,O
2
4 R 2,O
2
5
y1 y1 y1 y1 y1
1 2 3 4 5
s1 s1 s1 s1 s1 s1
0 R 1,O
1
1 R 1,O
1
2 R 1,O
1
3 R 1,O
1
4 R 1,O
1
5
x x x x x
1 2 3 4 5
Figure14.7: Athree-layer(“deep”)RNNarchitecture.
Whileitisnottheoreticallyclearwhatistheadditionalpowergainedbythedeeperarchi-
tecture, it was observed empirically that deep RNNs work better than shallower ones on some
tasks.Inparticular,Sutskeveretal.[2014]reportthatafour-layersdeeparchitecturewascrucial
inachievinggoodmachine-translationperformanceinanencoder-decoderframework.Irsoyand
Cardie[2014]alsoreportimprovedresultsfrommovingfromaone-layerbiRNNtoanarchitec-
turewithseverallayers.ManyotherworksreportresultusinglayeredRNNarchitectures,butdo
notexplicitlycomparetoone-layerRNNs.Intheexperimentofmyresearchgroup,usingtwoor
morelayersindeedoftenimprovesoverusingasingleone.
14.6 RNNSFORREPRESENTINGSTACKS
Some algorithms in language processing, including those for transition-based parsing [Nivre,
2008],requireperformingfeatureextractionoverastack.Insteadofbeingconfinedtolookingat
thek top-mostelementsofthestack,theRNNframeworkcanbeusedtoprovideafixed-sized
vectorencodingoftheentirestack.
e main intuition is that a stack is essentially a sequence, and so the stack state can be
representedbytakingthestackelementsandfeedingtheminorderintoanRNN,resultingina
finalencodingoftheentirestack.Inordertodothiscomputationefficiently(withoutperforming
an O.n/ stack encoding operation each time the stack changes), the RNN state is maintained
together with the stack state. If the stack was push-only, this would be trivial: whenever a new14.6. RNNSFORREPRESENTINGSTACKS 173
elementxispushedintothestack,thecorrespondingvectorxwillbeusedtogetherwiththeRNN
states inordertoobtainanewstates .Dealingwithpopoperationismorechallenging,but
i i 1
C
canbesolvedbyusingthepersistent-stackdata-structure[Goldbergetal.,2013,Okasaki,1999].
Persistent, or immutable, data-structures keep old versions of themselves intact when modified.
e persistent stack construction represents a stack as a pointer to the head of a linked list. An
empty stack is the empty list. e push operation appends an element to the list, returning the
new head. e pop operation then returns the parent of the head, but keeping the original list
intact.Fromthepointofviewofsomeonewhoheldapointertotheprevioushead,thestackdid
not change. A subsequent push operationwill add a new child to the same node.Applying this
procedurethroughoutthelifetimeofthestackresultsinatree,wheretherootisanemptystack
andeachpathfromanodetotherootrepresentsanintermediarystackstate.Figure14.8provides
anexampleofsuchatree.esameprocesscanbeappliedinthecomputationgraphconstruction,
creatinganRNNwithatreestructureinsteadofachainstructure.Backpropagatingtheerrorfrom
a given node will then affect all the elements that participated in the stack when the node was
created,inorder.Figure14.9showsthecomputationgraphforthestack-RNNcorrespondingto
thelaststateinFigure14.8.ismodelingapproachwasproposedindependentlybyDyeretal.
[2015]andWatanabeandSumita[2015]fortransition-baseddependencyparsing.
head
d
head head head head
⊥ a ⊥ a b ⊥ a b c ⊥ a b c ⊥ a b c
(1) push a (2) push b (3) push c (4) pop (5) push d
head head
e e f
d d d d
⊥ a b c ⊥ a b c ⊥ a b c ⊥ a b c
head head
(6) pop (7) pop (8) push e (9) push f
Figure14.8: An immutable stack construction for the sequence of operations pusha;pushb;pushc;
pop;pushd;pop;pop;pushe;pushf.174 14. RECURRENTNEURALNETWORKS:MODELINGSEQUENCESANDSTACKS
y y
a,e a,e,f
s
R, O a,e R, O s
a,e,f
y a,b,d x e x f
s a R, O s a,b,d
s
a,b
y a y a,b y a,b,c x d
s s
s R, O a R, O a,b R, O s
0 a,b,c
x x x
a b c
Figure14.9: estack-RNNcorrespondingtothefinalstateinFigure14.8.
14.7 ANOTEONREADINGTHELITERATURE
Unfortunately,itisoftenthecasethatinferringtheexactmodelformfromreadingitsdescription
inaresearchpapercanbequitechallenging.Manyaspectsofthemodelsarenotyetstandardized,
and different researchers use the same terms to refer to slightly different things. To list a few
examples, the inputs to the RNN can be either one-hot vectors (in which case the embedding
matrixisinternaltotheRNN)orembeddedrepresentations;theinputsequencecanbepadded
with start-of-sequence and/or end-of-sequence symbols, or not; while the output of an RNN
is usually assumed to be a vector which is expected to be fed to additional layers followed by a
softmax for prediction (as is the case in the presentation in this tutorial), some papers assume
the softmax to be part of the RNN itself; in multi-layer RNN, the “state vector” can be either
the output of the top-most layer, or a concatenation of the outputs from all layers; when using
theencoder-decoderframework,conditioningontheoutputoftheencodercanbeinterpretedin
various different ways; and so on. On top of that, the LSTM architecture described in the next14.7. ANOTEONREADINGTHELITERATURE 175
sectionhasmanysmallvariants,whichareallreferredtounderthecommonnameLSTM.Some
ofthesechoicesaremadeexplicitinthepapers,otherrequirecarefulreading,andothersstillare
notevenmentioned,orarehiddenbehindambiguousfiguresorphrasing.
Asareader,beawareoftheseissueswhenreadingandinterpretmodeldescriptions.Asa
writer,beawareoftheseissuesaswell:eitherfullyspecifyyourmodelinmathematicalnotation,
or refer to a different source in which the model is fully specified, if such a source is available.
If using the default implementation from a software package without knowing the details, be
explicit of that fact and specify the software package you use. In any case, don’t rely solely on
figuresornaturallanguagetextwhendescribingyourmodel,astheseareoftenambiguous.177
C H A P T E R 15
Concrete Recurrent Neural
Network Architectures
AfterdescribingtheRNNabstraction,wearenowinplacetodiscussspecificinstantiationsofit.
Recall that we are interested in a recursive function s R.x ;s / such that s encodes the
i i i 1 i
D (cid:0)
sequencex .WewillpresentseveralconcreteinstantiationsoftheabstractRNNarchitecture,
1n
W
providing concrete definitions of the functions R and O. ese include the Simple RNN (S-
RNN),theLongShort-TermMemory(LSTM)andtheGatedRecurrentUnit (GRU).
15.1 CBOWASANRNN
OnparticularlysimplechoiceofRistheadditionfunction:
s R .x ;s / s x
i  i i 1 i 1 i
D (cid:0) D (cid:0) C (15.1)
y O .s / s
i  i i
D D
s ;y Rds; x Rds:
i i i
2 2
Following the definition in Equation (15.1), we get the continuous-bag-of-words model:
the state resulting from inputs x is the sum of these inputs. While simple, this instantiation
1n
W
of the RNN ignores the sequential nature of the data. e Elman RNN, described next, adds
dependenceonthesequentialorderingoftheelements.¹
15.2 SIMPLERNN
e simplest RNN formulation that is sensitive to the ordering of elements in the sequence is
known as an Elman Network or Simple-RNN (S-RNN). e S-RNN was proposed by Elman
[1990] and explored for use in language modeling by Mikolov [2012]. e S-RNN takes the
followingform:
s R .x ;s / g.s Ws x Wx b/
i  i i 1 i 1 i
D (cid:0) D (cid:0) C C (15.2)
y O .s / s
i  i i
D D
s ;y Rds; x Rdx; Wx Rdx ds; Ws Rds ds; b Rds:
i i i (cid:2) (cid:2)
2 2 2 2 2
¹eviewoftheCBOWrepresentationasanRNNisnotacommononeintheliterature.However,wefindittobeagood
steppingstoneintotheElmanRNNdefinition.ItisalsousefultohavethesimpleCBOWencoderinthesameframeworkas
theRNNsasitcanalsoservetheroleofanencoderinaconditionedgenerationnetworksuchasthosedescribedinChapter17.178 15. CONCRETERECURRENTNEURALNETWORKARCHITECTURES
atis,thestates andtheinputx areeachlinearlytransformed,theresultsareadded
i 1 i
(cid:0)
(togetherwithabiasterm)andthenpassedthroughanonlinearactivationfunctiong(commonly
tanhorReLU).eoutputatpositioni isthesameasthehiddenstateinthatposition.²
An equivalent way of writing Equation (15.2) is Equation (15.3), both are used in the
literature:
s R .x ;s / g.(cid:140)s x (cid:141)W b/
i  i i 1 i 1 i
D (cid:0) D (cid:0) I C (15.3)
y O .s / s
i  i i
D D
s ;y Rds; x Rdx; W R.dx ds/ ds; b Rds:
i i i C (cid:2)
2 2 2 2
e S-RNN is only slightly more complex than the CBOW, with the major difference
beingthenonlinearactivationfunctiong.However,thisdifferenceisacrucialone,asaddingthe
lineartransformationfollowedbythenonlinearitymakesthenetworksensitivetotheorderofthe
inputs. Indeed, the Simple RNN provides strong results for sequence tagging [Xu et al., 2015]
aswellaslanguagemodeling.ForcomprehensivediscussiononusingSimpleRNNsforlanguage
modeling,seethePh.D.thesisbyMikolov[2012].
15.3 GATEDARCHITECTURES
e S-RNN is hard to train effectively because of the vanishing gradients problem [Pascanu
etal.,2012].Errorsignals(gradients)inlaterstepsinthesequencediminishquicklyintheback-
propagation process, and do not reach earlier input signals, making it hard for the S-RNN to
capture long-range dependencies. Gating-based architectures, such as the LSTM [Hochreiter
andSchmidhuber,1997]andtheGRU[Choetal.,2014b]aredesignedtosolvethisdeficiency.
ConsidertheRNNasageneralpurposecomputingdevice,wherethestates representsa
i
finite memory. Each application of the function R reads in an input x , reads in the current
i 1
C
memorys ,operatesontheminsomeway,andwritestheresultintomemory,resultinginanew
i
memorystates .Viewedthisway,anapparentproblemwiththeS-RNNarchitectureisthat
i 1
C
thememoryaccessisnotcontrolled.Ateachstepofthecomputation,theentirememorystateis
read,andtheentirememorystateiswritten.
How does one provide more controlled memory access? Consider a binary vector g
2
0;1 n. Such a vector can act as a gate for controlling access to n-dimensional vectors, using
f g
the hadamard-product operation x g:³ Consider a memory s Rd, an input x Rd and a
(cid:12) 2 2
gate g 0;1d. e computation s g x .1 g/ .s/ “reads” the entries in x that cor-
0
2 (cid:12) C (cid:0) (cid:12)
respondtothe1valuesing,andwritesthemtothenewmemorys .en,locationsthatweren’t
0
²Someauthorstreattheoutputatpositioni asamorecomplicatedfunctionofthestate,e.g.,alineartransformation,oran
MLP.Inourpresentation,suchfurthertransformationoftheoutputarenotconsideredpartoftheRNN,butasseparate
computationsthatareappliedtotheRNNsoutput.
³ehadamard-productisafancynameforelement-wisemultiplicationoftwovectors:thehadamardproductx u v
resultsinx(cid:140)i(cid:141) u(cid:140)i(cid:141) v(cid:140)i(cid:141). D (cid:12)
D (cid:1)15.3. GATEDARCHITECTURES 179
readtoarecopiedfromthememorystothenewmemorys throughtheuseofthegate(1 g).
0
(cid:0)
Figure15.1showsthisprocessforupdatingthememorywithpositions2and5fromtheinput.
8 0 10 1 8
11 1 11 0 9
3 0 12 1 3
⨀ + ⨀
7 0 13 1 7
5 0 14 1 5
15 1 15 0 8
s g x (1 – g) s
Figure15.1: Usingbinarygatevectorg tocontrolaccesstomemorys.
0
e gating mechanism described above can serve as a building block in our RNN: gate
vectors can be used to control access to the memory state s . However, we are still missing two
i
important (and related) components: the gates should not be static, but be controlled by the
current memory state and the input, and their behavior should be learned. is introduced an
obstacle,aslearninginourframeworkentailsbeingdifferentiable(becauseofthebackpropagation
algorithm)andthebinary0-1valuesusedinthegatesarenotdifferentiable.⁴
A solution to the above problem is to approximate the hard gating mechanism with a
soft—butdifferentiable—gatingmechanism.Toachievethesedifferentiablegates,wereplacethe
requirement that g 0;1 n and allow arbitrary real numbers, g Rn, which are then pass
0
2f g 2
through a sigmoid function (cid:27).g /. is bounds the value in the range .0;1/, with most values
0
near the borders. When using the gate (cid:27).g / x, indices in x corresponding to near-one val-
0
(cid:12)
uesin(cid:27).g /areallowedtopass,whilethosecorrespondingtonear-zerovaluesareblocked.e
0
gate values can then be conditioned on the input and the current memory, and trained using a
gradient-basedmethodtoperformadesiredbehavior.
iscontrollablegatingmechanismisthebasisoftheLSTMandtheGRUarchitectures,
tobedefinednext:ateachtimestep,differentiablegatingmechanismsdecidewhichpartsofthe
inputs will be written to memory, and which parts of memory will be overwritten (forgotten).
isratherabstractdescriptionwillbemadeconcreteinthenextsections.
15.3.1 LSTM
eLongShort-TermMemory(LSTM)architecture[HochreiterandSchmidhuber,1997]was
designedtosolvethevanishinggradientsproblem,andisthefirsttointroducethegatingmech-
anism.eLSTMarchitectureexplicitlysplitsthestatevectors intotwohalves,whereonehalf
i
⁴Itisinprinciplepossibletolearnalsomodelswithnon-differentiablecomponentssuchasbinarygatesusingreinforcement-
learningtechniques.However,asthetimeofthiswritingsuchtechniquesarebrittletotrain.Reinforcementlearningtech-
niquesarebeyondthescopeofthisbook.180 15. CONCRETERECURRENTNEURALNETWORKARCHITECTURES
istreatedas“memorycells”andtheotherisworkingmemory.ememorycellsaredesignedto
preservethememory,andalsotheerrorgradients,acrosstime,andarecontrolledthroughdiffer-
entiablegatingcomponents—smooth mathematical functions that simulate logical gates. At each
inputstate,agateisusedtodecidehowmuchofthenewinputshouldbewrittentothememory
cell,andhowmuchofthecurrentcontentofthememorycellshouldbeforgotten.Mathemati-
cally,theLSTMarchitectureisdefinedas:⁵
s R .s ;x / (cid:140)c h (cid:141)
j  j 1 j j j
D (cid:0) D I
c f c i z
j j 1
D (cid:12) (cid:0) C (cid:12)
h o tanh.c /
j j
D (cid:12)
i (cid:27).x Wxi h Whi/
j j 1
D C (cid:0)
f (cid:27).x Wxf h Whf/ (15.4)
j j 1
D C (cid:0)
o (cid:27).x Wxo h Who/
j j 1
D C (cid:0)
z tanh.x Wxz h Whz/
j j 1
D C (cid:0)
y O .s / h
j  j j
D D
s R2dh; x Rdx; c ;h ;i;f;o;z Rdh; Wx Rdx dh; Wh Rdh dh:
j (cid:1) i j j (cid:14) (cid:2) (cid:14) (cid:2)
2 2 2 2 2
estateattimej iscomposedoftwovectors,c andh ,wherec isthememorycom-
j j j
ponent and h is the hidden state component. ere are three gates, i, f, and o, controlling
j
forinput,forget,andoutput.egatevaluesarecomputedbasedonlinearcombinationsofthe
currentinputx andthepreviousstateh ,passedthroughasigmoidactivationfunction.An
j j 1
(cid:0)
update candidate z is computedas a linear combination ofx and h ,passed througha tanh
j j 1
(cid:0)
activation function. e memory c is then updated: the forget gate controls how much of the
j
previous memory to keep (f c ), and the input gate controls how much of the proposed
j 1
(cid:12) (cid:0)
updatetokeep(i z).Finally,thevalueofh (whichisalsotheoutputy )isdeterminedbased
j j
(cid:12)
onthecontentofthememoryc ,passedthroughatanhnonlinearityandcontrolledbytheoutput
j
gate.egatingmechanismsallowforgradientsrelatedtothememorypartc tostayhighacross
j
verylongtimeranges.
For further discussion on the LSTM architecture see the Ph.D. thesis by Alex Graves
[2008], as well as Chris Olah’s description.⁶ For an analysis of the behavior of an LSTM when
usedasacharacter-levellanguagemodel,seeKarpathyetal.[2015].
⁵erearemanyvariantsontheLSTMarchitecturepresentedhere.Forexample,forgetgateswerenotpartoftheoriginal
proposalinHochreiterandSchmidhuber[1997],butareshowntobeanimportantpartofthearchitecture.Othervariants
includepeepholeconnectionsandgate-tying.ForanoverviewandcomprehensiveempiricalcomparisonofvariousLSTM
architectures,seeGreffetal.[2015].
⁶http://colah.github.io/posts/2015-08-Understanding-LSTMs/15.3. GATEDARCHITECTURES 181
e vanishing gradients problem in Recurrent Neural Networks and its Solution Intu-
itively, recurrent neural networks can be thought of as very deep feed-forward networks,
withsharedparametersacrossdifferentlayers.FortheSimple-RNN[Equation(15.3)],the
gradientsthenincluderepeatedmultiplicationofthematrixW,makingitverylikelyforthe
valuestovanishorexplode.egatingmechanismmitigatethisproblemtoalargeextentby
gettingridofthisrepeatedmultiplicationofasinglematrix.
For further discussion of the exploding and vanishing gradient problem in RNNs,
see Section 10.7 in Bengio et al. [2016]. For further explanation of the motivation behind
thegatingmechanismintheLSTM(andtheGRU)anditsrelationtosolvingthevanishing
gradientprobleminrecurrentneuralnetworks,seeSections4.2and4.3inthedetailedcourse
notesofCho[2015].
..
LSTMsarecurrentlythemostsuccessfultypeofRNNarchitecture,andtheyarerespon-
sible for many state-of-the-art sequence modeling results. e main competitor of the LSTM-
RNNistheGRU,tobediscussednext.
Practical Considerations When training LSTM networks, Jozefowicz et al. [2015] strongly
recommendtoalwaysinitializethebiastermoftheforgetgatetobeclosetoone.
15.3.2 GRU
eLSTMarchitectureisveryeffective,butalsoquitecomplicated.ecomplexityofthesystem
makes it hard to analyze, and also computationally expensive to work with. e gated recurrent
unit(GRU)wasrecentlyintroducedbyChoetal.[2014b]asanalternativetotheLSTM.Itwas
subsequentlyshownbyChungetal.[2014]toperformcomparablytotheLSTMonseveral(non
textual)datasets.
Like the LSTM, the GRU is also based on a gating mechanism, but with substantially
fewergatesandwithoutaseparatememorycomponent.
s R .s ;x / .1 z/ s z s
j GRU j 1 j j 1 j
D (cid:0) D (cid:0) (cid:12) (cid:0) C (cid:12) Q
z (cid:27).x Wxz s Wsz/
j j 1
D C (cid:0)
r (cid:27).x Wxr s Wsr/
j j 1
D C (cid:0) (15.5)
s tanh.x Wxs .r s /Wsg/
j j j 1
Q D C (cid:12) (cid:0)
y O .s / s
j GRU j j
D D
s ;s Rds; x Rdx; z;r Rds; Wx Rdx ds; Ws Rds ds:
j j i (cid:14) (cid:2) (cid:14) (cid:2)
Q 2 2 2 2 2182 15. CONCRETERECURRENTNEURALNETWORKARCHITECTURES
One gate (r) is used to control access to the previous state s and compute a proposed up-
j 1
(cid:0)
date s . e updated state s (which also serves as the output y ) is then determined based on
j j j
Q
an interpolation of the previous state s and the proposal s , where the proportions of the
j 1 j
(cid:0) Q
interpolationarecontrolledusingthegatez.⁷
eGRUwasshowntobeeffectiveinlanguagemodelingandmachinetranslation.How-
ever,thejuryisstilloutbetweentheGRU,theLSTMandpossiblealternativeRNNarchitectures,
and the subject is actively researched. For an empirical exploration of the GRUand the LSTM
architectures,seeJozefowiczetal.[2015].
15.4 OTHERVARIANTS
Improvementstonon-gatedarchitectures egatedarchitecturesoftheLSTMandtheGRU
helpinalleviatingthevanishinggradientsproblemoftheSimpleRNN,andallowtheseRNNsto
capturedependenciesthatspanlongtimeranges.Someresearchersexploresimplerarchitectures
thantheLSTMandtheGRUforachievingsimilarbenefits.
Mikolov et al. [2014] observed that the matrix multiplication s Ws coupled with the
i 1
(cid:0)
nonlinearitygintheupdateruleRoftheSimpleRNNcausesthestatevectors toundergolarge
i
changesateachtimestep,prohibitingitfromrememberinginformationoverlongtimeperiods.
eyproposetosplitthestatevectors intoaslowchangingcomponentc (“contextunits”)and
i i
afastchangingcomponenth .⁸eslowchangingcomponentc isupdatedaccordingtoalinear
i i
interpolationoftheinputandthepreviouscomponent:c .1 (cid:11)/x Wx1 (cid:11)c ,where(cid:11)
i i i 1
D (cid:0) C (cid:0) 2
.0;1/.isupdateallowsc toaccumulatethepreviousinputs.efastchangingcomponenth
i i
is updated similarly to the Simple RNN update rule, but changed to take c into account as
i
well:⁹ h (cid:27).x Wx2 h Wh c Wc/. Finally, the output y is the concatenation of the
i i i 1 i i
D C (cid:0) C
slowandthefastchangingpartsofthestate:y (cid:140)c h (cid:141).Mikolovetal.demonstratethatthis
i i i
D I
architecture provides competitive perplexities to the much more complex LSTM on language
modelingtasks.
eapproachofMikolovetal.canbeinterpretedasconstrainingtheblockofthematrix
Ws intheS-RNNcorrespondingtoc tobeamultipleoftheidentitymatrix(seeMikolovetal.
i
[2014] for the details). Le et al. [2015] propose an even simpler approach: set the activation
functionoftheS-RNNtoaReLU,andinitializethebiasesbaszeroesandthematrixWs asthe
identifymatrix.iscausesanuntrainedRNNtocopythepreviousstatetothecurrentstate,add
theeffectofthecurrentinputx andsetthenegativevaluestozero.Aftersettingthisinitialbias
i
toward state copying, the training procedure allows Ws to change freely. Le et al. demonstrate
thatthissimplemodificationmakestheS-RNNcomparabletoanLSTMwiththesamenumber
ofparametersonseveraltasks,includinglanguagemodeling.
⁷estatessareoftencalledhintheGRUliterature.
⁸WedepartfromthenotationinMikolovetal.[2014]andreusethesymbolsusedintheLSTMdescription.
⁹eupdateruledivergesfromtheS-RNNupdaterulealsobyfixingthenonlinearitytobeasigmoidfunction,andbynot
usingabiasterm.However,thesechangesarenotdiscussedascentraltotheproposal.15.5. DROPOUTINRNNS 183
Beyond differential gates e gating mechanism is an example of adapting concepts from the
theory of computation (memory access, logical gates) into differentiable—and hence gradient-
trainable—systems. ere is considerable research interest in creating neural network architec-
tures to simulate and implement further computational mechanisms, allowing better and more
fine grained control. One such example is the work on a differentiablestack [Grefenstette et al.,
2015]inwhichastackstructurewithpushandpopoperationsiscontrolledusinganend-to-end
differentiablenetwork,andtheneuralturingmachine[Gravesetal.,2014]whichallowsreadand
writeaccesstocontent-addressablememory,again,inadifferentiablesystem.Whiletheseefforts
areyettoresultinrobustandgeneral-purposearchitecturesthatcanbeusedinnon-toylanguage
processingapplications,theyarewellworthkeepinganeyeon.
15.5 DROPOUTINRNNS
ApplyingdropouttoRNNscanbeabittricky,asdroppingdifferentdimensionsatdifferenttime
stepsharmstheabilityoftheRNNtocarryinformativesignalsacrosstime.ispromptedPham
etal.[2013],Zarembaetal.[2014]tosuggestapplyingdropoutonlyonthenon-recurrentcon-
nection,i.e.,onlytoapplyitbetweenlayersindeep-RNNsandnotbetweensequencepositions.
More recently, following a variational analysis of the RNN architecture, Gal [2015] sug-
gests applying dropout to all the components of the RNN (both recurrent and non-recurrent),
butcruciallyretainthesamedropoutmaskacrosstimesteps.atis,thedropoutmasksaresam-
pled once per sequence, and not once per time step. Figure 15.2 contrasts this form of dropout
(“variationalRNN”)withthearchitectureproposedbyPhametal.[2013],Zarembaetal.[2014].
e variational RNN dropout method of Gal is the current best-practice for applying
dropoutinRNNs.184 15. CONCRETERECURRENTNEURALNETWORKARCHITECTURES
y y y y y y
t–1 t t+1 t–1 t t+1
x x x x x x
t–1 t t+1 t–1 t t+1
(a) Naive dropout RNN (b) Variational RNN
Figure15.2: Gal’sproposalforRNNdropout(b),vs.theprevioussuggestionbyPhametal.[2013],
Zaremba et al. [2014] (a). Figure from Gal [2015], used with permission. Each square represents
anRNNunit,withhorizontalarrowsrepresentingtimedependence(recurrentconnections).Vertical
arrowsrepresenttheinputandoutputtoeachRNNunit.Coloredconnectionsrepresentdropped-out
inputs, with different colors corresponding to different dropout masks. Dashed lines correspond to
standardconnectionswithnodropout.Previoustechniques(naivedropout,left)usedifferentmasks
atdifferenttimesteps,withnodropoutontherecurrentlayers.Gal’sproposedtechnique(Variational
RNN,right)usesthesamedropoutmaskateachtimestep,includingtherecurrentlayers.185
C H A P T E R 16
Modeling
with Recurrent Networks
After enumerating common usage patterns in Chapter 14 and learning the details of concrete
RNNarchitecturesinChapter15,wenowexploretheuseofRNNsinNLPapplicationsthrough
someconcreteexamples.WhileweusethegenerictermRNN,weusuallymeangatedarchitec-
turessuchastheLSTMortheGRU.eSimpleRNNconsistentlyresultsinloweraccuracies.
16.1 ACCEPTORS
e simplest use of RNNs is as acceptors: read in an input sequence, and produce a binary or
multi-classanswerattheend.RNNsareverystrongsequencelearners,andcanpick-uponvery
intricatepatternsinthedata.
is power is often not needed for many natural language classification tasks: the word-
orderandsentencestructureturnouttonotbeveryimportantinmanycases,andbag-of-words
orbag-of-ngramsclassifieroftenworksjustaswellorevenbetterthanRNN-acceptors.
is section presents two examples of acceptor usages for language problems. e first is
acanonicalone:sentimentclassification.eapproachworkswell,butlesspowerfulapproaches
can also prove competitive. e second is a somewhat contrived example: it does not solve any
“useful”taskonitsown,butdemonstratesthepowerofRNNsandthekindofpatternstheyare
capableoflearning.
16.1.1 SENTIMENTCLASSIFICATION
Sentence-LevelSentimentClassification
In the sentence-level sentiment classification task, we are given a sentence, often as part of a
review, and need to assign it one of two values: P or N.¹ is is a somewhat
simplisticviewofthesentimentdetectiontask—butonewhichisoftenusednonetheless.isis
alsothetaskthatmotivatedourdiscussionofconvolutionalneuralnetworks,inChapter13.An
exampleofnaturallyoccurringPandNsentencesinthemovie-reviewsdomain
wouldbethefollowing:²
¹Inamorechallengingvariant,thegoalisathree-wayclassificationintoP,N,andN.
²eseexamplesaretakenfromtheStanfordSentimentTreebank[Socheretal.,2013b].186 16. MODELINGWITHRECURRENTNETWORKS
P:It’snotlife-affirming—it’svulgarandmean,butIlikedit.
N:It’sadisappointingthatitonlymanagestobedecentinsteadofdeadbrilliant.
Note that the positive example contains some negative phrases (not life affirming, vulgar, and
mean), while the negative examples contains some positive ones (dead brilliant). Correctly pre-
dictingthesentimentrequiresunderstandingnotonlytheindividualphrasesbutalsothecontext
in which they occur, linguistic constructs such as negation, and the overall structure of the sen-
tence. Sentiment classification is a tricky and challenging task, and properly solving it involves
handling such issues as sarcasm and metaphor. e definition of sentiment is also not straight-
forward.Foragoodoverviewofthechallengesinsentimentclassificationanditsdefinition,see
the comprehensive review by Pang and Lee [2008]. For our current purpose, however, we will
ignorethecomplexitiesindefinitionandtreatitasadata-driven,binaryclassificationtask.
etaskisstraightforwardtomodelusinganRNN-acceptor:aftertokenization,theRNN
reads in the words of the sentence one at a time. e final RNN state is then fed into an MLP
followed by a softmax-layer with two outputs. e network is trained with cross-entropy loss
based on the gold sentiment labels. For a finer-grained classification task, where one needs to
assign a sentiment on scale of 1–5 or 1–10 (a “star rating”), it is straightforward to change the
MLPtoproduce5outputsinsteadof2.Tosummarizethearchitecture:
p.label k w / y
D j 1 Wn D O(cid:140)k(cid:141)
y softmax.MLP.RNN.x /// (16.1)
1n
O D W
x E ;:::;E :
1 Wn
D
(cid:140)w1(cid:141) (cid:140)wn(cid:141)
ewordembeddingsmatrixE isinitializedusingpre-trainedembeddingslearnedoveralarge
externalcorpususinganalgorithmsuchasW2VorGVwitharelativelywidewindow.
ItisoftenhelpfultoextendthemodelinEquation(16.1)byconsideringtwoRNNs,one
readingthe sentence in itsgivenorderand the otheronereadingit in reverse.eend states of
thetwoRNNsarethenconcatenatedandfedintotheMLPforclassification:
p.label k w / y
D j 1 Wn D O(cid:140)k(cid:141)
y softmax.MLP.(cid:140)RNNf.x / RNNb.x /(cid:141)// (16.2)
1n n1
O D W I W
x E ;:::;E :
1 Wn
D
(cid:140)w1(cid:141) (cid:140)wn(cid:141)
esebidirectionalmodelsproducestrongresultsforthetask[Lietal.,2015].
For longer sentences, Li et al. [2015] found it useful to use a hierarchical architecture, in
whichthesentenceissplitintosmallerspansbasedonpunctuation.en,eachspanisfedintoa
forwardandabackwardRNNasdescribedinEquation(16.2).Sequenceofresultingvectors(one
foreachspan)arethenfedintoanRNNacceptorsuchastheoneinEquation(16.1).Formally,16.1. ACCEPTORS 187
givenasentencew whichissplitintomspans,w1 ;:::;wm ,thearchitectureisgivenby:
1 Wn 1 W‘1 1 W‘m
p.label k w / y
D j 1 Wn D O(cid:140)k(cid:141)
y softmax.MLP.RNN.z ///
1m
O D W
(16.3)
z (cid:140)RNNf.xi / RNNb.xi /(cid:141)
i D 1 W‘ i I ‘ iW1
xi E ;:::;E :
1 W‘ i D (cid:140)w 1i(cid:141) (cid:140)w ‘i i(cid:141)
Eachofthemdifferentspansmayconveyadifferentsentiment.ehigher-levelacceptorreads
thesummaryz producedbythelowerlevelencoders,anddecidesontheoverallsentiment.
1m
W
Sentimentclassificationisalsousedasatest-bedforhierarchical,tree-structuredrecursive
neuralnetworks,asdescribedinChapter18.
DocumentLevelSentimentClassification
edocumentlevelsentimentclassificationissimilartothesentencelevelone,buttheinputtext
is much longer—consisting of multiple sentences—and the supervision signal (sentiment label)
is given only at the end, not for the individual sentences. e task is harder than sentence-level
classification, as the individual sentences may convey different sentiments than the overall one
conveyedbythedocument.
Tangetal.[2015]founditusefultouseahierarchicalarchitectureforthistask,similarto
theoneusedbyLietal.[2015].[Equation(16.3)]:eachsentences isencodedusingagatedRNN
i
producing a vector z , and the vectors z are then fed into a second gated RNN, producing a
i 1n
W
vectorh RNN.z /whichisthenusedforprediction:y softmax.MLP.h//.
1n
D W O D
e authors experimented also with a variant in which all the intermediate vectors from
thedocument-levelRNNarekept,andtheiraverageisfedintotheMLP(h RNN?.z /,
1n 1n
y
O
Dsoftmax.MLP. n1 Pn
i
1h i//.isproducedslightlyhigherresultsinsomW ecD ases. W
D
16.1.2 SUBJECT-VERBAGREEMENTGRAMMATICALITYDETECTION
GrammaticalEnglishsentencesobeytheconstraintthattheheadofthesubjectofapresent-tense
verbmustagreewithinonthenumberinflection(*denoteungrammaticalsentences):
(1) a. ekeyisonthetable.
b. *ekeyareonthetable.
c. *ekeysisonthetable.
d. ekeysareonthetable.
is relationship is non-trivial to infer from the sequence alone, as the two elements can be
separatedbyarbitrarylongsententialmaterial,whichmayincludenounsoftheoppositenumber:188 16. MODELINGWITHRECURRENTNETWORKS
(2) a. ekeystothecabinetinthecorneroftheroomareonthetable.
b. *ekeystothecabinetinthecorneroftheroomisonthetable.
Given the difficulty in identifying the subject from the linear sequence of the sentence,
dependencies such as subject-verb agreement serve as an argument for structured syntactic
representationsinhumans[Everaertetal.,2015].Indeed,givenacorrectsyntacticparsetreeof
thesentence,therelationbetweentheverbanditssubjectbecomestrivialtoextract:
nsubj
root
pobj pobj pobj pobj
det prep det prep det prep det prep det
The keys to the cabinet in the corner of the room are on the table
In work with Linzen and Dupoux [Linzen et al., 2016], we set out to find if RNNs,
which are sequential learners, can pick up on this rather syntactic regularity, by learning from
wordsequencesalone.Wesetupseveralpredictiontasksbasedonnaturallyoccurringtextfrom
Wikipedia to test this. One of the tasks was grammaticality detection: the RNN is tasked with
readingasentence,andattheenddecidingifitisgrammaticalornot.Inoursetup,grammatical
sentenceswereWikipediasentencesthatcontainapresent-tenseverb,whileungrammaticalones
areWikipediasentenceswithapresent-tenseverbinwhichwepickeduponeofthepresent-tense
verbsatrandomandflippeditsformfromsingulartopluralortheotherwayaround.³Notethat
abag-of-wordsorbag-of-ngramsmodelislikelytohaveaveryhardtimesolvingthisparticular
problem,asthedependencybetweentheverbandthesubjectreliesonthestructureofthesen-
tencewhichislostwhenmovingtoabag-of-wordsrepresentation,andcanalsospanmorethan
anynumberofwordsn.
emodelwastrainedasastraightforwardacceptor:
y softmax.MLP.RNN.E ;:::;E ///
O D
(cid:140)w1(cid:141) (cid:140)wn(cid:141)
usingcross-entropyloss.Wehadtensofthousandstrainingsentences,andhundredsofthousands
testsentences(manyoftheagreementcasesarenothard,andwewantedthetestsettocontaina
substantialamountofhardcases).
isisahardtask,withveryindirectsupervision:thesupervisionsignaldidnotincludeany
clueastowherethegrammaticalityclueis.eRNNhadtolearntheconceptofnumber(that
pluralandsingularwordsbelongtodifferentgroups),theconceptofagreement(thattheformof
theverbshouldmatchtheformofthesubject)andtheconceptofsubjecthood(toidentifywhich
³Somefinedetails:weidentifiedverbsusingautomaticallyassignedPOS-tags.Weusedavocabularyofthemostfrequent
10,000wordsinthecorpus,andwordsnotinthevocabularywerereplacedwiththeirautomaticallyassignedPOStag.16.2. RNNSASFEATUREEXTRACTORS 189
ofthenounsprecedingtheverbdeterminestheverb’sform).Identificationofthecorrectsubject
requires learning to identify syntactic markers of nested structures, in order to be able to skip
over distracting nouns in nested clauses. e RNN handled the learning task remarkably well,
andmanagedtosolvethevastmajority(>99%accuracy)ofthetestsetcases.Whenfocusingon
thereallyhardcases,inwhichtheverbanditssubjectwereseparatedby4nounsoftheopposite
number, the RNN still managed to get accuracy of over 80%. Note that if it were to learn a
heuristic of predicting the number of the last noun, its accuracy would have been 0% on these
cases, and for a heuristic of choosing a random preceding noun the accuracy would have been
20%.
Tosummarize,thisexperimentdemonstratesthelearningpowerofgatedRNNs,andthe
kindsofsubtlepatternsandregularitiesinthedatatheycanpickupon.
16.2 RNNSASFEATUREEXTRACTORS
AmajorusecaseofRNNsisasflexible,trainablefeatureextractors,thatcanreplacepartsofthe
moretraditionalfeatureextractionpipelineswhenworkingwithsequences.Inparticular,RNNs
aregoodreplacementsforwindow-basedextractors.
16.2.1 PART-OF-SPEECHTAGGING
Let’sre-considerthepart-of-speechtaggingproblemundertheRNNsetup.
eskeleton:deepbiRNN POS-taggingisaspecialcaseofthesequencetaggingtask,assigning
anoutputtagtoeachoftheninputwords.ismakesabiRNNanidealcandidateforthebasic
structure.
Givenasentencewithwordss w ,wewilltranslatethemintoinputvectorsx using
1n 1n
D W W
afeaturefunctionx (cid:30).s;i/.einputvectorswillbefedintoadeepbiRNN,producingoutput
i
D
vectorsy biRNN?.x /.Eachofthevectorsy willthenbefedintoanMLPwhichwill
1n 1n i
W D W
predict one of k possible output tags for the word. Each vector y is focused on position i in
i
the sequence, but also has information regarding the entire sequence surrounding that position
(an “infinite window”). rough the training procedure, the biRNN will learn to focus on the
sequential aspects that are informative for predicting the label for w , and encode them in the
i
vectory .
i
Fromwordstoinputswithcharacter-levelRNNs Howdowemapawordw toaninputvector
i
x ? One possibility is to use an embedding matrix, which can be either randomly initialized or
i
pre-trained using a technique such as W2V with positional window contexts. Such map-
pingwillbeperformedthroughanembeddingmatrixE,mappingwordstoembeddingvectors
e E .Whilethisworkswell,itcanalsosufferfromcoverageproblemsforvocabularyitems
i
D
(cid:140)wi(cid:141)
notseenduringtrainingorpre-training.Wordsaremadeofcharacters,andcertainsuffixesand
prefixes,aswellasotherorthographiccuessuchasthepresenceofcapitalization,hyphens,ordig-190 16. MODELINGWITHRECURRENTNETWORKS
itscanprovidestronghintsregardingtheword’sambiguityclass.InChapters7and8wediscussed
integratingsuchinformationusingdesignatedfeatures.Here,wewillreplacethesemanuallyde-
signed feature extractors with RNNs. Specifically, we will use two character-level RNNs. For a
wordw madeof charactersc ;:::;c , wewill mapeach characterinto a correspondingembed-
1 ‘
dingvectorc .ewordwillthenbeencodedusingaforwardRNNandreverseRNNoverthe
i
characters. ese RNNs can then either replace the word embedding vector, or, better yet, be
concatenatedtoit:
x (cid:30).s;i/ (cid:140)E RNNf.c / RNNb.c /(cid:141):
i
D D
(cid:140)wi(cid:141)
I
1 W‘
I
‘ W1
Notethattheforward-runningRNNfocusesoncapturingsuffixes,thebackward-running
RNN focuses on prefixes, and both RNNs can be sensitive to capitalization, hyphens, and even
wordlength.
efinalmodel etaggingmodelsthenbecomes:
p.t j w ;:::;w / softmax.MLP.biRNN.x ;i///
i 1 n 1n (cid:140)j(cid:141)
D j D W (16.4)
x (cid:30).s;i/ (cid:140)E RNNf.c / RNNb.c /(cid:141):
i
D D
(cid:140)wi(cid:141)
I
1 W‘
I
‘ W1
e model is trained using cross-entropy loss. Making use of word dropout (Section 8.4.2) for
thewordembeddingsisbeneficial.AnillustrationofthearchitectureisgiveninFigure16.1.
A similar tagging model is described in the work of Plank et al. [2016], in which it was
showntoproduceverycompetitiveresultsforawiderangeoflanguages.
Character-levelConvolutionandPooling Inthearchitectureabove,wordsaremappedtovec-
torsusingforward-movingandbackward-movingRNNsovertheword’scharacters.Analterna-
tiveistorepresentwordsusingcharacter-levelconvolutionandpoolingneuralnetworks(CNN,
Chapter13).MaandHovy[2016]demonstratethatusingaone-layerconvolutional-and-pooling
layer with a window-size of k 3 over each word’s characters is indeed effective for part-of-
D
speechtaggingandnamed-entityrecognitiontasks.
Structured models In the above model, the tagging prediction for word i is performed inde-
pendently of the other tags. is may work well, but one could also condition the ith tag on
the previous model predictions. e conditioning can be either the previous k tags (following a
markovassumption),inwhichcaseweusetagembeddingsE ,resultingin:
(cid:140)t(cid:141)
p.t j w ;:::;w ;t ;:::;t / softmax.MLP.(cid:140)biRNN.x ;i/ E ::: E (cid:141)// ;
i
D j
1 n i (cid:0)1 i (cid:0)k
D
1 Wn
I
(cid:140)ti (cid:0)1(cid:141)
I I
(cid:140)ti (cid:0)k(cid:141) (cid:140)j(cid:141)
orontheentiresequenceofpreviouspredictionst ,inwhichcaseanRNNisusedforencoding
1i 1
W (cid:0)
thetagsequence:
p.t j w ;:::;w ;t / softmax.MLP.(cid:140)biRNN.x ;i/ RNNt.t /(cid:141)// :
i 1 n 1i 1 1n 1i 1 (cid:140)j(cid:141)
D j W (cid:0) D W I W (cid:0)16.2. RNNSASFEATUREEXTRACTORS 191
DET ADJ NN VB IN
pred pred pred pred pred
BI BI BI BI BI
BI BI BI BI BI
BI BI BI BI BI
ø ø ø ø ø
(the) (brown) (fox) (jumped) (over)
… … … …
concat
E
[brown]
Rf Rf Rf Rf Rf Rf Rf Rb Rb Rb Rb Rb Rb Rb
c
*S*
cb cr co cw cn c
*E*
c
*E*
cn cw co cr cb c
*S*
Figure16.1: Illustrationof theRNNtaggingarchitecture.Eachwordw isconvertedinto avector
i
(cid:30).w /whichisaconcatenationofanembeddingvectorandtheendstatesofforward-andbackward-
i
movingcharacterlevelRNNs.ewordvectorsarethenfedintoadeepbiRNN.eoutputofeach
of the outer layer biRNN states is then fed into a predicting network (MLP followed by softmax)
resulting in a tag prediction. Note that each tagging prediction can conditions on the entire input
sentence.
Inbothcases,themodelcanberuningreedymode,predictingthetagst insequence,or
i
usingdynamicprogrammingsearch(inthemarkovcase)orbeam-search(inbothcases)tofind
ahigh-scoringtaggingsequence.SuchamodelwasusedforCCG-supertagging(assigningeach
wordoneofalargenumberoftagsencodingarichsyntacticstructure)byVaswanietal.[2016].
StructuredpredictiontrainingforsuchmodelsisdiscussedinChapter19.
16.2.2 RNN–CNNDOCUMENTCLASSIFICATION
In the sentiment classification examples in Section 16.1.1, we had embedding vectors feeding
into a forward-moving RNN and a backward-moving RNN, followed by a classification layer
[Equation (16.2)]. In the tagger example in Section 16.2.1, we saw that the word embeddings
can be supplemented (or replaced) with character-level models such as RNNs or CNNs over192 16. MODELINGWITHRECURRENTNETWORKS
the characters, in order to improve the model’s coverage and help it deal with unseen words,
inflections,andtypos.
e same approach can be effective also for document classification: instead of feeding
word-embeddings into the two RNNs, we feed vectors that result either from character-level
RNNsovereachword,orfromaconvolutional-and-poolinglayerappliedovereachword.
Another alternative is to apply a hierarchical convolution-and-pooling network (Sec-
tion 13.3) on the characters, in order to get a shorter sequence of vectors that represent units
that are beyond characters but are not necessarily words (the captured information may capture
either more or less than a single word), and then feed the resulting sequence of vectors into the
twoRNNsandtheclassificationlayer.SuchanapproachisexploredbyXiaoandCho[2016]on
several document classification tasks. More specifically, their hierarchical architecture includes
a series of convolutional and pooling layers. At each layer, a convolution with window size k
is applied to the sequence of input vectors, and then max-pooling is applied between each two
neighboring resulting vectors, halving the sequence length. After several such layers (with win-
dowsizesvaryingbetween5and3asafunctionofthelayer,i.e.,widthsof5,5,3),theresulting
vectorsarefedintoforward-runningandbackward-runningGRURNNs,whicharethenfedinto
aclassificationcomponent(afullyconnectedlayerfollowedbysoftmax).eyalsoapplydropout
betweenthelastconvolutionallayerandtheRNNs,andbetweentheRNNandtheclassification
component.isapproachiseffectiveforseveraldocumentclassificationtasks.
16.2.3 ARC-FACTOREDDEPENDENCYPARSING
We revisit the arc-factored dependency-parsing task from Section 7.7. Recall that we are given
a sentence sent with words w and corresponding POS-tags t , and need to assign, for each
1n 1n
W W
wordpair.w ;w /ascoreindicatingthestrengthassignedtowordw beingtheheadofwordw .
i j i j
InSection8.6wederivedanintricatefeaturefunctionforthetask,basedonwindowssurrounding
the head and modifier words, the words between the head and modifier words, and their POS
tags. is intricate feature function can be replaced by a concatenation of two biRNN vectors,
correspondingtotheheadandthemodifierwords.
Specifically, given words and POS-tags w and t with the corresponding embedding
1n 1n
W W
vectorsw andt ,wecreateabiRNNencodingv foreachsentencepositionbyconcatenating
1n 1n i
W W
thewordandPOSvectors,andfeedingthemintoadeep-biRNN:
v biRNN?.x /
1n 1n
W D W (16.5)
x (cid:140)w t (cid:141):
i i i
D I
We then score a head-modifier candidate by passing the concatenation of the biRNN vectors
throughanMLP:
AS.h;m;w ;t / MLP.(cid:30).h;m;s// MLP.(cid:140)v v (cid:141)/: (16.6)
1n 1n h m
W W D D I16.2. RNNSASFEATUREEXTRACTORS 193
Illustration of the architecture is given in Figure 16.2. Notice that the biRNN vectors v
i
encodethewordsincontext,essentiallyforminganinfinitewindowtoeachsideofthewordw ,
i
which is sensitive to both the POS-tag sequence and the word sequence. Moreover, the con-
catenation (cid:140)v v (cid:141) include RNNs running up to each word in each direction, and in particular
h m
I
it covers the sequence of positions between w and w , and the distance between them. e
h m
biRNN is trained as part of the larger network, and learns to focus on the important aspects of
thesequencethesyntacticparsingtask(structured-trainingofthearc-factoredparserisexplained
inSection19.4.1).
ø (over, fox, s)
concat
BI BI BI BI BI BI BI BI BI
BI BI BI BI BI BI BI BI BI
concat concat concat concat concat concat concat concat concat
E E E E E E E E E
[the] [fox] [who] [likes] [apples] [jumped] [over] [a] [dog]
E E E E E E E E E
[D] [N] [P] [V] [N] [V] [P] [D] [N]
the/D fox/N who/P likes/V apples/N jumped/V over/P a/D dog/N
Figure16.2: Illustrationofthearc-factoredparserfeatureextractorforthearcbetweenfoxandover.
Such a feature extractor was used in the work of Kiperwasser and Goldberg [2016b], in
which it was shown to produce state-of-the-art parsing results for the arc-factored approach,
rivalingthescoresofmuchmorecomplexparsingmodels.Asimilarapproachwastakenalsoby
Zhangetal.[2016],achievingsimilarresultswithadifferenttrainingregime.
Ingeneral,wheneveroneisusingwordsasfeaturesinataskthatissensitivetowordorderor
sentencestructure,thewordscanbereplacedbytheirtrainedbiLSTMvectors.Suchanapproach
wastakenbyKiperwasserandGoldberg[2016b]andCrossandHuang[2016a,b]inthecontext
oftransition-based syntacticparsing,withimpressiveresults.195
C H A P T E R 17
Conditioned Generation
As discussed in Chapter 14, RNNs can act as non-markovian language models, conditioning
on the entire history. is ability makes them suitable for use as generators (generating natural
languagesequences)andconditionedgenerators,inwhichthegeneratedoutputisconditionedon
acomplexinput.ischapterdiscussesthesearchitectures.
17.1 RNNGENERATORS
AspecialcaseofusingtheRNN-transducerarchitectureforlanguagemodeling(Section14.3.3)
issequencegeneration.Anylanguagemodelcanbeusedforgeneration,asdescribedinSection9.5.
FortheRNN-transducer,generationworksbytyingtheoutputofthetransducerattimei withits
inputattimei 1:afterpredictingadistributionoverthenextoutputsymbolsp.t k t /,
i 1i 1
C D j W (cid:0)
atokent ischosenanditscorrespondingembeddingvectorisfedastheinputtothenextstep.
i
e process stops when generating a special end-of-sequence symbol, often denoted as </s>.
eprocessisdepictedinFigure17.1.
the black fox jumped </s>
predict predict predict predict predict
y y y y y
1 2 3 4 5
s 0 R, O s 1 R, O s 2 R, O s 3 R, O s 4 R, O
E E E E E
[<s>] [the] [black] [fox] [jumped]
<s> the black fox jumped
Figure17.1: TransducerRNNusedasagenerator.
Similartothecaseofgenerationfromanngramlanguagemodel(Section9.5),whengen-
erating from a trained RNN transducer one can either choose the highest probability item at196 17. CONDITIONEDGENERATION
eachstep,sampleanitemaccordingtothemodel’spredicteddistribution,orusebeam-searchfor
findingagloballyhigh-probabilityoutput.
AnimpressivedemonstrationoftheabilityofgatedRNNtoconditiononarbitrarilylong
histories is through a RNN-based language model that is trained on characters rather than on
words. When used as a generator, the trained RNN language model is tasked with generat-
ing random sentences character by character, each character conditioning on the previous ones
[Sutskeveretal.,2011].Workingonthecharacterlevelforcesthemodeltolookfurtherbackinto
thesequenceinordertoconnectletterstowordsandwordstosentences,andtoformmeaningful
patterns.egeneratedtextsnotonlyresemblefluentEnglish,butalsoshowsensitivitytoprop-
ertiesthatarenotcapturedbyngramlanguagemodels,includinglinelengthsandnestedparen-
thesisbalancing.WhentrainedonCsourcecode,thegeneratedsequencesadheretoindentation
patterns,andthegeneralsyntacticconstraintstheClanguage.Foraninterestingdemonstration
andanalysisofthepropertiesofRNN-basedcharacterlevellanguagemodels,seeKarpathyetal.
[2015].
17.1.1 TRAININGGENERATORS
Whentrainingthegenerator,thecommonapproachistosimplytrainitasatransducerthataims
toputalargeprobabilitymassonthenexttokenintheobservedsequencebasedonthepreviously
observedtokens(i.e.,trainingasalanguagemodel).
Moreconcretely,foreverynwordssentencew ;:::;w inthetrainingcorpus,weproduce
1 n
anRNNtransducerwithn 1inputsandn 1correspondingoutputs,wherethefirstinputis
C C
thestart-of-sentencesymbol,followedbythenwordsofthesentence.efirstexpectedoutput
isthenw ,thesecondexpectedoutputisw ,andsoon,andthen 1thexpectedoutputisthe
1 2
C
end-of-sentencesymbol.
is training approach is often called teacher-forcing, as the generator is fed the observed
wordevenifitsownpredictionputasmallprobabilitymassonit,andintesttimeitwouldhave
generatedadifferentwordatthisstate.
Whilethisworks,itdoesnothandlewelldeviationsfromthegoldsequences.Indeed,when
appliedasagenerator,feedingonitsownpredictionsratherthanongoldsequences,thegener-
ator will be required to assign probabilities given states not observed in training. Searching for
a high-probability output sequence using beam-search may also benefit from a dedicated train-
ingprocedure.Asofthiswriting,copingwiththesesituationsisstillanopenresearchquestion,
which is beyond the scope of this book. We briefly touch upon this when discussing structured
predictioninChapter19.3.
17.2 CONDITIONEDGENERATION(ENCODER-DECODER)
WhileusingtheRNNasageneratorisacuteexercisefordemonstratingitsstrength,thepower
oftheRNNtransducerisreallyrevealedwhenmovingtoaconditionedgenerationframework.17.2. CONDITIONEDGENERATION(ENCODER-DECODER) 197
egenerationframeworkgeneratesthenexttokent basedonthepreviouslygenerated
j 1
C
tokenst :
O1j
W
t p.t k t /: (17.1)
Oj C1
(cid:24)
j C1
D j
O1 Wj
isismodeledintheRNNframeworkas:
p.t k t / f.RNN.t //
j C1
D j
O1 Wj
D
O1 Wj
(17.2)
t p.t t /;
Oj
(cid:24)
j
j
O1 Wj (cid:0)1
or,ifusingthemoredetailedrecursivedefinition:
p.t k t / f.O.s //
j C1
D j
O1 Wj
D
j C1
s R.t ;s / (17.3)
j C1
D
Oj j
t p.t t /;
Oj
(cid:24)
j
j
O1 Wj (cid:0)1
where f is a parameterized function that maps the RNN state to a distribution over words, for
examplef.x/ softmax.xW b/orf.x/ softmax.MLP.x//.
D C D
Intheconditionedgenerationframework,thenexttokenisgeneratedbasedontheprevi-
ouslygeneratedtokens,andanadditionalconditioningcontextc.
t p.t k t ;c/: (17.4)
Oj C1
(cid:24)
j C1
D j
O1 Wj
WhenusingtheRNNframework,thecontextc isrepresentedasavectorc:
p.t k t ;c/ f.RNN.v //
j C1
D j
O1 Wj
D
1 Wj
v (cid:140)t c(cid:141) (17.5)
i
D
Oi
I
t p.t t ;c/;
Oj
(cid:24)
j
j
O1 Wj (cid:0)1
or,usingtherecursivedefinition:
p.t k t ;c/ f.O.s //
j C1
D j
O1 Wj
D
j C1
s R.s ;(cid:140)t c(cid:141)/ (17.6)
j C1
D
j Oj
I
t p.t t ;c/:
Oj
(cid:24)
i
j
O1 Wj (cid:0)1
Ateachstageofthegenerationprocessthecontextvectorcisconcatenatedtotheinputt ,
Oj
andtheconcatenationisfedintotheRNN,resultinginthenextprediction.Figure17.2illustrates
thearchitecture.198 17. CONDITIONEDGENERATION
the black fox jumped </s>
predict predict predict predict predict
y y y y y
1 2 3 4 5
s 0 R, O s 1 R, O s 2 R, O s 3 R, O s 4 R, O
concat concat concat concat concat
c c c c c
E E E E E
[<s>] [the] [black] [fox] [jumped]
<s> the black fox jumped
Figure17.2: ConditionedRNNgenerator.
Whatkindofinformationcanbeencodedinthecontextc?Prettymuchanydatawecan
putourhandsonduringtraining,andthatwefinduseful.Forexample,ifwehavealargecorpusof
newsitemscategorizedintodifferenttopics,wecantreatthetopicasaconditioningcontext.Our
languagemodelwillthenbeabletogeneratetextsconditionedonthetopic.Ifweareinterested
in movie reviews, we can condition the generation on the genre of the movie, the rating of the
review,andperhapsthegeographicregionoftheauthor.Wecanthencontroltheseaspectswhen
generatingtext.Wecanalsoconditiononinferred properties,thatweautomaticallyextractfrom
the text. For example, we can derive heuristics to tell us if a given sentence is written in first
person,ifitcontainsapassive-voiceconstruction,andthelevelofvocabularyusedinit.Wecan
thenusetheseaspectsasconditioningcontextfortraining,and,later,fortextgeneration.
17.2.1 SEQUENCETOSEQUENCEMODELS
econtextc canhavemanyforms.Intheprevioussubsection,wedescribedsomefixed-length,
set-like examples of conditioning contexts. Another popular approach takes c to be itself a se-
quence, most commonly a piece of text. is gives rise to the sequence to sequence conditioned
generation framework, also called the encoder-decoder framework [Cho et al., 2014a, Sutskever
etal.,2014].17.2. CONDITIONEDGENERATION(ENCODER-DECODER) 199
In sequence to sequence conditioned generation, we have a source sequence x (for ex-
1n
W
ample reflecting a sentence in French) and we are interested in generating a target output se-
quence t (for example the translation of the sentence into English). is works by encoding
1m
W
the source sentence x into a vector using an encoder function c E.x /, commonly an
1n 1n
RNN:c
RNNenc.xW /.AconditionedgeneratorRNN(decoder)D isthenuseW
dtogeneratethe
1n
D W
desiredoutputt accordingtoEquation(17.5).earchitectureisillustratedinFigure17.3.
1m
W
the black fox jumped </s>
predict predict predict predict predict
y y y y y
1 2 3 4 5
s 0 R , O s 1 R , O s 2 R , O s 3 R , O s 4 R , O
D D D D D D D D D D
concat concat concat concat concat
c c c c c
E E E E E
[<s>] [the] [black] [fox] [jumped]
<s> the black fox jumped
R , O R , O R , O R , O R , O
E E E E E E E E E E
E E E E E
[<s>] [a] [conditioning] [sequence] [</s>]
<s> a conditioning sequence </s>
Figure17.3: Sequence-to-sequenceRNNgenerator.
issetupisusefulformappingsequencesoflengthntosequencesoflengthm.eencoder
summarizesthesourcesentenceasavectorc,andthedecoderRNNisthenusedtopredict(using
alanguagemodelingobjective)thetargetsequencewordsconditionedonthepreviouslypredicted
wordsaswellastheencodedsentencec.eencoderanddecoderRNNsaretrainedjointly.e200 17. CONDITIONEDGENERATION
supervisionhappensonlyforthedecoderRNN,butthegradientsarepropagatedallthewayback
totheencoderRNN(seeFigure17.4).
loss
sum
predict and predict and predict and predict and predict and
calculate calculate calculate calculate calculate
loss loss loss loss loss
t t t t t
1 2 3 4 5
d d d d d
s 0 R D, O D s 1 R D, O D s 2 R D, O D s 3 R D, O D s 4 R D, O D
<s> t t t t
1 2 3 4
se
0 R E, O E
s 1e
R E, O E
s 2e
R E, O E
s 3e
R E, O E
s 4e
R E, O E
s 5e
x x x x x
1 2 3 4 5
Figure17.4: Sequence-to-sequenceRNNtraininggraph.
17.2.2 APPLICATIONS
esequence-to-sequenceapproachisverygeneral,andcanpotentiallyfitanycasewhereamap-
ping from an input sequence to an output sequence is needed. We list some example use cases
fromtheliterature.17.2. CONDITIONEDGENERATION(ENCODER-DECODER) 201
Machine Translation esequence-to-sequence approach was shownto be surprisingly effec-
tiveforMachineTranslation[Sutskeveretal.,2014]usingdeepLSTMRNNs.Inorderforthe
techniquetowork,Sutskeveretal.founditeffectivetoinputthesourcesentenceinreverse,such
thatx correspondstothefirstwordofthesentence.Inthisway,itiseasierforthesecondRNN
n
to establish the relation between the first word of the source sentence to the first word of the
targetsentence.
Whilethesuccessofthesequence-to-sequenceapproachinFrench-to-Englishtranslation
isimpressive,itisworthnotingthattheapproachofSutskeveretal.[2014]requiredeightlayers
ofhigh-dimensionalLSTMs,isverycomputationallyexpensive,andisnon-trivialtotrainwell.
Later in this chapter (Section 17.4) we describe attention-based architectures, an elaboration on
thesequence-to-sequencearchitecturethatismuchmoreusefulformachinetranslation.
Email Auto-response Here, the task is to map an email, that can be potentially long, into a
shortanswersuchasYes,I’lldoit,Great,seeyouonWednesdayorItwon’tworkout.Kannanetal.
[2016] describe an implementation of the auto-response feature for the Google Inbox product.
ecoreofthesolutionisastraightforwardsequencetosequenceconditionedgenerationmodel
based on an LSTM encoder that reads in the email, and an LSTM decoder that generates an
appropriate response. is component is trained on many email-response pairs. Of course, in
ordertosuccessfullyintegratetheresponsegenerationcomponentintoaproduct,itneedstobe
supplemented by additional modules, to schedule the triggering of the response component, to
ensurediversityofresponsesandbalancenegativeandpositiveresponses,maintainuserprivacy,
andsoon.Fordetails,seeKannanetal.[2016].
Morphological Inflection In the morphological inflection task, the input is a base word and a
desiredinflection request,and the output is an inflected form of the word.Forexample,for the
Finnishwordbruttoarvoandthedesiredinflectionpos=N,case=IN+ABL,num=PLthedesired
outputisbruttoarvoista.Whilethetaskhastraditionallybeenapproachedusinghand-craftedlex-
iconsandfinite-statetransducers,itisalsoaverygoodfitforcharacterlevelsequence-to-sequence
conditioned generation models [Faruqui et al., 2016]. Results of the SIGMORPHON 2016
shared task on inflection generation indicate that recurrent neural network approaches outper-
formallotherparticipatingapproaches[Cotterelletal.,2016].esecond-placesystem[Aharoni
etal.,2016]usedasequence-to-sequencemodelwithafewenhancementsforthetask,whilethe
winningsystem[KannandSchütze,2016]usedanensembleofattentive sequence-to-sequence
models,suchastheonesdescribedinSection17.4.
OtherUses Mappingasequenceofnitemstoasequenceofmitemsisverygeneral,andalmost
any task can be formulated in an encode-and-generate solution. However, the fact that a task
can be formulated this way, does not mean that it should be—perhaps better architectures are
more suitable for it, or are easier to learn. We now describe several applications that seem to
be needlessly hard to learn under the encoder-decoder framework, and for which other, better-202 17. CONDITIONEDGENERATION
suited architectures exist. e fact that the authors managed to get decent accuracies with the
encoder-decoderframeworkatteststothepoweroftheframework.
Filippova et al. [2015] use the architecturefor performing sentencecompressionbydeletion.
Inthistask,wearegivenasentencesuchas“AlanTuring,knownasthefatherofcomputerscience,the
codebreakerthathelpedwinWorldWar2,andthemantorturedbythestateforbeinggay,istoreceivea
pardonnearly60yearsafterhisdeath”andarerequiredtoproduceashorter(“compressed”)version
containing the main information in the sentence by deleting words from the original sentence.
An example compression would be “Alan Turing is to receive a pardon.” Filippova et al. [2015]
modeltheproblemasasequence-to-sequencemappinginwhichtheinputsequenceistheinput
sentence (possibly coupled with syntactic information derived from an automatically produced
parse-tree),andtheoutputisasequenceof K,D,andSdecisions.emodelwas
trained on a corpus of about 2 million sentence-and-compression pairs extracted automatically
fromnewsarticles[FilippovaandAltun,2013],producingstate-of-the-artresults.¹
Gillicketal.[2016]performpart-of-speechtaggingandnamed-entityrecognitionbytreat-
ingitasasequence-to-sequenceproblemmappingasequenceofunicodebytestoasequenceof
spans predictions of the form S12,L13,PER,S40,L11,LOC indicating a 13-bytes long P
entitystartingatoffset12,andan11-byteslongLentitystartingatoffset40.²
Vinyalsetal.[2014]performsyntacticparsingasasequence-to-sequencetaskmappinga
sentencetoasetofconstituencybracketingdecisions.
17.2.3 OTHERCONDITIONINGCONTEXTS
econditioned-generationapproachisveryflexible—theencoderneedn’tbeanRNN.Indeed,
theconditioningcontextvectorcanbebasedonasingleword,aCBOWencoding,begenerated
byaconvolutionalnetwork,orbasedonsomeothercomplexcomputation.
Furthermore,theconditioningcontextneednotevenbetext-based.Inadialogsetting(in
whichtheRNNistrainedtoproduceresponsestomessagesinadialog)Lietal.[2016]useascon-
textatrainableembeddingvectorwhichisassociatedwiththeuser whowrotetheresponse.e
intuition is that different users have different communication styles, based on their age, gender,
socialrole,backgroundknowledge,personalitytraitsandmanyotherlatentfactors.Bycondition-
ingontheuserwhengeneratingtheresponse,thenetworkcanlearntoadaptitspredictionswhile
stillusinganunderlyinglanguagemodelasabackbone.Moreover,asasideeffectoftrainingthe
generator, the network also learns user embeddings, producing similar vectors to users who have
similarcommunicationstyles.Attesttime,onecaninfluencethestyleofthegeneratedresponse
byfeedinginaparticularuser(oraverageuservector)asaconditioningcontext.
¹Whileimpressive,thesequence-to-sequenceapproachisarguablyanoverkillforthistask,inwhichwemapasequenceofn
wordsintoasequenceofndecisions,wheretheithdecisionrelatesdirectlytotheithword.isisinessenceasequence
taggingtask,andabiLSTMtransducer,suchasthosedescribedinthepreviouschapter,couldbeabetterfit.Indeed,thework
ofKlerkeetal.[2016]showsthatsimilar(thoughabitlower)accuraciescanbeobtainedusingabiRNNtransducertrained
onseveralordersofmagnitudelessdata.
²isis,again,asequencetaggingtaskwhichcanbeperformedwellusingabiLSTMtransducer,orastructuredbiLSTM
transducer(biLSTM-CRF),asdescribedinSection19.4.2.17.3. UNSUPERVISEDSENTENCESIMILARITY 203
Departingfurtherawayfromlanguage,apopularuse-caseisinimagecaptioning:aninput
imageisencodedasavector(usuallyusingamulti-layerconvolutionalnetwork³)andthisvectoris
usedasaconditioningcontextforanRNNgeneratorthatistrainedtopredictimagedescriptions
[KarpathyandLi,2015,Maoetal.,2014,Vinyalsetal.,2015].
Work by Huang et al. [2016] extend the captioning task to the more elaborate one of
visualstorytelling, in which the input is a series of images, and the output is a story describing
the progression in the images. Here, the encoder is an RNN that reads in a sequence of image
vectors.
17.3 UNSUPERVISEDSENTENCESIMILARITY
It is often desired to have vector representations of sentences such that similar sentences have
similarvectors.isproblemissomewhatilldefined(whatdoesitmeanforsentencestobesimi-
lar?),andisstillanopenresearchquestion,butsomeapproachesproducereasonableresults.Here,
we focus on unsupervised approaches, in the sense that they can be trained from un-annotated
data. e result of the training is an encoder function E.w / such that similar sentences are
1n
W
encodedtosimilarvectors.
Most approaches are based on the sequence-to-sequence framework: an encoder RNN is
trained to produced context vectors c that will then be used by an RNN decoder to perform a
task.Asaconsequence,theimportantinformationfromthesentencewithrespecttothetaskmust
be captured in c. en, the decoder RNN is thrown away, and the encoder is used to generate
sentencerepresentationsc,underthepremisethatsimilarsentenceswillhavesimilarvectors.e
resultingsimilarityfunctionacrosssentences,then,cruciallyreliesonthetaskofthedecoderwas
trainedtoperform.
Auto Encoding e auto-encoding approach is a conditioned generation model in which a
sentenceisencodedusinganRNN,andthenthedecoderattemptstoreconstructtheinputsen-
tence.isway,themodelistrainedtoencodetheinformationthatisneededtoreconstructthe
sentence,again,hopefullyresultinginsimilarsentenceshavingsimilarvectors.esentencere-
constructionobjectivemaynotbeidealforgeneralsentencesimilarity,however,asitislikelyto
pushapartrepresentationsofsentencesthatconveysimilarmeaningsbutusedifferentwords.
Machine Translation Here, a sequence-to-sequence network is trained to translate sentences
fromEnglishtoanotherlanguage.Intuitively,thevectorsproducedbytheencoderareusefulfor
translation,andsotheyencodetheessenceofthesentencethatisneededtotranslateitproperly,
resultinginsentencesthatwillbetranslatedsimilarlytohavesimilarvectors.ismethodrequires
a large corpus for the conditioned generation task, such as a parallel corpus used in machine
translation.
³Mappingimagestovectorsusingneuralarchitecturesisawell-studiedtopicwithestablishedbestpracticesandmanysuccess
stories.Italsofallsoutsidethescopeofthisbook.204 17. CONDITIONEDGENERATION
Skip-thoughts e model of Kiros et al. [2015], assigned the name skip-thoughtvectors by its
authors, presents an interesting objective to the sentence similarity problem. e model extend
thedistributionalhypothesisfromwordstosentences,arguingthatsentencesaresimilarifthey
appear in similar contexts, where a context of a sentence are the sentences surrounding it. e
skip-thoughts model is thus a conditioned generation model where an RNN encoder maps a
sentencetoavector,andthenonedecoderistrainedtoreconstructtheprevioussentencebasedon
theencodedrepresentation,andaseconddecoderistrainedtoreconstructthefollowingsentence.
etrainedskip-thoughtencoderproducesimpressiveresultsinpractice,mappingsentencessuch
as:
(a)heranhishandinsidehiscoat,double-checkingthattheunopenedletterwasstillthere;and
(b)heslippedhishandbetweenhiscoatandhisshirt,wherethefoldedcopieslayinabrownenvelope.
tosimilarvectors.
SyntacticSimilarity eworkofVinyalsetal.[2014]demonstratethatanencoder-decodercan
producedecentresultsforphrase-basedsyntacticparsing,byencodingthesentenceandrequiring
thedecodertoreconstructalinearizedparsetreeasastreamofbracketingdecisions,i.e.,mapping
from:
theboyopenedthedoor
to:
(S (NP DT NN ) (VP VBD (NP DT NN ) ) )
eencodedsentencerepresentationsundersuchtrainingarelikelytocapturethesyntacticstruc-
tureofthesentence.
17.4 CONDITIONEDGENERATIONWITHATTENTION
In the encoder-decoder networks described in Section 17.2 the input sentence is encoded into
a single vector, which is then used as a conditioning context for an RNN-generator. is archi-
tecturesforcestheencodedvectorc RNNenc.x /tocontainalltheinformationrequiredfor
1n
D W
generation,andrequiresthegeneratortobeabletoextractthisinformationfromthefixed-length
vector. Given these rather strong requirements, the architecture works surprisingly well. How-
ever, in many cases it can be substantially improved by the addition of an attention mechanism.
e conditionedgenerationwithattention architecture[Bahdanauet al., 2014] relaxesthe condi-
tion that the entire source sentence be encoded as a single vector. Instead, the input sentence
is encoded as a sequence of vectors, and the decoder uses a soft attention mechanism in order to
decideonwhichpartsoftheencodinginputitshouldfocus.eencoder,decoder,andattention
mechanismarealltrainedjointlyinordertoplaywellwitheachother.17.4. CONDITIONEDGENERATIONWITHATTENTION 205
Moreconcretely,theencoder-decoderwithattentionarchitectureencodesalengthninput
sequencex usingabiRNN,producingnvectorsc :
1n 1n
W W
c E.x / biRNN?.x /:
1n 1n 1n
W D W D W
egenerator(decoder)canthenusethesevectorsasaread-onlymemoryrepresentingthecon-
ditioningsentence:atanystagej ofthegenerationprocess,itchooseswhichofthevectorsc
1n
itshouldattendto,resultinginafocusedcontextvectorcj attend.c ;t /. W
efocusedcontextvectorcj isthenusedforconditiD
oningtheg1 eWn nerO1
aW
tj
ionatstepj:
p.t k t ;x / f.O.s //
j C1
D j
O1 Wj 1 Wn
D
j C1
s R.s ;(cid:140)t cj(cid:141)/
j C1
D
j Oj
I (17.7)
cj attend.c ;t /
D
1 Wn O1 Wj
t p.t t ;x /:
Oj
(cid:24)
j
j
O1 Wj (cid:0)1 1 Wn
In terms of representation power, this architectures subsumes the previous encoder-decoder ar-
chitecture:bysettingattend.c ;t / c ,wegetEquation(17.6).
1 Wn O1 Wj
D
n
How does the function attend.; / look like? As you may have guessed by this point, it
(cid:1) (cid:1)
is a trainable, parameterized function. is text follows the attention mechanism described by
Bahdanau et al. [2014], who were the first to introduce attention in the context of sequence to
sequencegeneration.⁴Whilethisparticularattentionmechanismispopularandworkswell,many
variants are possible. e work of Luong et al. [2015] explores some of them in the context of
machinetranslation.
eimplementedattentionmechanismissoft,meaningthatateachstagethedecodersees
aweightedaverageofthevectorsc ,wheretheweightsarechosenbytheattentionmechanism.
1n
Moreformally,atstagej theW softattentionproducesamixturevectorcj:
n
cj X(cid:11)j c :
D (cid:140)i(cid:141)(cid:1) i
i 1
D
(cid:11)j Rn is the vector of attention weights for stage j, whose elements (cid:11)j are all positive and
2 (cid:140)i(cid:141)
sumtooCne.
evalues(cid:11)j
areproducedinatwostageprocess:first,unnormalizedattentionweights
(cid:140)i(cid:141)
(cid:11)j areproducedusingafeed-forwardnetworkMLPatt takingintoaccountthedecoderstateat
N(cid:140)i(cid:141)
timej andeachofthevectorsc :
i
(cid:11)j (cid:11)j ;:::;(cid:11)j
N D N(cid:140)1(cid:141) N(cid:140)n(cid:141) D
(17.8)
MLPatt.(cid:140)s c (cid:141)/;:::;MLPatt.(cid:140)s c (cid:141)/:
j 1 j n
D I I
⁴edescriptionofthedecoderpartofthemodeldiffersinsomesmallaspectsfromthatofBahdanauetal.[2014],andis
moresimilartothatofLuongetal.[2015].206 17. CONDITIONEDGENERATION
eunnormalizedweights(cid:11)j arethennormalizedintoaprobabilitydistributionusingthesoft-
N
maxfunction:
(cid:11)j softmax.(cid:11)j ;:::;(cid:11)j /:
D N(cid:140)1(cid:141) N(cid:140)n(cid:141)
In the context of machine translation, one can think of MLPatt as computing a soft alignment
between the current decoder state s (capturing the recently produced foreign words) and each
j
ofthesourcesentencecomponentsc .
i
ecompleteattendfunctionisthen:
attend.c ;t / cj
1 Wn O1 Wj
D
n
cj X(cid:11)j c
D (cid:140)i(cid:141)(cid:1) i
i 1 (17.9)
D
(cid:11)j softmax.(cid:11)j ;:::;(cid:11)j /
D N(cid:140)1(cid:141) N(cid:140)n(cid:141)
(cid:11)j MLPatt.(cid:140)s c (cid:141)/;
N(cid:140)i(cid:141) D j I i
andtheentiresequence-to-sequencegenerationwithattentionisgivenby:
p.t k t ;x / f.O .s //
j C1
D j
O1 Wj 1 Wn
D
dec j C1
s R .s ;(cid:140)t cj(cid:141)/
j C1
D
dec j Oj
I
n
cj X(cid:11)j c
D (cid:140)i(cid:141)(cid:1) i
i 1
D
c biRNN? .x /
1 Wn D enc 1 Wn
(cid:11)j softmax.(cid:11)j ;:::;(cid:11)j / (17.10)
D N(cid:140)1(cid:141) N(cid:140)n(cid:141)
(cid:11)j MLPatt.(cid:140)s c (cid:141)/
N(cid:140)i(cid:141) D j I i
t p.t t ;x /
Oj
(cid:24)
j
j
O1 Wj (cid:0)1 1 Wn
f.z/ softmax.MLPout.z//
D
MLPatt.(cid:140)s c (cid:141)/ vtanh.(cid:140)s c (cid:141)U b/:
j i j i
I D I C
AsketchofthearchitectureisgiveninFigure17.5.
WhyusethebiRNNencodertotranslatetheconditioningsequencex intothecontext
1n
W
vectorsc insteadoflettingtheattentionmechanismlookdirectlyatx ?Couldn’twejustuse
1n 1n
cj DPn iW 1(cid:11)j (cid:140)i(cid:141)(cid:1)x
i
and(cid:11) Nj
(cid:140)i(cid:141)
DMLPatt.(cid:140)s
j
Ix i(cid:141)/?Wecould,butwegetimW portantbenefitsfrom
theencodDingprocess.First,thebiRNNvectorsc representtheitemsx intheirsententialcontext,
i i17.4. CONDITIONEDGENERATIONWITHATTENTION 207
the black fox jumped </s>
predict predict predict predict predict
y y y y y
1 2 3 4 5
s 0 R , O s 1 R , O s 2 R , O s 3 R , O s 4 R , O
D D D D D D D D D D
concat concat concat concat concat
c c c c c
0 1 2 3 4
E E E E E
[<s>] [the] [black] [fox] [jumped]
<s> the black fox jumped
attend attend attend attend attend
BI BI BI BI BI
E E E E E
E E E E E
[<s>] [a] [conditioning] [sequence] [</s>]
<s> a conditioning sequence </s>
Figure17.5: Sequence-to-sequenceRNNgeneratorwithattention.
thatis,theyrepresentawindowfocusedaroundtheinputitemx andnottheitemitself.Second,
i
by having a trainable encoding component that is trained jointly with the decoder, the encoder
anddecoderevolvetogetherandthenetworkcanlearntoencoderelevantpropertiesoftheinput
thatareusefulfordecoding,andthatmaynotbepresentatthesourcesequencex directly.For
1n
W
example,thebiRNNencodermaylearntoencodethepositionofx withinthesequence,andthe
i
decodercouldusethisinformationtoaccesstheelementsinorder,orlearntopaymoreattention
toelementsinthebeginningofthesequencethentoelementsatitsend.208 17. CONDITIONEDGENERATION
Attentive conditioned generation models are very powerful, and work very well on many
sequencetosequencegenerationtasks.
17.4.1 COMPUTATIONALCOMPLEXITY
e conditioned generation without attention is relatively cheap: the encoding is performed in
lineartimeintheinputlength(O.n/),andthedecodingisperformedinlineartimeintheoutput
length(O.m/).Whilegeneratingadistributionoverwordsfromalargevocabularyisinitselfex-
pensive,thisisanorthogonalissuetothisanalysis,inwhichweconsiderthevocabularyscoringas
aconstanttimeoperation.eoverallcomplexityofthesequencetosequencegenerationprocess
isthenO.m n/.⁵
C
Whatisthecostofaddingtheattentionmechanism?eencodingoftheinputsequence
remains an O.n/ linear time operation. However, each step of the decoding process now needs
tocomputecj.isentailsnevaluationsofMLPattfollowedbyanormalizationstepandasum-
mation of n vectors. e complexity of a decoding step grew from a constant time operation to
linearinthelengthoftheconditioningsentence(O.n/),resultinginatotalruntimeofO.m n/.
(cid:2)
17.4.2 INTERPRETABILITY
Non-attentive encoder-decoder networks (much like most other neural architectures) are ex-
tremelyopaque:wedonothaveaclearunderstandingonwhatexactlyisencodedintheencoded
vector,howthisinformationisexploitedinthedecoder,andwhatpromptedaparticulardecoder
behavior. An important benefit of the attentive architecture is that it provides a simple way of
peeking inside some of the reasoning in the decoder and what it learned. At each stage of the
decodingprocess,onecanlookattheproducedattentionweights(cid:11)j andseewhichareasofthe
sourcesequencethedecoderfoundrelevantwhenproducingthegivenoutput.Whilethisisstilla
weakformofinterpretability,itisleapsandboundsbeyondtheopaquenessofthenon-attentive
models.
17.5 ATTENTION-BASEDMODELSINNLP
Conditioned-generation with attention is a very powerful architecture. It is the main algorithm
driving state-of-the-art machine translation, and provides strong results on many other NLP
tasks.issectionprovidesafewexamplesofitsusage.
⁵Whiletheoutputlengthmisinprinciplenotbounded,inpracticethetraineddecodersdolearntoproduceoutputswith
alengthdistributionsimilartolengthsinthetrainingdataset,andintheworstcaseonecanalwaysputahardlimitonthe
lengthofgeneratedsentences.17.5. ATTENTION-BASEDMODELSINNLP 209
17.5.1 MACHINETRANSLATION
Whileweinitiallydescribedmachinetranslationinthecontextofplainsequencetosequencegen-
eration,currentstate-of-the-artmachinetranslationsystemsarepoweredbymodelsthatemploy
attention.
e first results with attentive sequence-to-sequence models for machine translation are
due to Bahdanau et al. [2014], who essentially used the architecture described in the previous
section as is (using a GRU-flavored RNN), employing beam-search when generating from the
decoderattesttime.WhileLuongetal.[2015]exploredvariationsontheattentionmechanism
leading to some gains, most progress in neural machine translation use the attentive sequence-
to-sequencearchitectureasis(eitherwithLSTMsorGRUs),whilechangingitsinputs.
While we cannot expect to cover neural machine translation in this rather short section,
welistsomeimprovementsduetoSennrichandcolleaguesthatpushtheboundariesofthestate-
of-the-art.
Sub-word Units In order to deal with vocabularies of highly inflected languages (as well as to
restrictthevocabularysizeingeneral),Sennrichetal.[2016a]proposemovingtoworkingwith
sub-wordunitsthataresmallerthanatoken.eiralgorithmprocessesthesourceandtargetside
texts using an algorithm called B in search for prominent subword units (the algorithm itself
isdescribedattheendofSection10.5.5).WhenrunonEnglish,thisstageislikelytofindunits
suchaser,est,un,lowandwid.esourceandtargetsentencesarethenprocessedtosplitwords
accordingtotheinducedsegmentation(i.e.,convertingthe widest networkintothe wid_ _est
net_ _work).isprocessedcorpusisthenfedintoanattentivesequence-to-sequencetraining.
Afterdecodingtestsentences,theoutputisprocessedoncemoretoun-splitthesub-wordunits
backintowords.isprocessreducesthenumberofunknowntokens,makesiteasiertogeneralize
tonewvocabularyitems,andimprovestranslationquality.Relatedresearcheffortattempttowork
directlyonthecharacterlevel(encodinganddecodingcharactersinsteadofwords),withnotable
success[Chungetal.,2016].
Incorporatingmonolingualdata esequence-to-sequencemodelsaretrainedonparallelcor-
poraofalignedsentencesinthesourceandtargetlanguages.Suchcorporaexist,butarenaturally
much smaller than available monolingualdata, which is essentially infinite. Indeed, the previous
generationofstatisticalmachinetranslationsystems⁶trainatranslationmodel ontheparalleldata,
and a separate languagemodel on much larger monolingual data. e sequence-to-sequence ar-
chitecturedoesnotcurrentlyallowsuchaseparation,trainingthelanguagemodel(decoder)and
translationmodel(encoder-decoderinteraction)jointly.
Howcanwemakeuseoftarget-sidemonolingualdatainasequence-to-sequenceframe-
work?Sennrichetal.[2016b]proposethefollowingtrainingprotocol:whenattemptingtotrans-
late from source to target, first train a translation model from target to source, and use it to
⁶Foranoverview,seethebookofKoehn[2010]aswellasthebookonsyntax-basedmachinetranslationinthisseries[Williams
etal.,2016].210 17. CONDITIONEDGENERATION
translate a large monolingual corpus of target sentences. en, add the resulting (target,source)
pairstotheparallelcorpusas(source,target)examples.TrainasourcetotargetMTsystemonthe
combined corpus. Note that while the system now trains on automatically produced examples,
allofthetargetsidesentencesitseesareoriginal,sothelanguagemodelingcomponentisnever
trainedonautomaticallyproducedtext.Whilesomewhatofahack,thistrainingprotocolbrings
substantialimprovementsintranslationquality.Furtherresearchwilllikelyyieldcleanersolutions
forintegratingmonolingualdata.
Linguistic Annotations Finally, Sennrich and Haddow [2016] show that the attentive
sequence-to-sequencearchitecturecanlearnbettertranslationmodelifitsinputissupplemented
withlinguisticannotations.atis,givenasourcesentencew ;:::;w ,ratherthancreatingthe
1 n
inputvectorsx bysimplyassigninganembeddingvectortoeachword(x E ),thesen-
1 Wn i
D
(cid:140)wi(cid:141)
tence is run through a linguistic annotation pipeline that includes part-of-speech tagging, syn-
tacticdependencyparsingandlemmatization.Eachwordisthensupplementedwithanencoding
vectorofitspartofspeechtag(p ),it’sdependencylabelwithrespecttoitshead(r ),itslemma
i i
(l ), and morphological features (m ). e input vectors x is then defined as concatenation
i i 1n
W
ofthesefeatures:x (cid:140)w p r l m (cid:141).eseadditionalfeaturesconsistentlyimprovetrans-
i i i i i i
D I I I I
lation quality, indicating that linguistic information is helpful even in the presence of powerful
models than can in theory learn the linguistic concepts on their own. Similarly, Aharoni and
Goldberg [2017] show that by training the decoder in a German to English translation system
to produce linearized syntactic trees instead of a sequence of words, the resulting translations
exhibit more consistent reordering behavior, and better translation quality. ese works barely
scratchthesurfacewithrespecttointegratinglinguisticinformation.Furtherresearchmaycome
up with additional linguistic cues that could be integrated, or improved ways of integrating the
linguisticinformation.
Open issues As of the time of this writing, major open issues in neural machine translation
includescalingupthesizeoftheoutputvocabulary(orremovingthedependenceonitbymoving
to character-based outputs), training while taking the beam-search decoding into account, and
speeding up training and decoding. Another topic that becomes popular is the move to models
that make use of syntactic information. at said, the field is moving extremely fast, and this
paragraphmaynotberelevantbythetimethebookgetstopress.
17.5.2 MORPHOLOGICALINFLECTION
emorphologicalinflectiontaskdiscussedaboveinthecontextofsequencetosequencemodels
also work better when used with an attentive sequence-to-sequence architecture, as evident by
the architecture of the winning system in the SIGMORPHON shared task on morphological
reinflection [Cotterell et al., 2016]. e winning system [Kann and Schütze, 2016] essentially
useanoff-the-shelfattentivesequencetosequencemodel.einputtothesharedtaskisaword
form and a desired inflection, given as a list of target part-of-speech tags and morphological17.5. ATTENTION-BASEDMODELSINNLP 211
features, e.g., NOUN Gender=Male Number=Plural, and the desired output as an inflected
form. is is translated to a sequence to sequence model by creating an input sequence that is
thelistofinflectioninformation,followedbythelistofcharactersoftheinputword.edesired
outputisthenthelistofcharactersinthetargetword.
17.5.3 SYNTACTICPARSING
While more suitable architectures exist, the work of Vinyals et al. [2014] show that attentive
sequence to sequence models can produce competitive syntactic parsing results, by reading in a
sentence(awordatatime)andoutputtingasequenceofbracketingdecisions.ismaynotseem
like an ideal architecture for parsing—indeed, one can get superior results with better tailored
architectures, as evident by the work of Cross and Huang [2016a]. However, considering the
generalityofthearchitecture,thesystemworkssurprisinglywell,andproducesimpressiveparsing
results.Inordertogetfullycompetitiveresults,someextrastepsmustbetaken:thearchitecture
needsalotoftrainingdata.Itistrainedonparse-treesproducedbyrunningtwotreebank-trained
parsersonalargetextcorpus,andselectingtreesonwhichthetwoparsersagree(high-confidence
parses).Inaddition,forthefinalparser,anensemble(Section5.2.3)ofseveralattentionnetworks
isused.PART IV
Additional Topics215
C H A P T E R 18
Modeling Trees
with Recursive Neural Networks
e RNN is very useful for modeling sequences. In language processing, it is often natural and
desirabletoworkwithtreestructures.etreescanbesyntactictrees,discoursetrees,oreventrees
representing the sentiment expressed by various parts of a sentence [Socher et al., 2013b]. We
may want to predict values based on specific tree nodes, predict values based on the root nodes,
orassignaqualityscoretoacompletetreeorpartofatree.Inothercases,wemaynotcareabout
thetreestructuredirectlybutratherreasonaboutspansinthesentence.Insuchcases,thetreeis
merelyusedasabackbonestructurewhichhelpsguidetheencodingprocessofthesequenceinto
afixedsizevector.
erecursiveneuralnetworkabstraction(RecNN)[Pollack,1990],popularizedinNLPby
RichardSocherandcolleagues[Socher,2014,Socheretal.,2010,2011,2013a]isageneralization
oftheRNNfromsequencesto(binary)trees.¹
Much like the RNN encodes each sentence prefix as a state vector, the RecNN encodes
eachtree-nodeasastatevectorinRd.Wecanthenusethesestatevectorseithertopredictvalues
ofthecorrespondingnodes,assignqualityvaluestoeachnode,orasasemanticrepresentationof
thespansrootedatthenodes.
emainintuitionbehindtherecursiveneuralnetworksisthateachsubtreeisrepresented
asad-dimensionalvector,andtherepresentationofanodep withchildrenc andc isafunc-
1 2
tionoftherepresentationofthenodes:vec.p/ f.vec.c /;vec.c //,wheref isacomposition
1 2
D
function taking two d-dimensional vectors and returning a single d-dimensional vector. Much
like the RNN state s is used to encode the entire sequence x , the RecNN state associated
i 1i
W
withatreenodep encodestheentiresubtreerootedatp.SeeFigure18.1foranillustration.
18.1 FORMALDEFINITION
Consider a binary parse tree T over an n-word sentence. As a reminder, an ordered, unlabeled
treeoverastringx ;:::;x canberepresentedasauniquesetoftriplets.i;k;j/,s.t.i k j.
1 n
(cid:20) (cid:20)
Eachsuchtripletindicatesthatanodespanningwordsx isparentofthenodesspanningx
i j i k
W W
and x . Triplets of the form .i;i;i/ correspond to terminal symbols at the tree leaves (the
k 1j
C W
wordsx ).Movingfromtheunlabeledcasetothelabeledone,wecanrepresentatreeasasetof
i
¹Whilepresentedintermsofbinaryparsetrees,theconceptseasilytransfertogeneralrecursivelydefineddatastructures,with
themajortechnicalchallengeisthedefinitionofaneffectiveformforR,thecombinationfunction.216 18. MODELINGTREESWITHRECURSIVENEURALNETWORKS
S =
combine
NP = VP =
2
combine
V = NP =
1
Figure18.1: Illustrationof a recursive neural network.e representationsof V and NP arecom-
1
bined to form the representation of VP. e representations of VP and NP are then combined to
2
formtherepresentationofS.
6-tuples .A B;C;i;k;j/, whereas i, k, and j indicate the spans as before, and A, B, and C
!
arethenodelabelsofofthenodesspanningx ,x ,andx ,respectively.Here,leafnodes
i j i k k 1j
W W C W
have the form .A A;A;i;i;i/, where A is a pre-terminal symbol. We refer to such tuples as
!
production rules. For an example, consider the syntactic tree for the sentence “the boy saw her
duck.”
S
NP VP
Det Noun Verb NP
the boy saw Det Noun
her duck
ItscorrespondingunlabeledandlabeledrepresentationsareasshowninTable18.1.18.1. FORMALDEFINITION 217
Table18.1: Unlabeledandlabeledrepresentations
Unlabeled Labeled Corresponding Span
(1,1,1) (Det, Det, Det, 1, 1, 1) x the
1:1
(2.2.2) (Nound, Noun, Noun, 2, 2, 2) x boy
2:2
(3,3,3) (Verb, Verb, Verb, 3, 3, 3) x saw
3:3
(4, 4, 4) (Det, Det, Det, 4, 4, 4) x her
4:4
(5, 5, 5) (Noun, Noun, Noun, 5, 5, 5) x duck
5:5
(4, 4, 5) (NP, Det, Noun, 4, 4, 5) x her duck
4:5
(3, 3, 5) (VP, Verb, NP, 3, 3, 5) x saw her duck
3:5
(1, 1, 2) (NP, Det, Nound, 1, 1, 2) x the boy
1:2
(1, 2, 5) (S, NP, VP, 1 2, 5) x the boy saw her duck
1:5
e set of production rules above can be uniquely converted to a set of tree nodes qA
i j
(indicatinganodewithsymbolAoverthespanx )bysimplyignoringtheelements.B;C;kW/
i j
W
ineachproductionrule.Wearenowinpositiontodefinetherecursiveneuralnetwork.
Arecursiveneuralnetwork(RecNN)isafunctionthattakesasinputaparsetreeoverann-
wordsentencex ;:::;x .Eachofthesentence’swordsisrepresentedasad-dimensionalvector
1 n
x ,andthetreeisrepresentedasasetT ofproductionrules.A B;C;i;j;k/.Denotethenodes
i
!
ofT byqA .eRecNNreturnsasoutputacorrespondingsetofinsidestatevectorssA ,where
i j i j
each insideW state vector sA Rd represents the corresponding tree node qA , and encW odes the
i j 2 i j
entirestructurerootedattWhatnode.LikethesequenceRNN,thetree-shapedW RecNNisdefined
recursivelyusingafunctionR,wheretheinsidevectorofagivennodeisdefinedasafunctionof
theinsidevectorsofitsdirectchildren.²Formally:
RecNN.x ;:::;x ;T/ sA Rd qA T
1 n Df i j 2 j i j 2 g
W W
sA v.x / (18.1)
i i D i
W
sA R.A;B;C;sB ;sC / qB T; qC T:
i j D i k k 1j i k 2 k 1j 2
W W C W W C W
efunctionRusuallytakestheformofasimplelineartransformation,whichmayormay
notbefollowedbyanonlinearactivationfunctiong:
R.A;B;C;sB ;sC / g.(cid:140)sB sC (cid:141)W/: (18.2)
i k k 1j D i kI k 1j
W C W W C W
²LeandZuidema[2014]extendtheRecNNdefinitionsuchthateachnodehas,inadditiontoitsinsidestatevector,alsoan
outsidestatevectorrepresentingtheentirestructurearoundthesubtreerootedatthatnode.eirformulationisbasedonthe
recursivecomputationoftheclassicinside-outsidealgorithm,andcanbethoughtofasthebiRNNcounterpartofthetree
RecNN.Fordetails,seeLeandZuidema[2014].218 18. MODELINGTREESWITHRECURSIVENEURALNETWORKS
isformulationofRignoresthetreelabels,usingthesamematrixW R2d d forallcombina-
(cid:2)
2
tions.ismaybeausefulformulationincasethenodelabelsdonotexist(e.g.,whenthetreedoes
notrepresentasyntacticstructurewithclearlydefinedlabels)orwhentheyareunreliable.How-
ever,ifthelabelsareavailable,itisgenerallyusefultoincludetheminthecompositionfunction.
Oneapproachwouldbetointroducelabelembeddings v.A/mappingeachnon-terminalsymbol
toad dimensionalvector,andchangeRtoincludetheembeddedsymbolsinthecombination
nt
function:
R.A;B;C;sB ;sC / g.(cid:140)sB sC v.A/ v.B/(cid:141)W/ (18.3)
i k k 1j D i kI k 1jI I
W C W W C W
(here,W R2d 2dnt d).SuchapproachistakenbyQianetal.[2015].Analternativeapproach,
C (cid:2)
2
duetoSocheretal.[2013a]istountietheweightsaccordingtothenon-terminals,usingadif-
ferentcompositionmatrixforeachB;C pairofsymbols:³
R.A;B;C;sB ;sC / g.(cid:140)sB sC (cid:141)WBC/: (18.4)
i k k 1j D i kI k 1j
W C W W C W
isformulationisusefulwhenthenumberofnon-terminalsymbols(orthenumberofpossible
symbolcombinations)isrelativelysmall,asisusuallythecasewithphrase-structureparsetrees.A
similarmodelwasalsousedbyHashimotoetal.[2013]toencodesubtreesinsemantic-relation
classificationtask.
18.2 EXTENSIONSANDVARIATIONS
As all of the definitions of R above suffer from the vanishing gradients problem of the Simple
RNN, several authors sought to replace it with functions inspired by the LSTM gated archi-
tecture, resulting in Tree-shaped LSTMs [Tai et al., 2015, Zhu et al., 2015b]. e question of
optimaltreerepresentationisstillverymuchanopenresearchquestion,andthevastspaceofpos-
sible combination functions R is yet to be explored. Other proposed variants on tree-structured
RNNs includes a recursive matrix-vector model [Socher et al., 2012] and recursive neural tensor
network [Socher et al., 2013b]. In the first variant, each word is represented as a combination
of a vector and a matrix, where the vector defines the word’s static semantic content as before,
while the matrix acts as a learned “operator” for the word, allowing more subtle semantic com-
positions than the addition and weighted averaging implied by the concatenation followed by
lineartransformationfunction.Inthesecondvariant,wordsareassociatedwithvectorsasusual,
but the composition function becomes more expressive by basing it on tensor instead of matrix
operations.
In our own work [Kiperwasser and Goldberg, 2016a], we propose a tree encoder that is
not restricted to binary trees but instead can work with arbitrary branching trees. e encoding
is based on RNNs (specifically LSTMs), where each subtree encoding is recursively defined as
themergingoftwoRNNstates,onerunningovertheencodingsoftheleftsubtrees(fromleftto
³Whilenotexploredintheliterature,atrivialextensionwouldconditionthetransformationmatrixalsoonA.18.3. TRAININGRECURSIVENEURALNETWORKS 219
right)andendingintherootnode,andtheotherrunningovertheencodingsoftherightsubtrees
(fromrighttoleft),andendingintherootnode.
18.3 TRAININGRECURSIVENEURALNETWORKS
e training procedure for a recursive neural network follows the same recipe as training other
formsofnetworks:definealoss,spelloutthecomputationgraph,computegradientsusingback-
propagation,⁴andtraintheparametersusingSGD.
With regard to the loss function, similar to the sequence RNN one can associate a loss
either with the root of the tree, with any given node, or with a set of nodes, in which case the
individual node’s losses are combined, usually by summation. e loss function is based on the
labeledtrainingdatawhichassociatesalabelorotherquantitywithdifferenttreenodes.
Additionally,onecantreattheRecNNasanEncoder,whereastheinside-vectorassociated
withanodeistakentobeanencodingofthetreerootedatthatnode.eencodingcanpotentially
besensitivetoarbitrarypropertiesofthestructure.evectoristhenpassedasinputtoanother
network.
Forfurtherdiscussiononrecursiveneuralnetworksandtheiruseinnaturallanguagetasks,
refertothePh.D.thesisofSocher[2014].
18.4 ASIMPLEALTERNATIVE–LINEARIZEDTREES
e RecNN abstraction provides a flexible mechanism for encoding trees as vectors, using a re-
cursive, compositional approach. e RecNN encodes not only the given tree, but also all of its
subtrees.Ifthisrecursivenessoftheencodingisnotneeded,andallweneedisavectorrepresen-
tationofanentiretree,thatissensitivetothetreestructure,simpleralternativesmayworkwell.
Inparticular,linearizingtreesintolinearsequencethatisthenfedintoagatedRNNacceptor(or
a biRNN encoder) has proven to be very effective in several works [Choe and Charniak, 2016,
Luongetal.,2016,Vinyalsetal.,2014].Concretely,thetreeforthesentencetheboysawherduck,
presentedabove,willbetranslatedintothelinearstring:
(S (NP (Det the Det) (Noun boy Noun) NP) (VP (Verb saw Verb) (NP (Det her Det)
(Noun duck Noun) NP) VP) S)
whichwillthenbefedintoagatedRNNsuchasanLSTM.efinalstateoftheRNNcanthen
beusedasthevectorrepresentationofthetree.Alternatively,thetreestructurecanbescoredby
traininganRNNlanguagemodeloversuchlinearizedparse-trees,andtakingthelanguage-model
probabilityofthelinearizedparsetreetostandforitsquality.
⁴Beforetheintroductionofthecomputationgraphabstraction,thespecificbackpropagationprocedureforcomputingthe
gradientsinaRecNNasdefinedabovewasreferredtoastheback-propagationthroughstructure(BPTS)algorithm[Goller
andKüchler,1996].220 18. MODELINGTREESWITHRECURSIVENEURALNETWORKS
18.5 OUTLOOK
econceptofrecursive,tree-structurednetworksispowerful,intriguing,andseemsverysuited
for dealing with the recursive nature of language. However, as of the end of 2016, it is safe to
say that they don’t yet show any real and consistent benefits over simpler architectures. Indeed,
in many cases sequence-level models such as RNNs capture the desired regularities just as well.
Eitherwehavenotyetfoundthekiller-applicationfortree-structurednetworks,orwehavenot
yetfoundthecorrectarchitectureortrainingregimes.Somecomparisonandanalysisoftheuse
oftree-structuredvs.sequence-structurednetworksforlanguagetaskscanbefoundinthework
ofLietal.[2015].Asitstands,theuseoftree-structurednetworksforprocessinglanguagedata
isstillanopenresearcharea.Findingthekiller-appforsuchnetworks,providingbettertraining
regimes,orshowingthattree-likearchitecturesarenotneededareallexcitingresearchdirections.221
C H A P T E R 19
Structured Output Prediction
ManyproblemsinNLPinvolvestructuredoutputs:caseswherethedesiredoutputisnotaclass
labelordistributionoverclasslabels,butastructuredobjectsuchasasequence,atree,oragraph.
Canonical examples are sequence tagging (e.g., part-of-speech tagging) sequence segmentation
(chunking, NER), syntactic parsing, and machine translation. In this chapter, we discuss the
applicationofneuralnetworkmodelsforstructuredtasks.
19.1 SEARCH-BASEDSTRUCTUREDPREDICTION
e common approach to structured data prediction is search based. For in-depth discussion of
search-basedstructurepredictioninpre-deep-learningNLP,seethebookbySmith[2011].e
techniquescaneasilybeadaptedtouseaneuralnetwork.Intheneuralnetworksliterature,such
modelswerediscussedundertheframeworkofenergy-basedlearning[LeCunetal.,2006,Section
7].eyarepresentedhereusingsetupandterminologyfamiliartotheNLPcommunity.
Search-based structured prediction is formulated as a search problem over possible struc-
tures:
predict.x/ argmaxscore .x;y/; (19.1)
global
D
y Y.x/
2
wherex isaninputstructure,y isanoutputoverx (inatypicalexamplex isasentenceandy is
atag-assignmentoraparse-treeoverthesentence),Y.x/isthesetofallvalidstructuresoverx,
andwearelookingforanoutputy thatwillmaximizethescoreofthex;y pair.
19.1.1 STRUCTUREDPREDICTIONWITHLINEARMODELS
Intherichliteratureonstructurepredictionwithlinearandlog-linearmodels,thescoringfunc-
tionismodeledasalinearfunction:
score .x;y/ w (cid:136).x;y/; (19.2)
global
D (cid:1)
where(cid:136)isafeatureextractionfunctionandwisaweightvector.
Inordertomakethesearchfortheoptimaly tractable,thestructurey isdecomposedinto
parts,andthefeaturefunctionisdefinedintermsoftheparts,where(cid:30).p/isapart-localfeature
extractionfunction:
(cid:136).x;y/ X(cid:30).p/: (19.3)
D
p parts.x;y/
2222 19. STRUCTUREDOUTPUTPREDICTION
Each part is scored separately, and the structure score is the sum of the component parts
scores:
score .x;y/ w (cid:136).x;y/ w X(cid:30).p/ Xw (cid:30).p/ Xscore .p/; (19.4)
global local
D (cid:1) D (cid:1) D (cid:1) D
p y p y p y
2 2 2
where p y is a shorthand for p parts.x;y/. e decomposition of y into parts is such that
2 2
there exists an inference algorithm that allows for efficient search for the best scoring structure
giventhescoresoftheindividualparts.
19.1.2 NONLINEARSTRUCTUREDPREDICTION
Onecannowtriviallyreplacethelinearscoringfunctionoverpartswithaneuralnetwork:
score .x;y/ Xscore .p/ XNN.(cid:30).p//; (19.5)
global local
D D
p y p y
2 2
where(cid:30).p/mapsthepartp intoad dimensionalvector.
in
Incaseofaonehidden-layerfeed-forwardnetwork:
score .x;y/ XMLP .(cid:30).p// X.g.(cid:30).p/W1 b1//w (19.6)
global 1
D D C
p y p y
2 2
(cid:30).p/ Rdin, W1 Rdin d1, b1 Rd1, w Rd1. A common objective in structured prediction
(cid:2)
2 2 2 2
ismakingthegoldstructurey scorehigherthananyotherstructurey ,leadingtothefollowing
0
(generalizedperceptron[Collins,2002])loss:
maxscore .x;y / score .x;y/: (19.7)
global 0 global
y (cid:0)
0
e maximization is performed using a dedicated search algorithm, which is often based
ondynamicprogrammingorasimilarsearchtechnique.
Intermsofimplementation,thismeans:createacomputationgraphCG foreachofthe
p
possible parts, and calculate its score. en, run inference (i.e., search) to find the best scoring
structure y according to the scores of its parts. Connect the output nodes of the computation
0
graphscorrespondingtopartsinthegold(predicted)structurey (y )intoasummingnodeCG
0 y
(CG ).ConnectCG andCG usinga“minus”node,CG ,andcomputethegradients.
y0 y y0 l
As argued in LeCun et al. [2006, Section 5], the generalized perceptron loss may not be
a good loss function when training structured prediction neural networks as it does not have a
margin,andamargin-basedhingelossispreferred:
max.0;m maxscore .x;y / score .x;y//: (19.8)
global 0 global
Cy y (cid:0)
0⁄19.1. SEARCH-BASEDSTRUCTUREDPREDICTION 223
Itistrivialtomodifytheimplementationabovetoworkwiththehingeloss.
Note that in both cases we lose the nice properties of the linear model. In particular, the
modelisnolongerconvex.isistobeexpected,aseventhesimplestnonlinearneuralnetworkis
alreadynon-convex.Nonetheless,wecouldstillusestandardneuralnetworkoptimizationtech-
niquestotrainthestructuredmodel.
Trainingandinferenceisslower,aswehavetoevaluatetheneuralnetwork(andtakegra-
dients)onceforeachpart,atotalof parts.x;y/ times.
j j
CostAugmentedTraining Structuredpredictionisavastfield,andthisbookdoesnotattempt
tocoveritfully.Forthemostpart,thelossfunctions,regularizersandmethodsdescribedin,e.g.,
Smith[2011],areeasilytransferabletotheneuralnetworkframework,althoughlosingconvexity
andmanyoftheassociatedtheoreticalguarantees.Onetechniquethatisworthmentioningspecif-
icallyiscostaugmentedtraining,alsocalledlossaugmentedinference.Whileitbringsmodestgains
when used in linear structured prediction, my research group found it essential to successfully
trainingneuralnetwork-basedstructured-predictionmodelsusingthegeneralizedperceptronor
themarginbasedlosses,especiallywhenusingstrongfeatureextractorssuchasRNNs.
e maximization term in Equations (19.7) and (19.8) is looking for a structure y that
0
receives a high score according to the current model, and is also wrong. en the loss reflects
thedifferenceinscoresbetweeny andthegoldstructurey.Oncethemodelissufficientlywell
0
trained, the incorrect structure y and the correct one y are likely to be similar to each other
0
(because the model learned to assign high scores to structures that are reasonably good). Recall
thattheglobalscorefunctionisinfactcomposedofasumoflocalpartscores.Partsthatappear
inbothscoringterms(ofy andofy)willcanceleachotherout,andwillresultingradientsof0
0
fortheassociatednetworkparameters.Ify andy aresimilartoeachother,thenmostpartswill
0
overlapandcanceloutthisway,leadingtoanoverallverysmallupdatefortheexample.
eideabehindcost-augmentedtrainingistochangethemaximizationtofindstructures
y thatscorewellunderthemodelandarealsorelativelywrong inthesensethattheyhavemany
0
incorrectparts.Formally,thehingeobjectivechangesto:
(cid:18) (cid:19)
max 0;m max(cid:0)score global.x;y 0/ (cid:26)(cid:129).y;y 0/(cid:1) score global.x;y/ ; (19.9)
Cy y C (cid:0)
0⁄
where (cid:26) is a scalar hyperparameter indicating the relative importance of (cid:129) vs. the model score,
and(cid:129).y;y /isafunctioncountingthenumberofincorrectpartsiny withrespecttoy:
0 0
(cid:129).y;y / p p y ;p y : (19.10)
0 0
Djf W 2 62 gj
Practically, the new maximization can be implemented by increasing the local score of each in-
correctpartby(cid:26)beforecallingthemaximizationprocedure.
euseofcostaugmentedinferencesurfaceshighlyincorrectexamplesandresultinmore
losstermsthatdonotcancelout,causingmoreeffectivegradientupdates.224 19. STRUCTUREDOUTPUTPREDICTION
19.1.3 PROBABILISTICOBJECTIVE(CRF)
e error-based and margin-based losses above attempt to score the correct structure above in-
correct ones, but does not tell anything about the ordering of the structures below the highest
scoringone,orthescoredistancesbetweenthem.
Incontrast,adiscriminativeprobabilisticlossattemptstoassignaprobabilitytoeachpos-
siblestructuregiventheinput,suchthattheprobabilityofthecorrectstructureismaximized.e
probabilistic losses are concerned with the scores of all possible structures, not just the highest
scoringone.
In a probabilistic framework (also known as conditional random fields, or CRF), each of
the parts scores is treated as a cliquepotential (see Lafferty et al. [2001], Smith [2011]) and the
scoreofeachstructurey isdefinedtobe:
escoreglobal.x;y/
score .x;y/ P.y x/
 D j D P
y
Y.x/escoreglobal.x;y 0/
02
exp.P
p
yscore local.p//
2 (19.11)
D P
y
Y.x/exp.P
p y
score local.p//
02 2 0
exp.P NN.(cid:30).p///
p y
2 :
D P
y
Y.x/exp.P
p y
NN.(cid:30).p///
02 2 0
e scoring function defines a conditional distribution P.y x/, and we wish to set the param-
j
eters of the network such that corpus conditional log likelihood P .xi;yi/ traininglogP.y i jx i/ is
maximized. 2
elossforagiventrainingexample.x;y/isthen:
L .y ;y/ logscore .x;y/: (19.12)
CRF 0 
D(cid:0)
at is, the loss is related to the distance of the probability of the correct structure from 1. e
CRFlosscanbeseenasanextensionofthehard-classificationcross-entropylosstothestructured
case.
TakingthegradientwithrespecttothelossinEquation(19.12)isasinvolvedasbuilding
the associated computation graph. e tricky part is the denominator (the partition function)
Y
which requires summing over the potentially exponentially many structures in . However, for
some problems, a dynamic programming algorithm exists for efficiently solving the summation
in polynomial time (i.e., the forward-backward viterbi recurrences for sequences and the CKY
inside-outsiderecurrencesfortreestructures).Whensuchanalgorithmexists,itcanbeadapted
toalsocreateapolynomial-sizecomputationgraph.
19.1.4 APPROXIMATESEARCH
Sometimes,efficientsearchalgorithmsforthepredictionproblemarenotavailable.Wemaynot
have an efficient way of finding the best scoring structure (solving the maximization) in Equa-19.1. SEARCH-BASEDSTRUCTUREDPREDICTION 225
tions (19.7), (19.8), or (19.9), or not have an efficient algorithm for computing the partition
function (denominator) in Equation (19.11). In such cases, one can resort to approximateinfer-
encealgorithms,suchasbeamsearch.Whenusingbeamsearch,themaximizationandsummation
arewithrespecttotheitemsinthebeam.Forexample,onemayusebeamsearchforlookingfor
astructurewithanapproximatelyhighscore,andforthepartitionfunctionsumoverthestruc-
tures remaining in the beam instead of over the exponentially large
Y.x/.
A related technique
when working with inexact search is early-update: instead of computing the loss over complete
structures,computeitoverpartialstructuresassoonasthegolditemsfallsoffthebeam.Foran
analysis of the early update techniques and alternative loss-computation and update strategies
whenlearningunderapproximatesearch,seeHuangetal.[2012].
19.1.5 RERANKING
When searching over all possible structures is intractable, inefficient, or hard to integrate into a
model, another alternative to beam search is the use of reranking. In the reranking framework
[Charniak and Johnson, 2005, Collins and Koo, 2005] a base model is used to produce a list of
thek-bestscoringstructures.Amorecomplexmodelisthentrainedtoscorethecandidatesinthe
k-bestlistsuchthatthebeststructurewithrespecttothegoldoneisscoredhighest.Asthesearch
isnowperformedoverkitemsratherthanoveranexponentialspace,thecomplexmodelcancon-
ditionon(extractfeaturesfrom)arbitraryaspectsofthescoredstructure.ebasemodelthatis
used for predicting the k-best structures can be based on a simpler model, with stronger inde-
pendenceassumptions,whichcanproducereasonable,butnotgreat,results.Rerankingmethods
are natural candidates for structured prediction using neural network models, as they allow the
modelertofocusonthefeatureextractionandnetworkstructure,whileremovingtheneedtoin-
tegratetheneuralnetworkscoringintoadecoder.Indeed,rerankingmethodsareoftenusedfor
experimentingwithneuralmodelsthatarenotstraightforwardtointegrateintoadecoder,such
asconvolutional,recurrent,andrecursivenetworks.Worksusingthererankingapproachinclude
Auli et al. [2013], Le and Zuidema [2014], Schwenk et al. [2006], Socher et al. [2013a], Zhu
etal.[2015a],andChoeandCharniak[2016].
19.1.6 SEEALSO
BeyondtheexamplesinSection19.4,sequence-levelCRFswithneuralnetworkcliquepotentials
are discussed in Peng et al. [2009] and Do et al. [2010], where they are applied to sequence la-
belingofbiologicaldata,OCRdata,andspeechsignals,andbyWangandManning[2013]who
applythemontraditionalnaturallanguagetaggingtasks(chunkingandNER).Similarsequence
taggingarchitectureisalsodescribedinCollobertandWeston[2008],Collobertetal.[2011].A
hinge-based approach was used by Pei et al. [2015] for arc-factored dependency parsing with a
manuallydefinedfeatureextractor,andbyKiperwasserandGoldberg[2016b]usingabiLSTM
feature extractor. e probabilistic approach was used by Durrett and Klein [2015] for a CRF
constituencyparser.eapproximatebeam-basedpartitionfunction(approximateCRF)wasef-226 19. STRUCTUREDOUTPUTPREDICTION
fectivelyusedbyZhouetal.[2015]inatransition-basedparser,andlaterbyAndoretal.[2016]
forvarioustasks.
19.2 GREEDYSTRUCTUREDPREDICTION
In contrast to the search-based structured prediction approaches, there are greedy approaches
that decompose the structured problem into a sequence of local prediction problems and train-
ing a classifier to perform each local decision well. At test time, the trained classifier is used
in a greedy manner. Examples of this approach are left-to-right tagging models [Giménez and
Màrquez,2004]andgreedytransition-basedparsing[Nivre,2008].¹Becausetheydonotassume
search,greedyapproachesarenotrestrictedinthekindoffeaturesthatareavailabletothem,and
canuserichconditioningstructures.ismakegreedyapproachesquitecompetitiveintermsof
predictionaccuraciesformanyproblems.
However,thegreedyapproachesareheuristicbydefinition,andhavethepotentialofsuf-
fering from error-propagation: prediction errors that are made early in the sequence cannot be
fixed, and can propagate into larger errors later on. e problem is especially severe when using
a method with a limitedhorizon into the sentence, such as common with window-based feature
extractors.Suchmethodsprocessthesentencetokensinafixedorder,andonlyseealocalwindow
aroundthepredictionpoint.eyhavenowayofknowingwhatthefutureofthesequencehold,
andarelikelytobemisledbythelocalcontextintoincorrectdecisions.
Fortunately,theuseofRNNs(andespeciallybiRNNs)mitigatetheeffectconsiderably.A
feature extractor which is based on a biRNN can essentially see through the end of the input,
and be trained to extract useful information from arbitrarily far sequence positions. is abil-
ity turn greedy local models that are trained with biRNN extractor into greedy global models:
eachdecisioncanconditionontheentiresentence,makingtheprocesslesssusceptibletobeing
“surprised”lateronbyanunexpectedoutput.Aseachpredictioncanbecomemoreaccurate,the
overallaccuracygrowsconsiderably.
Indeed,worksinsyntacticparsingshowthatgreedypredictionmodelsthataretrainedwith
globalbiRNNfeatureextractorsrivaltheaccuracyofsearch-basedmethodsthatcombineglobal
searchwithlocalfeatureextractors[CrossandHuang,2016a,Dyeretal.,2015,Kiperwasserand
Goldberg,2016b,Lewisetal.,2016,Vaswanietal.,2016].
Inadditiontoglobalfeatureextractors,thegreedymethodscanbenefitfromtrainingtech-
niques that aim to mitigate the error propagation problem by either attempting to take easier
predictionsbeforeharderones(theeasy-firstapproach[GoldbergandElhadad,2010])ormak-
ingtrainingconditionsmoresimilartotestingconditionsbyexposingthetrainingprocedureto
inputsthatresultfromlikelymistakes[HalDauméIIIetal.,2009,GoldbergandNivre,2013].
eseareeffectivealsofortraininggreedyneuralnetworkmodels,asdemonstratedbyMaetal.
¹Transition-basedparsersarebeyondthescopeofthisbook,butseeKübleretal.[2008],Nivre[2008],andGoldbergand
Nivre[2013]foranoverview.19.3. CONDITIONALGENERATIONASSTRUCTUREDOUTPUTPREDICTION 227
[2014] (easy-first tagger) and Ballesteros et al. [2016], Kiperwasser and Goldberg [2016b] (dy-
namicoracletrainingforgreedydependencyparsing).
19.3 CONDITIONALGENERATIONASSTRUCTURED
OUTPUTPREDICTION
Finally,RNNgenerators,especiallyintheconditionedgeneratorsetup(Chapter17),canalsobe
seenasaninstanceofstructured-prediction.eseriesofpredictionsmadebythegeneratorpro-
ducesastructuredoutputt .Eachindividualpredictionhasanassociatedscore(orprobability)
O1n
W
score.t t /andweareinterestedinoutputsequencewithmaximalscore(ormaximalprob-
a nb ai tl uit ry e)Oi o, fj i. tO e h1 .W e,i (cid:0) s R1 u Nch Nt mha et anP sn i tDh1 ats tc ho ere s. ct O oi j rt O i1 nWi g(cid:0) f1 u/ ni cs tim onax caim nniz oe td b. eU dn ecf oo mrtu pn oa st ee dly i, ntt ohe fan cto on r- sm tha ar tko alv li oa wn
forexactsearchusingstandarddynamicprogrammingtechniques,andapproximatesearchmust
beused.
One popular approximate technique is using greedyprediction, taking the highest scoring
item at each stage. While this approach is often effective, it is obviously non-optimal. Indeed,
usingbeamsearchasanapproximatesearchoftenworksfarbetterthanthegreedyapproach.
At this stage, it is important to consider how conditioned generators are trained. As de-
scribedinSection17.1.1,generatorsaretrainedusingateacher-forcingtechnique:theyaretrained
usingaprobabilisticobjectivethatattemptstoassignhighprobabilitymasstogoldobservedse-
quences.Givenagoldsequencet ,ateachstageithemodelistrainedtoassignahighprobability
1n
W
masstothegoldeventt t conditionedonthegoldhistoryt .
Oi
D
i 1 Wi (cid:0)1
ere are two shortcomings with this approach: first, it is based on the goldhistory t
1i 1
W (cid:0)
while in practice the generator will be tasked with assigning scores based on its predictedhistory
t .Second,itisalocallynormalizedmodel:themodelassignsaprobabilitydistributionover
O1i 1
W (cid:0)
eachevent,andthussusceptibletothelabelbiasproblem,²whichcanhurtthequalityofsolutions
returnedbybeamsearch.BothoftheseproblemsweretackledintheNLPandmachine-learning
communities,butarenotyetfullyexploredintheRNNgenerationsetting.
efirstproblemcanbemitigatedusingtrainingprotocolssuchSEARN[HalDauméIII
et al., 2009], DAGGER [Ross and Bagnell, 2010, Ross et al., 2011], and exploration-training
withdynamicoracles[GoldbergandNivre,2013].Applicationofthesetechniquesinthecontext
ofRNNgeneratorsisproposedbyBengioetal.[2015]underthetermscheduledsampling.
e second problem can be treated by discarding of the locally normalized objective and
moving to global, sequence-level objectives that are more suitable for beam decoding. Such ob-
jectivesincludethebeamapproximationsofthestructuredhingeloss[Equation(19.8)]andthe
CRFloss[Equation(19.11)]discussedinSection19.1.4above.WisemanandRush[2016]dis-
cussglobalsequence-levelscoringobjectivesforRNNgenerators.
²Foradiscussionofthelabelbiasproblem,seeSection3ofAndoretal.[2016]andthereferencestherein.228 19. STRUCTUREDOUTPUTPREDICTION
19.4 EXAMPLES
19.4.1 SEARCH-BASEDSTRUCTUREDPREDICTION:FIRST-ORDER
DEPENDENCYPARSING
Consider the dependency-parsing task, described in Section 7.7. e input is an n-words sen-
tences w ;:::;w ,andweareinterestedinfindingadependencyparsetreey overthesentence
1 n
D
(Figure 7.1). A dependency parse tree is a rooted directed tree over the words in the sentence.
Every word in the tree is assigned a single parent (its head), that can be either another word in
thesentenceorspecialROOTelement.eparentwordiscalledahead anditsdaughterwords
arecalledmodifiers.
Dependency parsing fits nicely in the search-based structured prediction framework de-
scribedinSection19.1.Specifically,Equation(19.5)statesthatweshouldassignscorestotrees
by decomposing them into parts and scoring each part individually. e parsing literature de-
scribesmanypossiblefactorizations[KooandCollins,2010,ZhangandMcDonald,2012],here
we focus on the simplest one, due to McDonald et al. [2005]: the arc-factored decomposition.
Eachpartwillbeanarcinthetree(i.e.,pairofheadwordw andmodifierwordw ).Eacharc
h m
.w ;w / will be scored individually based on a local scoring function that will asses the quality
h m
oftheattachment.Afterassigningascoretoeachofthepossiblen2 arcs,wecanrunaninference
alogorithmsuchastheEisneralgorithm[EisnerandSatta,1999,Kübleretal.,2008,McDonald
etal.,2005]tofindthevalidprojectivetree³whosesumofarcscoresismaximal.
Equation(19.5)thenbecomes:
score .x;y/ X score .w ;w / X NN.(cid:30).h;m;s//; (19.13)
global local h m
D D
.wh;wm/ y .wh;wm/ y
2 2
where (cid:30).h;m;s/ is a feature function translating the sentence indices h and m into real-valued
vectors.WediscussedfeatureextractorsfortheparsingtaskinSections7.7and8.6(usingman-
ually designed features) and in Section 16.2.3 (using a biRNN feature extractor). Here, assume
thefeatureextractorisgivenandfocusonthetrainingprocedure.
Once we decide on a particular form for the NN component (say an MLP, NN.x/
D
.tanh.xU b// v),wecaneasilycomputethescorea ofeachpossiblearch(assumingthe
(cid:140)h;m(cid:141)
C (cid:1)
indexofROOTis0):
a .tanh.(cid:30).h;m;s//U b/ v h 0;:::;n
(cid:140)h;m(cid:141)
D C (cid:1) 8 2 (19.14)
m 1;:::;n:
8 2
³Parsingpeopletalkaboutprojectiveandnon-projectivetrees.Projectivetreesposeadditionalconstraintsontheformofthe
tree:thatitcanbedrawnoveralinearizationofthewordsinthesentenceintheiroriginalorder,withoutcrossingarcs.While
thedistinctionisanimportantoneintheparsingworld,itisbeyondthescopeofthisbook.Formoredetails,seeKübleretal.
[2008]andNivre[2008].19.4. EXAMPLES 229
WethenruntheEisneralgorithm,resultinginapredictedtreey withmaximalscore:
0
y max X a Eisner.n;a/:
0 (cid:140)h;m(cid:141)
D y Y D
2 .h;m/ y
2
Ifweweretousecost-augmentedinference,wewouldhaveusedinsteadthescoresa:
N
(0 if.h;m/ y
a a 2
(cid:140)h;m(cid:141) (cid:140)h;m(cid:141)
N D C (cid:26) otherwise:
Once we have the predicted tree y and gold tree y, we can create a computation graph for the
0
structuredhingelossofthetrees,accordingto:
max.0;1 X tanh.(cid:30).h;m;s//U b/ v X tanh.(cid:30).h;m;s//U b/ v/:
0 0
C C (cid:1) (cid:0) C (cid:1)
.h;m/ y .h;m/ y
0 0 2 0 2
(cid:132) maxy 0⁄ys(cid:131)co(cid:130)reglobal.s;y 0/ (cid:133) (cid:132) scoreg(cid:131) lob(cid:130) al.s;y/ (cid:133)
(19.15)
We then compute the gradients with respect to the loss using backprop, update the parameters
accordingly,andmovetothenexttreeinthetrainingset.
isparsingapproachisdescribedinPeietal.[2015](usingthemanuallydesignedfeature
function from Section 8.6) and Kiperwasser and Goldberg [2016b] (using the biRNN feature
extractorfromSection16.2.3).
19.4.2 NEURAL-CRFFORNAMEDENTITYRECOGNITION
Independent Classification Consider the named entity recognition task described in Sec-
tion7.5.Itisasequencesegmentationtaskwhichisoftenmodeledassequencetagging:eachword
inthesentenceisassignedoneofKBIO-tagsdescribedinTable7.1,andthetaggingdecisionsare
thendeterministicallytranslatedintospans.InSection7.5wetreatedNERasaword-in-context
classificationproblem,assumingeachtaggingdecisionforeachwordisperformedindependently
oftheothers.
Undertheindependentclassificationframework,wearegivenasentences w ;:::;w ,
1 n
D
anduseafeaturefunction(cid:30).i;s/tocreateafeaturevectorrepresentingthewordw inthecontext
i
ofthesentence.en,aclassifiersuchasanMLPisusedtopredictascore(oraprobability)to
eachtag:
t softmax.MLP.(cid:30).i;s/// i 1;:::;n (19.16)
Oi
D 8 2 I
here,t
Oi
isavectorofpredictedtagscores,andt
Oi(cid:140)k(cid:141)
isthescoreoftaggingwordi withtagk.
epredictedtaggingy ;:::;y forthesentenceisthenobtainedbyindependentlychoosingthe
1 n
O O
highestscoringtagforeachsentenceposition:
y Oi Dargmaxt Oi(cid:140)k(cid:141) 8i 21;:::;n; (19.17)
k230 19. STRUCTUREDOUTPUTPREDICTION
andthescoreoftheassignmenty y ;:::;y is:
1 n
O D O O
n
score.s;y/ Xt : (19.18)
O D
i(cid:140)y Oi(cid:141)
i 1
D
Structured Tagging by Coupling Tag-Pair Decisions e independent classification approach
mayworkreasonablywellinmanycases,butissub-optimalbecauseneighboringdecisionsinflu-
enceeachother.ConsiderasequencesuchasParisHilton:thefirstwordcanbeeitheralocation
oraperson,andthesecondwordcanbeeitheranorganizationoraperson,butifwechoseone
ofthemtobeaperson,thesecondoneshouldbetaggedpersonwithcertainty.Wewouldliketo
have the different tagging decisions influence each other, and have this reflected in the score. A
commonwaytodothisisbyintroducingtag-tagfactors:compatibilityscoresforpairsofneigh-
boring tags. Intuitively, a pair such as B-PER I-PER should receive a high score, while a pair
B-PER I-ORGshouldreceiveaverylow,orevennegativescore.ForatagsetofK possibletags,
we introduce a scoring matrix A RK K in which A is the compatibility score of the tag
(cid:2) (cid:140)g;h(cid:141)
2
sequenceg h.
e scoring function for a tagging assignment is updated to take the tagging factors into
account:
n n 1
C
score.s;y/ Xt XA ; (19.19)
O D i(cid:140)y Oi(cid:141) C (cid:140)y Oi (cid:0)1;y Oi(cid:141)
i 1 i 1
D D
wherethetagsatlocations0andn 1arespecial*START*and*END*symbols.Giventagging
C
scores for individual words t and the values in A, one can find the sequence y maximizing
1n
W O
Equation(19.19)usingtheViterbidynamic-programmingalgorithm.
Aswedonotneedthetagscoresineachpositiontobepositiveandsumtoone,weremove
thesoftmaxwhencomputingthescorest :
i
t MLP.(cid:30).i;s// i 1;:::;n: (19.20)
Oi
D 8 2
e tagging scores t are determined by a neural network according to Equation (19.20),
i
and the matrix A can be considered as additional model parameters. We can now proceed to
trainastructuredmodelusingthestructuredhinge-loss[Equation(19.8)]orthecost-augmented
structuredhingeloss[Equation(19.9)].
Instead,wewillfollowLampleetal.[2016]andusetheprobabilisticCRFobjective.
StructuredCRFTraining UndertheCRFobjective,ourgoalistoassignaprobabilitytoeach
possibletagsequencey y ;:::;y overasentences.isismodeledbytakingasoftmaxover
1 n
D19.4. EXAMPLES 231
allthepossibletaggings:
escore.s;y/
score .s;y/ P.y s/
 D j D
P
y
Y.s/escore.s;y 0/
02 (19.21)
exp.Pn
i
1t
i(cid:140)yi(cid:141)
CPn
i
1A
(cid:140)yi;yi
1(cid:141)/
:
D Py 0 2Y.s/expD .Pn i D1t i(cid:140)y i0(cid:141) CD Pn i D1AC (cid:140)y i0;y i0 C1(cid:141)/
edenominatoristhesameforallpossibletaggingsy,sofindingthebestsequence(withoutits
probability)amountstofindingthesequencethatmaximizesscore.s;y/,andcanbedoneusing
Viterbiasabove.
elossisthendefinedasthenegativeloglikelihoodofthecorrectstructurey:
n 1 n 1 ! n 1 n 1 !
C C C C
logP.y s/ Xt XA log X exp Xt XA
(cid:0) j D(cid:0)
i(cid:140)yi(cid:141)
C
(cid:140)yi 1;yi(cid:141)
C
i(cid:140)y i0(cid:141)
C
(cid:140)y
i0
1;y i0(cid:141)
i D1 i D1 (cid:0) y 0 2Y.s/ i D1 i D1 (cid:0)
n 1 n 1 ! n 1 n 1 !
C C C C
Xt XA M Xt XA ;
D(cid:0)
i(cid:140)yi(cid:141)
C
(cid:140)yi 1;yi(cid:141)
C
i(cid:140)y i0(cid:141)
C
(cid:140)y
i0
1;y i0(cid:141)
i D1 i D1 (cid:0) y 0 2Y.s/ i D1 i D1 (cid:0)
(cid:132) score(cid:131)o(cid:130)fgold (cid:133)
(cid:132) usingdyn(cid:131)am(cid:130)icprogram (cid:133)
(19.22)
whereLdenotesadditioninlog-space(logadd)andL.a;b;c;d/ log.ea eb ec ed/.
D C C C
efirsttermcanbeeasilyconstructedasacomputationgraph,butthesecondisabitlesstrivial
to construct, as it requires summing over the nk different sequences in Y.s/. Fortunately, it can
besolvedusingavariantoftheViterbialgorithm⁴whichwedescribebelow.
Properties of Log-addition e log-add operation performs addition in log-space. It has
the following properties that we use in constructing the dynamic program. ey are trivial
toprovewithbasicmathematicmanipulation,andthereaderisencouragedtodoso.
M.a;b/ M.b;a/ Commutativity (19.23)
D
M.a;M.b;c// M.a;b;c/ Associativity (19.24)
D
M.a c;b c/ M.a b/ c Distributivity (19.25)
C C D C C
..
⁴isalgorithmisknownastheforward algorithm,whichisdifferentthanthealgorithmforcomputingtheforward passin
thecomputationgraph.232 19. STRUCTUREDOUTPUTPREDICTION
DenotebyY.s;r;k/thesetofsequencesoflengthr thatendwithsymbolk.esetofall
possible sequences over s is then Y.s/ Y.s;n 1;*END*/. Further denote by Y.s;r;‘;k/
j j D C
the sequences of length r where the last symbol is k and the second to last symbol is ‘. Let
(cid:128) as(cid:140)r h; ok r(cid:141) thD andL ,dy efi0 2nY e.s f;r .;k i/ ;yPr i
D
;1 y.t /i(cid:140)y i0(cid:141) tCA (cid:140)y i0 (cid:0)A1;y i0(cid:141)/.Ou .r Wgo eal ni os wco gm etp :uting(cid:128)(cid:140)n C1;*END*(cid:141).As
i0 1 i0 D i(cid:140)y i0(cid:141) C (cid:140)y i0 1;y i0(cid:141)
(cid:0) (cid:0)
r
(cid:128)(cid:140)r;k(cid:141) M Xf.i;y ;y /
D i0 1 i0
(cid:0)
y Y.s;r;k/i 1
02 D
r 1 !
C
(cid:128)(cid:140)r 1;k(cid:141) M M Xf.i;y ;y /
C D i0 1 i0
(cid:0)
‘ y Y.s;r 1;‘;k/ i 1
02 C D
r !
DM M X(cid:0)f.i;y
i0
1;y i0/(cid:1) Cf.r C1;y
r0 1
D‘;y
r0
Dk/
(cid:0) (cid:0)
‘ y Y.s;r 1;‘;k/ i 1
02 C D
0 r ! 1
M M Xf.i;y ;y / f.r 1;y ‘;y k/
D @ i0 1 i0 C C r0 1 D r0 D A
(cid:0) (cid:0)
‘ y Y.s;r 1;‘;k/ i 1
02 C D
DM(cid:0)(cid:128)(cid:140)r;‘(cid:141) Cf.r C1;y
r0 1
D‘;y
r0
Dk/(cid:1)
(cid:0)
‘
DM(cid:0)(cid:128)(cid:140)r;‘(cid:141) Ct
r
C1(cid:140)k(cid:141)CA (cid:140)‘;k(cid:141)(cid:1):
‘
Weobtainedtherecurrence:
(cid:128)(cid:140)r C1;k(cid:141) DM(cid:0)(cid:128)(cid:140)r;l(cid:141) Ct r C1(cid:140)k(cid:141)CA (cid:140)‘;k(cid:141)(cid:1) (19.26)
‘
which we can use to construct the computation graph for computing the denominator, (cid:128)(cid:140)n
C
1;*END*(cid:141).⁵ After building the computation graph, we can compute the gradients using back-
propagation.
19.4.3 APPROXIMATENER-CRFWITHBEAM-SEARCH
Intheprevioussection,wetransformedtheNERpredictionintoastructuredtaskbycouplingthe
outputtagsatpositionsi andi 1usingascorematrixA assigningascoretoeachconsecutive
(cid:0)
tag pair. is is akin to using a first-order markov assumption in which the tag in position i is
independentofthetagsatpositions<i 1giventhetagati 1.isindependenceassumption
(cid:0) (cid:0)
allowed us to decompose the sequence scoring and derive efficient algorithms for finding the
highestscoringsequenceaswellasthesumoverallpossibletagsequences.
⁵Observethatthisrecursionisthesameastheoneforthebest-pathViterbialgorithm,withLreplacedwithamax.19.4. EXAMPLES 233
We may want to relax this markov independence assumption and instead condition the
tag y at all previous tags y . is can be incorporated into the tagging model by adding an
i 1i 1
W (cid:0)
additionalRNNoverthetaghistory.Wenowscoreatagsequencey y ;:::;y as:
1 n
D
n 1
C
score.s;y/ Xf.(cid:140)(cid:30).s;i/ RNN.y /(cid:141)/; (19.27)
1i
O D I O W
i 1
D
where f is a parametric function such as a linear transformation or an MLP, and (cid:30) is a feature
functionmappingthewordaspositioni inthesentences toavector.⁶Inwords,wecomputethe
localscoreoftaggingpositioni withtagk byconsideringfeaturesofsentencepositioni,aswell
asanRNNencodingofthetagsequencey ;y ;y ;k.Wethencomputetheglobalscoreasa
1 2 i 1
(cid:0)
sumoflocalscores.
Unfortunately,theRNNcomponenttiesthedifferentlocalscoresoverallprevioustagging
decisions, preventing us from using efficient dynamic programming algorithms for finding the
exact best tagging sequence or the sum of all possible tag sequences under the model. Instead,
we must resort to approximation such as beamsearch. Using a beam of size r, we can develop r
differenttagsequencesy1;:::;yr.⁷eapproximatebesttagsequenceisthenthehighestscoring
O O
ofther beamsequences:
argmaxscore.s;yi/:
O
i 1;:::;r
2
Fortraining,wecanusetheapproximateCRFobjective:
escore.s;y/
score .s;y/ P.y s/ (19.28)
AC D Q j D
Py
0
Y.s;r/escore.s;y 0/
2Q
L .y ;y/ logP.y s/
CRF 0 D(cid:0) Q j
score.s;y/ log X escore.s;y 0/ (19.29)
D(cid:0) C
y 0 Y.s;r/
2Q
Y.s;r/ y1;:::;yr y :
Q Df g[f g
InsteadofnormalizingbysummingovertheentiresetofsequencesY.s/,wesumoverY.s;r/:the
Q
unionofthegoldsequenceandther beamsequences.r isasmallnumber,makingthesummation
istrivial.Asr approachesnK weapproachthetrueCRFobjective.
⁶efeaturefunction(cid:30)canbebasedonawordwindoworabiLSTM,similartothefeaturefunctionsforPOS-taggingin
Sections8.5and16.2.1.
⁷ebeamsearchalgorithmworksinstages.Afterobtainingr possibletagsequencesoflengthi (y1 ;:::;yr )corre-
O1i O1i
spondingtothefirstiwordsofthesentence,weextendeachsequencewithallpossibletags,scoreeachofWtheresultiWngr K
sequences,andretainthetopscoringrsequences.eprocesscontinuesuntilwehavertagsequencesovertheentiresente(cid:2)
nce.235
C H A P T E R 20
Cascaded, Multi-task and
Semi-supervised Learning
When processing natural language, it is often the case that we have several tasks that feed into
each other. For example, the syntactic parser we discussed in Sections 7.7, 16.2.3, and 19.4.1
takesasinputpartsofspeechtags,thatareinthemselvesautomaticallypredictedbyastatistical
model. Feeding the predictions of one model as the input of another, when the two models are
independent, is called a pipeline system. An alternative approach is model cascading. In model
cascading,ratherthanfeedingthepredictionsofmodelA(thetagger)intomodelB(theparser),
weinsteadfeedintotheparsertheintermediaterepresentationsthatareinformativeforpredicting
thetags.atis,ratherthancommittingtoaparticulartaggingdecision,wepassonthetagging
uncertaintytotheparser.Modelcascadingisveryeasytoimplementindeeplearningsystem,by
simplypassingthevectorbeforetheargmax,orevenoneofthehiddenvectors.
A related technique is multi-task learning [Caruana, 1997], in which we have several re-
lated predictions tasks (that may or may not feed into each other). We would like to leverage
theinformationinoneofthetasksinordertoimprovetheaccuracyontheothertasks.Indeep
learning,theideaistohavedifferentnetworksforthedifferenttasks,butletthenetworksshare
someoftheirstructureandparameters.isway,acommonpredictivecore(thesharedstructure)
isinfluencedbyallthetasks,andtrainingdataforonetaskmayhelpimprovethepredictionsof
theotherones.
Acascadingapproachlendsitselfnaturallytothemulti-tasklearningframework:insteadof
justpassingintheintermediateoutputofthetaggertotheparser,wecaninsteadpluginthesub-
graphofthecomputationgraphthatisresponsiblefortheintermediatetaggingrepresentationas
inputtotheparser’scomputationgraph,andbackpropagatetheparser’serrorallthewaybackto
the(nowshared)baseofthetaggingcomponent.
Another related and similar case is that of semi-supervisedlearning, in which we have su-
pervisedtrainingdatafortaskA,andwhattouseannotatedorunannotateddataforothertasks
inordertoimprovetheperformanceontaskA.
ischapterdealswiththesethreetechniques.
20.1 MODELCASCADING
Inmodel-cascading,largenetworksarebuiltbycomposingthemoutofsmallercomponentnet-
works.Forexample,inSection16.2.1wedescribeanRNN-basedneuralnetworkforpredicting236 20. CASCADED,MULTI-TASKANDSEMI-SUPERVISEDLEARNING
thepartofspeechofawordbasedonitssententialcontextandthecharactersthatcomposeit.In
apipelineapproach,wewouldusethisnetworkforpredictingpartsofspeech,andthenfeedthe
predictionsasinputfeaturestoaneuralnetworkthatdoessyntacticchunkingorparsing.
Instead, we could think of the hidden layers of this network as an encoding that captures
therelevantinformationforpredictingthepartofspeech.Inacascadingapproach,wetakethe
hiddenlayersofthisnetworkandconnectthem(andnotthepartofspeechpredictionthemselves)
astheinputsforthesyntacticnetwork.Wenowhavealargernetworkthattakesasinputsequences
ofwordsandcharacters,andoutputsasyntacticstructure.
As a concrete example, consider the tagging and parsing networks described in Sec-
tions 16.2.1 and 16.2.3. e tagging network [Equation (16.4)], reproduced here, predicts the
tagoftheithwordaccordingto:
t argmaxsoftmax.MLP.biRNN.x ;i///
i 1n (cid:140)j(cid:141)
D j W
(20.1)
x (cid:30).s;i/ (cid:140)E RNNf.c / RNNb.c /(cid:141)
i
D D
(cid:140)wi(cid:141)
I
1 W‘
I
‘ W1
whiletheparsingnetwork[Equation(16.6)]assignsarc-scoresaccordingto:
AS.h;m;w ;t / MLP.(cid:30).h;m;s// MLP.(cid:140)v v (cid:141)/
1n 1n h m
W W D D I
v biRNN?.x / (20.2)
1n 1n
W D W
x (cid:140)w t (cid:141):
i i i
D I
e important thing to note here is that the parser takes as input words w andtags t , and
1n 1n
W W
then converts the words and tags into embedding vectors, and concatenates them to form its
correspondinginputrepresentationsx .
1n
W
In the cascading approach, we’ll feed the tagger’s pre-prediction state directly into the
parser, in one joint network. Concretely, denote by z the tagger’s pre-prediction for word i:
i
z MLP.biRNN.x ;i//.Wecannowusez astheinputrepresentationoftheithwordin
i 1n i
D W
theparser,resultingin:
AS.h;m;w / MLP .(cid:30).h;m;s// MLP .(cid:140)v v (cid:141)/
1n parser parser h m
W D D I
v biRNN? .z /
1 Wn D parser 1 Wn
(20.3)
z MLP .biRNN .x ;i//
i tagger tagger 1n
D W
x (cid:30) .s;i/ hE RNNf .c / RNNb .c /i:
i D tagger D (cid:140)wi(cid:141) I tagger 1 W‘ I tagger ‘ W1
e computation graph abstraction allows us to easily propagate the error gradients from the
syntactictasklossallthewaybacktothecharacters.¹
Figure20.1presentsasketchoftheentirenetwork.
¹Dependingonthesituation,wemayormaynotwanttobackpropagatetheerrorallthewayback.20.1. MODELCASCADING 237
ø
(jumped, brown, s)
concat
BI BI BI BI BI
parse parse parse parse parse
BI BI BI BI BI
parse parse parse parse parse
DET ADJ NN VB IN
pred pred pred pred pred
BI BI BI BI BI
tag tag tag tag tag
BI BI BI BI BI
tag tag tag tag tag
BI BI BI BI BI
tag tag tag tag tag
ø ø ø ø ø
(the) (brown) (fox) (jumped) (over)
… … … …
concat
E
[brown] Rf Rf Rf Rf Rf Rf Rf Rb Rb Rb Rb Rb Rb Rb
c
*S*
cb cr co cw cn c
*E*
c
*E*
cn cw co cr cb c
*S*
Figure20.1: Tagging-parsingcascadenetwork[Equation(20.3)].238 20. CASCADED,MULTI-TASKANDSEMI-SUPERVISEDLEARNING
While the parser has access to the word identities, they may be diluted by the time they pass
throughallthetaggerRNNlayers.Toremedythis,wemayuseaskip-connection,andpassthe
wordembeddingsE directlytotheparser,inadditiontothetagger’soutput:
(cid:140)wi(cid:141)
AS.h;m;w / MLP .(cid:30).h;m;s// MLP .(cid:140)v v (cid:141)/
1n parser parser h m
W D D I
v biRNN? .z /
1 Wn D parser 1 Wn
z (cid:140)E z (cid:141)
i
D
(cid:140)wi(cid:141)
I
0i (20.4)
z MLP .biRNN .x ;i//
0i
D
tagger tagger 1 Wn
x (cid:30) .s;i/ hE RNNf .c / RNNb .c /i:
i D tagger D (cid:140)wi(cid:141) I tagger 1 W‘ I tagger ‘ W1
isarchitectureisdepictedinFigure20.2.
To combat the vanishing gradient problem of deep networks, as well as to make better use of
availabletrainingmaterial,theindividualcomponentnetwork’sparameterscanbebootstrapped
bytrainingthemseparatelyonarelevanttask,beforepluggingthemintothelargernetworkfor
furthertuning.Forexample,thepart-of-speechpredictingnetworkcanbetrainedtoaccurately
predict parts-of-speech on a relatively large annotated corpus, before plugging its hidden layer
into the syntactic parsing network for which less training data is available. In case the training
dataprovidedirectsupervisionforbothtasks,wecanmakeuseofitduringtrainingbycreatinga
networkwithtwooutputs,oneforeachtask,computingaseparatelossforeachoutput,andthen
summingthelossesintoasinglenodefromwhichwebackpropagatetheerrorgradients.
Modelcascadingisverycommonwhenusingconvolutional,recursive,andrecurrentneural
networks,where,forexample,arecurrentnetworkisusedtoencodeasentenceintoafixedsized
vector,whichisthenusedastheinputofanothernetwork.esupervisionsignaloftherecurrent
networkcomesprimarilyfromtheuppernetworkthatconsumestherecurrentnetwork’soutput
asitinputs.
Inourexample,boththetaggerandtheparserwerebasedonabiRNNbackbone.isis
notnecessary—eitherorbothofthenetworkscouldjustaswellbeafeed-forwardnetworkthat
gets a word-window as input, a convolutional network, or any other architecture that produces
vectorsandthatcanpassgradients.
20.2 MULTI-TASKLEARNING
Multi-tasklearning(MTL)isarelatedtechnique,inwhichwehaveseveralrelatedtasksthatwe
assumearecorrelated,inthesensethatlearningtosolveoneislikelytoprovide“intuitions”about
solving the other. For example, consider the syntactic chunking task (see Linguistic Annotation
frame in Section 6.2.2), in which we annotate a sentence with chunk boundaries, producing
outputsuchas:
[ theboy][ with][ theblackshirt][ opened][ thedoor][ with][ akey]
NP PP NP VP NP PP NP20.2. MULTI-TASKLEARNING 239
ø
(jumped, brown, s)
concat
BI BI BI BI BI
parse parse parse parse parse
BI BI BI BI BI
parse parse parse parse parse
concat concat concat concat concat
E E E E E
[the] [brown] [fox] [jumped] [over]
DET ADJ NN VB IN
pred pred pred pred pred
BI BI BI BI BI
tag tag tag tag tag
BI BI BI BI BI
tag tag tag tag tag
BI BI BI BI BI
tag tag tag tag tag
ø ø ø ø ø
(the) (brown) (fox) (jumped) (over)
… … … …
concat
E
[brown] Rf Rf Rf Rf Rf Rf Rf Rb Rb Rb Rb Rb Rb Rb
c
*S*
cb cr co cw cn c
*E*
c
*E*
cn cw co cr cb c
*S*
Figure 20.2: Tagging-parsing cascade with skip-connections for the word embeddings [Equa-
tion(20.4)].240 20. CASCADED,MULTI-TASKANDSEMI-SUPERVISEDLEARNING
Like named-entity recognition, chunking is a sequence-segmentation task, and can be reduced
toataggingtaskusingtheBIOencodingscheme(seeSection7.5).Anetworkforchunkingthen
maybemodeledasadeepbiRNN,followedbyanMLPforindividualtagpredictions:
p.chunkTag j/ softmax.MLP .biRNN .x ;i///
i D D chunk chunk 1 Wn (cid:140)j(cid:141) (20.5)
x (cid:30).s;i/ Ecnk
i
D D
(cid:140)wi(cid:141)
(forbrevity,weremovedthecharacter-levelRNNsfromtheinput,buttheycanbetriviallyadded.)
NotethatthisisverysimilartoaPOS-taggingnetwork:
p.posTag j/ softmax.MLP .biRNN .x ;i///
i D D tag tag 1 Wn (cid:140)j(cid:141) (20.6)
x (cid:30).s;i/ Etag :
i
D D
(cid:140)wi(cid:141)
DET ADJ NN VB IN B-NP I-NP I-NP B-VP B-PP
pred pred pred pred pred pred pred pred pred pred
BI BI BI BI BI BI BI BI BI BI
BI BI BI BI BI BI BI BI BI BI
BI BI BI BI BI BI BI BI BI BI
x x x x x x x x x x
the brown fox jumped over the brown fox jumped over
Figure20.3: Left:POStaggingnetwork.Right:Chunktaggingnetwork.
BothnetworksaredepictedinFigure20.3.Differentcolorsindicatedifferentsetsofparameters.
e syntactic chunking task is synergistic with part-of-speech tagging. Information for
predicting chunk boundaries, or the part-of-speech of a word, rely on some shared underlying
syntacticrepresentation.Insteadoftrainingaseparatenetworkforeachtask,wecancreateasingle
networkwithseveraloutputs.ecommonapproachwouldbetosharethebiRNNparameters,
but have a dedicated MLP predictor for each task (or have also a shared MLP, in which only
thefinalmatrixandbiastermsarespecializedforatask).iswillresultinthefollowing,shared
network:
p.chunkTag j/ softmax.MLP .biRNN .x ;i///
i D D chunk shared 1 Wn (cid:140)j(cid:141)
p.posTag j/ softmax.MLP .biRNN .x ;i/// (20.7)
i D D tag shared 1 Wn (cid:140)j(cid:141)
x (cid:30).s;i/ Eshared :
i
D D
(cid:140)wi(cid:141)
e two networks use the same deep biRNN and embedding layers, but separate final output
predictors.isisdepictedinFigure20.4.20.2. MULTI-TASKLEARNING 241
DET B-NP ADJ I-NP NN I-NP VB B-VP IN B-PP
pred pred pred pred pred pred pred pred pred pred
BI BI BI BI BI
BI BI BI BI BI
BI BI BI BI BI
x x x x x
the brown fox jumped over
Figure20.4: AjointPOS-taggingandChunkingnetwork.ebiRNNparametersareshared,and
thebiRNNcomponentisspecializedforbothtasks.efinalpredictorsareseparate.
Most of the parameters of the network are shared between the different tasks. Useful in-
formationlearnedfromonetaskcanthenhelptodisambiguateothertasks.
20.2.1 TRAININGINAMULTI-TASKSETUP
e computation graph abstraction makes it very easy to construct such networks and compute
the gradients for them, by computing a separate loss for each available supervision signals, and
then summing the losses into a single loss that is used for computing the gradients. In case we
haveseveralcorpora,eachwithdifferentkindofsupervisionsignal(e.g.,wehaveonecorpusfor
POS and another for chunking), the preferred training protocol would be to choose a corpus
at random, pass the example through the relevant part of the computation graph, compute the
loss,backpropagatetheerror,andupdatetheparameters.en,onthenextstep,againchoosea
corpusatrandomandsoon.Inpractice,thisisoftenachievedbyshufflingalltheavailabletraining
examples and going through them in order. e important part is that we potentially compute
thegradientswithrespecttoadifferentloss(andusingadifferentsub-network)foreachtraining
example.
Insomecases,wemayhaveseveraltasks,butcaremoreaboutoneofthem.atis,wehave
oneormoremaintasks,andafewothersupportingtaskwhichwebelievecanhelpthemaintask,
but whose predictionswe do not careabout. In such cases, wemaywant to scale the loss of the
supportingtasktobesmallerthanthelossofthemaintasks.Anotheroptionistofirstpre-train242 20. CASCADED,MULTI-TASKANDSEMI-SUPERVISEDLEARNING
a network on the supporting tasks, and then take the shared components of this network and
continuetrainingitonlyonthemaintask.
20.2.2 SELECTIVESHARING
Going back to the POS-tagging and Chunking example, we could argue that while the tasks
share information, the POS-tagging task is in fact somewhat morelowlevel than the chunking
task:theinformationneededforperformingchunkingismorerefinedthanthatneededforPOS-
tagging.Insuchcases,wemayprefertonotsharetheentiredeepbiRNNbetweenthetwotasks,
butratherhavethelowerlayerofthebiRNNbeshared,andtheupperlayersbededicatedtothe
chunkingtask(Figure20.5).
B-NP I-NP I-NP B-VP B-PP
pred pred pred pred pred
BI BI BI BI BI
BI BI BI BI BI
DET ADJ NN VB IN
pred pred pred pred pred
BI BI BI BI BI
x x x x x
the brown fox jumped over
Figure20.5: Aselectivelyshared POS-taggingandChunkingnetwork.elowerlayerofthebiRNN
issharedbetweenthetwotasks,buttheupperlayersarededicatedtochunking.
elowerlayerinthebiRNNissharedbetweenthetwotasks.Itisprimarilysupervisedby
thePOStask,butalsoreceivesgradientsfromthechunkingsupervision.eupperlayersofthe
networkarededicatedtothechunkingtask—butaretrainedtoworkwellwiththerepresentation
ofthelowerlayers.
is selective sharing suggestion follows the work of Søgaard and Goldberg [2016]. A
similarapproach,usingfeed-forwardratherthanrecurrentnetworks,istakenbyZhangandWeiss
[2016]underthenamestackpropagation.
eselectivelysharedMTLnetworkinFigure20.5isverysimilarinspirittothecascaded
setupindiscussedintheprevioussection(Figure20.1).Indeed,itisoftenhardtoproperlydraw
theboundarybetweenthetwoframeworks.20.2. MULTI-TASKLEARNING 243
Input-output Inversion Another view of multi-task and cascaded learning is one of input-
output inversion. Instead of thinking of some signal (say POS-tags) as inputs to a higher level
task(sayparsing),wecanthinkofthemasoutputs ofintermediatelayersinthenetworkforthe
higherleveltasks.atis,insteadofusingtheparts-of-speechtagsasinputs,theyareusedinstead
asasupervisionsignaltointermediatelayersofthenetwork.
20.2.3 WORD-EMBEDDINGSPRE-TRAININGASMULTI-TASKLEARNING
e chunking and POS-tagging tasks (and indeed, many others) are also synergistic with the
language modeling task. Information for predicting the chunk boundary are the part-of-speech
tagofawordisintimatelyconnectedwiththeabilitytopredicttheidentityofthenextword,or
thepreviousone:thetasksshareacommonsyntactic-semanticbackbone.
Viewedthisway,theuseofpre-trainedwordvectorsforinitializingtheembeddinglayerof
atask-specificnetworkisaninstanceofMTL,withlanguagemodelingasasupportingtask.e
word embedding algorithms are trained with an distributional objective that is a generalization
oflanguagemodeling,andthewordembeddinglayeroftheembeddingalgorithmsisthenshared
withtheothertask.
ekindofsupervisionforthepre-trainingalgorithm(i.e.,thechoiceofcontexts)should
be matched to the task the specialized network is trying to solve. Closer tasks results in larger
benefitsfromMTL.
20.2.4 MULTI-TASKLEARNINGINCONDITIONEDGENERATION
MTLcanbeseamlesslyintegratedintotheconditionedgenerationframeworkdiscussedinChap-
ter17.isisdonebyhavingasharedencoder feedingintodifferentdecoders,eachattemptingto
performadifferenttask.iswillforcetheencodertoencodeinformationthatisrelevanttoeach
of the tasks. Not only can this information then be shared by the different decoders, it also will
potentially allow for training different decoders on different training data, enlarging the overall
numberofexamplesavailablefortraining.WediscussaconcreteexampleinSection20.4.4.
20.2.5 MULTI-TASKLEARNINGASREGULARIZATION
Anotherviewofmulti-tasklearningisoneofaregularizer.esupervisionfromthesupporting
tasks prevent the network from overfitting on the main task, by forcing the shared representa-
tiontobemoregeneral,andusefulforpredictionbeyondthetraininginstancesofthemaintask.
Viewedthisway,andwhenthesupportingtasksaremeanttobeusedasregularizers,oneshould
notperformtheMTLinasequencewherethesupportingtasksaretunedfirstfollowedbyadapt-
ingtherepresentationtothemaintask(assuggestedinSection20.2.1).Rather,alltasksshould
belearnedinparallel.244 20. CASCADED,MULTI-TASKANDSEMI-SUPERVISEDLEARNING
20.2.6 CAVEATS
While the prospect of MTL is very appealing, some caveats are in order. MTL often does not
workwell.Forexample,ifthetasksarenotcloselyrelated,youmaynotseegainsfromMTL,and
mosttasksareindeednotrelated.ChoosingtherelatedtasksforperformingMTLcanbemore
ofanartthanascience.
Even if the tasks are related, but the shared network doesn’t have the capacity to support
all the tasks, the performance of all of them can degrade. When taking the regularization view,
thismeansthattheregularizationistoostrong,andpreventsthemodelfromfittingtheindivid-
ual tasks. In such cases, it is better to increase the model capacity (i.e., increase the number of
dimensionsinthesharedcomponentsofthenetwork).IfanMTLnetworkwithk tasksneedsa
k-foldincreaseincapacity(orclosetoit)inordertosupportalltasks,itmeansthatthereislikely
nosharingofpredictivestructureatallbetweenthetasks,andoneshouldforgotheMTLidea.
Whenthetasksareverycloselyrelated,suchasthePOStaggingandchunkingtasks,the
benefits from MTL could be very small. is is especially true when the networks are trained
onasingledatasetinwhicheachsentenceisannotatedforbothPOS-tagandChunklabel.e
chunking network can learn the representation it needs without the help of the intermediate
POSsupervision.WedostarttoseethebenefitsofMTLwhenthePOS-trainingdataandthe
chunking data are disjoint (but share sizable portions of the vocabulary), or when the POS-tag
data is a superset of the Chunk data. In this situation, the MTL allows to effectively enlarge
the amount of supervision for the chunking task by training on data with related labels for the
POS-taggingtask.isletstheChunkpartofthenetworkleverageonandinfluencetheshared
representationthatwaslearnedbasedonthePOSannotationsontheadditionaldata.
20.3 SEMI-SUPERVISEDLEARNING
A related framework to both multi-task and cascaded learning is semi-supervised learning, in
whichwehaveasmallamountoftrainingdataforataskwecareabout,andadditionaltraining
data for other tasks. e other tasks can be either supervised, or unsupervised (i.e., where the
supervision can be generated from unannotated corpora, such as in language modeling, word
embeddings,orsentenceencodings,asdiscussedinSection9.6,Chapters10and17.3).
We would like to use the supervision for the additional tasks (or to invent suitable addi-
tionaltasks)inordertoimprovethepredictionaccuracyonthemaintask.isisaverycommon
scenario, which is an active and important research area: we never have enough supervision for
thetaskswecareabout.
Foranoverviewofnon-neuralnetworkssemi-supervisedlearningmethodsinNLP,seethe
bookofSøgaard[2013]inthisseries.
Within the deep-learning framework, semi-supervised learning can be performed, much
like MTL, by learning a representation based on the additional tasks, that can then be used as
supplement input or as initialization to the main task. Concretely, one can pre-train word em-20.4. EXAMPLES 245
beddingsorsentencerepresentationsonunannotateddata,andusethesetoinitializeorfeedinto
aPOS-tagger,parseroradocumentsummarizationsystem.
In a sense, we have been doing semi-supervised learning ever since we introduced distri-
butionalrepresentationspre-trainedwordembeddingsinChapter10.Sometimes,problemslend
themselves to more specialized solutions, as we explore in Section 20.4.3. e similarities and
connectionstomulti-tasklearningarealsoclear:weareusingsupervisiondatafromonetaskto
improve performance on another.e main differenceseem to be in how the differenttasks are
integratedintothefinalmodel,andinthesourceoftheannotateddataforthedifferenttasks,but
the border between the approaches is rather blurry. In general, it is probably best not to debate
abouttheboundariesofcascadedlearning,multi-tasklearningandsemi-supervisedlearning,but
ratherseethemasasetofcomplimentaryandoverlappingtechniques.
Other approaches to semi-supervised learning include various regimes in which one or
more models are trained on the small labeled data, and are then used to assign labels to large
amounts of unlabeled data. e automatically annotated data (possibly following some quality
filteringstagebasedonagreementbetweenthemodelsareotherconfidencemeasures)isthenused
to train a new model, or provide additional features to an existing on. ese approaches can be
groupedunderthecollectivetermself-training.Othermethodsspecifyconstraintsonthesolution,
thatshouldhelpguidethemodel(i.e.,specifyingthatsomewordscanonlybetaggedwithcertain
tags, or that each sentence must contain at least one word tagged as X). Such methods are not
(yet)specializedforneuralnetworks,andarebeyondthescopeofthisbook.Foranoverview,see
thebookbySøgaard[2013].
20.4 EXAMPLES
WenowdescribeafewexamplesinwhichweMTLwasshowntobeeffective.
20.4.1 GAZE-PREDICTIONANDSENTENCECOMPRESSION
Inthesentencecompressionbydeletiontask,wearegivenasentencesuchas“AlanTuring,known
asthefatherofcomputerscience,thecodebreakerthathelpedwinWorldWarII,andthemantortured
by the state for being gay, is to receive a pardon nearly 60 years after his death” and are required to
produce a shorter (“compressed”) version containing the main information in the sentence by
deletingwordsfromtheoriginalsentence.Anexamplecompressionwouldbe“AlanTuringisto
receiveapardon.”iscanbemodeledasadeepbiRNNfollowedbyanMLPinwhichtheinputs
tothebiRNNarethewordsofthesentence,andtheoutputsoftheMLPsareKorD
decisionsforeachword.
InworkwithKlerkeetal.[2016],weshowedthattheperformanceonthesentencedeletion
by compression task can be improved by using two additional sequence prediction tasks: CCG
supertaggingandGazeprediction.etwotasksareaddedinaselective-sharingarchitecture,as
individualMLPsthatfeedfromthelowerlayerofthebiRNN.246 20. CASCADED,MULTI-TASKANDSEMI-SUPERVISEDLEARNING
e CCG supertagging task assigns each work with a CCGsupertag, which is a complex
syntactictagsuchas(S[dcl] NP)/PP,indicatingitssyntacticrolewithrespecttotherestofthe
n
sentence.²
e Gaze prediction task is a cognitive task that relates to the way people read written
language.Whenreading,oureyesmoveacrossthepage,fixatingonsomewords,skippingothers,
andoftenjumpingbacktopreviouswords.Itiswidelybelievedthateyemovementwhenreading
reflectsonthesentenceprocessingmechanismsinthebrain,whichinturnreflectsonthesentence
structure.Eye-trackersaremachinesthatcanaccuratelytrackeye-movementwhilereading,and
some eye-tracked corpora are available in which sentences are paired with exact eye-movement
measurements of several human subjects. In the gaze-prediction task, the network was trained
to predict aspects of the eye-movement behavior on the text (how long of a fixation each word
wouldreceive,orwhichwordswilltriggerbackmovements).eintuitionbeingthatpartsofthe
sentencewhicharelessimportantaremorelikelytobeskippedorglossedover,andpartsthatare
moreimportantarelikelytobefixatedmoreuponwhenprocessingthesentence.
ecompressiondata,thesyntacticCCGtaggingdata,andtheeye-movementdatawere
completely disjoint from each other, but we observed clear improvements to the compression
accuracywhenincludingtheadditionaltasksassupervision.
20.4.2 ARCLABELINGANDSYNTACTICPARSING
roughoutthebook,wedescribedanarchitectureforarc-standarddependencyparsing.Inpar-
ticular,inSection16.2.3wedescribedbiRNNbasedfeatures,andinSection19.4.1astructured
prediction learning framework. e parser we described was an unlabeldparser—the model as-
signed a score to each possible head-modifier pair, and the final prediction by the parser was a
collection of arcs, representing the best tree over the sentence. However, the scoring function,
and the resulting arcs, only took into consideration which words are syntactically connected to
eachother,andnotthenatureoftherelationbetweenthewords.
Recall from Section 6.2.2 that a dependency parse-tree usually contains also the relation
information, in term of a dependency label on each arc, i.e., the det, prep, pobj, nsubj, etc. label
annotationsinFigure20.6.
Givenanunlabeledparsing,thearc-labelscanbeassignedusinganarchitectureinwhicha
biRNNisusedtoreadthewordsofthesentence,andthen,forarc.h;m/inthetree,concatenate
thecorrespondingbiRNNencodingsandfeedthemintoanMLPforpredictingthearc’slabel.
Rather than training a separate network for the label prediction, we can treat Unlabeled
ParsingandArc-Labelingasrelatedtasksinamulti-tasksetting.WethenhaveasinglebiRNN
for the arc-labeler and the parser, and use the encoded biRNN states as inputs both to the arc-
scorerandtothearc-labeler.Intraining,thearc-labelerwillonlyseegoldarcs(becausewedonot
²CCGandCCGsupertagsarebeyondthescopeofthisbook.AgoodpointertostartlearningaboutCCGinNLPisthePh.D.
thesisofJuliaHockenmaier[Hockenmaier,2003].econceptofsupertaggingisintroducedbyJoshiandSrinivas[1994].20.4. EXAMPLES 247
root
nsubj
pobj prep
det dobj pobj
det prep amod det det
the boy with the black shirt opened the door with a key
Figure20.6: Labeleddependencytree.
havelabel informationfor theother,hypothetical arcs),while thearc-scorerwill seeall possible
arcs.
Indeed, in the work of Kiperwasser and Goldberg [2016b] we observe that the tasks are
indeedcloselyrelated.Trainingthejointnetworkforperformingbothunlabeledarc-scoringand
arc-labeling,usingthesamesharedbiRNNencoder,notonlyresultsinaccuratearclabeling,but
alsosubstantiallyimprovestheaccuracyoftheunlabeledparsetrees.
20.4.3 PREPOSITIONSENSEDISAMBIGUATIONANDPREPOSITION
TRANSLATION PREDICTION
Considerthepreposition-sensedisambiguationtaskdiscussedinSection7.6.Torecall,thisisa
word-in-context problem, in which we need to assign each preposition with one of K possible
sense labels (M, P, L, D, etc.). Annotated corpora for the task
exist[LitkowskiandHargraves,2007,Schneideretal.,2016],butaresmall.
In Section 7.6, we discussed a rich set of core features that can be used for training a
preposition-sensedisambiguator.Let’sdenotethefeatureextractortakingaprepositioninstance
andreturninganencodingofthesefeaturesasavectoras(cid:30) .s;i/,wheres istheinputsentence
sup
(includingwords,part-of-speechtags,lemmas,andsyntacticparse-treeinformation),andi isthe
indexoftheprepositionwithinthesentence.efeatureextractor(cid:30) basedonfeaturessimilarto
sup
thoseinfeaturesinSection7.6,withouttheWordNet-basedfeaturesbutwithpre-trainedword-
embeddings, is a strong one. Feeding it into an MLP for prediction performs reasonably well
(albeit still disappointingly low, below 80% accuracy), and attempts to replace or to supplement
itwithabiRNN-basedfeatureextractordoesnotimprovetheaccuracies.
Here, we show how the accuracy of the sense prediction can be improved further using a
semi-supervised approach,whichisbasedonlearningausefulrepresentationfromlargeamounts
ofunannotateddata,thatwetransformintorelatedandusefulpredictiontasks.
Specifically, we will be using tasks derived from sentence-aligned multilingual data. ese
arepairsofEnglishsentencesandtheirtranslationintootherlanguages.³Whentranslatingfrom
³Suchresourcesarereadilyavailablefrom,e.g.,proceedingsoftheEuropeanUnion(theEuroparlcorpus,[Koehn,2005]),or
canbeminedfromtheweb[Uszkoreitetal.,2010].esearetheresourcesthatdrivestatisticalmachinetranslation.248 20. CASCADED,MULTI-TASKANDSEMI-SUPERVISEDLEARNING
English to a different language, a preposition can be translated into one of several possible al-
ternatives.echoiceoftheforeignprepositionwillbebasedontheEnglishprepositionsense,
as reflected by the sentential context in which it appears. While prepositions are ambiguous in
alllanguages,theambiguitypatternsdifferacrosslanguages.us,predictingtheforeignprepo-
sition into which a given English preposition will be translated based on the English sentential
contextisagoodauxiliarytaskforprepositionsensedisambiguation.isistheapproachtaken
byGonenandGoldberg[2016].Weprovidehereahigh-leveloverview.Fordetails,refertothe
originalpaper.
Trainingdataiscreatedbasedonamultilingualsentence-alignedparallelcorpus.ecorpus
is word-aligned using a word-alignment algorithm [Dyer et al., 2013], and tuples of sentence,
h
preposition-position,foreign-language,foreign-prepositions areextractedastrainingexamples.
i
Given such a tuple s w ;i;L;f , the prediction task is to predict the translation of the
1n
h D W i
preposition w in the context of the sentence s. e possible outputs are taken from a set of
i
languagespecificoptionsp ,andthecorrectoutputisf.
L
ehopeisthatarepresentationofthecontextofw thatisgoodatpredictingtheforeign
i
preposition f will also be helpful for predicting the preposition sense. We model the task as an
encoder E.s;i/ that encodes the sentential context of w into a vector, and a predictor, which
i
attempts to predict the right preposition. e encoder is very similar to a biRNN, but does not
includetheprepositionitselfinordertoforcethenetworktopaymoreattentiontothecontext,
whilethepredictorisalanguagespecificMLP.
p.foreign f s;i;L/ softmax.MLPL .E.s;i///
D j D foreign (cid:140)f(cid:141)
(20.8)
E.s;i/ (cid:140)RNNf.w / RNNb.w /(cid:141):
1i 1 ni 1
D W (cid:0) I W C
eencoderissharedacrossthedifferentlanguages.Aftertrainingthenetworkonseveralmillion
Englishsentence,foreign-preposition pairs,weareleftwithapre-trainedcontext-encoderthat
h i
can then be used in the preposition-sense disambiguation network by concatenating it to the
supervisedfeaturerepresentation.Oursemi-superviseddisambiguatoristhen:
p.sense j s;i/ softmax.MLP .(cid:140)(cid:30) .s;i/ E.s;i/(cid:141)/ ; (20.9)
sup sup (cid:140)j(cid:141)
D j D I
whereEisthepre-trainedencoderthatisfurthertrainedbythesensepredictionnetwork,and
(cid:30) is the supervised feature extractor. e approach substantially and consistently improve the
sup
accuracyofthesensepredictionbyabout1–2accuracypoints,dependingondetailsofthesetup.⁴
⁴Whileanincreaseof1–2accuracypointsmaynotseemveryimpressive,itisunfortunatelyintheupperrangeofwhatone
couldrealisticallyexpectinsemi-supervisedscenariosinwhichthebaselinesupervisedsystemisalreadyrelativelystrong,using
currentsemi-supervisiontechniques.Improvementsoverweakerbaselinesarelarger.20.4. EXAMPLES 249
20.4.4 CONDITIONEDGENERATION:MULTILINGUALMACHINE
TRANSLATION,PARSING,ANDIMAGECAPTIONING
MTLcanalsobeeasilyperformedinanencoder-decoderframework.eworkofLuongetal.
[2016]demonstratedthisinthecontextofmachinetranslation.eirtranslationsystemfollows
thesequence-to-sequencearchitecture(Section17.2.1),withoutattention.Whilebettertransla-
tionsystemsexist(notablysystemsthatmakeuseofattention),thefocusoftheworkwastoshow
thatimprovementsfromthemulti-tasksetuparepossible.
Luong et al explore different setups of multi-task learning under this system. In the
first setup (one-to-many), the Encoder component (encoding English sentences into vectors)
is shared, and is used with two different decoders: one decoder is generating German transla-
tions, and the other decoder is generating linearized parse-trees for the English sentences (i.e.,
thepredictedsequencefortheboyopenedthedoor shouldbe (S (NP DT NN ) (VP VBD (NP
DT NN ) ) ) ).esystemistrainedonaparallelcorpusof English,German translationpairs,
h i
andongoldparsetreesfromthePennTreebank[Marcusetal.,1993].etranslationdataand
theparsingdataaredisjoint.roughthemulti-tasksetting,thesharedencoderlearnstoproduce
vectorsthatareinformativeforbothtasks.emulti-taskencoder-decodernetworkiseffective:
the network that is trained for both tasks (one encoder, two decoders) works better than the
individual networks consisting of a single encode-decoder pair. is setup likely works because
encodingbasicelementsofthesyntacticstructureofasentenceareinformativeforselectingthe
word order and syntactic structures in the resulting translation, and vice versa. e translation
andparsingtasksareindeedsynergistic.
Inthesecondsetup(many-to-one),thereisasingledecoder,butseveraldifferentencoders.
e tasks here are machine translation (German to English translation) and image captioning
(Image to English description). e decoder is tasked at producing English sentences. One en-
coderisencodingGermansentences,whiletheotherisencodingimages.Likebefore,thedatasets
forthetranslationandfortheimagecaptioningaredisjoint.Again,withsometuningofparam-
eters,trainingthejointsystemimprovesovertheindividualones,thoughthegainsaresomewhat
smaller.Here,thereisnorealconnectionbetweenthetaskofencodingGermansentences(which
expresselaboratepredicationsandcomplicatedsyntacticstructures)andencodingimagecontents
(whichencodesthemaincomponentsofsimplescenes).ebenefitseemtobefromthefactthat
bothtasksprovidesupervisionforthelanguage-modelingpartofthedecodernetwork,allowing
ittoproducebettersoundingEnglishsentences.Additionally,theimprovementmaystemfroma
regularizationeffect,inwhichone encoder,decoder pairpreventstheotherpairfromoverfitting
h i
toitstrainingdata.
Despitetheratherlowbaseline,theresultsofLuongetal.[2016]areencouraging,suggest-
ingtherearegainstobehadfrommulti-tasklearningintheconditionalgenerationframework,
whensuitablesynergistictasksarechosen.250 20. CASCADED,MULTI-TASKANDSEMI-SUPERVISEDLEARNING
20.5 OUTLOOK
Cascaded,multi-task,andsemi-supervisedlearningareexcitingtechniques.eneuralnetworks
framework,drivenbygradients-basedtrainingoveracomputationgraph,providemanyseamless
opportunitiesforusingthesetechniques.Inmanycases,suchapproachesbringrealandconsistent
gainsinaccuracy.Unfortunately,asofthetimeofthiswriting,thegainsareoftenrelativelymod-
est compared to the baseline performance, especially when the baselines are high. is should
not discourage you from using the techniques, as the gains often times are real. It should also
encourage you to actively work on improving and refining the techniques, so that we see could
expecttoseegreatergainsinthefuture.251
C H A P T E R 21
Conclusion
21.1 WHATHAVEWESEEN?
e introduction of neural networks methods has been transformative for NLP. It prompted a
move from linear-models with heavy feature engineering (and in particular the engineering of
backoffandcombinationfeatures)tomulti-layerperceptronsthatlearnthefeaturecombinations
(as discussed in the first part of the book); to architectures like convolutional neural networks
thatcanidentifygeneralizablengramsandgappy-ngrams(asdiscussedinChapter13);toarchi-
tectures like RNNs and bidirectional RNNs (Chapters 14–16) that can identify subtle patterns
andregularitiesinsequencesofarbitrarylengths;andtorecursiveneuralnetworks(Chapter18)
that can represent trees. ey also brought about methods for encoding words as vectors based
on distributional similarity, which can be effective for semi-supervised learning (Chapters 10–
11);andmethodsfornon-markovianlanguagemodeling,whichinturnpavethewaytoflexible
conditioned language generation models (Chapter 17), and revolutionized machine translation.
eneuralmethodsalsopresentmanyopportunitiesformulti-tasklearning(Chapter20).More-
over,establishedpre-neuralstructured-predictiontechniquescanbereadilyadaptedtoincorpo-
rateneuralnetworkbasedfeatureextractorsandpredictors(Chapter19).
21.2 THECHALLENGESAHEAD
Allinall,thefieldisprogressingveryquickly,anditishardtopredictwhatthefuturewillhold.
Onethingisclearthough,atleastinmyview—withalltheirimpressiveadvantages,neuralnet-
worksarenotasilverbulletfornatural-languageunderstandingandgeneration.Whiletheypro-
vide many improvements over the previous generation of statistical NLP techniques, the core
challengesremain:languageisdiscreteandambiguous,wedonothaveagoodunderstandingof
howitworks,anditisnotlikelythataneuralnetworkwilllearnallthesubtletiesonitsownwith-
outcarefulhumanguidance.echallengesmentionedintheintroductionareever-presentalso
with the neural techniques, and familiarity with the linguistic concepts and resources presented
in Chapter 6 is still as important as ever for designing good language processing systems. e
actual performance on many natural language tasks, even low-level and seemingly simple ones
suchaspronominalcoreferenceresolution[ClarkandManning,2016,Wisemanetal.,2016]or
coordination boundary disambiguation [Ficler and Goldberg, 2016] is still very far from being
perfect. Designing learning systems to target such low-level language understanding tasks is as
importantaresearchchallengeasitwasbeforetheintroductionofneuralNLPmethods.252 21. CONCLUSION
Another important challenge is the opaqueness of the learned representations, and the
lack of rigorous theory behind the architectures and the learning algorithms. Research into the
interpretability of neural network representations, as well as into better understanding of the
learning capacity and training dynamics of various architectures, is crucially needed in order to
progressevenfurther.
Asofthetimeofthiswriting,neuralnetworksareinessencestillsupervisedlearningmeth-
ods, and require relatively large amounts of labeled training data. While the use of pre-trained
word-embeddingsprovidesaconvenientplatformforsemi-supervisedlearning,wearestillinvery
preliminary stages of effectively utilizing unlabeled data and reducing the reliance on annotated
examples.Rememberthathumanscanoftengeneralizefromahandfulofexamples,whileneural
networksusually requireat least hundredsof labeled examples in orderto perform well, evenin
the most simple language tasks. Finding effective ways of leveraging small amounts of labeled
data together with large amounts of un-annotated data, as well as generalizing across domains,
willlikelyresultinanothertransformationofthefield.
Finally, an aspect which was only very briefly glossed over in this book is that language is
notanisolatedphenomena.Whenpeoplelearn,perceive,andproducelanguage,theydoitwith
a reference to the real world, and language utterances are more often than not grounded in real
worldentitiesorexperiences.Learninglanguageinagroundedsetting,eithercoupledwithsome
other modality such as images, videos, or robot movement control, or as part of an agent that
interactswiththeworldinordertoachieveconcretegoals,isanotherpromisingresearchfrontier.253
Bibliography
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,
GregS.Corrado,AndyDavis,JeffreyDean,MatthieuDevin,etal. TensorFlow:Large-scale
machinelearningonheterogeneoussystems,2015. http://tensorflow.org/
HeikeAdel,NgocangVu,andTanjaSchultz. Combinationofrecurrentneuralnetworksand
factored language models for code-switching language modeling. In Proc.ofthe51stAnnual
MeetingoftheAssociationforComputationalLinguistics—(Volume2:ShortPapers), pages 206–
211,Sofia,Bulgaria,August2013.
Roee Aharoni, Yoav Goldberg, and Yonatan Belinkov. Proc.ofthe14thSIGMORPHONWork-
shoponComputationalResearchinPhonetics,Phonology,andMorphology,chapterimprovingse-
quence to sequence learning for morphological inflection generation: e BIU-MIT systems
fortheSIGMORPHON2016sharedtaskformorphologicalreinflection,pages41–48. Asso-
ciationforComputationalLinguistics,2016.http://aclweb.org/anthology/W16-2007DOI:
10.18653/v1/W16-2007.
Roee Aharoni and Yoav Goldberg. Towards string-to-tree neural machine translation. Proc. of
ACL,2017.
M. A. Aizerman, E. A. Braverman, and L. Rozonoer. eoretical foundations of the potential
functionmethodinpatternrecognitionlearning.InAutomationandRemoteControl,number25
inAutomationandRemoteControl,pages821–837,1964.
ErinL.Allwein,RobertE.Schapire,andYoramSinger. Reducingmulticlasstobinary:Auni-
fyingapproachformarginclassifiers. JournalofMachineLearningResearch,1:113–141,2000.
Rie Ando and Tong Zhang. A high-performance semi-supervised learning method for text
chunking. In Proc.ofthe43rdAnnualMeetingoftheAssociationforComputationalLinguistics
(ACL’05),pages1–9,AnnArbor,Michigan,June2005a.DOI:10.3115/1219840.1219841.
RieKubotaAndoandTongZhang.Aframeworkforlearningpredictivestructuresfrommultiple
tasksandunlabeleddata. eJournalofMachineLearningResearch,6:1817–1853,2005b.
Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman
Ganchev,SlavPetrov,andMichaelCollins. Globallynormalizedtransition-basedneuralnet-
works. In Proc. of the 54th Annual Meeting of the Association for Computational Linguistics—
(Volume 1: Long Papers), pages 2442–2452, 2016. http://aclweb.org/anthology/P16-1231
DOI:10.18653/v1/P16-1231.254 BIBLIOGRAPHY
Michael Auli and Jianfeng Gao. Decoder integration and expected BLEU training for recur-
rent neural network language models. In Proc. of the 52nd Annual Meeting of the Association
forComputationalLinguistics—(Volume2:ShortPapers),pages136–142,Baltimore,Maryland,
June2014.DOI:10.3115/v1/p14-2023.
Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. Joint language and transla-
tion modeling with recurrent neural networks. In Proc. of the 2013 Conference on Empirical
MethodsinNaturalLanguageProcessing, pages 1044–1054, Seattle, Washington. Association
forComputationalLinguistics,October2013.
OdedAvrahamandYoavGoldberg.einterplayofsemanticsandmorphologyinwordembed-
dings.EACL,2017.
DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointly
learningtoalignandtranslate. arXiv:1409.0473[cs,stat],September2014.
Miguel Ballesteros, Chris Dyer, and Noah A. Smith. Improved transition-based parsing by
modeling characters instead of words with LSTMs. In Proc. of the 2015 Conference on Em-
piricalMethodsinNaturalLanguageProcessing,pages349–359,Lisbon,Portugal.Association
forComputationalLinguistics,September2015.DOI:10.18653/v1/d15-1041.
MiguelBallesteros,YoavGoldberg,ChrisDyer,andNoahA.Smith. Trainingwithexploration
improves a greedy stack-LSTM parser, EMNLP 2016. arXiv:1603.03793[cs], March 2016.
DOI:10.18653/v1/d16-1211.
MohitBansal,KevinGimpel,andKarenLivescu. Tailoringcontinuouswordrepresentationsfor
dependency parsing. In Proc.ofthe52ndAnnualMeetingoftheAssociationforComputational
Linguistics—(Volume2:ShortPapers),pages809–815,Baltimore,Maryland,June2014.DOI:
10.3115/v1/p14-2131.
Marco Baroni and Alessandro Lenci. Distributional memory: A general framework
for corpus-based semantics. Computational Linguistics, 36(4):673–721, 2010. DOI:
10.1162/coli_a_00016.
Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark
Siskind. Automatic differentiation in machine learning: A survey. arXiv:1502.05767 [cs],
February2015.
Emily M. Bender. LinguisticFundamentalsforNaturalLanguageProcessing:100Essentialsfrom
Morphology and Syntax. Synthesis Lectures on Human Language Technologies. Morgan &
ClaypoolPublishers,2013.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for se-
quence prediction with recurrent neural networks. CoRR, abs/1506.03099, 2015. http:
//arxiv.org/abs/1506.03099BIBLIOGRAPHY 255
Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures.
arXiv:1206.5533[cs],June2012.DOI:10.1007/978-3-642-35289-8_26.
YoshuaBengio,RéjeanDucharme,PascalVincent,andChristianJanvin. Aneuralprobabilistic
language model. Journal of Machine Learning Research, 3:1137–1155, March 2003. ISSN
1532-4435.DOI:10.1007/10985687_6.
Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning.
InProc.ofthe26thAnnualInternationalConferenceonMachineLearning,pages41–48.ACM,
2009.DOI:10.1145/1553374.1553380.
YoshuaBengio,IanJ.Goodfellow,andAaronCourville. DeepLearning. MITPress,2016.
JamesBergstra,OlivierBreuleux,FrédéricBastien,PascalLamblin,RazvanPascanu,Guillaume
Desjardins,JosephTurian,DavidWarde-Farley,andYoshuaBengio.eano:aCPUandGPU
mathexpressioncompiler.InProc.ofthePythonforScientificComputingConference(SciPy),June
2010.
Jeff A. Bilmes and Katrin Kirchhoff. Factored language models and generalized parallel
backoff. In Companion Volume of the Proc. of HLT-NAACL—Short Papers, 2003. DOI:
10.3115/1073483.1073485.
ZsoltBitvaiandTrevorCohn. Non-lineartextregressionwithadeepconvolutionalneuralnet-
work. In Proc.ofthe53rdAnnualMeetingoftheAssociationforComputationalLinguisticsand
the7thInternationalJointConferenceonNaturalLanguageProcessing—(Volume2:ShortPapers),
pages180–185,Beijing,China,July2015.DOI:10.3115/v1/p15-2030.
TolgaBolukbasi,Kai-WeiChang,JamesY.Zou,VenkateshSaligrama,andAdamTaumanKalai.
Quantifying and reducing stereotypes in word embeddings. CoRR, abs/1606.06121, 2016.
http://arxiv.org/abs/1606.06121
BernhardE.Boser,IsabelleM.Guyon,andVladimirN.Vapnik.Atrainingalgorithmforoptimal
marginclassifiers. InProc.ofthe5thAnnualACMWorkshoponComputationalLearningeory,
pages144–152.ACMPress,1992.DOI:10.1145/130385.130401.
JanA.BothaandPhilBlunsom. Compositionalmorphologyforwordrepresentationsandlan-
guage modelling. In Proc. of the 31st International Conference on Machine Learning (ICML),
Beijing,China,June2014.
Léon Bottou. Stochastic gradient descent tricks. In Neural Networks: Tricks of the Trade,
pages421–436.Springer,2012.DOI:10.1007/978-3-642-35289-8_25.
R. Samuel Bowman, Gabor Angeli, Christopher Potts, and D. Christopher Manning. A large
annotatedcorpusforlearningnaturallanguageinference.InProc.ofthe2015ConferenceonEm-
piricalMethodsinNaturalLanguageProcessing,pages632–642.AssociationforComputational
Linguistics,2015. http://aclweb.org/anthology/D15-1075DOI:10.18653/v1/D15-1075.256 BIBLIOGRAPHY
Peter Brown, Peter deSouza, Robert Mercer, T. Watson, Vincent Della Pietra, and Jenifer Lai.
Class-basedn-grammodelsofnaturallanguage. ComputationalLinguistics,18(4),December
1992. http://aclweb.org/anthology/J92-4003
John A. Bullinaria and Joseph P. Levy. Extracting semantic representations from word co-
occurrencestatistics:Acomputationalstudy. BehaviorResearchMethods,39(3):510–526,2007.
DOI:10.3758/bf03193020.
A. Caliskan-Islam, J. J. Bryson, and A. Narayanan. Semantics derived automatically from lan-
guagecorporanecessarilycontainhumanbiases. CoRR,abs/1608.07187,2016.
Rich Caruana. Multitask learning. Machine Learning, 28:41–75, 1997. DOI: 10.1007/978-1-
4615-5529-2_5.
Eugene Charniak and Mark Johnson. Coarse-to-fine n-best parsing and MaxEnt dis-
criminative reranking. In Proc. of the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL’05), pages 173–180, Ann Arbor, Michigan, June 2005. DOI:
10.3115/1219840.1219862.
Danqi Chen and Christopher Manning. A fast and accurate dependency parser using neural
networks. InProc.ofthe2014ConferenceonEmpiricalMethodsinNaturalLanguageProcessing
(EMNLP),pages740–750,Doha,Qatar.AssociationforComputationalLinguistics,October
2014.DOI:10.3115/v1/d14-1082.
StanleyF.ChenandJoshuaGoodman.Anempiricalstudyofsmoothingtechniquesforlanguage
modeling. In34thAnnualMeetingoftheAssociationforComputationalLinguistics,1996. http:
//aclweb.org/anthology/P96-1041DOI:10.1006/csla.1999.0128.
Stanley F. Chen and Joshua Goodman. An empirical study of smoothing techniques for lan-
guage modeling. Computer Speech and Language, 13(4):359–394, 1999. DOI: 10.1006/c-
sla.1999.0128.
WenlinChen,DavidGrangier,andMichaelAuli. Strategiesfortraininglargevocabularyneu-
ral language models. In Proc. of the 54th Annual Meeting of the Association for Computational
Linguistics—(Volume1:LongPapers),pages1975–1985,2016. http://aclweb.org/anthology
/P16-1186DOI:10.18653/v1/P16-1186.
YuboChen,LihengXu,KangLiu,DaojianZeng,andJunZhao. Eventextractionviadynamic
multi-poolingconvolutionalneuralnetworks. InProc.ofthe53rdAnnualMeetingoftheAsso-
ciationforComputationalLinguisticsandthe7thInternationalJointConferenceonNaturalLan-
guage Processing—(Volume 1: Long Papers), pages 167–176, Beijing, China, July 2015. DOI:
10.3115/v1/p15-1017.BIBLIOGRAPHY 257
Kyunghyun Cho. Natural language understanding with distributed representation.
arXiv:1511.07916[cs,stat],November2015.
KyunghyunCho,BartvanMerrienboer,DzmitryBahdanau,andYoshuaBengio.Ontheproper-
tiesofneuralmachinetranslation:Encoder-decoderapproaches. InProc.ofSSST-8,8thWork-
shoponSyntax,SemanticsandStructureinStatisticalTranslation,pages103–111,Doha,Qatar.
AssociationforComputationalLinguistics,October2014a.DOI:10.3115/v1/w14-4012.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
HolgerSchwenk,andYoshuaBengio. LearningphraserepresentationsusingRNNencoder-
decoderforstatisticalmachinetranslation.InProc.ofthe2014ConferenceonEmpiricalMethods
in Natural Language Processing (EMNLP), pages 1724–1734, Doha, Qatar. Association for
ComputationalLinguistics,October2014b.DOI:10.3115/v1/d14-1179.
DoKookChoeandEugeneCharniak. Parsingaslanguagemodeling. InProc.oftheConference
onEmpiricalMethodsinNaturalLanguageProcessing,pages2331–2336,Austin,Texas.Associ-
ationforComputationalLinguistics,November2016. https://aclweb.org/anthology/D16-
1257DOI:10.18653/v1/d16-1257.
Grzegorz Chrupala. Normalizing tweets with edit scripts and recurrentneural embeddings. In
Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics—(Volume 2:
ShortPapers),pages680–686,Baltimore,Maryland,June2014.DOI:10.3115/v1/p14-2111.
JunyoungChung,CaglarGulcehre,KyungHyunCho,andYoshuaBengio. Empiricalevaluation
of gated recurrent neural networks on sequence modeling. arXiv:1412.3555 [cs], December
2014.
Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio. A character-level decoder without ex-
plicit segmentation for neural machine translation. In Proc.ofthe54thAnnualMeetingofthe
Association for Computational Linguistics—(Volume 1: Long Papers), pages 1693–1703, 2016.
http://aclweb.org/anthology/P16-1160DOI:10.18653/v1/P16-1160.
Kenneth Ward Church and Patrick Hanks. Word association norms, mutual information, and
lexicography. ComputationalLinguistics,16(1):22–29,1990.DOI:10.3115/981623.981633.
KevinClarkandChristopherD.Manning. Improvingcoreferenceresolutionbylearningentity-
level distributed representations. In Association for Computational Linguistics (ACL), 2016.
/u/apache/htdocs/static/pubs/clark2016improving.pdfDOI:10.18653/v1/p16-1061.
MichaelCollins.DiscriminativetrainingmethodsforhiddenMarkovmodels:eoryandexper-
imentswithperceptronalgorithms. InProc.oftheConferenceonEmpiricalMethodsinNatural
LanguageProcessing, pages 1–8. Association for Computational Linguistics, July 2002. DOI:
10.3115/1118693.1118694.258 BIBLIOGRAPHY
Michael Collins and Terry Koo. Discriminative reranking for natural language pars-
ing. Computational Linguistics, 31(1):25–70, March 2005. ISSN 0891-2017. DOI:
10.1162/0891201053630273.
RonanCollobertandJasonWeston.Aunifiedarchitecturefornaturallanguageprocessing:Deep
neuralnetworkswithmultitasklearning.InProc.ofthe25thInternationalConferenceonMachine
Learning,pages160–167.ACM,2008.DOI:10.1145/1390156.1390177.
Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel
Kuksa. Natural language processing (almost) from scratch. eJournalofMachineLearning
Research,12:2493–2537,2011.
Alexis Conneau, Holger Schwenk, Loïc Barrault, and Yann LeCun. Very deep convolutional
networks for natural language processing. CoRR, abs/1606.01781, 2016. http://arxiv.org/
abs/1606.01781
RyanCotterellandHinrichSchutze.Morphologicalwordembeddings.NAACL,2015.
Ryan Cotterell, Christo Kirov, John Sylak-Glassman, David Yarowsky, Jason Eisner, and Mans
Hulden. Proc. of the 14th SIGMORPHON Workshop on Computational Research in Phonetics,
Phonology,andMorphology,chaptereSIGMORPHON2016SharedTask—Morphological
Reinflection,pages10–22. AssociationforComputationalLinguistics,2016. http://aclweb
.org/anthology/W16-2002DOI:10.18653/v1/W16-2002.
Koby Crammer and Yoram Singer. On the algorithmic implementation of multiclass kernel-
basedvectormachines. eJournalofMachineLearningResearch,2:265–292,2002.
MathiasCreutzandKristaLagus. Unsupervisedmodelsformorphemesegmentationandmor-
phologylearning.ACMTransactionsofSpeechandLanguageProcessing,4(1):3:1–3:34,February
2007. ISSN1550-4875.DOI:10.1145/1187415.1187418.
James Cross and Liang Huang. Incremental parsing with minimal features using bi-directional
LSTM. In Proc.ofthe54thAnnualMeetingoftheAssociationforComputationalLinguistics—
(Volume2:ShortPapers),pages32–37,2016a. http://aclweb.org/anthology/P16-2006DOI:
10.18653/v1/P16-2006.
James Cross and Liang Huang. Span-based constituency parsing with a structure-label sys-
tem and dynamic oracles. In Proc. of the 2016 Conference on Empirical Methods in Natu-
ralLanguageProcessing(EMNLP). Association for Computational Linguistics, 2016b. DOI:
10.18653/v1/d16-1001.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Con-
trol,SignalsandSystems,2(4):303–314,December1989. ISSN0932-4194,1435-568X.DOI:
10.1007/BF02551274.BIBLIOGRAPHY 259
Ido Dagan and Oren Glickman. Probabilistic textual entailment: Generic applied modeling of
language variability. In PASCAL Workshop on Learning Methods for Text Understanding and
Mining,2004.
IdoDagan,FernandoPereira,andLillianLee. Similarity-basedestimationofwordcooccurrence
probabilities. InACL,1994.DOI:10.3115/981732.981770.
Ido Dagan, Oren Glickman, and Bernardo Magnini. e PASCAL recognising textual en-
tailment challenge. In Machine Learning Challenges, Evaluating Predictive Uncertainty, Vi-
sualObjectClassificationandRecognizingTextualEntailment,FirstPASCALMachineLearning
ChallengesWorkshop,MLCW,pages177–190,Southampton,UK,April11–13,2005.(revised
selectedpapers).DOI:10.1007/11736790_9.
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. Recognizing Textual
Entailment:ModelsandApplications. Synthesis Lectures on Human Language Technologies.
Morgan&ClaypoolPublishers,2013.DOI:10.2200/s00509ed1v01y201305hlt023.
G. E. Dahl, T. N. Sainath, and G. E. Hinton. Improving deep neural networks for LVCSR
using rectified linear units and dropout. In 2013 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pages 8609–8613, May 2013. DOI: 10.1109/I-
CASSP.2013.6639346.
HalDauméIII,JohnLangford,andDanielMarcu.Search-basedstructuredprediction.Machine
LearningJournal(MLJ),2009.DOI:10.1007/s10994-009-5106-x.
HalDauméIII. ACourseInMachineLearning. SelfPublished,2015.
Yann N. Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and
YoshuaBengio. Identifyingandattackingthesaddlepointprobleminhigh-dimensionalnon-
convexoptimization. InZ.Ghahramani,M.Welling,C.Cortes,N.D.Lawrence,andK.Q.
Weinberger, Eds., Advances in Neural Information Processing Systems 27, pages 2933–2941.
CurranAssociates,Inc.,2014.
AdriàdeGispert,GonzaloIglesias,andBillByrne. FastandaccuratepreorderingforSMTusing
neural networks. In Proc.ofthe2015ConferenceoftheNorthAmericanChapteroftheAssocia-
tion for Computational Linguistics: Human Language Technologies, pages 1012–1017, Denver,
Colorado,2015.DOI:10.3115/v1/n15-1105.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, omas Lamar, Richard Schwartz, and John
Makhoul. Fast and robust neural network joint models for statistical machine translation.
InProc.ofthe52ndAnnualMeetingoftheAssociationforComputationalLinguistics—(Volume1:
LongPapers),pages1370–1380,Baltimore,Maryland,June2014.DOI:10.3115/v1/p14-1129.260 BIBLIOGRAPHY
TrinhDo,ierryArti,andothers.Neuralconditionalrandomfields.InInternationalConference
onArtificialIntelligenceandStatistics,pages177–184,2010.
PedroDomingos. eMasterAlgorithm. BasicBooks,2015.
LiDong,FuruWei,ChuanqiTan,DuyuTang,MingZhou,andKeXu.Adaptiverecursiveneural
network for target-dependent twitter sentiment classification. In Proc. of the 52nd Annual
MeetingoftheAssociationforComputationalLinguistics—(Volume2:ShortPapers),pages49–54,
Baltimore,Maryland,June2014.DOI:10.3115/v1/p14-2009.
Li Dong, Furu Wei, Ming Zhou, and Ke Xu. Question answering over freebase with multi-
column convolutional neural networks. In Proc. of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics and the 7th International Joint Conference on Natural Lan-
guage Processing—(Volume 1: Long Papers), pages 260–269, Beijing, China, July 2015. DOI:
10.3115/v1/p15-1026.
CicerodosSantosandMairaGatti. Deepconvolutionalneuralnetworksforsentimentanalysis
of short texts. In Proc. of COLING, the 25th International Conference on Computational Lin-
guistics: Technical Papers, pages 69–78, Dublin City University, Dublin, Ireland. Association
forComputationalLinguistics,August2014.
Cicero dos Santos and Bianca Zadrozny. Learning character-level representations for part-of-
speech tagging. In Proc. of the 31st International Conference on Machine Learning (ICML),
pages1818–1826,2014.
Cicero dos Santos, Bing Xiang, and Bowen Zhou. Classifying relations by ranking with con-
volutional neural networks. In Proc.ofthe53rdAnnualMeetingoftheAssociationforCompu-
tationalLinguisticsandthe7thInternationalJointConferenceonNaturalLanguageProcessing—
(Volume 1: Long Papers), pages 626–634, Beijing, China, July 2015. DOI: 10.3115/v1/p15-
1061.
JohnDuchi,EladHazan,andYoramSinger. Adaptivesubgradientmethodsforonlinelearning
andstochasticoptimization. eJournalofMachineLearningResearch,12:2121–2159,2011.
KevinDuh,GrahamNeubig,KatsuhitoSudoh,andHajimeTsukada. Adaptationdataselection
usingneurallanguagemodels:experimentsinmachinetranslation. InProc.ofthe51stAnnual
MeetingoftheAssociationforComputationalLinguistics—(Volume2:ShortPapers), pages 678–
683,Sofia,Bulgaria,August2013.
Greg Durrett and Dan Klein. Neural CRF parsing. In Proc.ofthe53rdAnnualMeetingofthe
Association for Computational Linguistics and the 7th International Joint Conference on Natural
LanguageProcessing—(Volume1:LongPapers),pages302–312,Beijing,China,July2015.DOI:
10.3115/v1/p15-1030.BIBLIOGRAPHY 261
ChrisDyer,VictorChahuneau,andA.NoahSmith. Asimple,fast,andeffectivereparameter-
ization of IBM model 2. In Proc.ofthe2013ConferenceoftheNorthAmericanChapterofthe
AssociationforComputationalLinguistics:HumanLanguageTechnologies,pages644–648,2013.
http://aclweb.org/anthology/N13-1073
ChrisDyer,MiguelBallesteros,WangLing,AustinMatthews,andNoahA.Smith. Transition-
based dependency parsing with stack long short-term memory. In Proc. of the 53rd Annual
MeetingoftheAssociationforComputationalLinguisticsandthe7thInternationalJointConference
onNaturalLanguageProcessing—(Volume1:LongPapers),pages334–343,Beijing,China,July
2015.DOI:10.3115/v1/p15-1033.
C.EckartandG.Young. eapproximationofonematrixbyanotheroflowerrank. Psychome-
trika,1:211–218,1936.DOI:10.1007/bf02288367.
JasonEisnerandGiorgioSatta.Efficientparsingforbilexicalcontext-freegrammarsandheadau-
tomatongrammars.InProc.ofthe37thAnnualMeetingoftheAssociationforComputationalLin-
guistics,1999. http://aclweb.org/anthology/P99-1059DOI:10.3115/1034678.1034748.
Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179–211, March 1990.
ISSN1551-6709.DOI:10.1207/s15516709cog1402_1.
Martin B. H. Everaert, Marinus A. C. Huybregts, Noam Chomsky, Robert C. Berwick, and
JohanJ.Bolhuis. Structures,notstrings:Linguisticsaspartofthecognitivesciences. Trends
inCognitiveSciences,19(12):729–743,2015.DOI:10.1016/j.tics.2015.09.008.
ManaalFaruquiandChrisDyer. Improvingvectorspacewordrepresentationsusingmultilingual
correlation. InProc.ofthe14thConferenceoftheEuropeanChapteroftheAssociationforCompu-
tationalLinguistics,pages462–471,Gothenburg,Sweden,April2014.DOI:10.3115/v1/e14-
1049.
Manaal Faruqui, Jesse Dodge, Kumar Sujay Jauhar, Chris Dyer, Eduard Hovy, and A. Noah
Smith. Retrofitting word vectors to semantic lexicons. In Proc. of the 2015 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies, pages 1606–1615, 2015. http://aclweb.org/anthology/N15-1184 DOI:
10.3115/v1/N15-1184.
Manaal Faruqui, Yulia Tsvetkov, Graham Neubig, and Chris Dyer. Morphological inflection
generation using character sequence to sequence learning. In Proc. of the 2016 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies, pages 634–643, 2016. http://aclweb.org/anthology/N16-1077 DOI:
10.18653/v1/N16-1077.
ChristianeFellbaum. WordNet:AnElectronicLexicalDatabase. BradfordBooks,1998.262 BIBLIOGRAPHY
Jessica Ficler and Yoav Goldberg. A neural network for coordination boundary prediction. In
Proc.ofthe2016ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages23–32,
Austin, Texas. Association for Computational Linguistics, November 2016. https://aclweb
.org/anthology/D16-1003DOI:10.18653/v1/d16-1003.
KatjaFilippovaandYaseminAltun. Overcomingthelackofparalleldatainsentencecompres-
sion. In Proc. of the 2013 Conference on Empirical Methods in Natural Language Processing,
pages1481–1491.AssociationforComputationalLinguistics,2013. http://aclweb.org/ant
hology/D13-1155
Katja Filippova, Enrique Alfonseca, Carlos A. Colmenares, Lukasz Kaiser, and Oriol Vinyals.
Sentence compression by deletion with LSTMs. In Proc. of the 2015 Conference on Empiri-
calMethodsinNaturalLanguageProcessing,pages360–368,Lisbon,Portugal.Associationfor
ComputationalLinguistics,September2015.DOI:10.18653/v1/d15-1042.
Charles J. Fillmore, Josef Ruppenhofer, and Collin F. Baker. FrameNet and representing the
link between semantic and syntactic relations. LanguageandLinguisticsMonographsSeriesB,
pages19–62,InstituteofLinguistics,AcademiaSinica,Taipei,2004.
JohnR.Firth.Asynopsisoflinguistictheory1930–1955.InStudiesinLinguisticAnalysis,Special
volume of the Philological Society, pages 1–32. Firth, John Rupert, Haas William, Halliday,
MichaelA.K.,Oxford,BlackwellEd.,1957.
John R. Firth. e technique of semantics. TransactionsofthePhilologicalSociety, 34(1):36–73,
1935. ISSN1467-968X.DOI:10.1111/j.1467-968X.1935.tb01254.x.
MikelL.ForcadaandRamónP.Ñeco. Recursivehetero-associativememoriesfortranslation. In
BiologicalandArtificialComputation:FromNeurosciencetoTechnology,pages453–462.Springer,
1997.DOI:10.1007/bfb0032504.
Philip Gage. A new algorithm for data compression. C Users Journal, 12(2):23–38, February
1994. ISSN0898-9788. http://dl.acm.org/citation.cfm?id=177910.177914
YarinGal. Atheoreticallygroundedapplicationofdropoutinrecurrentneuralnetworks. CoRR,
abs/1512.05287,December2015.
Kuzman Ganchev and Mark Dredze. Proc. of the ACL-08: HLT Workshop on Mobile Language
Processing,chapterSmallStatisticalModelsbyRandomFeatureMixing,pages19–20. Asso-
ciationforComputationalLinguistics,2008. http://aclweb.org/anthology/W08-0804
Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. PPDB: e paraphrase
database. In Proc. of the 2013 Conference of the North American Chapter of the Association for
ComputationalLinguistics:HumanLanguageTechnologies, pages 758–764, 2013. http://aclw
eb.org/anthology/N13-1092BIBLIOGRAPHY 263
JianfengGao,PatrickPantel,MichaelGamon,XiaodongHe,andLiDeng. Modelinginterest-
ingnesswithdeepneuralnetworks. InProc.oftheConferenceonEmpiricalMethodsinNatural
LanguageProcessing(EMNLP),pages2–13,Doha,Qatar.AssociationforComputationalLin-
guistics,October2014.DOI:10.3115/v1/d14-1002.
Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag Subramanya. Multilingual language
processing from bytes. In Proc. of the Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Language Technologies, pages 1296–1306, 2016.
http://aclweb.org/anthology/N16-1155DOI:10.18653/v1/N16-1155.
JesúsGiménezandLluisMàrquez. SVMTool:AgeneralPOStaggergeneratorbasedonsupport
vectormachines. InProc.ofthe4thLREC,Lisbon,Portugal,2004.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward
neuralnetworks. InInternationalConferenceonArtificialIntelligenceandStatistics,pages249–
256,2010.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
InternationalConferenceonArtificialIntelligenceandStatistics,pages315–323,2011.
YoavGoldberg. Aprimeronneuralnetworkmodelsfornaturallanguageprocessing. Journalof
ArtificialIntelligenceResearch,57:345–420,2016.
YoavGoldbergandMichaelElhadad.Anefficientalgorithmforeasy-firstnon-directionaldepen-
dencyparsing. InHumanLanguageTechnologies:eAnnualConferenceoftheNorthAmerican
ChapteroftheAssociationforComputationalLinguistics,pages742–750,LosAngeles,Califor-
nia,June2010.
YoavGoldbergandJoakimNivre. Trainingdeterministicparserswithnon-deterministicoracles.
TransactionsoftheAssociationforComputationalLinguistics,1(0):403–414,October2013.ISSN
2307-387X.
Yoav Goldberg, Kai Zhao, and Liang Huang. Efficient implementation of beam-search in-
cremental parsers. In Proc. of the 51st Annual Meeting of the Association for Computational
Linguistics—(Volume2:ShortPapers),pages628–633,Sofia,Bulgaria,August2013.
ChristophGollerandAndreasKüchler. Learningtask-dependentdistributedrepresentationsby
backpropagationthroughstructure. InInProc.oftheICNN-96,pages347–352.IEEE,1996.
HilaGonenandYoavGoldberg. Semisupervisedpreposition-sensedisambiguationusingmulti-
lingualdata. InProc.ofCOLING,the26thInternationalConferenceonComputationalLinguis-
tics: Technical Papers, pages 2718–2729, Osaka, Japan, December 2016. e COLING 2016
OrganizingCommittee. http://aclweb.org/anthology/C16-1256264 BIBLIOGRAPHY
JoshuaGoodman. Abitofprogressinlanguagemodeling. CoRR,cs.CL/0108005,2001. http:
//arxiv.org/abs/cs.CL/0108005DOI:10.1006/csla.2001.0174.
StephanGouws,YoshuaBengio,andGregCorrado. BilBOWA:Fastbilingualdistributedrep-
resentationswithoutwordalignments. InProc.ofthe32ndInternationalConferenceonMachine
Learning,pages748–756,2015.
A.Graves. SupervisedSequenceLabellingwithRecurrentNeuralNetworks. Ph.D.thesis,Technis-
cheUniversitätMünchen,2008.DOI:10.1007/978-3-642-24797-2.
AlexGraves,GregWayne,andIvoDanihelka. Neuralturingmachines. CoRR,abs/1410.5401,
2014. http://arxiv.org/abs/1410.5401
EdwardGrefenstette,KarlMoritzHermann,MustafaSuleyman,andPhilBlunsom.Learningto
transducewithunboundedmemory. InC.Cortes,N.D.Lawrence,D.D.Lee,M.Sugiyama,
andR.Garnett,Eds.,AdvancesinNeuralInformationProcessingSystems28,pages1828–1836.
Curran Associates, Inc., 2015. http://papers.nips.cc/paper/5648-learning-to-transduce-
with-unbounded-memory.pdf
Klaus Greff, Rupesh Kumar Srivastava, Jan Koutník, Bas R. Steunebrink, and Jürgen Schmid-
huber. LSTM: A search space odyssey. arXiv:1503.04069 [cs], March 2015. DOI:
10.1109/tnnls.2016.2582924.
MichaelGutmannandAapoHyvärinen. Noise-contrastiveestimation:Anewestimationprin-
ciple for unnormalized statistical models. In InternationalConferenceonArtificialIntelligence
andStatistics,pages297–304,2010.
Zellig Harris. Distributional structure. Word, 10(23):146–162, 1954. DOI:
10.1080/00437956.1954.11659520.
KazumaHashimoto,MakotoMiwa,YoshimasaTsuruoka,andTakashiChikayama. Simplecus-
tomization of recursive neural networks for semantic relation classification. In Proc. of the
Conference on Empirical Methods in Natural Language Processing, pages 1372–1376, Seattle,
Washington.AssociationforComputationalLinguistics,October2013.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Sur-
passinghuman-levelperformanceonImageNetclassification.arXiv:1502.01852[cs],February
2015.DOI:10.1109/iccv.2015.123.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. IneIEEEConferenceonComputerVisionandPatternRecognition(CVPR),June
2016.DOI:10.1109/cvpr.2016.90.BIBLIOGRAPHY 265
MatthewHenderson,Blaiseomson,andSteveYoung. Deepneuralnetworkapproachforthe
dialog state tracking challenge. In Proc. of the SIGDIAL Conference, pages 467–471, Metz,
France.AssociationforComputationalLinguistics,August2013.
Karl Moritz Hermann and Phil Blunsom. e role of syntax in vector space models of com-
positional semantics. In Proc. of the 51st Annual Meeting of the Association for Computational
Linguistics—(Volume1:LongPapers),pages894–904,Sofia,Bulgaria,August2013.
Karl Moritz Hermann and Phil Blunsom. Multilingual models for compositional distributed
semantics.InProc.ofthe52ndAnnualMeetingoftheAssociationforComputationalLinguistics—
(Volume1:LongPapers),pages58–68,Baltimore,Maryland,June2014.DOI:10.3115/v1/p14-
1006.
SalahElHihiandYoshuaBengio. Hierarchicalrecurrentneuralnetworksforlong-termdepen-
dencies. In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, Eds., Advances in Neural
InformationProcessingSystems8,pages493–499.MITPress,1996.
FelixHill,KyunghyunCho,SebastienJean,ColineDevin,andYoshuaBengio.Embeddingword
similaritywithneuralmachinetranslation. arXiv:1412.6448[cs],December2014.
Geoffrey E. Hinton, J. L. McClelland, and D. E. Rumelhart. Distributed representations. In
D.E.Rumelhart,J.L.McClelland,etal.,Eds.,ParallelDistributedProcessing:Volume1:Foun-
dations,pages77–109.MITPress,Cambridge,1987.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R.
Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors.
arXiv:1207.0580[cs],July2012.
SeppHochreiterandJürgenSchmidhuber. Longshort-termmemory. NeuralComputation,9(8):
1735–1780,1997.DOI:10.1162/neco.1997.9.8.1735.
JuliaHockenmaier.DataandModelsforStatisticalParsingwithCombinatoryCategorialGrammar.
Ph.D.thesis,UniversityofEdinburgh,2003.DOI:10.3115/1073083.1073139.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks
are universal approximators. NeuralNetworks, 2(5):359–366, 1989. ISSN 0893-6080. DOI:
10.1016/0893-6080(89)90020-8.
Dirk Hovy, Stephen Tratz, and Eduard Hovy. What’s in a preposition? dimensions of sense
disambiguationforaninterestingwordclass. InColingPosters,pages454–462,Beijing,China,
August 2010. Coling 2010 Organizing Committee. http://www.aclweb.org/anthology/C
10-2052266 BIBLIOGRAPHY
(Kenneth) Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aish-
warya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra,
LawrenceC.Zitnick,DeviParikh,LucyVanderwende,MichelGalley,andMargaretMitchell.
Visual storytelling. In Proc.ofthe2016ConferenceoftheNorthAmericanChapteroftheAsso-
ciation for Computational Linguistics: Human Language Technologies, pages 1233–1239, 2016.
http://aclweb.org/anthology/N16-1147DOI:10.18653/v1/N16-1147.
Liang Huang, Suphan Fayong, and Yang Guo. Structured perceptron with inexact search. In
Proc.oftheConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguis-
tics:HumanLanguageTechnologies,pages142–151,2012.http://aclweb.org/anthology/N12-
1015
SergeyIoffeandChristianSzegedy. Batchnormalization:Acceleratingdeepnetworktrainingby
reducinginternalcovariateshift. arXiv:1502.03167[cs],February2015.
Ozan Irsoy and Claire Cardie. Opinion mining with deep recurrent neural networks. In
Proc. of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),
pages720–728,Doha,Qatar.AssociationforComputationalLinguistics,October2014.DOI:
10.3115/v1/d14-1080.
MohitIyyer,JordanBoyd-Graber,LeonardoClaudino,RichardSocher,andHalDauméIII. A
neural network for factoid question answering over paragraphs. In Proc. of the Conference on
Empirical Methods in Natural Language Processing (EMNLP), pages 633–644, Doha, Qatar.
AssociationforComputationalLinguistics,October2014a.DOI:10.3115/v1/d14-1070.
Mohit Iyyer, Peter Enns, Jordan Boyd-Graber, and Philip Resnik. Political ideology detection
usingrecursiveneuralnetworks. InProc.ofthe52ndAnnualMeetingoftheAssociationforCom-
putationalLinguistics—(Volume1:LongPapers),pages1113–1122,Baltimore,Maryland,June
2014b.DOI:10.3115/v1/p14-1105.
Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daumé III. Deep unordered
compositionrivalssyntacticmethodsfortextclassification. InProc.ofthe53rdAnnualMeet-
ingoftheAssociationforComputationalLinguisticsandthe7thInternationalJointConferenceon
NaturalLanguageProcessing—(Volume1:LongPapers),pages1681–1691,Beijing,China,July
2015.DOI:10.3115/v1/p15-1162.
Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large
target vocabulary for neural machine translation. In Proc. of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Conference on Natural
LanguageProcessing—(Volume1:LongPapers), pages 1–10, 2015. http://aclweb.org/antho
logy/P15-1001DOI:10.3115/v1/P15-1001.
FrederickJelinekandRobertMercer. InterpolatedestimationofMarkovsourceparametersfrom
sparsedata. InWorkshoponPatternRecognitioninPractice,1980.BIBLIOGRAPHY 267
Rie Johnson and Tong Zhang. Effective use of word order for text categorization with convo-
lutional neural networks. In Proc.ofthe2015ConferenceoftheNorthAmericanChapterofthe
AssociationforComputationalLinguistics:HumanLanguageTechnologies,pages103–112,Den-
ver,Colorado,2015.DOI:10.3115/v1/n15-1011.
AravindK.JoshiandBangaloreSrinivas. Disambiguationofsuperpartsofspeech(orsupertags):
Allnost parsing. In COLING Volume 1: e 15th International Conference on Computational
Linguistics,1994. http://aclweb.org/anthology/C94-1024DOI:10.3115/991886.991912.
ArmandJoulin,EdouardGrave,PiotrBojanowski,andTomasMikolov.Bagoftricksforefficient
textclassification. CoRR,abs/1607.01759,2016. http://arxiv.org/abs/1607.01759
Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recur-
rent network architectures. In Proc.ofthe32ndInternationalConferenceonMachineLearning
(ICML-15),pages2342–2350,2015.
RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploringthe
limitsoflanguagemodeling. arXiv:1602.02410[cs],February2016.
Daniel Jurafsky and James H. Martin. Speech and Language Processing, 2nd ed. Prentice Hall,
2008.
NalKalchbrenner,EdwardGrefenstette,andPhilBlunsom. Aconvolutionalneuralnetworkfor
modelling sentences. In Proc.ofthe52ndAnnualMeetingoftheAssociationforComputational
Linguistics—(Volume1:LongPapers),pages655–665,Baltimore,Maryland,June2014.DOI:
10.3115/v1/p14-1062.
Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aäron van den Oord, Alex Graves, and
KorayKavukcuoglu. Neuralmachinetranslationinlineartime. CoRR,abs/1610.10099,2016.
http://arxiv.org/abs/1610.10099
KatharinaKannandHinrichSchütze. Proc.ofthe14thSIGMORPHONWorkshoponComputa-
tionalResearchinPhonetics,Phonology,andMorphology,chapterMED:eLMUSystemfor
theSIGMORPHON2016SharedTaskonMorphologicalReinflection,pages62–70.Associ-
ation for Computational Linguistics, 2016. http://aclweb.org/anthology/W16-2010 DOI:
10.18653/v1/W16-2010.
AnjuliKannan,KarolKurach,SujithRavi,TobiasKaufmann,AndrewTomkins,BalintMiklos,
Greg Corrado, Laszlo Lukacs, Marina Ganea, Peter Young, and Vivek Ramavajjala. Smart
reply:Automatedresponsesuggestionforemail. InProc.oftheACMSIGKDDConferenceon
KnowledgeDiscoveryandDataMining(KDD),2016. https://arxiv.org/pdf/1606.04870.pdf
DOI:10.1145/2939672.2939801.268 BIBLIOGRAPHY
AndrejKarpathyandFei-FeiLi. Deepvisual-semanticalignmentsforgeneratingimagedescrip-
tions. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 3128–
3137,Boston,MA,June7–12,2015.DOI:10.1109/cvpr.2015.7298932.
AndrejKarpathy,JustinJohnson,andFei-FeiLi. Visualizingandunderstandingrecurrentnet-
works. arXiv:1506.02078[cs],June2015.
Douwe Kiela and Stephen Clark. A systematic study of semantic vector space model param-
eters. In Workshop on Continuous Vector Space Models and their Compositionality, 2014. DOI:
10.3115/v1/w14-1503.
YoonKim.Convolutionalneuralnetworksforsentenceclassification.InProc.oftheConferenceon
EmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages1746–1751,Doha,Qatar.
AssociationforComputationalLinguistics,October2014.DOI:10.3115/v1/d14-1181.
YoonKim,YacineJernite,DavidSontag,andAlexanderM.Rush. Character-awareneurallan-
guagemodels. arXiv:1508.06615[cs,stat],August2015.
Diederik Kingma and Jimmy Ba. ADAM: A method for stochastic optimization.
arXiv:1412.6980[cs],December2014.
Eliyahu Kiperwasser and Yoav Goldberg. Easy-first dependency parsing with hierarchical tree
LSTMs. Transactions of the Association of Computational Linguistics—(Volume 4, Issue 1),
pages445–461,2016a. http://aclweb.org/anthology/Q16-1032
Eliyahu Kiperwasser and Yoav Goldberg. Simple and accurate dependency parsing using bidi-
rectional LSTM feature representations. Transactions of the Association of Computational
Linguistics—(Volume4,Issue1),pages313–327,2016b. http://aclweb.org/anthology/Q16-
1023
KarinKipper,HoaT.Dang,andMarthaPalmer. Class-basedconstructionofaverblexicon. In
AAAI/IAAI,pages691–696,2000.
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio
Torralba,andSanjaFidler. Skip-thoughtvectors. InC.Cortes,N.D.Lawrence,D.D.Lee,
M. Sugiyama, and R. Garnett, Eds., Advances in Neural Information Processing Systems 28,
pages 3294–3302. Curran Associates, Inc., 2015. http://papers.nips.cc/paper/5950-skip-
thought-vectors.pdf
SigridKlerke,YoavGoldberg,andAndersSøgaard.Improvingsentencecompressionbylearning
to predict gaze. In Proc. of the Conference of the North American Chapter of the Association for
ComputationalLinguistics:HumanLanguageTechnologies,pages1528–1533,2016. http://ac
lweb.org/anthology/N16-1179DOI:10.18653/v1/N16-1179.BIBLIOGRAPHY 269
Reinhard Kneser and Hermann Ney. Improved backing-off for m-gram language modeling.
In Acoustics,Speech,andSignalProcessing,ICASSP-95,InternationalConferenceon, volume 1,
pages181–184,May1995.DOI:10.1109/ICASSP.1995.479394.
Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proc.ofMT
Summit,volume5,pages79–86,2005.
Philipp Koehn. Statistical Machine Translation. Cambridge University Press, 2010. DOI:
10.1017/cbo9780511815829.
Terry Koo and Michael Collins. Efficient third-order dependency parsers. In Proc.ofthe48th
AnnualMeetingoftheAssociationforComputationalLinguistics, pages 1–11, 2010. http://ac
lweb.org/anthology/P10-1001
MosheKoppel,JonathanSchler,andShlomoArgamon. Computationalmethodsinauthorship
attribution. JournaloftheAmericanSocietyforinformationScienceandTechnology, 60(1):9–26,
2009.DOI:10.1002/asi.20961.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep
convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Wein-
berger, Eds., AdvancesinNeuralInformationProcessingSystems25, pages 1097–1105. Curran
Associates,Inc.,2012.DOI:10.1007/978-3-319-46654-5_20.
R.A.KronmalandA.V.Peterson,Jr. Onthealiasmethodforgeneratingrandomvariablesfrom
adiscretedistribution. eAmericanStatistician,33:214–218,1979.DOI:10.2307/2683739.
Sandra Kübler, Ryan McDonald, and Joakim Nivre. Dependency Parsing. Synthesis Lec-
tures on Human Language Technologies. Morgan & Claypool Publishers, 2008. DOI:
10.2200/s00169ed1v01y200901hlt002.
Taku Kudo and Yuji Matsumoto. Fast methods for Kernel-based text analysis. In Proc. of the
41st Annual Meeting on Association for Computational Linguistics—(Volume 1), pages 24–31,
Stroudsburg,PA,2003.DOI:10.3115/1075096.1075100.
JohnLafferty,AndrewMcCallum,andFernandoCNPereira. Conditionalrandomfields:Prob-
abilisticmodelsforsegmentingandlabelingsequencedata. InProc.ofICML,2001.
Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris
Dyer. Neuralarchitecturesfornamedentityrecognition. InProc.oftheConferenceoftheNorth
AmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,
pages 260–270, 2016. http://aclweb.org/anthology/N16-1030 DOI: 10.18653/v1/N16-
1030.270 BIBLIOGRAPHY
PhongLeandWillemZuidema. einside-outsiderecursiveneuralnetworkmodelfordepen-
dencyparsing. InProc.oftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing
(EMNLP),pages729–739,Doha,Qatar.AssociationforComputationalLinguistics,October
2014.DOI:10.3115/v1/d14-1081.
PhongLeandWillemZuidema.eforestconvolutionalnetwork:Compositionaldistributional
semanticswithaneuralchartandwithoutbinarization. InProc.oftheConferenceonEmpirical
MethodsinNaturalLanguageProcessing, pages 1155–1164, Lisbon, Portugal. Association for
ComputationalLinguistics,September2015.DOI:10.18653/v1/d15-1137.
Quoc V. Le, Navdeep Jaitly, and Geoffrey E. Hinton. A simple way to initialize recurrent net-
worksofrectifiedlinearunits. arXiv:1504.00941[cs],April2015.
Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time-series.
InM.A.Arbib,Ed.,eHandbookofBraineoryandNeuralNetworks.MITPress,1995.
YannLeCun,LeonBottou,G.Orr,andK.Muller. EfficientBackProp. InG.OrrandMullerK,
Eds.,NeuralNetworks:TricksoftheTrade.Springer,1998a.DOI:10.1007/3-540-49430-8_2.
YannLeCun,LeonBottou,YoshuaBengio,andPatrickHaffner.Gradientbasedlearningapplied
topatternrecognition. Proc.oftheIEEE,86(11):2278–2324,November1998b.
YannLeCunandF.Huang. Lossfunctionsfordiscriminativetrainingofenergy-basedmodels.
InProc.ofAISTATS,2005.
Yann LeCun, Sumit Chopra, Raia Hadsell, M. Ranzato, and F. Huang. A tutorial on energy-
basedlearning. PredictingStructuredData,1:0,2006.
Geunbae Lee, Margot Flowers, and Michael G. Dyer. Learning distributed representations of
conceptual knowledge and their application to script-based story processing. In Connection-
istNaturalLanguageProcessing, pages 215–247. Springer, 1992. DOI: 10.1007/978-94-011-
2624-3_11.
MosheLeshno,VladimirYa.Lin,AllanPinkus,andShimonSchocken. Multilayerfeedforward
networks with a nonpolynomial activation function can approximate any function. Neural
Networks, 6(6):861–867, 1993. ISSN 0893-6080. http://www.sciencedirect.com/science/
article/pii/S0893608005801315DOI:10.1016/S0893-6080(05)80131-5.
Omer Levy and Yoav Goldberg. Dependency-based word embeddings. In Proc. of the 52nd
Annual Meeting of the Association for Computational Linguistics—(Volume 2: Short Papers),
pages302–308,Baltimore,Maryland,June2014.DOI:10.3115/v1/p14-2050.
OmerLevyandYoavGoldberg.Linguisticregularitiesinsparseandexplicitwordrepresentations.
In Proc. of the 18th Conference on Computational Natural Language Learning, pages 171–180.BIBLIOGRAPHY 271
AssociationforComputationalLinguistics,2014. http://aclweb.org/anthology/W14-1618
DOI:10.3115/v1/W14-1618.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In
Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, Eds., Ad-
vancesinNeuralInformationProcessingSystems27,pages2177–2185.CurranAssociates,Inc.,
2014.
Omer Levy, Yoav Goldberg, and Ido Dagan. Improving distributional similarity with lessons
learnedfromwordembeddings. TransactionsoftheAssociationforComputationalLinguistics,3
(0):211–225,May2015. ISSN2307-387X.
Omer Levy, Anders Søgaard, and Yoav Goldberg. A strong baseline for learning cross-lingual
word embeddings from sentence alignments. In Proc. of the 15th Conference of the European
ChapteroftheAssociationforComputationalLinguistics,2017.
Mike Lewis and Mark Steedman. Improved CCG parsing with semi-supervised supertagging.
TransactionsoftheAssociationforComputationalLinguistics,2(0):327–338,October2014.ISSN
2307-387X.
Mike Lewis, Kenton Lee, and Luke Zettlemoyer. LSTM CCG parsing. In Proc. of the Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human
LanguageTechnologies,pages221–231,2016. http://aclweb.org/anthology/N16-1026DOI:
10.18653/v1/N16-1026.
Jiwei Li, Rumeng Li, and Eduard Hovy. Recursive deep models for discourse parsing.
In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP),
pages 2061–2069, Doha, Qatar. Association for Computational Linguistics, October 2014.
DOI:10.3115/v1/d14-1220.
Jiwei Li, ang Luong, Dan Jurafsky, and Eduard Hovy. When are tree structures necessary
fordeeplearningofrepresentations? InProc.oftheConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages2304–2314.AssociationforComputationalLinguistics,2015.http:
//aclweb.org/anthology/D15-1278DOI:10.18653/v1/D15-1278.
JiweiLi,MichelGalley,ChrisBrockett,GeorgiosSpithourakis,JianfengGao,andBillDolan. A
persona-basedneuralconversationmodel. InProc.ofthe54thAnnualMeetingoftheAssociation
forComputationalLinguistics—(Volume1:LongPapers), pages 994–1003, 2016. http://aclw
eb.org/anthology/P16-1094DOI:10.18653/v1/P16-1094.
G.J.Lidstone.NoteonthegeneralcaseoftheBayes-Laplaceformulaforinductiveoraposteriori
probabilities. TransactionsoftheFacultyofActuaries,8:182–192,1920.272 BIBLIOGRAPHY
Wang Ling, Chris Dyer, Alan W. Black, and Isabel Trancoso. Two/too simple adaptations of
Word2Vec for syntax problems. In Proc. of the Conference of the North American Chapter of
theAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages1299–1304,
Denver,Colorado,2015a.DOI:10.3115/v1/n15-1142.
WangLing,ChrisDyer,AlanW.Black,IsabelTrancoso,RamonFermandez,SilvioAmir,Luis
Marujo,andTiagoLuis. Findingfunctioninform:Compositionalcharactermodelsforopen
vocabulary word representation. In Proc. of the Conference on Empirical Methods in Natural
LanguageProcessing,pages1520–1530,Lisbon,Portugal.AssociationforComputationalLin-
guistics,September2015b.DOI:10.18653/v1/d15-1176.
Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of LSTMs to learn
syntax-sensitive dependencies. TransactionsoftheAssociationforComputationalLinguistics, 4:
521–535,2016. ISSN2307-387X. https://www.transacl.org/ojs/index.php/tacl/article/
view/972
KenLitkowskiandOrinHargraves. eprepositionproject. InProc.ofthe2ndACL-SIGSEM
WorkshopontheLinguisticDimensionsofPrepositionsandeirUseinComputationalLinguistics
FormalismsandApplications,pages171–179,2005.
Ken Litkowski and Orin Hargraves. SemEval-2007 task 06: Word-sense disambiguation of
prepositions. InProc.ofthe4thInternationalWorkshoponSemanticEvaluations,pages24–29,
2007.DOI:10.3115/1621474.1621479.
Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou, and Houfeng Wang. A dependency-
based neural network for relation classification. In Proc. of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Conference on Natural
LanguageProcessing—(Volume2:ShortPapers),pages285–290,Beijing,China,July2015.DOI:
10.3115/v1/p15-2047.
Minh-ang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to
attention-basedneuralmachinetranslation. arXiv:1508.04025[cs],August2015.
Minh-angLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser. Multi-task
sequencetosequencelearning. InProc.ofICLR,2016.
Ji Ma, Yue Zhang, and Jingbo Zhu. Tagging the web: Building a robust web tagger with
neural network. In Proc. of the 52nd Annual Meeting of the Association for Computational
Linguistics—(Volume1:LongPapers),pages144–154,Baltimore,Maryland,June2014.DOI:
10.3115/v1/p14-1014.
Mingbo Ma, Liang Huang, Bowen Zhou, and Bing Xiang. Dependency-based convolutional
neural networks for sentence embedding. In Proc. of the 53rd Annual Meeting of the Associ-BIBLIOGRAPHY 273
ationforComputationalLinguisticsandthe 7thInternationalJoint ConferenceonNaturalLan-
guage Processing—(Volume 2: Short Papers), pages 174–179, Beijing, China, July 2015. DOI:
10.3115/v1/p15-2029.
XuezheMaandEduardHovy. End-to-endsequencelabelingviabi-directionalLSTM-CNNs-
CRF. In Proc. of the 54th Annual Meeting of the Association for Computational Linguistics—
(Volume1:LongPapers),pages1064–1074,Berlin,Germany,August2016. http://www.aclw
eb.org/anthology/P16-1101DOI:10.18653/v1/p16-1101.
ChristopherManningandHinrichSchütze. FoundationsofStatisticalNaturalLanguageProcess-
ing. MITPress,1999.
Christopher Manning, Prabhakar Raghavan, and Hinrich Schütze. IntroductiontoInformation
Retrieval. CambridgeUniversityPress,2008.DOI:10.1017/cbo9780511809071.
JunhuaMao,WeiXu,YiYang,JiangWang,andAlanL.Yuille.Explainimageswithmultimodal
recurrentneuralnetworks. CoRR,abs/1410.1090,2014. http://arxiv.org/abs/1410.1090
Ryan McDonald, Koby Crammer, and Fernando Pereira. Online large-margin training of de-
pendency parsers. In Proc. of the 43rd Annual Meeting of the Association for Computational
Linguistics (ACL’05), pages 91–98, 2005. http://aclweb.org/anthology/P05-1012 DOI:
10.3115/1219840.1219852.
RyanMcDonald,JoakimNivre,YvonneQuirmbach-Brundage,YoavGoldberg,DipanjanDas,
KuzmanGanchev,KeithB.Hall,SlavPetrov,HaoZhang,OscarTäckström,ClaudiaBedini,
NúriaBertomeuCastelló,andJungmeeLee.Universaldependencyannotationformultilingual
parsing. InACL(2),pages92–97,2013.
TomášMikolov.Statisticallanguagemodelsbasedonneuralnetworks.Ph.D.thesis,BrnoUniversity
ofTechnology,2012.
Tomáš Mikolov. Martin Karafiát, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Re-
currentneuralnetworkbasedlanguagemodel. InINTERSPEECH,11thAnnualConferenceof
theInternationalSpeechCommunicationAssociation,pages1045–1048,Makuhari,Chiba,Japan,
September26–30,2010.
TomášMikolov,StefanKombrink,LukášBurget,JanHonzaČernocky,andSanjeevKhudanpur.
Extensions of recurrent neural network language model. In Acoustics, Speech and Signal Pro-
cessing(ICASSP),IEEEInternationalConferenceon,pages5528–5531,2011.DOI:10.1109/i-
cassp.2011.5947611.
Tomáš Mikolov.Kai Chen, GregCorrado, and Jeffrey Dean. Efficient estimation of wordrep-
resentationsinvectorspace. arXiv:1301.3781[cs],January2013.274 BIBLIOGRAPHY
Tomáš Mikolov. Quoc V. Le, and Ilya Sutskever. Exploiting similarities among languages for
machinetranslation. CoRR,abs/1309.4168,2013. http://arxiv.org/abs/1309.4168
Tomáš Mikolov. Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed repre-
sentations of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Weinberger, Eds., Advances in Neural Information
ProcessingSystems26,pages3111–3119.CurranAssociates,Inc.,2013.
TomášMikolov.Wen-tauYih,andGeoffreyZweig. Linguisticregularitiesincontinuousspace
wordrepresentations. InProc.oftheConferenceoftheNorthAmericanChapteroftheAssociation
forComputationalLinguistics:HumanLanguageTechnologies,pages746–751,2013. http://ac
lweb.org/anthology/N13-1090
Tomáš Mikolov. Armand Joulin, Sumit Chopra, Michael Mathieu, and Marc’Aurelio Ranzato.
Learninglongermemoryinrecurrentneuralnetworks. arXiv:1412.7753[cs],December2014.
Scott Miller, Jethran Guinness, and Alex Zamanian. Name tagging with word clusters and
discriminative training. In Proc. of the Human Language Technology Conference of the North
American Chapter of the Association for Computational Linguistics: HLT-NAACL, 2004. http:
//aclweb.org/anthology/N04-1043
Andriy Mnih and Koray Kavukcuoglu. Learning word embeddings efficiently with noise-
contrastive estimation. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and
K. Q. Weinberger, Eds., Advances in Neural Information Processing Systems 26, pages 2265–
2273.CurranAssociates,Inc.,2013.
Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic
language models. In John Langford and Joelle Pineau, Eds., Proc. of the 29th International
Conference on Machine Learning (ICML-12), pages 1751–1758, New York, NY, July 2012.
Omnipress.
MehryarMohri,AfshinRostamizadeh,andAmeetTalwalkar. FoundationsofMachineLearning.
MITPress,2012.
Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model.
In RobertG. Cowell and ZoubinGhahramani, Eds., Proc.ofthe10thInternationalWorkshop
onArtificialIntelligenceandStatistics,pages246–252,2005. http://www.iro.umontreal.ca/~
lisa/pointeurs/hierarchical-nnlm-aistats05.pdf
Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise omson, Milica Gasic, Pei-Hao Su, David
Vandyke, Tsung-Hsien Wen, and Steve Young. Multi-domain dialog state tracking usingBIBLIOGRAPHY 275
recurrent neural networks. In Proc. of the 53rd Annual Meeting of the Association for Compu-
tationalLinguisticsandthe7thInternationalJointConferenceonNaturalLanguageProcessing—
(Volume2:ShortPapers),pages794–799,Beijing,China.AssociationforComputationalLin-
guistics,July2015.DOI:10.3115/v1/p15-2130.
MasamiNakamuraandKiyohiroShikano. AstudyofEnglishwordcategorypredictionbasedon
neuralnetworks. eJournaloftheAcousticalSocietyofAmerica,84(S1):S60–S61,1988.DOI:
10.1121/1.2026400.
R. Neidinger. Introduction to automatic differentiation and MATLAB object-oriented
programming. SIAM Review, 52(3):545–563, January 2010. ISSN 0036-1445. DOI:
10.1137/080743627.
Y.Nesterov.AmethodofsolvingaconvexprogrammingproblemwithconvergencerateO(1/k2).
InSovietMathematicsDoklady,27:372–376,1983.
Y.Nesterov. IntroductoryLecturesonConvexOptimization. KluwerAcademicPublishers,2004.
DOI:10.1007/978-1-4419-8853-9.
Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed Ammar, Antonios
Anastasopoulos, Miguel Ballesteros, David Chiang, Daniel Clothiaux, Trevor Cohn, Kevin
Duh,Manaal Faruqui,Cynthia Gan,Dan Garrette,Yangfeng Ji,Lingpeng Kong,Adhiguna
Kuncoro,GauravKumar,ChaitanyaMalaviya,PaulMichel,YusukeOda,MatthewRichard-
son,NaomiSaphra,SwabhaSwayamdipta,andPengchengYin. DyNet:edynamicneural
networktoolkit. CoRR,abs/1701.03980,2017. http://arxiv.org/abs/1701.03980
ienHuuNguyenandRalphGrishman. Eventdetectionanddomainadaptationwithconvo-
lutional neural networks. In Proc. of the 53rd Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint Conference on Natural Language Processing—
(Volume 2: Short Papers), pages 365–371, Beijing, China, July 2015. DOI: 10.3115/v1/p15-
2060.
Joakim Nivre. Algorithms for deterministic incremental dependency parsing. Computa-
tional Linguistics, 34(4):513–553, December 2008. ISSN 0891-2017, 1530-9312. DOI:
10.1162/coli.07-056-R1-07-027.
Joakim Nivre, Željko Agić, Maria Jesus Aranzabe, Masayuki Asahara, Aitziber Atutxa, Miguel
Ballesteros,JohnBauer,KepaBengoetxea,RiyazAhmadBhat,CristinaBosco,SamBowman,
Giuseppe G. A. Celano, Miriam Connor, Marie-Catherine de Marneffe, Arantza Diaz de
Ilarraza, Kaja Dobrovoljc, Timothy Dozat, Tomaž Erjavec, Richárd Farkas, Jennifer Foster,
DanielGalbraith,FilipGinter,IakesGoenaga,KoldoGojenola,YoavGoldberg,BertaGon-
zales,BrunoGuillaume,JanHajič,DagHaug,RaduIon,ElenaIrimia,AndersJohannsen,Hi-
roshi Kanayama, Jenna Kanerva, Simon Krek, Veronika Laippala, Alessandro Lenci, Nikola276 BIBLIOGRAPHY
Ljubešić, Teresa Lynn, Christopher Manning, Cătălina Mărănduc, David Mareček, Héctor
Martínez Alonso, Jan Mašek, Yuji Matsumoto, Ryan McDonald, Anna Missilä, Verginica
Mititelu, Yusuke Miyao, Simonetta Montemagni, Shunsuke Mori, Hanna Nurmi, Petya
Osenova, Lilja Øvrelid, Elena Pascual, Marco Passarotti, Cenel-Augusto Perez, Slav Petrov,
Jussi Piitulainen, Barbara Plank, Martin Popel, Prokopis Prokopidis, Sampo Pyysalo, Lo-
ganathan Ramasamy, Rudolf Rosa, Shadi Saleh, Sebastian Schuster, Wolfgang Seeker, Moj-
ganSeraji,NataliaSilveira,MariaSimi,RaduSimionescu,KatalinSimkó,KirilSimov,Aaron
Smith, Jan Štěpánek, Alane Suhr, Zsolt Szántó, Takaaki Tanaka, Reut Tsarfaty, Sumire Ue-
matsu, Larraitz Uria, Viktor Varga, Veronika Vincze, Zdeněk Žabokrtský, Daniel Zeman,
and Hanzhi Zhu. Universal dependencies 1.2, 2015. http://hdl.handle.net/11234/1-1548
LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles
UniversityinPrague.
ChrisOkasaki. PurelyFunctionalDataStructures. CambridgeUniversityPress,Cambridge,UK,
June1999.DOI:10.1017/cbo9780511530104.
MitchellP.Marcus,BeatriceSantorini,andMaryAnnMarcinkiewicz.Buildingalargeannotated
corpus of English: e Penn Treebank. ComputationalLinguistics, 19(2), June 1993, Special
IssueonUsingLargeCorpora:II,1993. http://aclweb.org/anthology/J93-2004
Martha Palmer, Daniel Gildea, and Nianwen Xue. SemanticRoleLabeling. Synthesis Lectures
onHumanLanguageTechnologies.Morgan&ClaypoolPublishers,2010.DOI:10.1093/ox-
fordhb/9780199573691.013.023.
Bo Pang and Lillian Lee. Opinion mining and sentiment analysis. Foundation and Trends in
InformationRetrieval,2:1–135,2008.DOI:10.1561/1500000011.
AnkurP.Parikh,OscarTäckström,DipanjanDas,andJakobUszkoreit. Adecomposableatten-
tionmodelfornaturallanguageinference. InProc.ofEMNLP,2016.DOI:10.18653/v1/d16-
1244.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent
neuralnetworks. arXiv:1211.5063[cs],November2012.
ElliePavlick,PushpendreRastogi,JuriGanitkevitch,BenjaminVanDurme,andChrisCallison-
Burch. PPDB2.0:Betterparaphraseranking,fine-grainedentailmentrelations,wordembed-
dings,andstyleclassification. InProc.ofthe53rdAnnualMeetingoftheAssociationforCompu-
tationalLinguisticsandthe7thInternationalJointConferenceonNaturalLanguageProcessing—
(Volume 2: Short Papers), pages 425–430. Association for Computational Linguistics, 2015.
http://aclweb.org/anthology/P15-2070DOI:10.3115/v1/P15-2070.
WenzhePei,TaoGe,andBaobaoChang.Aneffectiveneuralnetworkmodelforgraph-basedde-
pendencyparsing. InProc.ofthe53rdAnnualMeetingoftheAssociationforComputationalLin-BIBLIOGRAPHY 277
guisticsandthe7thInternationalJointConferenceonNaturalLanguageProcessing—(Volume1:
LongPapers),pages313–322,Beijing,China,July2015.DOI:10.3115/v1/p15-1031.
Joris Pelemans, Noam Shazeer, and Ciprian Chelba. Sparse non-negative matrix language
modeling. Transactions of the Association of Computational Linguistics, 4(1):329–342, 2016.
http://aclweb.org/anthology/Q16-1024
JianPeng,LiefengBo,andJinboXu. Conditionalneuralfields. InY.Bengio,D.Schuurmans,
J. D. Lafferty, C. K. I. Williams, and A. Culotta, Eds., AdvancesinNeuralInformationPro-
cessingSystems22,pages1419–1427.CurranAssociates,Inc.,2009.
JeffreyPennington,RichardSocher,andChristopherManning. GloVe:globalvectorsforword
representation. In Proc. of the Conference on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 1532–1543, Doha, Qatar. Association for Computational Linguistics,
October2014.DOI:10.3115/v1/d14-1162.
VuPham,ChristopherKermorvant,andJérômeLouradour. Dropoutimprovesrecurrentneural
networks for handwriting recognition. CoRR, abs/1312.4569, 2013. http://arxiv.org/abs/
1312.4569DOI:10.1109/icfhr.2014.55.
Barbara Plank, Anders Søgaard, and Yoav Goldberg. Multilingual part-of-speech tagging with
bidirectional long short-term memory models and auxiliary loss. In Proc.ofthe 54thAnnual
MeetingoftheAssociationforComputationalLinguistics—(Volume2:ShortPapers), pages 412–
418. Association for Computational Linguistics, 2016. http://aclweb.org/anthology/P16-
2067DOI:10.18653/v1/P16-2067.
JordanB.Pollack. Recursivedistributedrepresentations. ArtificialIntelligence,46:77–105,1990.
DOI:10.1016/0004-3702(90)90005-k.
B.T.Polyak. Somemethodsofspeedinguptheconvergenceofiterationmethods. USSRCom-
putational Mathematics and Mathematical Physics, 4(5):1–17, 1964. ISSN 0041-5553. DOI:
10.1016/0041-5553(64)90137-5.
Qiao Qian, Bo Tian, Minlie Huang, Yang Liu, Xuan Zhu, and Xiaoyan Zhu. Learning tag
embeddingsandtag-specificcompositionfunctionsinrecursiveneuralnetwork. InProc.ofthe
53rd Annual Meeting of the Association for Computational Linguistics and the 7th International
JointConferenceonNaturalLanguageProcessing—(Volume1:LongPapers), pages 1365–1374,
Beijing,China,July2015.DOI:10.3115/v1/p15-1132.
Lev Ratinov and Dan Roth. Proc. of the 13th Conference on Computational Natural Language
Learning (CoNLL-2009), chapter Design Challenges and Misconceptions in Named Entity
Recognition,pages147–155. AssociationforComputationalLinguistics,2009. http://aclw
eb.org/anthology/W09-1119278 BIBLIOGRAPHY
RonaldRosenfeld.Amaximumentropyapproachtoadaptivestatisticallanguagemodeling.Com-
puter, Speech and Language, 10:187–228, 1996. Longe version: Carnegie Mellon Technical
ReportCMU-CS-94-138.DOI:10.1006/csla.1996.0011.
StéphaneRossandJ.AndrewBagnell. Efficientreductionsforimitationlearning. InProc.ofthe
13thInternationalConferenceonArtificialIntelligenceandStatistics,pages661–668,2010.
StéphaneRoss,GeoffreyJ.Gordon,andJ.AndrewBagnell.Areductionofimitationlearningand
structuredpredictiontono-regretonlinelearning. InProc.ofthe14thInternationalConference
onArtificialIntelligenceandStatistics,pages627–635,2011.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning represen-
tations by back-propagating errors. Nature, 323(6088):533–536, October 1986. DOI:
10.1038/323533a0.
Ivan A. Sag, omas Wasow, and Emily M. Bender. Syntactic eory, 2nd ed., CSLI Lecture
Note152,2003.
MagnusSahlgren.edistributionalhypothesis.ItalianJournalofLinguistics,20(1):33–54,2008.
NathanSchneider,VivekSrikumar,JenaD.Hwang,andMarthaPalmer. Ahierarchywith,of,
andforprepositionsupersenses. InProc.ofthe9thLinguisticAnnotationWorkshop,pages112–
123,2015.DOI:10.3115/v1/w15-1612.
NathanSchneider,JenaD.Hwang,VivekSrikumar,MeredithGreen,AbhijitSuresh,Kathryn
Conger, Tim O’Gorman, and Martha Palmer. A corpus of preposition supersenses. In Proc.
ofthe10thLinguisticAnnotationWorkshop,2016.DOI:10.18653/v1/w16-1712.
BernhardSchölkopf.ekerneltrickfordistances.InT.K.Leen,T.G.Dietterich,andV.Tresp,
Eds.,AdvancesinNeuralInformationProcessingSystems13,pages301–307.MITPress,2001.
http://papers.nips.cc/paper/1862-the-kernel-trick-for-distances.pdf
M. Schuster and Kuldip K. Paliwal. Bidirectional recurrent neural networks. IEEE Trans-
actions on Signal Processing, 45(11):2673–2681, November 1997. ISSN 1053-587X. DOI:
10.1109/78.650093.
Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gauvain. Continuous space language mod-
els for statistical machine translation. In Proc. of the COLING/ACL on Main Confer-
ence Poster Sessions, pages 723–730. Association for Computational Linguistics, 2006. DOI:
10.3115/1273073.1273166.
Rico Sennrich and Barry Haddow. Proc. of the 1st Conference on Machine Translation: Vol-
ume1,ResearchPapers,chapterLinguisticInputFeaturesImproveNeuralMachineTranslation,
pages 83–91. Association for Computational Linguistics, 2016. http://aclweb.org/antholo
gy/W16-2209DOI:10.18653/v1/W16-2209.BIBLIOGRAPHY 279
RicoSennrich, Barry Haddow,and Alexandra Birch. Neural machine translationof rarewords
with subword units. In Proc. of the 54th Annual Meeting of the Association for Computational
Linguistics—(Volume 1: Long Papers), pages 1715–1725, 2016a. http://aclweb.org/antholo
gy/P16-1162DOI:10.18653/v1/P16-1162.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation
modelswithmonolingualdata. InProc.ofthe54thAnnualMeetingoftheAssociationforCom-
putationalLinguistics—(Volume1:LongPapers),pages86–96.AssociationforComputational
Linguistics,2016b. http://aclweb.org/anthology/P16-1009DOI:10.18653/v1/P16-1009.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From eory to
Algorithms. CambridgeUniversityPress,2014.DOI:10.1017/cbo9781107298019.
JohnShawe-TaylorandNelloCristianini. KernelMethodsforPatternAnalysis. CambridgeUni-
versityPress,Cambridge,UK,June2004.DOI:10.4018/9781599040424.ch001.
Q.Shi, J.Petterson,G. Dror,J.Langford,A. J.Smola, A.Strehl,and V.Vishwanathan. Hash
kernels. InArtificialIntelligenceandStatisticsAISTATS’09,Florida,April2009.
KarenSimonyanandAndrewZisserman.Verydeepconvolutionalnetworksforlarge-scaleimage
recognition. InICLR,2015.
NoahA.Smith. LinguisticStructurePrediction. SynthesisLecturesonHumanLanguageTech-
nologies.Morgan&Claypool,May2011.DOI:10.2200/s00361ed1v01y201105hlt013.
RichardSocher. RecursiveDeepLearningForNaturalLanguageProcessingandComputerVision.
Ph.D.thesis,StanfordUniversity,August2014.
RichardSocher,ChristopherManning,andAndrewNg. Learningcontinuousphraserepresen-
tationsandsyntacticparsingwithrecursiveneuralnetworks. InProc.oftheDeepLearningand
UnsupervisedFeatureLearningWorkshopof{NIPS},pages1–9,2010.
Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng, and Christopher D. Manning. Parsing
natural scenes and natural language with recursive neural networks. In Lise Getoor and To-
bias Scheffer, Eds., Proc. of the 28th International Conference on Machine Learning, ICML ,
pages129–136,Bellevue,Washington,June28–July2,Omnipress,2011.
Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic com-
positionality through recursive matrix-vector spaces. In Proc. of the Joint Conference on Em-
piricalMethodsinNaturalLanguageProcessingandComputationalNaturalLanguageLearning,
pages1201–1211,JejuIsland,Korea.AssociationforComputationalLinguistics,July2012.
RichardSocher,JohnBauer,ChristopherD.Manning,andAndrewY.Ng. Parsingwithcompo-
sitionalvectorgrammars.InProc.ofthe51stAnnualMeetingoftheAssociationforComputational
Linguistics—(Volume1:LongPapers),pages455–465,Sofia,Bulgaria,August2013a.280 BIBLIOGRAPHY
RichardSocher,AlexPerelygin,JeanWu,JasonChuang,ChristopherD.Manning,AndrewNg,
andChristopherPotts. Recursivedeepmodelsforsemanticcompositionalityoverasentiment
treebank. InProc.ofthe2013ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,
pages 1631–1642, Seattle, Washington. Association for Computational Linguistics, October
2013b.
AndersSøgaard. Semi-SupervisedLearningandDomainAdaptationinNaturalLanguageProcess-
ing. SynthesisLecturesonHumanLanguageTechnologies.Morgan&ClaypoolPublishers,
2013.DOI:10.2200/s00497ed1v01y201304hlt021.
Anders Søgaard and Yoav Goldberg. Deep multi-task learning with low level tasks super-
vised at lower layers. In Proc.of the 54th Annual Meeting of the Associationfor Computational
Linguistics—(Volume2:ShortPapers),pages231–235,2016. http://aclweb.org/anthology/P
16-2038DOI:10.18653/v1/P16-2038.
Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret
Mitchell,Jian-YunNie,JianfengGao,andBillDolan. Aneuralnetworkapproachtocontext-
sensitive generation of conversational responses. In Proc.oftheConferenceoftheNorthAmer-
ican Chapter of the Association for Computational Linguistics: Human Language Technologies,
pages196–205,Denver,Colorado,2015.DOI:10.3115/v1/n15-1020.
VivekSrikumarandDanRoth. Aninventoryofprepositionrelations. arXiv:1305.5785,2013a.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. Dropout:Asimplewaytopreventneuralnetworksfromoverfitting. JournalofMachine
LearningResearch,15:1929–1958,2014. http://jmlr.org/papers/v15/srivastava14a.html
E.Strubell,P.Verga,D.Belanger,andA.McCallum. Fastandaccuratesequencelabelingwith
iterateddilatedconvolutions. ArXive-prints,February2017.
Martin Sundermeyer, Ralf Schlüter, and Hermann Ney. LSTM neural networks for language
modeling. InINTERSPEECH,2012.
MartinSundermeyer,TamerAlkhouli,JoernWuebker,andHermannNey. Translationmodel-
ingwithbidirectionalrecurrentneuralnetworks.InProc.oftheConferenceonEmpiricalMethods
inNaturalLanguageProcessing(EMNLP), pages 14–25, Doha, Qatar. Association for Com-
putationalLinguistics,October2014.DOI:10.3115/v1/d14-1003.
Ilya Sutskever, James Martens, and Geoffrey E. Hinton. Generating text with recurrent neu-
ral networks. In Proc. of the 28th International Conference on Machine Learning (ICML-11),
pages1017–1024,2011.DOI:10.1109/icnn.1993.298658.
IlyaSutskever,JamesMartens,GeorgeDahl,andGeoffreyHinton. Ontheimportanceofini-
tialization and momentum in deep learning. In Proc. of the 30th International Conference on
MachineLearning(ICML-13),pages1139–1147,2013.BIBLIOGRAPHY 281
Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le. Sequence to sequence learning with neural
networks. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Wein-
berger, Eds., AdvancesinNeuralInformationProcessingSystems27, pages 3104–3112. Curran
Associates,Inc.,2014.
Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic represen-
tations from tree-structured long short-term memory networks. In Proc. of the 53rd Annual
MeetingoftheAssociationforComputationalLinguisticsandthe7thInternationalJointConference
on Natural LanguageProcessing—(Volume 1: Long Papers), pages 1556–1566, Beijing, China,
July2015.DOI:10.3115/v1/p15-1150.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita. Recurrent neural networks for word
alignment model. In Proc. of the 52nd Annual Meeting of the Association for Computational
Linguistics—(Volume 1: Long Papers), pages 1470–1480, Baltimore, Maryland, June 2014.
DOI:10.3115/v1/p14-1138.
DuyuTang,BingQin,andTingLiu. Documentmodelingwithgatedrecurrentneuralnetwork
forsentimentclassification.InProc.oftheConferenceonEmpiricalMethodsinNaturalLanguage
Processing, pages 1422–1432. Association for Computational Linguistics, 2015. http://aclw
eb.org/anthology/D15-1167DOI:10.18653/v1/D15-1167.
Matus Telgarsky. Benefits of depth in neural networks. arXiv:1602.04485 [cs, stat], February
2016.
RobertTibshirani.Regressionshrinkageandselectionviathelasso.JournaloftheRoyalStatistical
Society,SeriesB,58:267–288,1994.DOI:10.1111/j.1467-9868.2011.00771.x.
T.TielemanandG.Hinton. Lecture6.5—RmsProp:Dividethegradientbyarunningaverage
ofitsrecentmagnitude. COURSERA:NeuralNetworksforMachineLearning,2012.
JosephTurian,Lev-ArieRatinov,andYoshuaBengio. Wordrepresentations:Asimpleandgen-
eralmethodforsemi-supervisedlearning. InProc.ofthe48thAnnualMeetingoftheAssociation
forComputationalLinguistics,pages384–394,2010.http://aclweb.org/anthology/P10-1040
PeterD.Turney. Miningthewebforsynonyms:PMI-IRvs.LSAonTOEFL. InECML,2001.
DOI:10.1007/3-540-44795-4_42.
PeterD.TurneyandPatrickPantel. Fromfrequencytomeaning:Vectorspacemodelsofseman-
tics. JournalofArtificialIntelligenceResearch,37(1):141–188,2010.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Dubiner. Large scale parallel document
miningformachinetranslation. InProc.ofthe23rdInternationalConferenceonComputational
Linguistics(Coling2010),pages1101–1109,OrganizingCommittee,2010. http://aclweb.o
rg/anthology/C10-1124282 BIBLIOGRAPHY
Tim Van de Cruys. A neural network approach to selectional preference acquisition. In
Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP),
pages 26–35, Doha, Qatar. Association for Computational Linguistics, October 2014. DOI:
10.3115/v1/d14-1004.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. Decoding with large-
scaleneurallanguagemodelsimprovestranslation.InProc.oftheConferenceonEmpiricalMeth-
ods in Natural Language Processing, pages 1387–1392, Seattle, Washington. Association for
ComputationalLinguistics,October2013.
AshishVaswani,YinggongZhao,VictoriaFossum,andDavidChiang.Decodingwithlarge-scale
neural language models improves translation. In Proc.oftheConferenceonEmpiricalMethods
inNaturalLanguageProcessing,pages1387–1392.AssociationforComputationalLinguistics,
2013. http://aclweb.org/anthology/D13-1140
Ashish Vaswani, Yonatan Bisk, Kenji Sagae, and Ryan Musa. Supertagging with LSTMs. In
Proc.oftheConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguis-
tics:HumanLanguageTechnologies,pages232–237.AssociationforComputationalLinguistics,
2016. http://aclweb.org/anthology/N16-1027DOI:10.18653/v1/N16-1027.
Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton.
Grammarasaforeignlanguage. arXiv:1412.7449[cs,stat],December2014.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural
image caption generator. In IEEE Conference on Computer Vision and Pattern Recognition,
CVPR,pages3156–3164,Boston,MA,June7–12,2015.DOI:10.1109/cvpr.2015.7298935.
Stefan Wager, Sida Wang, and Percy S Liang. Dropout training as adaptive regularization. In
C.J.C.Burges,L.Bottou,M.Welling,Z.Ghahramani,andK.Q.Weinberger,Eds.,Advances
inNeuralInformationProcessingSystems26,pages351–359.CurranAssociates,Inc.,2013.
MengqiuWangandChristopherD.Manning.Effectofnon-lineardeeparchitectureinsequence
labeling. InIJCNLP,pages1285–1291,2013.
Peng Wang, Jiaming Xu, Bo Xu, Chenglin Liu, Heng Zhang, Fangyuan Wang, and Hongwei
Hao. Semantic clustering and convolutional neural network for short text categorization. In
Proc.ofthe53rdAnnualMeetingoftheAssociationforComputationalLinguisticsandthe7thInter-
nationalJointConferenceonNaturalLanguageProcessing—(Volume2:ShortPapers),pages352–
357,Beijing,China,July2015a.DOI:10.3115/v1/p15-2058.
XinWang,YuanchaoLiu,ChengjieSun,BaoxunWang,andXiaolongWang. Predictingpolar-
ities of tweets by composing word embeddings with long short-term memory. In Proc.ofthe
53rd Annual Meeting of the Association for Computational Linguistics and the 7th InternationalBIBLIOGRAPHY 283
JointConferenceonNaturalLanguageProcessing—(Volume1:LongPapers), pages 1343–1353,
Beijing,China,July2015b.DOI:10.3115/v1/p15-1130.
TaroWatanabeandEiichiroSumita. Transition-basedneuralconstituentparsing. InProc.ofthe
53rd Annual Meeting of the Association for Computational Linguistics and the 7th International
JointConferenceonNaturalLanguageProcessing—(Volume1:LongPapers), pages 1169–1179,
Beijing,China,July2015.DOI:10.3115/v1/p15-1113.
K. Weinberger, A. Dasgupta, J. Attenberg, J. Langford, and A. J. Smola. Feature hashing for
large scale multitask learning. In International Conference on Machine Learning, 2009. DOI:
10.1145/1553374.1553516.
DavidWeiss,ChrisAlberti,MichaelCollins,andSlavPetrov. Structuredtrainingforneuralnet-
worktransition-basedparsing. InProc.ofthe53rdAnnualMeetingoftheAssociationforCompu-
tationalLinguisticsandthe7thInternationalJointConferenceonNaturalLanguageProcessing—
(Volume 1: Long Papers), pages 323–333, Beijing, China, July 2015. DOI: 10.3115/v1/p15-
1032.
P.J.Werbos. Backpropagationthroughtime:Whatitdoesandhowtodoit. Proc.oftheIEEE,
78(10):1550–1560,1990. ISSN0018-9219.DOI:10.1109/5.58337.
JasonWeston,AntoineBordes,OksanaYakhnenko,andNicolasUsunier. Connectinglanguage
andknowledgebaseswithembeddingmodelsforrelationextraction. InProc.oftheConference
onEmpiricalMethodsinNaturalLanguageProcessing,pages1366–1371,Seattle,Washington.
AssociationforComputationalLinguistics,October2013.
PhilipWilliams,RicoSennrich,MattPost,andPhilippKoehn. Syntax-basedStatisticalMachine
Translation.SynthesisLecturesonHumanLanguageTechnologies.Morgan&ClaypoolPub-
lishers,2016.DOI:10.2200/s00716ed1v04y201604hlt033.
Sam Wiseman and Alexander M. Rush. Sequence-to-sequence learning as beam-search op-
timization. In Proc. of the Conference on Empirical Methods in Natural Language Processing
(EMNLP).AssociationforComputationalLinguistics,2016.DOI:10.18653/v1/d16-1137.
Sam Wiseman, M. Alexander Rush, and M. Stuart Shieber. Learning global features for
coreference resolution. In Proc. of the Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Language Technologies, pages 994–1004, 2016.
http://aclweb.org/anthology/N16-1114DOI:10.18653/v1/N16-1114.
YijunXiaoandKyunghyunCho. Efficientcharacter-leveldocumentclassificationbycombining
convolutionandrecurrentlayers. CoRR,abs/1602.00367,2016. http://arxiv.org/abs/1602.
00367284 BIBLIOGRAPHY
Wenduan Xu, Michael Auli, and Stephen Clark. CCG supertagging with a recurrent neural
network. In Proc. of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing—(Volume 2: Short
Papers),pages250–255,Beijing,China,July2015.DOI:10.3115/v1/p15-2041.
Wenpeng Yin and Hinrich Schütze. Convolutional neural network for paraphrase identifica-
tion. In Proc. of the Conference of the North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies, pages 901–911, Denver, Colorado, 2015.
DOI:10.3115/v1/n15-1091.
FisherYuandVladlenKoltun.Multi-scalecontextaggregationbydilatedconvolutions.InICLR,
2016.
WojciechZaremba,IlyaSutskever,andOriolVinyals. Recurrentneuralnetworkregularization.
arXiv:1409.2329[cs],September2014.
Matthew D. Zeiler. ADADELTA: An adaptive learning rate method. arXiv:1212.5701 [cs],
December2012.
DaojianZeng,KangLiu,SiweiLai,GuangyouZhou,andJunZhao. Relationclassificationvia
convolutionaldeepneuralnetwork. InProc.ofCOLING,the25thInternationalConferenceon
Computational Linguistics: Technical Papers, pages 2335–2344, Dublin, Ireland, Dublin City
UniversityandAssociationforComputationalLinguistics,August2014.
Hao Zhang and Ryan McDonald. Generalized higher-order dependency parsing with cube
pruning. In Proc.oftheJointConferenceonEmpiricalMethodsinNaturalLanguageProcessing
andComputationalNaturalLanguageLearning,pages320–331.AssociationforComputational
Linguistics,2012. http://aclweb.org/anthology/D12-1030
TongZhang. Statisticalbehaviorandconsistencyofclassificationmethodsbasedonconvexrisk
minimization. eAnnalsofStatistics,32:56–85,2004.DOI:10.1214/aos/1079120130.
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text
classification. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,
Eds.,AdvancesinNeuralInformationProcessingSystems28,pages649–657.CurranAssociates,
Inc., 2015. http://papers.nips.cc/paper/5782-character-level-convolutional-networks-fo
r-text-classification.pdf
Xingxing Zhang, Jianpeng Cheng, and Mirella Lapata. Dependency parsing as head selection.
CoRR,abs/1606.01280,2016. http://arxiv.org/abs/1606.01280
Yuan Zhang and David Weiss. Stack-propagation: Improved representation learning for syn-
tax. In Proc. of the 54th Annual Meeting of the Association for Computational Linguistics—
(Volume 1: Long Papers), pages 1557–1566, 2016. http://aclweb.org/anthology/P16-1147
DOI:10.18653/v1/P16-1147.BIBLIOGRAPHY 285
Hao Zhou, Yue Zhang, Shujian Huang, and Jiajun Chen. A neural probabilistic structured-
prediction model for transition-based dependency parsing. In Proc.ofthe53rdAnnualMeet-
ingoftheAssociationforComputationalLinguisticsandthe7thInternationalJointConferenceon
NaturalLanguageProcessing—(Volume1:LongPapers),pages1213–1222,Beijing,China,July
2015.DOI:10.3115/v1/p15-1117.
ChenxiZhu, XipengQiu,XinchiChen,andXuanjingHuang. Are-rankingmodelfordepen-
dencyparserwithrecursiveconvolutionalneuralnetwork.InProc.ofthe53rdAnnualMeetingof
theAssociationforComputationalLinguisticsandthe7thInternationalJointConferenceonNatural
LanguageProcessing—(Volume1:LongPapers),pages1159–1168,Beijing,China,July2015a.
DOI:10.3115/v1/p15-1112.
XiaodanZhu,ParinazSobhani,andHongyuGuo.Longshort-termmemoryovertreestructures.
March2015b.
HuiZouandTrevorHastie.Regularizationandvariableselectionviatheelasticnet.Journalofthe
RoyalStatisticalSociety,SeriesB,67:301–320,2005.DOI:10.1111/j.1467-9868.2005.00503.x.287
Author’s Biography
YOAVGOLDBERG
YoavGoldberghasbeenworkinginnaturallanguageprocessingforoveradecade.HeisaSenior
Lecturer at the Computer Science Department at Bar-Ilan University, Israel. Prior to that, he
was a researcher at Google Research, New York. He received his Ph.D. in Computer Science
andNaturalLanguageProcessingfromBenGurionUniversity(2011).Heregularlyreviewsfor
NLPandmachinelearningvenues,andservesattheeditorialboardofComputationalLinguistics.
He published over 50 research papers and received best paper and outstanding paper awards at
major natural language processing conferences. His research interests include machine learning
fornaturallanguage,structuredprediction,syntacticparsing,processingofmorphologicallyrich
languages, and, in the past two years, neural network models with a focus on recurrent neural
networks.