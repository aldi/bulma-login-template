
[Image: extracted_image_0_0.png]
Neural Network Methods for
Natural Language Processing

Synthesis Lectures on Human
Language Technologies
Editor
Graeme Hirst, University of Toronto
Synthesis Lectures on Human Language Technologies is edited by Graeme Hirst of the University
of Toronto. e series consists of 50- to 150-page monographs on topics relating to natural language
processing, computational linguistics, information retrieval, and spoken language understanding.
Emphasis is on important new techniques, on new applications, and on topics that combine two or
more HLT subﬁelds.
Neural Network Methods for Natural Language Processing
Yoav Goldberg
2017
Syntax-based Statistical Machine Translation
Philip Williams, Rico Sennrich, Matt Post, and Philipp Koehn
2016
Domain-Sensitive Temporal Tagging
Jannik Strötgen and Michael Gertz
2016
Linked Lexical Knowledge Bases: Foundations and Applications
Iryna Gurevych, Judith Eckle-Kohler, and Michael Matuschek
2016
Bayesian Analysis in Natural Language Processing
Shay Cohen
2016
Metaphor: A Computational Perspective
Tony Veale, Ekaterina Shutova, and Beata Beigman Klebanov
2016
Grammatical Inference for Computational Linguistics
Jeﬀrey Heinz, Colin de la Higuera, and Menno van Zaanen
2015

iii
Automatic Detection of Verbal Deception
Eileen Fitzpatrick, Joan Bachenko, and Tommaso Fornaciari
2015
Natural Language Processing for Social Media
Atefeh Farzindar and Diana Inkpen
2015
Semantic Similarity from Natural Language and Ontology Analysis
Sébastien Harispe, Sylvie Ranwez, Stefan Janaqi, and Jacky Montmain
2015
Learning to Rank for Information Retrieval and Natural Language Processing, Second
Edition
Hang Li
2014
Ontology-Based Interpretation of Natural Language
Philipp Cimiano, Christina Unger, and John McCrae
2014
Automated Grammatical Error Detection for Language Learners, Second Edition
Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault
2014
Web Corpus Construction
Roland Schäfer and Felix Bildhauer
2013
Recognizing Textual Entailment: Models and Applications
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto
2013
Linguistic Fundamentals for Natural Language Processing: 100 Essentials from
Morphology and Syntax
Emily M. Bender
2013
Semi-Supervised Learning and Domain Adaptation in Natural Language Processing
Anders Søgaard
2013
Semantic Relations Between Nominals
Vivi Nastase, Preslav Nakov, Diarmuid Ó Séaghdha, and Stan Szpakowicz
2013

iv
Computational Modeling of Narrative
Inderjeet Mani
2012
Natural Language Processing for Historical Texts
Michael Piotrowski
2012
Sentiment Analysis and Opinion Mining
Bing Liu
2012
Discourse Processing
Manfred Stede
2011
Bitext Alignment
Jörg Tiedemann
2011
Linguistic Structure Prediction
Noah A. Smith
2011
Learning to Rank for Information Retrieval and Natural Language Processing
Hang Li
2011
Computational Modeling of Human Language Acquisition
Afra Alishahi
2010
Introduction to Arabic Natural Language Processing
Nizar Y. Habash
2010
Cross-Language Information Retrieval
Jian-Yun Nie
2010
Automated Grammatical Error Detection for Language Learners
Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault
2010
Data-Intensive Text Processing with MapReduce
Jimmy Lin and Chris Dyer
2010

v
Semantic Role Labeling
Martha Palmer, Daniel Gildea, and Nianwen Xue
2010
Spoken Dialogue Systems
Kristiina Jokinen and Michael McTear
2009
Introduction to Chinese Natural Language Processing
Kam-Fai Wong, Wenjie Li, Ruifeng Xu, and Zheng-sheng Zhang
2009
Introduction to Linguistic Annotation and Text Analytics
Graham Wilcock
2009
Dependency Parsing
Sandra Kübler, Ryan McDonald, and Joakim Nivre
2009
Statistical Language Models for Information Retrieval
ChengXiang Zhai
2008

© Springer Nature Switzerland AG 2022
Reprint of original edition © Morgan & Claypool 2017
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in
any form or by any means—electronic, mechanical, photocopy, recording, or any other except for brief quotations
in printed reviews, without the prior permission of the publisher.
Neural Network Methods for Natural Language Processing 
Yoav Goldberg
ISBN: 978-3-031-01037-8 
paperback 
ISBN: 978-3-031-02165-7 
ebook
DOI 10.1007/978-3-031-02165-7
A Publication in the Springer series
SYNTHESIS LECTURES ON HUMAN LANGUAGE TECHNOLOGIES
Lecture #37
Series Editor: Graeme Hirst, University of Toronto
Series ISSN
Print 1947-4040
Electronic 1947-4059

Neural Network Methods for
Natural Language Processing
Yoav Goldberg
Bar Ilan University
SYNTHESIS LECTURES ON HUMAN LANGUAGE TECHNOLOGIES #37

ABSTRACT
Neural networks are a family of powerful machine learning models. is book focuses on the
application of neural network models to natural language data. e ﬁrst half of the book (Parts I
and II) covers the basics of supervised machine learning and feed-forward neural networks, the
basics of working with machine learning over language data, and the use of vector-based rather
than symbolic representations for words. It also covers the computation-graph abstraction, which
allows to easily deﬁne and train arbitrary neural networks, and is the basis behind the design of
contemporary neural network software libraries.
e second part of the book (Parts III and IV) introduces more specialized neural net-
work architectures, including 1D convolutional neural networks, recurrent neural networks,
conditioned-generation models, and attention-based models. ese architectures and techniques
are the driving force behind state-of-the-art algorithms for machine translation, syntactic parsing,
and many other applications. Finally, we also discuss tree-shaped networks, structured prediction,
and the prospects of multi-task learning.
KEYWORDS
natural language processing, machine learning, supervised learning, deep learning,
neural networks, word embeddings, recurrent neural networks, sequence to sequence
models

ix
Contents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xvii
Acknowledgments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxi
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1
e Challenges of Natural Language Processing. . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2
Neural Networks and Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.3
Deep Learning in NLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.3.1 Success Stories. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.4
Coverage and Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.5
What’s not Covered . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.6
A Note on Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.7
Mathematical Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
PART I
Supervised Classiﬁcation and Feed-forward
Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2
Learning Basics and Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.1
Supervised Learning and Parameterized Functions . . . . . . . . . . . . . . . . . . . . . . 13
2.2
Train, Test, and Validation Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.3
Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.3.1 Binary Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.3.2 Log-linear Binary Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.3.3 Multi-class Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.4
Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.5
One-Hot and Dense Vector Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.6
Log-linear Multi-class Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.7
Training as Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.7.1 Loss Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.7.2 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

x
2.8
Gradient-based Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
2.8.1 Stochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
2.8.2 Worked-out Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
2.8.3 Beyond SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3
From Linear Models to Multi-layer Perceptrons . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.1
Limitations of Linear Models: e XOR Problem . . . . . . . . . . . . . . . . . . . . . . . 37
3.2
Nonlinear Input Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.3
Kernel Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.4
Trainable Mapping Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4
Feed-forward Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.1
A Brain-inspired Metaphor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.2
In Mathematical Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
4.3
Representation Power . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.4
Common Nonlinearities. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
4.5
Loss Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
4.6
Regularization and Dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
4.7
Similarity and Distance Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
4.8
Embedding Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
5
Neural Network Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
5.1
e Computation Graph Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
5.1.1 Forward Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
5.1.2 Backward Computation (Derivatives, Backprop) . . . . . . . . . . . . . . . . . . . 54
5.1.3 Software. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
5.1.4 Implementation Recipe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
5.1.5 Network Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
5.2
Practicalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
5.2.1 Choice of Optimization Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
5.2.2 Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
5.2.3 Restarts and Ensembles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
5.2.4 Vanishing and Exploding Gradients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
5.2.5 Saturation and Dead Neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
5.2.6 Shuﬄing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
5.2.7 Learning Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
5.2.8 Minibatches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61

xi
PART II
Working with Natural Language Data . . . . . . . . 63
6
Features for Textual Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
6.1
Typology of NLP Classiﬁcation Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
6.2
Features for NLP Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
6.2.1 Directly Observable Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
6.2.2 Inferred Linguistic Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
6.2.3 Core Features vs. Combination Features . . . . . . . . . . . . . . . . . . . . . . . . . 74
6.2.4 Ngram Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
6.2.5 Distributional Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
7
Case Studies of NLP Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
7.1
Document Classiﬁcation: Language Identiﬁcation . . . . . . . . . . . . . . . . . . . . . . . 77
7.2
Document Classiﬁcation: Topic Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . 77
7.3
Document Classiﬁcation: Authorship Attribution . . . . . . . . . . . . . . . . . . . . . . . 78
7.4
Word-in-context: Part of Speech Tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
7.5
Word-in-context: Named Entity Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . 81
7.6
Word in Context, Linguistic Features: Preposition Sense Disambiguation . . . . 82
7.7
Relation Between Words in Context: Arc-Factored Parsing. . . . . . . . . . . . . . . . 85
8
From Textual Features to Inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
8.1
Encoding Categorical Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
8.1.1 One-hot Encodings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
8.1.2 Dense Encodings (Feature Embeddings) . . . . . . . . . . . . . . . . . . . . . . . . . 90
8.1.3 Dense Vectors vs. One-hot Representations . . . . . . . . . . . . . . . . . . . . . . 90
8.2
Combining Dense Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
8.2.1 Window-based Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
8.2.2 Variable Number of Features: Continuous Bag of Words . . . . . . . . . . . . 93
8.3
Relation Between One-hot and Dense Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . 94
8.4
Odds and Ends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
8.4.1 Distance and Position Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
8.4.2 Padding, Unknown Words, and Word Dropout . . . . . . . . . . . . . . . . . . . 96
8.4.3 Feature Combinations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
8.4.4 Vector Sharing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
8.4.5 Dimensionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
8.4.6 Embeddings Vocabulary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
8.4.7 Network’s Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99

xii
8.5
Example: Part-of-Speech Tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
8.6
Example: Arc-factored Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
9
Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
9.1
e Language Modeling Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
9.2
Evaluating Language Models: Perplexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
9.3
Traditional Approaches to Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . 107
9.3.1 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
9.3.2 Limitations of Traditional Language Models. . . . . . . . . . . . . . . . . . . . . 108
9.4
Neural Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
9.5
Using Language Models for Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
9.6
Byproduct: Word Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
10
Pre-trained Word Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
10.1
Random Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
10.2
Supervised Task-speciﬁc Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
10.3
Unsupervised Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
10.3.1 Using Pre-trained Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
10.4
Word Embedding Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
10.4.1 Distributional Hypothesis and Word Representations. . . . . . . . . . . . . . 118
10.4.2 From Neural Language Models to Distributed Representations . . . . . . 122
10.4.3 Connecting the Worlds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
10.4.4 Other Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
10.5
e Choice of Contexts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
10.5.1 Window Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
10.5.2 Sentences, Paragraphs, or Documents . . . . . . . . . . . . . . . . . . . . . . . . . . 129
10.5.3 Syntactic Window . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
10.5.4 Multilingual. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
10.5.5 Character-based and Sub-word Representations . . . . . . . . . . . . . . . . . . 131
10.6
Dealing with Multi-word Units and Word Inﬂections . . . . . . . . . . . . . . . . . . . 132
10.7
Limitations of Distributional Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
11
Using Word Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
11.1
Obtaining Word Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
11.2
Word Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
11.3
Word Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136

xiii
11.4
Finding Similar Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
11.4.1 Similarity to a Group of Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
11.5
Odd-one Out . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
11.6
Short Document Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
11.7
Word Analogies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
11.8
Retroﬁtting and Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
11.9
Practicalities and Pitfalls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
12
Case Study: A Feed-forward Architecture for Sentence Meaning Inference . . 141
12.1
Natural Language Inference and the SNLI Dataset . . . . . . . . . . . . . . . . . . . . . 141
12.2
A Textual Similarity Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
PART III
Specialized Architectures . . . . . . . . . . . . . . . . . 147
13
Ngram Detectors: Convolutional Neural Networks . . . . . . . . . . . . . . . . . . . . . . 151
13.1
Basic Convolution + Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
13.1.1 1D Convolutions Over Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
13.1.2 Vector Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
13.1.3 Variations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
13.2
Alternative: Feature Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
13.3
Hierarchical Convolutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
14
Recurrent Neural Networks: Modeling Sequences and Stacks . . . . . . . . . . . . . 163
14.1
e RNN Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
14.2
RNN Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
14.3
Common RNN Usage-patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
14.3.1 Acceptor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
14.3.2 Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
14.3.3 Transducer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
14.4
Bidirectional RNNs (biRNN) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
14.5
Multi-layer (stacked) RNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
14.6
RNNs for Representing Stacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
14.7
A Note on Reading the Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174

xiv
15
Concrete Recurrent Neural Network Architectures . . . . . . . . . . . . . . . . . . . . . . 177
15.1
CBOW as an RNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
15.2
Simple RNN. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
15.3
Gated Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
15.3.1 LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
15.3.2 GRU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
15.4
Other Variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
15.5
Dropout in RNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
16
Modeling with Recurrent Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
16.1
Acceptors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
16.1.1 Sentiment Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
16.1.2 Subject-verb Agreement Grammaticality Detection . . . . . . . . . . . . . . . 187
16.2
RNNs as Feature Extractors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
16.2.1 Part-of-speech Tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
16.2.2 RNN–CNN Document Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . 191
16.2.3 Arc-factored Dependency Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
17
Conditioned Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
17.1
RNN Generators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
17.1.1 Training Generators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
17.2
Conditioned Generation (Encoder-Decoder) . . . . . . . . . . . . . . . . . . . . . . . . . . 196
17.2.1 Sequence to Sequence Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
17.2.2 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
17.2.3 Other Conditioning Contexts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
17.3
Unsupervised Sentence Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
17.4
Conditioned Generation with Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
17.4.1 Computational Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
17.4.2 Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
17.5
Attention-based Models in NLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
17.5.1 Machine Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
17.5.2 Morphological Inﬂection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
17.5.3 Syntactic Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211

xv
PART IV
Additional Topics . . . . . . . . . . . . . . . . . . . . . . . 213
18
Modeling Trees with Recursive Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 215
18.1
Formal Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
18.2
Extensions and Variations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
18.3
Training Recursive Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
18.4
A Simple Alternative–Linearized Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
18.5
Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
19
Structured Output Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
19.1
Search-based Structured Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
19.1.1 Structured Prediction with Linear Models . . . . . . . . . . . . . . . . . . . . . . . 221
19.1.2 Nonlinear Structured Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
19.1.3 Probabilistic Objective (CRF) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
19.1.4 Approximate Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
19.1.5 Reranking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
19.1.6 See Also . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
19.2
Greedy Structured Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
19.3
Conditional Generation as Structured Output Prediction . . . . . . . . . . . . . . . . 227
19.4
Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
19.4.1 Search-based Structured Prediction: First-order Dependency Parsing . 228
19.4.2 Neural-CRF for Named Entity Recognition . . . . . . . . . . . . . . . . . . . . . 229
19.4.3 Approximate NER-CRF With Beam-Search . . . . . . . . . . . . . . . . . . . . 232
20
Cascaded, Multi-task and Semi-supervised Learning . . . . . . . . . . . . . . . . . . . . 235
20.1
Model Cascading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
20.2
Multi-task Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
20.2.1 Training in a Multi-task Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
20.2.2 Selective Sharing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
20.2.3 Word-embeddings Pre-training as Multi-task Learning . . . . . . . . . . . . 243
20.2.4 Multi-task Learning in Conditioned Generation . . . . . . . . . . . . . . . . . 243
20.2.5 Multi-task Learning as Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . 243
20.2.6 Caveats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
20.3
Semi-supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
20.4
Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
20.4.1 Gaze-prediction and Sentence Compression . . . . . . . . . . . . . . . . . . . . . 245
20.4.2 Arc Labeling and Syntactic Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246

xvi
20.4.3 Preposition Sense Disambiguation and Preposition Translation
Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
20.4.4 Conditioned Generation: Multilingual Machine Translation,
Parsing, and Image Captioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
20.5
Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
21
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
21.1
What Have We Seen? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
21.2
e Challenges Ahead . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
Author’s Biography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287

xvii
Preface
Natural language processing (NLP) is a collective term referring to automatic computational pro-
cessing of human languages. is includes both algorithms that take human-produced text as
input, and algorithms that produce natural looking text as outputs. e need for such algorithms
is ever increasing: human produce ever increasing amounts of text each year, and expect com-
puter interfaces to communicate with them in their own language. Natural language processing
is also very challenging, as human language is inherently ambiguous, ever changing, and not well
deﬁned.
Natural language is symbolic in nature, and the ﬁrst attempts at processing language were
symbolic: based on logic, rules, and ontologies. However, natural language is also highly ambigu-
ous and highly variable, calling for a more statistical algorithmic approach. Indeed, the current-
day dominant approaches to language processing are all based on statistical machine learning. For
over a decade, core NLP techniques were dominated by linear modeling approaches to supervised
learning, centered around algorithms such as Perceptrons, linear Support Vector Machines, and
Logistic Regression, trained over very high dimensional yet very sparse feature vectors.
Around 2014, the ﬁeld has started to see some success in switching from such linear models
over sparse inputs to nonlinear neural network models over dense inputs. Some of the neural-
network techniques are simple generalizations of the linear models and can be used as almost
drop-in replacements for the linear classiﬁers. Others are more advanced, require a change of
mindset, and provide new modeling opportunities. In particular, a family of approaches based
on recurrent neural networks (RNNs) alleviates the reliance on the Markov Assumption that was
prevalent in sequence models, allowing to condition on arbitrarily long sequences and produce
eﬀective feature extractors. ese advances led to breakthroughs in language modeling, automatic
machine translation, and various other applications.
While powerful, the neural network methods exhibit a rather strong barrier of entry, for
various reasons. In this book, I attempt to provide NLP practitioners as well as newcomers with
the basic background, jargon, tools, and methodologies that will allow them to understand the
principles behind neural network models for language, and apply them in their own work. I also
hope to provide machine learning and neural network practitioners with the background, jargon,
tools, and mindset that will allow them to eﬀectively work with language data.
Finally, I hope this book can also serve a relatively gentle (if somewhat incomplete) intro-
duction to both NLP and machine learning for people who are newcomers to both ﬁelds.

xviii
PREFACE
INTENDED READERSHIP
is book is aimed at readers with a technical background in computer science or a related ﬁeld,
who want to get up to speed with neural network techniques for natural language processing.
While the primary audience of the book is graduate students in language processing and machine
learning, I made an eﬀort to make it useful also to established researchers in either NLP or ma-
chine learning (by including some advanced material), and to people without prior exposure to
either machine learning or NLP (by covering the basics from the grounds up). is last group of
people will, obviously, need to work harder.
While the book is self contained, I do assume knowledge of mathematics, in particular
undergraduate level of probability, algebra, and calculus, as well as basic knowledge of algorithms
and data structures. Prior exposure to machine learning is very helpful, but not required.
is book evolved out of a survey paper [Goldberg, 2016], which was greatly expanded and
somewhat re-organized to provide a more comprehensive exposition, and more in-depth coverage
of some topics that were left out of the survey for various reasons. is book also contains many
more concrete examples of applications of neural networks to language data that do not exist in
the survey. While this book is intended to be useful also for people without NLP or machine
learning backgrounds, the survey paper assumes knowledge in the ﬁeld. Indeed, readers who are
familiar with natural language processing as practiced between roughly 2006 and 2014, with heavy
reliance on machine learning and linear models, may ﬁnd the journal version quicker to read and
better organized for their needs. However, such readers may also appreciate reading the chapters
on word embeddings (10 and 11), the chapter on conditioned generation with RNNs (17), and
the chapters on structured prediction and multi-task learning (MTL) (19 and 20).
FOCUS OF THIS BOOK
is book is intended to be self-contained, while presenting the diﬀerent approaches under a uni-
ﬁed notation and framework. However, the main purpose of the book is in introducing the neural-
networks (deep-learning) machinery and its application to language data, and not in providing
an in-depth coverage of the basics of machine learning theory and natural language technology.
I refer the reader to external sources when these are needed.
Likewise, the book is not intended as a comprehensive resource for those who will go on
and develop the next advances in neural network machinery (although it may serve as a good
entry point). Rather, it is aimed at those readers who are interested in taking the existing, use-
ful technology and applying it in useful and creative ways to their favorite language-processing
problems.

PREFACE
xix
.
Further reading
For in-depth, general discussion of neural networks, the theory behind
them, advanced optimization methods, and other advanced topics, the reader is referred to
other existing resources. In particular, the book by Bengio et al. [2016] is highly recom-
mended.
For a friendly yet rigorous introduction to practical machine learning, the freely avail-
able book of Daumé III [2015] is highly recommended. For more theoretical treatment of
machine learning, see the freely available textbook of Shalev-Shwartz and Ben-David [2014]
and the textbook of Mohri et al. [2012].
For a strong introduction to NLP, see the book of Jurafsky and Martin [2008]. e
information retrieval book by Manning et al. [2008] also contains relevant information for
working with language data.
Finally, for getting up-to-speed with linguistic background, the book of Bender [2013]
in this series provides a concise but comprehensive coverage, directed at computationally
minded readers. e ﬁrst chapters of the introductory grammar book by Sag et al. [2003] are
also worth reading.
As of this writing, the progress of research in neural networks and Deep Learning is very
fast paced. e state-of-the-art is a moving target, and I cannot hope to stay up-to-date with the
latest-and-greatest. e focus is thus with covering the more established and robust techniques,
that were proven to work well in several occasions, as well as selected techniques that are not yet
fully functional but that I ﬁnd to be established and/or promising enough for inclusion.
Yoav Goldberg
March 2017

xxi
Acknowledgments
is book grew out of a survey paper I’ve written on the topic [Goldberg, 2016], which in turn
grew out of my frustration with the lack organized and clear material on the intersection of deep
learning and natural language processing, as I was trying to learn it and teach it to my students
and collaborators. I am thus indebted to the numerous people who commented on the survey
paper (in its various forms, from initial drafts to post-publication comments), as well as to the
people who commented on various stages of the book’s draft. Some commented in person, some
over email, and some in random conversations on Twitter. e book was also inﬂuenced by people
who did not comment on it per-se (indeed, some never read it) but discussed topics related to it.
Some are deep learning experts, some are NLP experts, some are both, and others were trying to
learn both topics. Some (few) contributed through very detailed comments, others by discussing
small details, others in between. But each of them inﬂuenced the ﬁnal form of the book. ey are,
in alphabetical order: Yoav Artzi, Yonatan Aumann, Jason Baldridge, Miguel Ballesteros, Mohit
Bansal, Marco Baroni, Tal Baumel, Sam Bowman, Jordan Boyd-Graber, Chris Brockett, Ming-
Wei Chang, David Chiang, Kyunghyun Cho, Grzegorz Chrupala, Alexander Clark, Raphael
Cohen, Ryan Cotterell, Hal Daumé III, Nicholas Dronen, Chris Dyer, Jacob Eisenstein, Jason
Eisner, Michael Elhadad, Yad Faeq, Manaal Faruqui, Amir Globerson, Fréderic Godin, Edward
Grefenstette, Matthew Honnibal, Dirk Hovy, Moshe Koppel, Angeliki Lazaridou, Tal Linzen,
ang Luong, Chris Manning, Stephen Merity, Paul Michel, Margaret Mitchell, Piero Molino,
Graham Neubig, Joakim Nivre, Brendan O’Connor, Nikos Pappas, Fernando Pereira, Barbara
Plank, Ana-Maria Popescu, Delip Rao, Tim Rocktäschel, Dan Roth, Alexander Rush, Naomi
Saphra, Djamé Seddah, Erel Segal-Halevi, Avi Shmidman, Shaltiel Shmidman, Noah Smith,
Anders Søgaard, Abe Stanway, Emma Strubell, Sandeep Subramanian, Liling Tan, Reut Tsarfaty,
Peter Turney, Tim Vieira, Oriol Vinyals, Andreas Vlachos, Wenpeng Yin, and Torsten Zesch.
e list excludes, of course, the very many researchers I’ve communicated with through
their academic writings on the topic.
e book also beneﬁted a lot from—and was shaped by—my interaction with the Natural
Language Processing Group at Bar-Ilan University (and its soft extensions): Yossi Adi, Roee Aha-
roni, Oded Avraham, Ido Dagan, Jessica Ficler, Jacob Goldberger, Hila Gonen, Joseph Keshet,
Eliyahu Kiperwasser, Ron Konigsberg, Omer Levy, Oren Melamud, Gabriel Stanovsky, Ori
Shapira, Micah Shlain, Vered Shwartz, Hillel Taub-Tabib, and Rachel Wities. Most of them
belong in both lists, but I tried to keep things short.
e anonymous reviewers of the book and the survey paper—while unnamed (and some-
times annoying)—provided a solid set of comments, suggestions, and corrections, which I can
safely say dramatically improved many aspects of the ﬁnal product. anks, whoever you are!

xxii
ACKNOWLEDGMENTS
And thanks also to Graeme Hirst, Michael Morgan, Samantha Draper, and C.L. Tondo for
orchestrating the eﬀort.
As usual, all mistakes are of course my own. Do let me know if you ﬁnd any, though, and
be listed in the next edition if one is ever made.
Finally, I would like to thank my wife, Noa, who was patient and supportive when I disap-
peared into writing sprees, my parents Esther and Avner and brother Nadav who were in many
cases more excited about the idea of me writing a book than I was, and the staﬀ at e Streets Cafe
(King George branch) and Shne’or Cafe who kept me well fed and served me drinks throughout
the writing process, with only very minimal distractions.
Yoav Goldberg
March 2017

1
C H A P T E R
1
Introduction
1.1
THE CHALLENGES OF NATURAL LANGUAGE
PROCESSING
Natural language processing (NLP) is the ﬁeld of designing methods and algorithms that take
as input or produce as output unstructured, natural language data. Human language is highly
ambiguous (consider the sentence I ate pizza with friends, and compare it to I ate pizza with
olives), and also highly variable (the core message of I ate pizza with friends can also be expressed as
friends and I shared some pizza). It is also ever changing and evolving. People are great at producing
language and understanding language, and are capable of expressing, perceiving, and interpreting
very elaborate and nuanced meanings. At the same time, while we humans are great users of
language, we are also very poor at formally understanding and describing the rules that govern
language.
Understanding and producing language using computers is thus highly challenging. Indeed,
the best known set of methods for dealing with language data are using supervised machine learn-
ing algorithms, that attempt to infer usage patterns and regularities from a set of pre-annotated
input and output pairs. Consider, for example, the task of classifying a document into one of four
categories: S, P, G, and E. Obviously, the words in the documents
provide very strong hints, but which words provide what hints? Writing up rules for this task is
rather challenging. However, readers can easily categorize a document into its topic, and then,
based on a few hundred human-categorized examples in each category, let a supervised machine
learning algorithm come up with the patterns of word usage that help categorize the documents.
Machine learning methods excel at problem domains where a good set of rules is very hard to
deﬁne but annotating the expected output for a given input is relatively simple.
Besides the challenges of dealing with ambiguous and variable inputs in a system with ill-
deﬁned and unspeciﬁed set of rules, natural language exhibits an additional set of properties that
make it even more challenging for computational approaches, including machine learning: it is
discrete, compositional, and sparse.
Language is symbolic and discrete. e basic elements of written language are characters.
Characters form words that in turn denote objects, concepts, events, actions, and ideas. Both
characters and words are discrete symbols: words such as “hamburger” or “pizza” each evoke in us
a certain mental representations, but they are also distinct symbols, whose meaning is external to
them and left to be interpreted in our heads. ere is no inherent relation between “hamburger”
and “pizza” that can be inferred from the symbols themselves, or from the individual letters they

2
1. INTRODUCTION
are made of. Compare that to concepts such as color, prevalent in machine vision, or acoustic
signals: these concepts are continuous, allowing, for example, to move from a colorful image to a
gray-scale one using a simple mathematical operation, or to compare two diﬀerent colors based
on inherent properties such as hue and intensity. is cannot be easily done with words—there
is no simple operation that will allow us to move from the word “red” to the word “pink” without
using a large lookup table or a dictionary.
Language is also compositional: letters form words, and words form phrases and sentences.
e meaning of a phrase can be larger than the meaning of the individual words that comprise it,
and follows a set of intricate rules. In order to interpret a text, we thus need to work beyond the
level of letters and words, and look at long sequences of words such as sentences, or even complete
documents.
e combination of the above properties leads to data sparseness. e way in which words
(discrete symbols) can be combined to form meanings is practically inﬁnite. e number of possi-
ble valid sentences is tremendous: we could never hope to enumerate all of them. Open a random
book, and the vast majority of sentences within it you have not seen or heard before. Moreover,
it is likely that many sequences of four-words that appear in the book are also novel to you. If you
were to look at a newspaper from just 10 years ago, or imagine one 10 years in the future, many
of the words, in particular names of persons, brands, and corporations, but also slang words and
technical terms, will be novel as well. ere is no clear way of generalizing from one sentence to
another, or deﬁning the similarity between sentences, that does not depend on their meaning—
which is unobserved to us. is is very challenging when we come to learn from examples: even
with a huge example set we are very likely to observe events that never occurred in the example
set, and that are very diﬀerent than all the examples that did occur in it.
1.2
NEURAL NETWORKS AND DEEP LEARNING
Deep learning is a branch of machine learning. It is a re-branded name for neural networks—a
family of learning techniques that was historically inspired by the way computation works in the
brain, and which can be characterized as learning of parameterized diﬀerentiable mathematical
functions.¹ e name deep-learning stems from the fact that many layers of these diﬀerentiable
function are often chained together.
While all of machine learning can be characterized as learning to make predictions based
on past observations, deep learning approaches work by learning to not only predict but also to
correctly represent the data, such that it is suitable for prediction. Given a large set of desired input-
output mapping, deep learning approaches work by feeding the data into a network that produces
successive transformations of the input data until a ﬁnal transformation predicts the output. e
transformations produced by the network are learned from the given input-output mappings,
such that each transformation makes it easier to relate the data to the desired label.
¹In this book we take the mathematical view rather than the brain-inspired view.

1.3. DEEP LEARNING IN NLP
3
While the human designer is in charge of designing the network architecture and training
regime, providing the network with a proper set of input-output examples, and encoding the input
data in a suitable way, a lot of the heavy-lifting of learning the correct representation is performed
automatically by the network, supported by the network’s architecture.
1.3
DEEP LEARNING IN NLP
Neural networks provide a powerful learning machinery that is very appealing for use in natural
language problems. A major component in neural networks for language is the use of an embed-
ding layer, a mapping of discrete symbols to continuous vectors in a relatively low dimensional
space. When embedding words, they transform from being isolated distinct symbols into math-
ematical objects that can be operated on. In particular, distance between vectors can be equated
to distance between words, making it easier to generalize the behavior from one word to another.
is representation of words as vectors is learned by the network as part of the training process.
Going up the hierarchy, the network also learns to combine word vectors in a way that is useful for
prediction. is capability alleviates to some extent the discreteness and data-sparsity problems.
ere are two major kinds of neural network architectures, that can be combined in various
ways: feed-forward networks and recurrent/recursive networks.
Feed-forward networks, in particular multi-layer perceptrons (MLPs), allow to work with
ﬁxed sized inputs, or with variable length inputs in which we can disregard the order of the ele-
ments. When feeding the network with a set of input components, it learns to combine them in
a meaningful way. MLPs can be used whenever a linear model was previously used. e nonlin-
earity of the network, as well as the ability to easily integrate pre-trained word embeddings, often
lead to superior classiﬁcation accuracy.
Convolutional feed-forward networks are specialized architectures that excel at extracting
local patterns in the data: they are fed arbitrarily sized inputs, and are capable of extracting mean-
ingful local patterns that are sensitive to word order, regardless of where they appear in the input.
ese work very well for identifying indicative phrases or idioms of up to a ﬁxed length in long
sentences or documents.
Recurrent neural networks (RNNs) are specialized models for sequential data. ese are
network components that take as input a sequence of items, and produce a ﬁxed size vector that
summarizes that sequence. As “summarizing a sequence” means diﬀerent things for diﬀerent tasks
(i.e., the information needed to answer a question about the sentiment of a sentence is diﬀerent
from the information needed to answer a question about its grammaticality), recurrent networks
are rarely used as standalone component, and their power is in being trainable components that
can be fed into other network components, and trained to work in tandem with them. For ex-
ample, the output of a recurrent network can be fed into a feed-forward network that will try
to predict some value. e recurrent network is used as an input-transformer that is trained to
produce informative representations for the feed-forward network that will operate on top of it.
Recurrent networks are very impressive models for sequences, and are arguably the most exciting

4
1. INTRODUCTION
oﬀer of neural networks for language processing. ey allow abandoning the markov assumption
that was prevalent in NLP for decades, and designing models that can condition on entire sen-
tences, while taking word order into account when it is needed, and not suﬀering much from
statistical estimation problems stemming from data sparsity. is capability leads to impressive
gains in language-modeling, the task of predicting the probability of the next word in a sequence
(or, equivalently, the probability of a sequence), which is a cornerstone of many NLP applications.
Recursive networks extend recurrent networks from sequences to trees.
Many of the problems in natural language are structured, requiring the production of com-
plex output structures such as sequences or trees, and neural network models can accommodate
that need as well, either by adapting known structured-prediction algorithms for linear models,
or by using novel architectures such as sequence-to-sequence (encoder-decoder) models, which
we refer to in this book as conditioned-generation models. Such models are at the heart of state-
of-the-art machine translation.
Finally, many language prediction tasks are related to each other, in the sense that knowing
to perform one of them will help in learning to perform the others. In addition, while we may
have a shortage of supervised (labeled) training data, we have ample supply of raw text (unlabeled
data). Can we learn from related tasks and un-annotated data? Neural network approaches pro-
vide exciting opportunities for both MTL (learning from related problems) and semi-supervised
learning (learning from external, unannotated data).
1.3.1
SUCCESS STORIES
Fully connected feed-forward neural networks (MLPs) can, for the most part, be used as a drop-in
replacement wherever a linear learner is used. is includes binary and multi-class classiﬁcation
problems, as well as more complex structured prediction problems. e nonlinearity of the net-
work, as well as the ability to easily integrate pre-trained word embeddings, often lead to superior
classiﬁcation accuracy. A series of works² managed to obtain improved syntactic parsing results
by simply replacing the linear model of a parser with a fully connected feed-forward network.
Straightforward applications of a feed-forward network as a classiﬁer replacement (usually cou-
pled with the use of pre-trained word vectors) provide beneﬁts for many language tasks, including
the very well basic task of language modeling³ CCG supertagging,⁴ dialog state tracking,⁵ and
pre-ordering for statistical machine translation.⁶ Iyyer et al. [2015] demonstrate that multi-layer
feed-forward networks can provide competitive results on sentiment classiﬁcation and factoid
question answering. Zhou et al. [2015] and Andor et al. [2016] integrate them in a beam-search
structured-prediction system, achieving stellar accuracies on syntactic parsing, sequence tagging
and other tasks.
²[Chen and Manning, 2014, Durrett and Klein, 2015, Pei et al., 2015, Weiss et al., 2015]
³See Chapter 9, as well as Bengio et al. [2003], Vaswani et al. [2013].
⁴[Lewis and Steedman, 2014]
⁵[Henderson et al., 2013]
⁶[de Gispert et al., 2015]

1.3. DEEP LEARNING IN NLP
5
Networks with convolutional and pooling layers are useful for classiﬁcation tasks in which
we expect to ﬁnd strong local clues regarding class membership, but these clues can appear in
diﬀerent places in the input. For example, in a document classiﬁcation task, a single key phrase
(or an ngram) can help in determining the topic of the document [Johnson and Zhang, 2015].
We would like to learn that certain sequences of words are good indicators of the topic, and do not
necessarily care where they appear in the document. Convolutional and pooling layers allow the
model to learn to ﬁnd such local indicators, regardless of their position. Convolutional and pool-
ing architecture show promising results on many tasks, including document classiﬁcation,⁷ short-
text categorization,⁸ sentiment classiﬁcation,⁹ relation-type classiﬁcation between entities,¹⁰ event
detection,¹¹ paraphrase identiﬁcation,¹² semantic role labeling,¹³ question answering,¹⁴ predict-
ing box-oﬃce revenues of movies based on critic reviews,¹⁵ modeling text interestingness,¹⁶ and
modeling the relation between character-sequences and part-of-speech tags.¹⁷
In natural language we often work with structured data of arbitrary sizes, such as sequences
and trees. We would like to be able to capture regularities in such structures, or to model similari-
ties between such structures. Recurrent and recursive architectures allow working with sequences
and trees while preserving a lot of the structural information. Recurrent networks [Elman, 1990]
are designed to model sequences, while recursive networks [Goller and Küchler, 1996] are gen-
eralizations of recurrent networks that can handle trees. Recurrent models have been shown to
produce very strong results for language modeling,¹⁸ as well as for sequence tagging,¹⁹ machine
translation,²⁰ parsing,²¹ and many other tasks including noisy text normalization,²² dialog state
tracking,²³ response generation,²⁴ and modeling the relation between character sequences and
part-of-speech tags.²⁵
⁷[Johnson and Zhang, 2015]
⁸[Wang et al., 2015a]
⁹[Kalchbrenner et al., 2014, Kim, 2014]
¹⁰[dos Santos et al., 2015, Zeng et al., 2014]
¹¹[Chen et al., 2015, Nguyen and Grishman, 2015]
¹²[Yin and Schütze, 2015]
¹³[Collobert et al., 2011]
¹⁴[Dong et al., 2015]
¹⁵[Bitvai and Cohn, 2015]
¹⁶[Gao et al., 2014]
¹⁷[dos Santos and Zadrozny, 2014]
¹⁸Some notable works are Adel et al. [2013], Auli and Gao [2014], Auli et al. [2013], Duh et al. [2013], Jozefowicz et al. [2016],
Mikolov [2012], Mikolov et al. [2010, 2011].
¹⁹[Irsoy and Cardie, 2014, Ling et al., 2015b, Xu et al., 2015]
²⁰[Cho et al., 2014b, Sundermeyer et al., 2014, Sutskever et al., 2014, Tamura et al., 2014]
²¹[Dyer et al., 2015, Kiperwasser and Goldberg, 2016b, Watanabe and Sumita, 2015]
²²[Chrupala, 2014]
²³[Mrkšić et al., 2015]
²⁴[Kannan et al., 2016, Sordoni et al., 2015]
²⁵[Ling et al., 2015b]

6
1. INTRODUCTION
Recursive models were shown to produce state-of-the-art or near state-of-the-art results
for constituency²⁶ and dependency²⁷ parse re-ranking, discourse parsing,²⁸ semantic relation clas-
siﬁcation,²⁹ political ideology detection based on parse trees,³⁰ sentiment classiﬁcation,³¹ target-
dependent sentiment classiﬁcation,³² and question answering.³³
1.4
COVERAGE AND ORGANIZATION
e book consists of four parts. Part I introduces the basic learning machinery we’ll be using
throughout the book: supervised learning, MLPs, gradient-based training, and the computation-
graph abstraction for implementing and training neural networks. Part II connects the machinery
introduced in the ﬁrst part with language data. It introduces the main sources of information that
are available when working with language data, and explains how to integrate them with the
neural networks machinery. It also discusses word-embedding algorithms and the distributional
hypothesis, and feed-forward approaches to language modeling. Part III deals with specialized
architectures and their applications to language data: 1D convolutional networks for working
with ngrams, and RNNs for modeling sequences and stacks. RNNs are the main innovation of
the application of neural networks to language data, and most of Part III is devoted to them,
including the powerful conditioned-generation framework they facilitate, and attention-based
models. Part IV is a collection of various advanced topics: recursive networks for modeling trees,
structured prediction models, and multi-task learning.
Part I, covering the basics of neural networks, consists of four chapters. Chapter 2 in-
troduces the basic concepts of supervised machine learning, parameterized functions, linear and
log-linear models, regularization and loss functions, training as optimization, and gradient-based
training methods. It starts from the ground up, and provides the needed material for the following
chapters. Readers familiar with basic learning theory and gradient-based learning may consider
skipping this chapter. Chapter 3 spells out the major limitation of linear models, motivates the
need for nonlinear models, and lays the ground and motivation for multi-layer neural networks.
Chapter 4 introduces feed-forward neural networks and the MLPs. It discusses the deﬁnition of
multi-layer networks, their theoretical power, and common subcomponents such as nonlinearities
and loss functions. Chapter 5 deals with neural network training. It introduces the computation-
graph abstraction that allows for automatic gradient computations for arbitrary networks (the
back-propagation algorithm), and provides several important tips and tricks for eﬀective network
training.
²⁶[Socher et al., 2013a]
²⁷[Le and Zuidema, 2014, Zhu et al., 2015a]
²⁸[Li et al., 2014]
²⁹[Hashimoto et al., 2013, Liu et al., 2015]
³⁰[Iyyer et al., 2014b]
³¹[Hermann and Blunsom, 2013, Socher et al., 2013b]
³²[Dong et al., 2014]
³³[Iyyer et al., 2014a]

1.4. COVERAGE AND ORGANIZATION
7
Part II introducing language data, consists of seven chapters. Chapter 6 presents a typol-
ogy of common language-processing problems, and discusses the available sources of information
(features) available for us when using language data. Chapter 7 provides concrete case studies,
showing how the features described in the previous chapter are used for various natural language
tasks. Readers familiar with language processing can skip these two chapters. Chapter 8 connects
the material of Chapters 6 and 7 with neural networks, and discusses the various ways of encoding
language-based features as inputs for neural networks. Chapter 9 introduces the language model-
ing task, and the feed-forward neural language model architecture. is also paves the way for dis-
cussing pre-trained word embeddings in the following chapters. Chapter 10 discusses distributed
and distributional approaches to word-meaning representations. It introduces the word-context
matrix approach to distributional semantics, as well as neural language-modeling inspired word-
embedding algorithms, such as GV and W2V, and discusses the connection between
them and the distributional methods. Chapter 11 deals with using word embeddings outside of
the context of neural networks. Finally, Chapter 12 presents a case study of a task-speciﬁc feed-
forward network that is tailored for the Natural Language Inference task.
Part III introducing the specialized convolutional and recurrent architectures, consists of
ﬁve chapters. Chapter 13 deals with convolutional networks, that are specialized at learning in-
formative ngram patterns. e alternative hash-kernel technique is also discussed. e rest of
this part, Chapters 14–17, is devoted to RNNs. Chapter 14 describes the RNN abstraction for
modeling sequences and stacks. Chapter 15 describes concrete instantiations of RNNs, including
the Simple RNN (also known as Elman RNNs) and gated architectures such as the Long Short-
term Memory (LSTM) and the Gated Recurrent Unit (GRU). Chapter 16 provides examples
of modeling with the RNN abstraction, showing their use within concrete applications. Finally,
Chapter 17 introduces the conditioned-generation framework, which is the main modeling tech-
nique behind state-of-the-art machine translation, as well as unsupervised sentence modeling and
many other innovative applications.
Part IV is a mix of advanced and non-core topics, and consists of three chapters. Chapter 18
introduces tree-structured recursive networks for modeling trees. While very appealing, this fam-
ily of models is still in research stage, and is yet to show a convincing success story. Nonetheless,
it is an important family of models to know for researchers who aim to push modeling techniques
beyond the state-of-the-art. Readers who are mostly interested in mature and robust techniques
can safely skip this chapter. Chapter 19 deals with structured prediction. It is a rather techni-
cal chapter. Readers who are particularly interested in structured prediction, or who are already
familiar with structured prediction techniques for linear models or for language processing, will
likely appreciate the material. Others may rather safely skip it. Finally, Chapter 20 presents multi-
task and semi-supervised learning. Neural networks provide ample opportunities for multi-task
and semi-supervised learning. ese are important techniques, that are still at the research stage.
However, the existing techniques are relatively easy to implement, and do provide real gains. e
chapter is not technically challenging, and is recommended to all readers.

8
1. INTRODUCTION
Dependencies
For the most part, chapters, depend on the chapters that precede them. An ex-
ception are the ﬁrst two chapters of Part II, which do not depend on material in previous chapters
and can be read in any order. Some chapters and sections can be skipped without impacting the
understanding of other concepts or material. ese include Section 10.4 and Chapter 11 that
deal with the details of word embedding algorithms and the use of word embeddings outside of
neural networks; Chapter 12, describing a speciﬁc architecture for attacking the Stanford Natural
Language Inference (SNLI) dataset; and Chapter 13 describing convolutional networks. Within
the sequence on recurrent networks, Chapter 15, dealing with the details of speciﬁc architectures,
can also be relatively safely skipped. e chapters in Part IV are for the most part independent of
each other, and can be either skipped or read in any order.
1.5
WHAT’S NOT COVERED
e focus is on applications of neural networks to language processing tasks. However, some sub-
areas of language processing with neural networks were deliberately left out of scope of this book.
Speciﬁcally, I focus on processing written language, and do not cover working with speech data
or acoustic signals. Within written language, I remain relatively close to the lower level, relatively
well-deﬁned tasks, and do not cover areas such as dialog systems, document summarization, or
question answering, which I consider to be vastly open problems. While the described techniques
can be used to achieve progress on these tasks, I do not provide examples or explicitly discuss these
tasks directly. Semantic parsing is similarly out of scope. Multi-modal applications, connecting
language data with other modalities such as vision or databases are only very brieﬂy mentioned.
Finally, the discussion is mostly English-centric, and languages with richer morphological sys-
tems and fewer computational resources are only very brieﬂy discussed.
Some important basics
are also not discussed. Speciﬁcally, two crucial aspects of good work in
language processing are proper evaluation and data annotation. Both of these topics are left outside
the scope of this book, but the reader should be aware of their existence.
Proper evaluation includes the choice of the right metrics for evaluating performance on a
given task, best practices, fair comparison with other work, performing error analysis, and assess-
ing statistical signiﬁcance.
Data annotation is the bread-and-butter of NLP systems. Without data, we cannot train
supervised models. As researchers, we very often just use “standard” annotated data produced by
someone else. It is still important to know the source of the data, and consider the implications
resulting from its creation process. Data annotation is a very vast topic, including proper for-
mulation of the annotation task; developing the annotation guidelines; deciding on the source
of annotated data, its coverage and class proportions, good train-test splits; and working with
annotators, consolidating decisions, validating quality of annotators and annotation, and various
similar topics.

1.6. A NOTE ON TERMINOLOGY
9
1.6
A NOTE ON TERMINOLOGY
e word “feature” is used to refer to a concrete, linguistic input such as a word, a suﬃx, or a part-
of-speech tag. For example, in a ﬁrst-order part-of-speech tagger, the features might be “current
word, previous word, next word, previous part of speech.” e term “input vector” is used to refer
to the actual input that is fed to the neural network classiﬁer. Similarly, “input vector entry” refers
to a speciﬁc value of the input. is is in contrast to a lot of the neural networks literature in
which the word “feature” is overloaded between the two uses, and is used primarily to refer to an
input-vector entry.
1.7
MATHEMATICAL NOTATION
We use bold uppercase letters to represent matrices (X, Y , Z), and bold lowercase letters to
represent vectors (b). When there are series of related matrices and vectors (for example, where
each matrix corresponds to a diﬀerent layer in the network), superscript indices are used (W 1,
W 2). For the rare cases in which we want indicate the power of a matrix or a vector, a pair of
brackets is added around the item to be exponentiated: .W /2; .W 3/2. We use Œ� as the index
operator of vectors and matrices: bŒi� is the ith element of vector b, and W Œi;j � is the element
in the ith column and jth row of matrix W . When unambiguous, we sometimes adopt the
more standard mathematical notation and use bi to indicate the ith element of vector b, and
similarly wi;j for elements of a matrix W . We use � to denote the dot-product operator: w � v D
P
i wivi D P
i wŒi�vŒi�. We use x1Wn to indicate a sequence of vectors x1; : : : ; xn, and similarly
x1Wn is the sequence of items x1; : : : ; xn. We use xnW1 to indicate the reverse sequence. x1WnŒi� D
xi, xnW1Œi� D xn�iC1. We use Œv1I v2� to denote vector concatenation.
While somewhat unorthodox, unless otherwise stated, vectors are assumed to be row vec-
tors. e choice to use row vectors, which are right multiplied by matrices (xW C b), is somewhat
non standard—a lot of the neural networks literature use column vectors that are left multiplied
by matrices (W x C b). We trust the reader to be able to adapt to the column vectors notation
when reading the literature.³⁴
³⁴e choice to use the row vectors notation was inspired by the following beneﬁts: it matches the way input vectors and network
diagrams are often drawn in the literature; it makes the hierarchical/layered structure of the network more transparent and
puts the input as the left-most variable rather than being nested; it results in fully connected layer dimensions being din � dout
rather than dout � din; and it maps better to the way networks are implemented in code using matrix libraries such as numpy.

PART I
Supervised Classiﬁcation and
Feed-forward Neural Networks

13
C H A P T E R
2
Learning Basics
and Linear Models
Neural networks, the topic of this book, are a class of supervised machine learning algorithms.
is chapter provides a quick introduction to supervised machine learning terminology and
practices, and introduces linear and log-linear models for binary and multi-class classiﬁcation.
e chapter also sets the stage and notation for later chapters. Readers who are familiar with
linear models can skip ahead to the next chapters, but may also beneﬁt from reading Sections 2.4
and 2.5.
Supervised machine learning theory and linear models are very large topics, and this chapter
is far from being comprehensive. For a more complete treatment the reader is referred to texts
such as Daumé III [2015], Shalev-Shwartz and Ben-David [2014], and Mohri et al. [2012].
2.1
SUPERVISED LEARNING AND PARAMETERIZED
FUNCTIONS
e essence of supervised machine learning is the creation of mechanisms that can look at exam-
ples and produce generalizations. More concretely, rather than designing an algorithm to perform
a task (“distinguish spam from non-spam email”), we design an algorithm whose input is a set
of labeled examples (“is pile of emails are spam. is other pile of emails are not spam.”), and
its output is a function (or a program) that receives an instance (an email) and produces the de-
sired label (spam or not-spam). It is expected that the resulting function will produce correct label
predictions also for instances it has not seen during training.
As searching over the set of all possible programs (or all possible functions) is a very hard
(and rather ill-deﬁned) problem, we often restrict ourselves to search over speciﬁc families of
functions, e.g., the space of all linear functions with din inputs and dout outputs, or the space of all
decision trees over din variables. Such families of functions are called hypothesis classes. By restrict-
ing ourselves to a speciﬁc hypothesis class, we are injecting the learner with inductive bias—a set
of assumptions about the form of the desired solution, as well as facilitating eﬃcient procedures
for searching for the solution. For a broad and readable overview of the main families of learning
algorithms and the assumptions behind them, see the book by Domingos [2015].
e hypothesis class also determines what can and cannot be represented by the learner.
One common hypothesis class is that of high-dimensional linear function, i.e., functions of the

14
2. LEARNING BASICS AND LINEAR MODELS
form:¹
f .x/ D x � W C b
(2.1)
x 2 Rdin
W 2 Rdin�dout
b 2 Rdout:
Here, the vector x is the input to the function, while the matrix W and the vector b are
the parameters. e goal of the learner is to set the values of the parameters W and b such that
the function behaves as intended on a collection of input values x1Wk D x1; : : : ; xk and the corre-
sponding desired outputs y1Wk D yi; : : : ; yk. e task of searching over the space of functions is
thus reduced to one of searching over the space of parameters. It is common to refer to parameters
of the function as ‚. For the linear model case, ‚ D W ; b. In some cases we want the notation
to make the parameterization explicit, in which case we include the parameters in the function’s
deﬁnition: f .xI W ; b/ D x � W C b.
As we will see in the coming chapters, the hypothesis class of linear functions is rather
restricted, and there are many functions that it cannot represent (indeed, it is limited to linear
relations). In contrast, feed-forward neural networks with hidden layers, to be discussed in Chap-
ter 4, are also parameterized functions, but constitute a very strong hypothesis class—they are
universal approximators, capable of representing any Borel-measurable function.² However, while
restricted, linear models have several desired properties: they are easy and eﬃcient to train, they
often result in convex optimization objectives, the trained models are somewhat interpretable,
and they are often very eﬀective in practice. Linear and log-linear models were the dominant
approaches in statistical NLP for over a decade. Moreover, they serve as the basic building blocks
for the more powerful nonlinear feed-forward networks which will be discussed in later chapters.
2.2
TRAIN, TEST, AND VALIDATION SETS
Before delving into the details of linear models, let’s reconsider the general setup of the machine
learning problem. We are faced with a dataset of k input examples x1Wk and their corresponding
gold labels y1Wk, and our goal is to produce a function f .x/ that correctly maps inputs x to
outputs Oy, as evidenced by the training set. How do we know that the produced function f ./ is
indeed a good one? One could run the training examples x1Wk through f ./, record the answers
Oy1Wk, compare them to the expected labels y1Wk, and measure the accuracy. However, this process
will not be very informative—our main concern is the ability of f ./ to generalize well to unseen
examples. A function f ./ that is implemented as a lookup table, that is, looking for the input x
in its memory and returning the corresponding value y for instances is has seen and a random
value otherwise, will get a perfect score on this test, yet is clearly not a good classiﬁcation function
as it has zero generalization ability. We rather have a function f ./ that gets some of the training
examples wrong, providing that it will get unseen examples correctly.
¹As discussed in Section 1.7. is book takes a somewhat un-orthodox approach and assumes vectors are row vectors rather
than column vectors.
²See further discussion in Section 4.3.

2.2. TRAIN, TEST, AND VALIDATION SETS
15
Leave-one out
We must assess the trained function’s accuracy on instances it has not seen during
training. One solution is to perform leave-one-out cross-validation: train k functions f1Wk, each
time leaving out a diﬀerent input example xi, and evaluating the resulting function fi./ on its
ability to predict xi. en train another function f ./ on the entire trainings set x1Wk. Assuming
that the training set is a representative sample of the population, this percentage of functions fi./
that produced correct prediction on the left-out samples is a good approximation of the accuracy
of f ./ on new inputs. However, this process is very costly in terms of computation time, and is
used only in cases where the number of annotated examples k is very small (less than a hundred
or so). In language processing tasks, we very often encounter training sets with well over 105
examples.
Held-out set
A more eﬃcient solution in terms of computation time is to split the training set
into two subsets, say in a 80%/20% split, train a model on the larger subset (the training set), and
test its accuracy on the smaller subset (the held-out set). is will give us a reasonable estimate on
the accuracy of the trained function, or at least allow us to compare the quality of diﬀerent trained
models. However, it is somewhat wasteful in terms training samples. One could then re-train a
model on the entire set. However, as the model is trained on substantially more data, the error
estimates of the model trained on less data may not be accurate. is is generally a good problem
to have, as more training data is likely to result in better rather than worse predictors.³
Some care must be taken when performing the split—in general it is better to shuﬄe the
examples prior to splitting them, to ensure a balanced distribution of examples between the train-
ing and held-out sets (for example, you want the proportion of gold labels in the two sets to be
similar). However, sometimes a random split is not a good option: consider the case where your
input are news articles collected over several months, and your model is expected to provide pre-
dictions for new stories. Here, a random split will over-estimate the model’s quality: the training
and held-out examples will be from the same time period, and hence on more similar stories,
which will not be the case in practice. In such cases, you want to ensure that the training set has
older news stories and the held-out set newer ones—to be as similar as possible to how the trained
model will be used in practice.
A three-way split
e split into train and held-out sets works well if you train a single model
and wants to assess its quality. However, in practice you often train several models, compare their
quality, and select the best one. Here, the two-way split approach is insuﬃcient—selecting the
best model according to the held-out set’s accuracy will result in an overly optimistic estimate of
the model’s quality. You don’t know if the chosen settings of the ﬁnal classiﬁer are good in general,
or are just good for the particular examples in the held-out sets. e problem will be even worse if
you perform error analysis based on the held-out set, and change the features or the architecture of
the model based on the observed errors. You don’t know if your improvements based on the held-
³Note, however, that some setting in the training procedure, in particular the learning rate and regularization weight may be
sensitive to the training set size, and tuning them based on some data and then re-training a model with the same settings on
larger data may produce sub-optimal results.

16
2. LEARNING BASICS AND LINEAR MODELS
out sets will carry over to new instances. e accepted methodology is to use a three-way split of
the data into train, validation (also called development), and test sets. is gives you two held-out
sets: a validation set (also called development set), and a test set. All the experiments, tweaks, error
analysis, and model selection should be performed based on the validation set. en, a single run
of the ﬁnal model over the test set will give a good estimate of its expected quality on unseen
examples. It is important to keep the test set as pristine as possible, running as few experiments
as possible on it. Some even advocate that you should not even look at the examples in the test
set, so as to not bias the way you design your model.
2.3
LINEAR MODELS
Now that we have established some methodology, we return to describe linear models for binary
and multi-class classiﬁcation.
2.3.1
BINARY CLASSIFICATION
In binary classiﬁcation problems we have a single output, and thus use a restricted version of
Equation (2.1) in which dout D 1, making w a vector and b a scalar.
f .x/ D x � w C b:
(2.2)
e range of the linear function in Equation (2.2) is Œ�1; C1�. In order to use it for
binary classiﬁcation, it is common to pass the output of f .x/ through the sign function, mapping
negative values to �1 (the negative class) and non-negative values to C1 (the positive class).
Consider the task of predicting which of two neighborhoods an apartment is located at,
based on the apartment’s price and size. Figure 2.1 shows a 2D plot of some apartments, where
the x-axis denotes the monthly rent price in USD, while the y-axis is the size in square feet.
e blue circles are for Dupont Circle, DC and the green crosses are in Fairfax, VA. It is evident
from the plot that we can separate the two neighborhoods using a straight line—apartments in
Dupont Circle tend to be more expensive than apartments in Fairfax of the same size.⁴ e dataset
is linearly separable: the two classes can be separated by a straight line.
Each data-point (an apartment) can be represented as a 2-dimensional (2D) vector x where
xŒ0� is the apartment’s size and xŒ1� is its price. We then get the following linear model:
Oy D sign.f .x// D sign.x � w C b/
D sign.size � w1 C price � w2 C b/;
where � is the dot-product operation, b and w D Œw1; w2� are free parameters, and we predict
Fairfax if Oy � 0 and Dupont Circle otherwise. e goal of learning is setting the values of w1,
⁴Note that looking at either size or price alone would not allow us to cleanly separate the two groups.

2.3. LINEAR MODELS
17
2500
2000
1500
1000
500
0 1000
2000
3000
4000
5000
Price
Size
Figure 2.1: Housing data: rent price in USD vs. size in square ft. Data source: Craigslist ads, collected
from June 7–15, 2015.
w2, and b such that the predictions are correct for all data-points we observe.⁵ We will discuss
learning in Section 2.7 but for now consider that we expect the learning procedure to set a high
value to w1 and a low value to w2. Once the model is trained, we can classify new data-points by
feeding them into this equation.
It is sometimes not possible to separate the data-points using a straight line (or, in higher di-
mensions, a linear hyperplane)—such datasets are said to be nonlinearly separable, and are beyond
the hypothesis class of linear classiﬁers. e solution would be to either move to a higher dimen-
sion (add more features), move to a richer hypothesis class, or allow for some mis-classiﬁcation.⁶
⁵Geometrically, for a given w the points x � w C b D 0 deﬁne a hyperplane (which in two dimensions corresponds to a line)
that separates the space into two regions. e goal of learning is then ﬁnding a hyperplane such that the classiﬁcation induced
by it is correct.
⁶Misclassifying some of the examples is sometimes a good idea. For example, if we have reason to believe some of the data-
points are outliers—examples that belong to one class, but are labeled by mistake as belonging to the other class.

[Image: extracted_image_36_0.png]
18
2. LEARNING BASICS AND LINEAR MODELS
..
Feature Representations
In the example above, each data-point was a pair of size and price
measurements. Each of these properties is considered a feature by which we classify the data-
point. is is very convenient, but in most cases the data-points are not given to us directly
as lists of features, but as real-world objects. For example, in the apartments example we
may be given a list of apartments to classify. We then need to make a concious decision and
select the measurable properties of the apartments that we believe will be useful features
for the classiﬁcation task at hand. Here, it proved eﬀective to focus on the price and the
size. We could also look at additional properties, such as the number of rooms, the height
of the ceiling, the type of ﬂoor, the geo-location coordinates, and so on. After deciding on
a set of features, we create a feature extraction function that maps a real world object (i.e.,
an apartment) to a vector of measurable quantities (price and size) which can be used as
inputs to our models. e choice of the features is crucial to the success of the classiﬁcation
accuracy, and is driven by the informativeness of the features, and their availability to us (the
geo-location coordinates are much better predictors of the neighborhood than the price and
size, but perhaps we only observe listings of past transactions, and do not have access to the
geo-location information). When we have two features, it is easy to plot the data and see the
underlying structures. However, as we see in the next example, we often use many more than
just two features, making plotting and precise reasoning impractical.
A central part in the design of linear models, which we mostly gloss over in this text, is
the design of the feature function (so called feature engineering). One of the promises of deep
learning is that it vastly simpliﬁes the feature-engineering process by allowing the model
designer to specify a small set of core, basic, or “natural” features, and letting the trainable
neural network architecture combine them into more meaningful higher-level features, or
representations. However, one still needs to specify a suitable set of core features, and tie
them to a suitable architecture. We discuss common features for textual data in Chapters 6
and 7.
We usually have many more than two features. Moving to a language setup, consider the
task of distinguishing documents written in English from documents written in German. It turns
out that letter frequencies make for quite good predictors (features) for this task. Even more
informative are counts of letter bigrams, i.e., pairs of consecutive letters.⁷ Assuming we have an
alphabet of 28 letters (a–z, space, and a special symbol for all other characters including digits,
punctuations, etc.) we represent a document as a 28 � 28 dimensional vector x 2 R784, where
each entry xŒi� represents a count of a particular letter combination in the document, normalized
by the document’s length. For example, denoting by xab the entry of x corresponding to the
⁷While one may think that words will also be good predictors, letters, or letter-bigrams are far more robust: we are likely to
encounter a new document without any of the words we observed in the training set, while a document without any of the
distinctive letter-bigrams is signiﬁcantly less likely.

2.3. LINEAR MODELS
19
letter-bigram ab:
xab D #ab
jDj;
(2.3)
where #ab is the number of times the bigram ab appears in the document, and jDj is the total
number of bigrams in the document (the document’s length).
_a
_d
_s
_t d_
de
e_
en
er
ie
in
n_
on
re
t_
th
_a
_d
_s
_t d_
de
e_
en
er
ie
in
n_
on
re
t_
th
_a
_d
_s
_t d_
de
e_
en
er
ie
in
n_
on
re
t_
th
_a
_d
_s
_t d_
de
e_
en
er
ie
in
n_
on
re
t_
th
_a
_d
_s
_t d_
de
e_
en
er
ie
in
n_
on
re
t_
th
_a
_d
_s
_t d_
de
e_
en
er
ie
in
n_
on
re
t_
th
_a
_d
_s
_t d_
de
e_
en
er
ie
in
n_
on
re
t_
th
_a
_d
_s
_t d_
de
e_
en
er
ie
in
n_
on
re
t_
th
_a
_d
_s
_t d_
de
e_
en
er
ie
in
n_
on
re
t_
th
_a
_d
_s
_t d_
de
e_
en
er
ie
in
n_
on
re
t_
th
Figure 2.2: Character-bigram histograms for documents in English (left, blue) and German (right,
green). Underscores denote spaces.
Figure 2.2 shows such bigram histograms for several German and English texts. For
readability, we only show the top frequent character-bigrams and not the entire feature vectors.
On the left, we see the bigrams of the English texts, and on the right of the German ones. ere
are clear patterns in the data, and, given a new item, such as:
_a
_d
_s
_t
d_
de
e_
en
er
ie
in
n_
on
re
t_
th

[Image: extracted_image_38_0.png]
[Image: extracted_image_38_1.png]
20
2. LEARNING BASICS AND LINEAR MODELS
you could probably tell that it is more similar to the German group than to the English one.
Note, however, that you couldn’t use a single deﬁnite rule such as “if it has th its English” or “if
it has ie its German”: while German texts have considerably less th than English, the th may
and does occur in German texts, and similarly the ie combination does occur in English. e
decision requires weighting diﬀerent factors relative to each other. Let’s formalize the problem in
a machine-learning setup.
We can again use a linear model:
Oy D sign.f .x// D sign.x � w C b/
D sign.xaa � waa C xab � wab C xac � wac : : : C b/:
(2.4)
A document will be considered English if f .x/ � 0 and as German otherwise. Intuitively,
learning should assign large positive values to w entries associated with letter pairs that are much
more common in English than in German (i.e., th) negative values to letter pairs that are much
more common in German than in English (ie, en), and values around zero to letter pairs that are
either common or rare in both languages.
Note that unlike the 2D case of the housing data (price vs. size), here we cannot easily
visualize the points and the decision boundary, and the geometric intuition is likely much less
clear. In general, it is diﬃcult for most humans to think of the geometries of spaces with more
than three dimensions, and it is advisable to think of linear models in terms of assigning weights
to features, which is easier to imagine and reason about.
2.3.2
LOG-LINEAR BINARY CLASSIFICATION
e output f .x/ is in the range Œ�1; 1�, and we map it to one of two classes f�1; C1g using
the sign function. is is a good ﬁt if all we care about is the assigned class. However, we may
be interested also in the conﬁdence of the decision, or the probability that the classiﬁer assigns to
the class. An alternative that facilitates this is to map instead to the range Œ0; 1�, by pushing the
output through a squashing function such as the sigmoid �.x/ D
1
1Ce�x , resulting in:
Oy D �.f .x// D
1
1 C e�.x�wCb/ :
(2.5)
Figure 2.3 shows a plot of the sigmoid function. It is monotonically increasing, and maps values
to the range Œ0; 1�, with 0 being mapped to 1
2. When used with a suitable loss function (discussed
in Section 2.7.1) the binary predictions made through the log-linear model can be interpreted as
class membership probability estimates �.f .x// D P. Oy D 1 j x/ of x belonging to the positive
class. We also get P. Oy D 0 j x/ D 1 � P. Oy D 1 j x/ D 1 � �.f .x//. e closer the value is to
0 or 1 the more certain the model is in its class membership prediction, with the value of 0.5
indicating model uncertainty.

2.3. LINEAR MODELS
21
σ(x)
1.0
0.8
0.6
0.4
0.2
0.0
-6
-4
-2
0
2
4
6
Figure 2.3: e sigmoid function �.x/.
2.3.3
MULTI-CLASS CLASSIFICATION
e previous examples were of binary classiﬁcation, where we had two possible classes. Binary-
classiﬁcation cases exist, but most classiﬁcation problems are of a multi-class nature, in which we
should assign an example to one of k diﬀerent classes. For example, we are given a document and
asked to classify it into one of six possible languages: English, French, German, Italian, Spanish,
Other. A possible solution is to consider six weight vectors wE; wF; : : : and biases, one for each
language, and predict the language resulting in the highest score:⁸
Oy D f .x/ D
argmax
L2fE;F;G;I;S;Og
x � wL C bL:
(2.6)
e six sets of parameters wL 2 R784; bL can be arranged as a matrix W 2 R784�6 and
vector b 2 R6, and the equation re-written as:
Oy D f .x/ D x � W C b
prediction D Oy D argmax
i
OyŒi�:
(2.7)
Here Oy 2 R6 is a vector of the scores assigned by the model to each language, and we again
determine the predicted language by taking the argmax over the entries of Oy.
⁸ere are many ways to model multi-class classiﬁcation, including binary-to-multi-class reductions. ese are beyond the
scope of this book, but a good overview can be found in Allwein et al. [2000].

[Image: extracted_image_40_0.png]
22
2. LEARNING BASICS AND LINEAR MODELS
2.4
REPRESENTATIONS
Consider the vector Oy resulting from applying Equation 2.7 of a trained model to a document.
e vector can be considered as a representation of the document, capturing the properties of the
document that are important to us, namely the scores of the diﬀerent languages. e represen-
tation Oy contains strictly more information than the prediction Oy D argmaxi OyŒi�: for example,
Oy can be used to distinguish documents in which the main language in German, but which also
contain a sizeable amount of French words. By clustering documents based on their vector rep-
resentations as assigned by the model, we could perhaps discover documents written in regional
dialects, or by multilingual authors.
e vectors x containing the normalized letter-bigram counts for the documents are also
representations of the documents, arguably containing a similar kind of information to the vec-
tors Oy. However, the representations in Oy is more compact (6 entries instead of 784) and more
specialized for the language prediction objective (clustering by the vectors x would likely reveal
document similarities that are not due to a particular mix of languages, but perhaps due to the
document’s topic or writing styles).
e trained matrix W 2 R784�6 can also be considered as containing learned representa-
tions. As demonstrated in Figure 2.4, we can consider two views of W , as rows or as columns.
Each of the 6 columns of W correspond to a particular language, and can be taken to be a 784-
dimensional vector representation of this language in terms of its characteristic letter-bigram pat-
terns. We can then cluster the 6 language vectors according to their similarity. Similarly, each of
the 784 rows of W correspond to a particular letter-bigram, and provide a 6-dimensional vector
representation of that bigram in terms of the languages it prompts.
Representations are central to deep learning. In fact, one could argue that the main power of
deep-learning is the ability to learn good representations. In the linear case, the representations are
interpretable, in the sense that we can assign a meaningful interpretation to each dimension in the
representation vector (e.g., each dimension corresponds to a particular language or letter-bigram).
is is in general not the case—deep learning models often learn a cascade of representations of
the input that build on top of each other, in order to best model the problem at hand, and these
representations are often not interpretable—we do not know which properties of the input they
capture. However, they are still very useful for making predictions. Moreover, at the boundaries of
the model, i.e., at the input and the output, we get representations that correspond to particular
aspects of the input (i.e., a vector representation for each letter-bigram) or the output (i.e., a
vector representation of each of the output classes). We will get back to this in Section 8.3 after
discussing neural networks and encoding categorical features as dense vectors. It is recommended
that you return to this discussion once more after reading that section.

2.5. ONE-HOT AND DENSE VECTOR REPRESENTATIONS
23
O
Sp
It
Gr
Fr
En
aa
ab
ac
ad
ae
af
ag
ah
zy
zz
(a)
(b)
W
W
Figure 2.4: Two views of the W matrix. (a) Each column corresponds to a language. (b) Each row
corresponds to a letter bigram.
2.5
ONE-HOT AND DENSE VECTOR REPRESENTATIONS
e input vector x in our language classiﬁcation example contains the normalized bigram counts
in the document D. is vector can be decomposed into an average of jDj vectors, each corre-
sponding to a particular document position i:
x D
1
jDj
jDj
X
iD1
xDŒi�I
(2.8)
here, DŒi� is the bigram at document position i, and each vector xDŒi� 2 R784 is a one-hot vector,
in which all entries are zero except the single entry corresponding to the letter bigram DŒi�, which
is 1.
e resulting vector x is commonly referred to as an averaged bag of bigrams (more gen-
erally averaged bag of words, or just bag of words). Bag-of-words (BOW) representations contain
information about the identities of all the “words” (here, bigrams) of the document, without con-
sidering their order. A one-hot representation can be considered as a bag-of-a-single-word.
e view of the rows of the matrix W as representations of the letter bigrams suggests an
alternative way of computing the document representation vector Oy in Equation (2.7). Denoting

[Image: extracted_image_42_0.png]
24
2. LEARNING BASICS AND LINEAR MODELS
by W DŒi� the row of W corresponding to the bigram DŒi�, we can take the representation y of a
document D to be the average of the representations of the letter-bigrams in the document:
Oy D
1
jDj
jDj
X
iD1
W DŒi�:
(2.9)
is representation is often called a continuous bag of words (CBOW), as it is composed of a
sum of word representations, where each “word” representation is a low-dimensional, continuous
vector.
We note that Equation (2.9) and the term x � W in Equation (2.7) are equivalent. To see
why, consider:
y D x � W
D
0
@ 1
jDj
jDj
X
iD1
xDŒi�
1
A � W
D
1
jDj
jDj
X
iD1
.xDŒi� � W /
D
1
jDj
jDj
X
iD1
W DŒi�:
(2.10)
In other words, the continuous-bag-of-words (CBOW) representation can be obtained
either by summing word-representation vectors or by multiplying a bag-of-words vector by a
matrix in which each row corresponds to a dense word representation (such matrices are also
called embedding matrices). We will return to this point in Chapter 8 (in particular Section 8.3)
when discussing feature representations in deep learning models for text.
2.6
LOG-LINEAR MULTI-CLASS CLASSIFICATION
In the binary case, we transformed the linear prediction into a probability estimate by passing it
through the sigmoid function, resulting in a log-linear model. e analog for the multi-class case
is passing the score vector through the softmax function:
softmax.x/Œi� D
exŒi�
P
j exŒj � :
(2.11)
Resulting in:

2.7. TRAINING AS OPTIMIZATION
25
Oy D softmax.xW C b/
OyŒi� D
e.xW Cb/Œi�
P
j e.xW Cb/Œj� :
(2.12)
e softmax transformation forces the values in Oy to be positive and sum to 1, making them
interpretable as a probability distribution.
2.7
TRAINING AS OPTIMIZATION
Recall that the input to a supervised learning algorithm is a training set of n training examples
x1Wn D x1; x2; : : : ; xn together with corresponding labels y1Wn D y1; y2; : : : ; yn. Without loss of
generality, we assume that the desired inputs and outputs are vectors: x1Wn, y1Wn.⁹
e goal of the algorithm is to return a function f ./ that accurately maps input examples
to their desired labels, i.e., a function f ./ such that the predictions Oy D f .x/ over the training
set are accurate. To make this more precise, we introduce the notion of a loss function, quantifying
the loss suﬀered when predicting Oy while the true label is y. Formally, a loss function L. Oy; y/
assigns a numerical score (a scalar) to a predicted output Oy given the true expected output y. e
loss function should be bounded from below, with the minimum attained only for cases where
the prediction is correct.
e parameters of the learned function (the matrix W and the biases vector b) are then set
in order to minimize the loss L over the training examples (usually, it is the sum of the losses over
the diﬀerent training examples that is being minimized).
Concretely, given a labeled training set .x1Wn; y1Wn/, a per-instance loss function L and a
parameterized function f .xI ‚/ we deﬁne the corpus-wide loss with respect to the parameters ‚
as the average loss over all training examples:
L.‚/ D 1
n
n
X
iD1
L.f .xiI ‚/; yi/:
(2.13)
In this view, the training examples are ﬁxed, and the values of the parameters determine
the loss. e goal of the training algorithm is then to set the values of the parameters ‚ such that
the value of L is minimized:
O‚ D argmin
‚
L.‚/ D argmin
‚
1
n
n
X
iD1
L.f .xiI ‚/; yi/:
(2.14)
Equation (2.14) attempts to minimize the loss at all costs, which may result in overﬁtting
the training data. To counter that, we often pose soft restrictions on the form of the solution. is
⁹In many cases it is natural to think of the expected output as a scalar (class assignment) rather than a vector. In such cases, y
is simply the corresponding one-hot vector, and argmaxi yŒi� is the corresponding class assignment.

26
2. LEARNING BASICS AND LINEAR MODELS
is done using a function R.‚/ taking as input the parameters and returning a scalar that reﬂect
their “complexity,” which we want to keep low. By adding R to the objective, the optimization
problem needs to balance between low loss and low complexity:
O‚ D argmin
‚
0
BBBB@
loss
‚
…„
ƒ
1
n
n
X
iD1
L.f .xiI ‚/; yi/
C
regularization
‚ …„ ƒ
�R.‚/
1
CCCCA
:
(2.15)
e function R is called a regularization term. Diﬀerent combinations of loss functions and
regularization criteria result in diﬀerent learning algorithms, with diﬀerent inductive biases.
We now turn to discuss common loss functions (Section 2.7.1), followed by a discussion of
regularization and regularizers (Section 2.7.2). en, in Section 2.8 we present an algorithm for
solving the minimization problem (Equation (2.15)).
2.7.1
LOSS FUNCTIONS
e loss can be an arbitrary function mapping two vectors to a scalar. For practical purposes of
optimization, we restrict ourselves to functions for which we can easily compute gradients (or sub-
gradients).¹⁰ In most cases, it is suﬃcient and advisable to rely on a common loss function rather
than deﬁning your own. For a detailed discussion and theoretical treatment of loss functions for
binary classiﬁcation, see Zhang [2004]. We now discuss some loss functions that are commonly
used with linear models and with neural networks in NLP.
Hinge (binary)
For binary classiﬁcation problems, the classiﬁer’s output is a single scalar Qy and
the intended output y is in fC1; �1g. e classiﬁcation rule is Oy D sign. Qy/, and a classiﬁcation
is considered correct if y � Qy > 0, meaning that y and Qy share the same sign. e hinge loss, also
known as margin loss or SVM loss, is deﬁned as:
Lhinge(binary). Qy; y/ D max.0; 1 � y � Qy/:
(2.16)
e loss is 0 when y and Qy share the same sign and j Qyj � 1. Otherwise, the loss is linear.
In other words, the binary hinge loss attempts to achieve a correct classiﬁcation, with a margin
of at least 1.
Hinge (multi-class)
e hinge loss was extended to the multi-class setting by Crammer and
Singer [2002]. Let Oy D OyŒ1�; : : : ; OyŒn� be the classiﬁer’s output vector, and y be the one-hot vector
for the correct output class.
e classiﬁcation rule is deﬁned as selecting the class with the highest score:
prediction D argmax
i
OyŒi�:
(2.17)
¹⁰A gradient of a function with k variables is a collection of k partial derivatives, one according to each of the variables. Gradients
are discussed further in Section 2.8.

2.7. TRAINING AS OPTIMIZATION
27
Denote by t D argmaxi yŒi� the correct class, and by k D argmaxi¤t OyŒi� the highest scoring
class such that k ¤ t. e multi-class hinge loss is deﬁned as:
Lhinge(multi-class). Oy; y/ D max.0; 1 � . OyŒt� � OyŒk�//:
(2.18)
e multi-class hinge loss attempts to score the correct class above all other classes with a margin
of at least 1.
Both the binary and multi-class hinge losses are intended to be used with linear outputs.
e hinge losses are useful whenever we require a hard decision rule, and do not attempt to model
class membership probability.
Log loss
e log loss is a common variation of the hinge loss, which can be seen as a “soft”
version of the hinge loss with an inﬁnite margin [LeCun et al., 2006]:
Llog. Oy; y/ D log.1 C exp.�. OyŒt� � OyŒk�//:
(2.19)
Binary cross entropy
e binary cross-entropy loss, also referred to as logistic loss is used in binary
classiﬁcation with conditional probability outputs. We assume a set of two target classes labeled
0 and 1, with a correct label y 2 f0; 1g. e classiﬁer’s output Qy is transformed using the sigmoid
(also called the logistic) function �.x/ D 1=.1 C e�x/ to the range Œ0; 1�, and is interpreted as the
conditional probability Oy D �. Qy/ D P.y D 1jx/. e prediction rule is:
prediction D
(
0
Oy < 0:5
1
Oy � 0:5:
e network is trained to maximize the log conditional probability log P.y D 1jx/ for each
training example .x; y/. e logistic loss is deﬁned as:
Llogistic. Oy; y/ D �y log Oy � .1 � y/ log.1 � Oy/:
(2.20)
e logistic loss is useful when we want the network to produce class conditional probability
for a binary classiﬁcation problem. When using the logistic loss, it is assumed that the output layer
is transformed using the sigmoid function.
Categorical cross-entropy loss
e categorical cross-entropy loss (also referred to as negative log
likelihood) is used when a probabilistic interpretation of the scores is desired.
Let y D yŒ1�; : : : ; yŒn� be a vector representing the true multinomial distribution over the
labels 1; : : : ; n,¹¹ and let Oy D OyŒ1�; : : : ; OyŒn� be the linear classiﬁer’s output, which was transformed
by the softmax function (Section 2.6), and represent the class membership conditional distribu-
tion OyŒi� D P.y D ijx/. e categorical cross entropy loss measures the dissimilarity between the
true label distribution y and the predicted label distribution Oy, and is deﬁned as cross entropy:
Lcross-entropy. Oy; y/ D �
X
i
yŒi� log. OyŒi�/:
(2.21)
¹¹is formulation assumes an instance can belong to several classes with some degree of certainty.

28
2. LEARNING BASICS AND LINEAR MODELS
For hard-classiﬁcation problems in which each training example has a single correct class
assignment, y is a one-hot vector representing the true class. In such cases, the cross entropy can
be simpliﬁed to:
Lcross-entropy(hard classiﬁcation). Oy; y/ D � log. OyŒt�/;
(2.22)
where t is the correct class assignment. is attempts to set the probability mass assigned to the
correct class t to 1. Because the scores Oy have been transformed using the softmax function to be
non-negative and sum to one, increasing the mass assigned to the correct class means decreasing
the mass assigned to all the other classes.
e cross-entropy loss is very common in the log-linear models and the neural networks
literature, and produces a multi-class classiﬁer which does not only predict the one-best class label
but also predicts a distribution over the possible labels. When using the cross-entropy loss, it is
assumed that the classiﬁer’s output is transformed using the softmax transformation.
Ranking losses
In some settings, we are not given supervision in term of labels, but rather as
pairs of correct and incorrect items x and x0, and our goal is to score correct items above incorrect
ones. Such training situations arise when we have only positive examples, and generate negative
examples by corrupting a positive example. A useful loss in such scenarios is the margin-based
ranking loss, deﬁned for a pair of correct and incorrect examples:
Lranking(margin).x; x0/ D max.0; 1 � .f .x/ � f .x0///;
(2.23)
where f .x/ is the score assigned by the classiﬁer for input vector x. e objective is to score (rank)
correct inputs over incorrect ones with a margin of at least 1.
A common variation is to use the log version of the ranking loss:
Lranking(log).x; x0/ D log.1 C exp.�.f .x/ � f .x0////:
(2.24)
Examples using the ranking hinge loss in language tasks include training with the auxiliary
tasks used for deriving pre-trained word embeddings (see Section 10.4.2), in which we are given a
correct word sequence and a corrupted word sequence, and our goal is to score the correct sequence
above the corrupt one [Collobert and Weston, 2008]. Similarly, Van de Cruys [2014] used the
ranking loss in a selectional-preferences task, in which the network was trained to rank correct
verb-object pairs above incorrect, automatically derived ones, and Weston et al. [2013] trained
a model to score correct (head, relation, tail) triplets above corrupted ones in an information-
extraction setting. An example of using the ranking log loss can be found in Gao et al. [2014]. A
variation of the ranking log loss allowing for a diﬀerent margin for the negative and positive class
is given in dos Santos et al. [2015].

2.7. TRAINING AS OPTIMIZATION
29
2.7.2
REGULARIZATION
Consider the optimization problem in Equation (2.14). It may admit multiple solutions, and,
especially in higher dimensions, it can also over-ﬁt. Consider our language identiﬁcation example,
and a setting in which one of the documents in the training set (call it xo) is an outlier: it is actually
in German, but is labeled as French. In order to drive the loss down, the learner can identify
features (letter bigrams) in xo that occur in only few other documents, and give them very strong
weights toward the (incorrect) French class. en, for other German documents in which these
features occur, which may now be mistakenly classiﬁed as French, the learner will ﬁnd other
German letter bigrams and will raise their weights in order for the documents to be classiﬁed as
German again. is is a bad solution to the learning problem, as it learns something incorrect, and
can cause test German documents which share many words with xo to be mistakenly classiﬁed
as French. Intuitively, we would like to control for such cases by driving the learner away from
such misguided solutions and toward more natural ones, in which it is OK to mis-classify a few
examples if they don’t ﬁt well with the rest.
is is achieved by adding a regularization term R to the optimization objective, whose job
is to control the complexity of the parameter value, and avoid cases of overﬁtting:
O‚ D argmin
‚
L.‚/ C �R.‚/
D argmin
‚
1
n
n
X
iD1
L.f .xiI ‚/; yi/ C �R.‚/:
(2.25)
e regularization term considers the parameter values, and scores their complexity. We
then look for parameter values that have both a low loss and low complexity. A hyperparameter¹²
� is used to control the amount of regularization: do we favor simple model over low loss ones,
or vice versa. e value of � has to be set manually, based on the classiﬁcation performance on a
development set. While Equation (2.25) has a single regularization function and � value for all
the parameters, it is of course possible to have a diﬀerent regularizer for each item in ‚.
In practice, the regularizers R equate complexity with large weights, and work to keep
the parameter values low. In particular, the regularizers R measure the norms of the parameter
matrices, and drive the learner toward solutions with low norms. Common choices for R are the
L2 norm, the L1 norm, and the elastic-net.
L2 regularization
In L2 regularization, R takes the form of the squared L2 norm of the param-
eters, trying to keep the sum of the squares of the parameter values low:
RL2.W / D jjW jj2
2 D
X
i;j
.W Œi;j�/2:
(2.26)
¹²A hyperparameter is a parameter of the model which is not learned as part of the optimization process, but needs to be set by
hand.

30
2. LEARNING BASICS AND LINEAR MODELS
e L2 regularizer is also called a gaussian prior or weight decay.
Note that L2 regularized models are severely punished for high parameter weights, but once
the value is close enough to zero, their eﬀect becomes negligible. e model will prefer to decrease
the value of one parameter with high weight by 1 than to decrease the value of ten parameters
that already have relatively low weights by 0.1 each.
L1 regularization
In L1 regularization, R takes the form of the L1 norm of the parameters,
trying to keep the sum of the absolute values of the parameters low:
RL1.W / D jjW jj1 D
X
i;j
jW Œi;j �j:
(2.27)
In contrast to L2, the L1 regularizer is punished uniformly for low and high values, and
has an incentive to decrease all the non-zero parameter values toward zero. It thus encourages
a sparse solutions—models with many parameters with a zero value. e L1 regularizer is also
called a sparse prior or lasso [Tibshirani, 1994].
Elastic-Net
e elastic-net regularization [Zou and Hastie, 2005] combines both L1 and L2
regularization:
Relastic-net.W / D �1RL1.W / C �2RL2.W /:
(2.28)
Dropout
Another form of regularization which is very eﬀective in neural networks is Dropout,
which we discuss in Section 4.6.
2.8
GRADIENT-BASED OPTIMIZATION
In order to train the model, we need to solve the optimization problem in Equation (2.25). A
common solution is to use a gradient-based method. Roughly speaking, gradient-based methods
work by repeatedly computing an estimate of the loss L over the training set, computing the
gradients of the parameters ‚ with respect to the loss estimate, and moving the parameters in the
opposite directions of the gradient. e diﬀerent optimization methods diﬀer in how the error
estimate is computed, and how “moving in the opposite direction of the gradient” is deﬁned. We
describe the basic algorithm, stochastic gradient descent (SGD), and then brieﬂy mention the other
approaches with pointers for further reading.
..
Motivating Gradient-based Optimization
Consider the task of ﬁnding the scalar value
x that minimizes a function y D f .x/. e canonical approach is computing the second
derivative f 00.x/ of the function, and solving for f 00.x/ D 0 to get the extrema points. For the
sake of example, assume this approach cannot be used (indeed, it is challenging to use this ap-
proach in function of multiple variables). An alternative approach is a numeric one: compute
the ﬁrst derivative f 0.x/. en, start with an initial guess value xi. Evaluating u D f 0.xi/

2.8. GRADIENT-BASED OPTIMIZATION
31
.
will give the direction of change. If u D 0, then xi is an optimum point. Otherwise, move
in the opposite direction of u by setting xiC1  xi � �u, where � is a rate parameter. With
a small enough value of �, f .xiC1/ will be smaller than f .xi/. Repeating this process (with
properly decreasing values of �) will ﬁnd an optimum point xi. If the function f ./ is convex,
the optimum will be a global one. Otherwise, the process is only guaranteed to ﬁnd a local
optimum.
Gradient-based optimization simply generalizes this idea for functions with multiple
variables. A gradient of a function with k variables is the collections of k partial derivatives,
one according to each of the variables. Moving the inputs in the direction of the gradient will
increase the value of the function, while moving them in the opposite direction will decrease
it. When optimizing the loss L.‚I x1Wn; y1Wn/, the parameters ‚ are considered as inputs to
the function, while the training examples are treated as constants.
..
Convexity
In gradient-based optimization, it is common to distinguish between convex (or
concave) functions and non-convex (non-concave) functions. A convex function is a function
whose second-derivative is always non-negative. As a consequence, convex functions have
a single minimum point. Similarly, concave functions are functions whose second-derivatives
are always negative or zero, and as a consequence have a single maximum point. Convex (con-
cave) functions have the property that they are easy to minimize (maximize) using gradient-
based optimization—simply follow the gradient until an extremum point is reached, and once
it is reached we know we obtained the global extremum point. In contrast, for functions that
are neither convex nor concave, a gradient-based optimization procedure may converge to a
local extremum point, missing the global optimum.
2.8.1
STOCHASTIC GRADIENT DESCENT
An eﬀective method for training linear models is using the SGD algorithm [Bottou, 2012, LeCun
et al., 1998a] or a variant of it. SGD is a general optimization algorithm. It receives a function
f parameterized by ‚, a loss function L, and desired input and output pairs x1Wn; y1Wn. It then
attempts to set the parameters ‚ such that the cumulative loss of f on the training examples is
small. e algorithm works, as shown in Algorithm 2.1.
e goal of the algorithm is to set the parameters ‚ so as to minimize the total loss
L.‚/ D Pn
iD1 L.f .xiI �/; yi/ over the training set. It works by repeatedly sampling a training
example and computing the gradient of the error on the example with respect to the parameters
‚ (line 4)—the input and expected output are assumed to be ﬁxed, and the loss is treated as a
function of the parameters ‚. e parameters ‚ are then updated in the opposite direction of the
gradient, scaled by a learning rate �t (line 5). e learning rate can either be ﬁxed throughout the

32
2. LEARNING BASICS AND LINEAR MODELS
Algorithm 2.1 Online stochastic gradient descent training.
Input:
- Function f .xI ‚/ parameterized with parameters ‚.
- Training set of inputs x1; : : : ; xn and desired outputs y1; : : : ; yn.
- Loss function L.
1: while stopping criteria not met do
2:
Sample a training example xi; yi
3:
Compute the loss L.f .xiI ‚/; yi/
4:
Og  gradients of L.f .xiI ‚/; yi/ w.r.t ‚
5:
‚  ‚ � �t Og
6: return ‚
training process, or decay as a function of the time step t.¹³ For further discussion on setting the
learning rate, see Section 5.2.
Note that the error calculated in line 3 is based on a single training example, and is thus
just a rough estimate of the corpus-wide loss L that we are aiming to minimize. e noise in
the loss computation may result in inaccurate gradients. A common way of reducing this noise is
to estimate the error and the gradients based on a sample of m examples. is gives rise to the
minibatch SGD algorithm (Algorithm 2.2).
In lines 3–6, the algorithm estimates the gradient of the corpus loss based on the minibatch.
After the loop, Og contains the gradient estimate, and the parameters ‚ are updated toward Og.
e minibatch size can vary in size from m D 1 to m D n. Higher values provide better estimates
of the corpus-wide gradients, while smaller values allow more updates and in turn faster con-
vergence. Besides the improved accuracy of the gradients estimation, the minibatch algorithm
provides opportunities for improved training eﬃciency. For modest sizes of m, some comput-
ing architectures (i.e., GPUs) allow an eﬃcient parallel implementation of the computation in
lines 3–6. With a properly decreasing learning rate, SGD is guaranteed to converge to a global
optimum if the function is convex, which is the case for linear and log-linear models coupled
with the loss functions and regularizers discussed in this chapter. However, it can also be used
to optimize non-convex functions such as multi-layer neural network. While there are no longer
guarantees of ﬁnding a global optimum, the algorithm proved to be robust and performs well in
practice.¹⁴
¹³Learning rate decay is required in order to prove convergence of SGD.
¹⁴Recent work from the neural networks literature argue that the non-convexity of the networks is manifested in a proliferation
of saddle points rather than local minima [Dauphin et al., 2014]. is may explain some of the success in training neural
networks despite using local search techniques.

2.8. GRADIENT-BASED OPTIMIZATION
33
Algorithm 2.2 Minibatch stochastic gradient descent training.
Input:
- Function f .xI ‚/ parameterized with parameters ‚.
- Training set of inputs x1; : : : ; xn and desired outputs y1; : : : ; yn.
- Loss function L.
1: while stopping criteria not met do
2:
Sample a minibatch of m examples f.x1; y1/; : : : ; .xm; ym/g
3:
Og  0
4:
for i D 1 to m do
5:
Compute the loss L.f .xiI ‚/; yi/
6:
Og  Og C gradients of 1
mL.f .xiI ‚/; yi/ w.r.t ‚
7:
‚  ‚ � �t Og
8: return ‚
2.8.2
WORKED-OUT EXAMPLE
As an example, consider a multi-class linear classiﬁer with hinge loss:
Oy D argmax
i
OyŒi�
Oy D f .x/ D xW C b
L. Oy; y/ D max.0; 1 � . OyŒt� � OyŒk�//
D max.0; 1 � ..xW C b/Œt� � .xW C b/Œk�//
t D argmax
i
yŒi�
k D argmax
i
OyŒi� i ¤ t:
We want to set the parameters W and b such that the loss is minimized. We need to compute
the gradients of the loss with respect to the values W and b. e gradient is the collection of the

34
2. LEARNING BASICS AND LINEAR MODELS
partial derivatives according to each of the variables:
@L. Oy; y/
@W
D
0
BBBBBBBBBB@
@L. Oy;y/
@W Œ1;1�
@L. Oy;y/
@W Œ1;2�
� � �
@L. Oy;y/
@W Œ1;n�
@L. Oy;y/
@W Œ2;1�
@L. Oy;y/
@W Œ2;2�
� � �
@L. Oy;y/
@W Œ2;n�
:::
:::
:::
:::
@L. Oy;y/
@W Œm;1�
@L. Oy;y/
@W Œm;2�
� � �
@L. Oy;y/
@W Œm;n�
1
CCCCCCCCCCA
@L. Oy; y/
@b
D
�
@L. Oy;y/
@bŒ1�
@L. Oy;y/
@bŒ2�
� � �
@L. Oy;y/
@bŒn�
�
:
More concretely, we will compute the derivate of the loss w.r.t each of the values W Œi;j � and bŒj�.
We begin by expanding the terms in the loss calculation:¹⁵
L. Oy; y/ D max.0; 1 � . OyŒt� � OyŒk�//
D max.0; 1 � ..xW C b/Œt� � .xW C b/Œk�//
D max
 
0; 1 �
  X
i
xŒi� � W Œi;t� C bŒt�
!
�
 X
i
xŒi� � W Œi;k� C bŒk�
!!!
D max
 
0; 1 �
X
i
xŒi� � W Œi;t� � bŒt� C
X
i
xŒi� � W Œi;k� C bŒk�
!
t D argmax
i
yŒi�
k D argmax
i
OyŒi� i ¤ t:
e ﬁrst observation is that if 1 � . OyŒt� � OyŒk�/ � 0 then the loss is 0 and so is the gradient (the
derivative of the max operation is the derivative of the maximal value). Otherwise, consider the
derivative of
@L
@bŒi� . For the partial derivative, bŒi� is treated as a variable, and all others are consid-
ered as constants. For i ¤ k; t, the term bŒi� does not contribute to the loss, and its derivative it
is 0. For i D k and i D t we trivially get:
@L
@bŒi�
D
8
ˆ<
ˆ:
�1
i D t
1
i D k
0
otherwise:
¹⁵More advanced derivation techniques allow working with matrices and vectors directly. Here, we stick to high-school level
techniques.

2.8. GRADIENT-BASED OPTIMIZATION
35
Similarly, for W Œi;j �, only j D k and j D t contribute to the loss. We get:
@L
@W Œi;j�
D
8
ˆˆˆˆˆ<
ˆˆˆˆˆ:
@.�xŒi��W Œi;t�/
@W Œi;t�
D �xŒi�
j D t
@.xŒi��W Œi;k�/
@W Œi;k�
D xŒi�
j D k
0
otherwise:
is concludes the gradient calculation.
As a simple exercise, the reader should try and compute the gradients of a multi-class linear
model with hinge loss and L2 regularization, and the gradients of multi-class classiﬁcation with
softmax output transformation and cross-entropy loss.
2.8.3
BEYOND SGD
While the SGD algorithm can and often does produce good results, more advanced algorithms
are also available. e SGD+Momentum [Polyak, 1964] and Nesterov Momentum [Nesterov, 1983,
2004, Sutskever et al., 2013] algorithms are variants of SGD in which previous gradients are ac-
cumulated and aﬀect the current update. Adaptive learning rate algorithms including AdaGrad
[Duchi et al., 2011], AdaDelta [Zeiler, 2012], RMSProp [Tieleman and Hinton, 2012], and
Adam [Kingma and Ba, 2014] are designed to select the learning rate for each minibatch, some-
times on a per-coordinate basis, potentially alleviating the need of ﬁddling with learning rate
scheduling. For details of these algorithms, see the original papers or [Bengio et al., 2016, Sec-
tions 8.3, 8.4].

37
C H A P T E R
3
From Linear Models to
Multi-layer Perceptrons
3.1
LIMITATIONS OF LINEAR MODELS: THE XOR
PROBLEM
e hypothesis class of linear (and log-linear) models is severely restricted. For example, it cannot
represent the XOR function, deﬁned as:
xor.0; 0/ D 0
xor.1; 0/ D 1
xor.0; 1/ D 1
xor.1; 1/ D 0:
at is, there is no parameterization w 2 R2; b 2 R such that:
.0; 0/ � w C b < 0
.0; 1/ � w C b � 0
.1; 0/ � w C b � 0
.1; 1/ � w C b < 0:
To see why, consider the following plot of the XOR function, where blue Os denote the
positive class and green Xs the negative class.
1
0
0
1

[Image: extracted_image_55_0.png]
[Image: extracted_image_55_1.png]
[Image: extracted_image_55_2.png]
38
3. FROM LINEAR MODELS TO MULTI-LAYER PERCEPTRONS
It is clear that no straight line can separate the two classes.
3.2
NONLINEAR INPUT TRANSFORMATIONS
However, if we transform the points by feeding each of them through the nonlinear function
�.x1; x2/ D Œx1 � x2; x1 C x2�, the XOR problem becomes linearly separable.
1
2
0
0
1
2
e function � mapped the data into a representation that is suitable for linear classiﬁcation.
Having � at our disposal, we can now easily train a linear classiﬁer to solve the XOR problem.
Oy D f .x/ D �.x/W C b:
In general, one can successfully train a linear classiﬁer over a dataset which is not linearly
separable by deﬁning a function that will map the data to a representation in which it is linearly
separable, and then train a linear classiﬁer on the resulting representation. In the XOR example
the transformed data has the same dimensions as the original one, but often in order to make the
data linearly separable one needs to map it to a space with a much higher dimension.
is solution has one glaring problem, however: we need to manually deﬁne the function
�, a process which is dependent on the particular dataset, and requires a lot of human intuition.
3.3
KERNEL METHODS
Kernelized Support Vectors Machines (SVMs) [Boser and et al., 1992], and Kernel Methods in
general [Shawe-Taylor and Cristianini, 2004], approach this problem by deﬁning a set of generic
mappings, each of them mapping the data into very high dimensional—and sometimes even
inﬁnite—spaces, and then performing linear classiﬁcation in the transformed space. Working
in very high dimensional spaces signiﬁcantly increase the probability of ﬁnding a suitable linear
separator.
One example mapping is the polynomial mapping, �.x/ D .x/d. For d D 2, we get
�.x1; x2/ D .x1x1; x1x2; x2x1; x2x2/. is gives us all combinations of the two variables, allow-
ing to solve the XOR problem using a linear classiﬁer, with a polynomial increase in the number
of parameters. In the XOR problem the mapping increased the dimensionality of the input (and

[Image: extracted_image_56_0.png]
3.4. TRAINABLE MAPPING FUNCTIONS
39
hence the number of parameters) from 2–4. For the language identiﬁcation example, the input
dimensionality would have increased from 784 to 7842 D614,656 dimensions.
Working in very high dimensional spaces can become computationally prohibitive, and
the ingenuity in kernel methods is the use of the kernel trick [Aizerman et al., 1964, Schölkopf,
2001] that allows one to work in the transformed space without ever computing the transformed
representation. e generic mappings are designed to work on many common cases, and the user
needs to select the suitable one for its task, often by trial and error. A downside of the approach
is that the application of the kernel trick makes the classiﬁcation procedure for SVMs dependent
linearly on the size of the training set, making it prohibitive for use in setups with reasonably
large training sets. Another downside of high dimensional spaces is that they increase the risk of
overﬁtting.
3.4
TRAINABLE MAPPING FUNCTIONS
A diﬀerent approach is to deﬁne a trainable nonlinear mapping function, and train it in con-
junction with the linear classiﬁer. at is, ﬁnding the suitable representation becomes the re-
sponsibility of the training algorithm. For example, the mapping function can take the form of a
parameterized linear model, followed by a nonlinear activation function g that is applied to each
of the output dimensions:
Oy D �.x/W C b
�.x/ D g.xW 0 C b0/:
(3.1)
By taking g.x/ D max.0; x/ and W 0 D
� 1 1
1 1
�, b0 D . �1 0 / we get an equivalent mapping
to .x1 � x2; x1 C x2/ for the our points of interest (0,0), (0,1), (1,0), and (1,1), successfully solv-
ing the XOR problem. e entire expression g.xW 0 C b0/W C b is diﬀerentiable (although not
convex), making it possible to apply gradient-based techniques to the model training, learning
both the representation function and the linear classiﬁer on top of it at the same time. is is
the main idea behind deep learning and neural networks. In fact, Equation (3.1) describes a very
common neural network architecture called a multi-layer perceptron (MLP). Having established
the motivation, we now turn to describe multi-layer neural networks in more detail.

41
C H A P T E R
4
Feed-forward Neural Networks
4.1
A BRAIN-INSPIRED METAPHOR
As the name suggests, neural networks were inspired by the brain’s computation mechanism,
which consists of computation units called neurons. While the connections between artiﬁcial
neural networks and the brain are in fact rather slim, we repeat the metaphor here for complete-
ness. In the metaphor, a neuron is a computational unit that has scalar inputs and outputs. Each
input has an associated weight. e neuron multiplies each input by its weight, and then sums¹
them, applies a nonlinear function to the result, and passes it to its output. Figure 4.1 shows such
a neuron.
Output
Neuron
Input
y1
x1
x2
x3
x4
∫
Figure 4.1: A single neuron with four inputs.
e neurons are connected to each other, forming a network: the output of a neuron may
feed into the inputs of one or more neurons. Such networks were shown to be very capable com-
putational devices. If the weights are set correctly, a neural network with enough neurons and a
nonlinear activation function can approximate a very wide range of mathematical functions (we
will be more precise about this later).
A typical feed-forward neural network may be drawn as in Figure 4.2. Each circle is a
neuron, with incoming arrows being the neuron’s inputs and outgoing arrows being the neuron’s
outputs. Each arrow carries a weight, reﬂecting its importance (not shown). Neurons are arranged
in layers, reﬂecting the ﬂow of information. e bottom layer has no incoming arrows, and is
¹While summing is the most common operation, other functions, such as a max, are also possible.

42
4. FEED-FORWARD NEURAL NETWORKS
the input to the network. e top-most layer has no outgoing arrows, and is the output of the
network. e other layers are considered “hidden.” e sigmoid shape inside the neurons in the
middle layers represent a nonlinear function (i.e., the logistic function 1=.1 C e�x/) that is applied
to the neuron’s value before passing it to the output. In the ﬁgure, each neuron is connected to all
of the neurons in the next layer—this is called a fully connected layer or an aﬃne layer.
Output layer
Hidden layer
Hidden layer
Input layer
y2
y3
y1
x1
x2
x3
x4
∫
∫
∫
∫
∫
∫
∫
∫
∫
∫
∫
Figure 4.2: Feed-forward neural network with two hidden layers.
While the brain metaphor is sexy and intriguing, it is also distracting and cumbersome
to manipulate mathematically. We therefore switch back to using more concise mathematical
notation. As will soon become apparent, a feed-forward network as the one in Figure 4.2 is simply
a stack of linear models separated by nonlinear functions.
e values of each row of neurons in the network can be thought of as a vector. In Figure 4.2
the input layer is a 4-dimensional vector (x), and the layer above it is a 6-dimensional vector (h1).
e fully connected layer can be thought of as a linear transformation from 4 dimensions to 6
dimensions. A fully connected layer implements a vector-matrix multiplication, h D xW where
the weight of the connection from the ith neuron in the input row to the jth neuron in the output
row is W Œi;j �.² e values of h are then transformed by a nonlinear function g that is applied to
each value before being passed on as input to the next layer. e whole computation from input
to output can be written as: .g.xW 1//W 2 where W 1 are the weights of the ﬁrst layer and W 2
are the weights of the second one. Taking this view, the single neuron in Figure 4.1 is equivalent
to a logistic (log-linear) binary classiﬁer �.xw/ without a bias term .
²To see why this is the case, denote the weight of the ith input of the jth neuron in h as W Œi;j�. e value of hŒj� is then
hŒj� D P4
iD1 xŒi� � W Œi;j�.

4.2. IN MATHEMATICAL NOTATION
43
4.2
IN MATHEMATICAL NOTATION
From this point on, we will abandon the brain metaphor and describe networks exclusively in
terms of vector-matrix operations.
e simplest neural network is called a perceptron. It is simply a linear model:
NNPerceptron.x/ D xW C b
(4.1)
x 2 Rdin; W 2 Rdin�dout; b 2 Rdout;
where W is the weight matrix and b is a bias term.³ In order to go beyond linear functions, we
introduce a nonlinear hidden layer (the network in Figure 4.2 has two such layers), resulting in
the Multi Layer Perceptron with one hidden-layer (MLP1). A feed-forward neural network with
one hidden-layer has the form:
NNMLP1.x/ D g.xW 1 C b1/W 2 C b2
(4.2)
x 2 Rdin; W 1 2 Rdin�d1; b1 2 Rd1; W 2 2 Rd1�d2; b2 2 Rd2:
Here W 1 and b1 are a matrix and a bias term for the ﬁrst linear transformation of the input,
g is a nonlinear function that is applied element-wise (also called a nonlinearity or an activation
function), and W 2 and b2 are the matrix and bias term for a second linear transform.
Breaking it down, xW 1 C b1 is a linear transformation of the input x from din dimensions
to d1 dimensions. g is then applied to each of the d1 dimensions, and the matrix W 2 together
with bias vector b2 are then used to transform the result into the d2 dimensional output vector.
e nonlinear activation function g has a crucial role in the network’s ability to represent complex
functions. Without the nonlinearity in g, the neural network can only represent linear transfor-
mations of the input.⁴ Taking the view in Chapter 3, the ﬁrst layer transforms the data into a
good representation, while the second layer applies a linear classiﬁer to that representation.
We can add additional linear-transformations and nonlinearities, resulting in an MLP with
two hidden-layers (the network in Figure 4.2 is of this form):
NNMLP2.x/ D .g2.g1.xW 1 C b1/W 2 C b2//W 3:
(4.3)
It is perhaps clearer to write deeper networks like this using intermediary variables:
NNMLP2.x/ Dy
h1 Dg1.xW 1 C b1/
h2 Dg2.h1W 2 C b2/
y Dh2W 3:
(4.4)
³e network in Figure 4.2 does not include bias terms. A bias term can be added to a layer by adding to it an additional neuron
that does not have any incoming connections, whose value is always 1.
⁴To see why, consider that a sequence of linear transformations is still a linear transformation.

44
4. FEED-FORWARD NEURAL NETWORKS
e vector resulting from each linear transform is referred to as a layer. e outer-most
linear transform results in the output layer and the other linear transforms result in hidden layers.
Each hidden layer is followed by a nonlinear activation. In some cases, such as in the last layer of
our example, the bias vectors are forced to 0 (“dropped”).
Layers resulting from linear transformations are often referred to as fully connected, or aﬃne.
Other types of architectures exist. In particular, image recognition problems beneﬁt from convo-
lutional and pooling layers. Such layers have uses also in language processing, and will be discussed
in Chapter 13. Networks with several hidden layers are said to be deep networks, hence the name
deep learning.
When describing a neural network, one should specify the dimensions of the layers and the
input. A layer will expect a din dimensional vector as its input, and transform it into a dout dimen-
sional vector. e dimensionality of the layer is taken to be the dimensionality of its output. For
a fully connected layer l.x/ D xW C b with input dimensionality din and output dimensionality
dout, the dimensions of x is 1 � din, of W is din � dout and of b is 1 � dout.
Like the case with linear models, the output of a neural network is a dout dimensional vector.
In case dout D 1, the network’s output is a scalar. Such networks can be used for regression (or
scoring) by considering the value of the output, or for binary classiﬁcation by consulting the sign
of the output. Networks with dout D k > 1 can be used for k-class classiﬁcation, by associating
each dimension with a class, and looking for the dimension with maximal value. Similarly, if the
output vector entries are positive and sum to one, the output can be interpreted as a distribution
over class assignments (such output normalization is typically achieved by applying a softmax
transformation on the output layer, see Section 2.6).
e matrices and the bias terms that deﬁne the linear transformations are the parameters of
the network. Like in linear models, it is common to refer to the collection of all parameters as ‚.
Together with the input, the parameters determine the network’s output. e training algorithm
is responsible for setting their values such that the network’s predictions are correct. Unlike linear
models, the loss function of multi-layer neural networks with respect to their parameters is not
convex,⁵ making search for the optimal parameter values intractable. Still, the gradient-based
optimization methods discussed in Section 2.8 can be applied, and perform very well in practice.
Training neural networks is discussed in detail in Chapter 5.
4.3
REPRESENTATION POWER
In terms of representation power, it was shown by Hornik et al. [1989] and Cybenko [1989] that
MLP1 is a universal approximator—it can approximate with any desired non-zero amount of er-
ror a family of functions that includes all continuous functions on a closed and bounded subset
of Rn, and any function mapping from any ﬁnite dimensional discrete space to another.⁶ is
⁵Strictly convex functions have a single optimal solution, making them easy to optimize using gradient-based methods.
⁶Speciﬁcally, a feed-forward network with linear output layer and at least one hidden layer with a “squashing” activation function
can approximate any Borel measurable function from one ﬁnite dimensional space to another. e proof was later extended
by Leshno et al. [1993] to a wider range of activation functions, including the ReLU function g.x/ D max.0; x/.

4.4. COMMON NONLINEARITIES
45
may suggest there is no reason to go beyond MLP1 to more complex architectures. However, the
theoretical result does not discuss the learnability of the neural network (it states that a represen-
tation exists, but does not say how easy or hard it is to set the parameters based on training data
and a speciﬁc learning algorithm). It also does not guarantee that a training algorithm will ﬁnd
the correct function generating our training data. Finally, it does not state how large the hidden
layer should be. Indeed, Telgarsky [2016] show that there exist neural networks with many layers
of bounded size that cannot be approximated by networks with fewer layers unless these layers
are exponentially large.
In practice, we train neural networks on relatively small amounts of data using local search
methods such as variants of stochastic gradient descent, and use hidden layers of relatively mod-
est sizes (up to several thousands). As the universal approximation theorem does not give any
guarantees under these non-ideal, real-world conditions, there is deﬁnitely beneﬁt to be had in
trying out more complex architectures than MLP1. In many cases, however, MLP1 does indeed
provide strong results. For further discussion on the representation power of feed-forward neural
networks, see Bengio et al. [2016, Section 6.5].
4.4
COMMON NONLINEARITIES
e nonlinearity g can take many forms. ere is currently no good theory as to which nonlin-
earity to apply in which conditions, and choosing the correct nonlinearity for a given task is for
the most part an empirical question. I will now go over the common nonlinearities from the liter-
ature: the sigmoid, tanh, hard tanh and the rectiﬁed linear unit (ReLU). Some NLP researchers
also experimented with other forms of nonlinearities such as cube and tanh-cube.
Sigmoid
e sigmoid activation function �.x/ D 1=.1 C e�x/, also called the logistic function,
is an S-shaped function, transforming each value x into the range Œ0; 1�. e sigmoid was the
canonical nonlinearity for neural networks since their inception, but is currently considered to be
deprecated for use in internal layers of neural networks, as the choices listed below prove to work
much better empirically.
Hyperbolic tangent (tanh)
e hyperbolic tangent tanh.x/ D e2x�1
e2xC1 activation function is an
S-shaped function, transforming the values x into the range Œ�1; 1�.
Hard tanh
e hard-tanh activation function is an approximation of the tanh function which
is faster to compute and to ﬁnd derivatives thereof:
hardtanh.x/ D
8
ˆ<
ˆ:
�1
x < �1
1
x > 1
x
otherwise:
(4.5)
Rectiﬁer (ReLU)
e rectiﬁer activation function [Glorot et al., 2011], also known as the recti-
ﬁed linear unit is a very simple activation function that is easy to work with and was shown many

46
4. FEED-FORWARD NEURAL NETWORKS
times to produce excellent results.⁷ e ReLU unit clips each value x < 0 at 0. Despite its sim-
plicity, it performs well for many tasks, especially when combined with the dropout regularization
technique (see Section 4.6):
ReLU.x/ D max.0; x/ D
(
0
x < 0
x
otherwise:
(4.6)
As a rule of thumb, both ReLU and tanh units work well, and signiﬁcantly outperform the
sigmoid. You may want to experiment with both tanh and ReLU activations, as each one may
perform better in diﬀerent settings.
Figure 4.3 shows the shapes of the diﬀerent activations functions, together with the shapes
of their derivatives.
1.0
0.5
0.0
-0.5
-1.0-6 -4 -2
0
2
4
6
1.0
0.5
0.0
-0.5
-1.0-6 -4 -2
0
2
4
6
1.0
0.5
0.0
-0.5
-1.0-6 -4 -2
0
2
4
6
-6 -4 -2
0
2
4
6
-6 -4 -2
0
2
4
6
1.0
0.5
0.0
-0.5
-1.0
1.0
0.5
0.0
-0.5
-1.0
1.0
0.5
0.0
-0.5
-1.0
1.0
0.5
0.0
-0.5
-1.0
-6 -4 -2
0
2
4
6
-6 -4 -2
0
2
4
6
1.0
0.5
0.0
-0.5
-1.0-6 -4 -2
0
2
4
6
sigmoid(x)
tanh(x)
hardtanh(x)
ReLU(x)
�f
�x
�f
�x
�f
�x
�f
�x
Figure 4.3: Activation functions (top) and their derivatives (bottom).
4.5
LOSS FUNCTIONS
When training a neural network (more on training in Chapter 5), much like when training a
linear classiﬁer, one deﬁnes a loss function L. Oy; y/, stating the loss of predicting Oy when the
true output is y. e training objective is then to minimize the loss across the diﬀerent training
examples. e loss L. Oy; y/ assigns a numerical score (a scalar) to the network’s output Oy given
the true expected output y. e loss functions discussed for linear models in Section 2.7.1 are
relevant and widely used also for neural networks. For further discussion on loss functions in the
⁷e technical advantages of the ReLU over the sigmoid and tanh activation functions is that it does not involve expensive-
to-compute functions, and more importantly that it does not saturate. e sigmoid and tanh activation are capped at 1, and
the gradients at this region of the functions are near zero, driving the entire gradient near zero. e ReLU activation does
not have this problem, making it especially suitable for networks with multiple layers, which are susceptible to the vanishing
gradients problem when trained with the saturating units.

[Image: extracted_image_63_0.png]
[Image: extracted_image_63_1.png]
[Image: extracted_image_63_2.png]
4.6. REGULARIZATION AND DROPOUT
47
context of neural networks, see LeCun and Huang [2005], LeCun et al. [2006] and Bengio et al.
[2016].
4.6
REGULARIZATION AND DROPOUT
Multi-layer networks can be large and have many parameters, making them especially prone to
overﬁtting. Model regularization is just as important in deep neural networks as it is in linear
models, and perhaps even more so. e regularizers discussed in Section 2.7.2, namely L2, L1
and the elastic-net, are also relevant for neural networks. In particular, L2 regularization, also
called weight decay is eﬀective for achieving good generalization performance in many cases, and
tuning the regularization strength � is advisable.
Another eﬀective technique for preventing neural networks from overﬁtting the training
data is dropout training [Hinton et al., 2012, Srivastava et al., 2014]. e dropout method is
designed to prevent the network from learning to rely on speciﬁc weights. It works by randomly
dropping (setting to 0) half of the neurons in the network (or in a speciﬁc layer) in each training
example in the stochastic-gradient training. For example, consider the multi-layer perceptron
with two hidden layers (MLP2):
NNMLP2.x/ Dy
h1 Dg1.xW 1 C b1/
h2 Dg2.h1W 2 C b2/
y Dh2W 3:
When applying dropout training to MLP2, we randomly set some of the values of h1 and
h2 to 0 at each training round:
NNMLP2.x/ Dy
h1 Dg1.xW 1 C b1/
m1 �Bernouli.r1/
Qh1 Dm1 ˇ h1
h2 Dg2. Qh1W 2 C b2/
m2 �Bernouli.r2/
Qh2 Dm2 ˇ h2
y D Qh2W 3:
(4.7)
Here, m1 and m2 are random masking vectors with the dimensions of h1 and h2, respectively,
and ˇ is the element-wise multiplication operation. e values of the elements in the masking

48
4. FEED-FORWARD NEURAL NETWORKS
vectors are either 0 or 1, and are drawn from a Bernouli distribution with parameter r (usually
r D 0:5). e values corresponding to zeros in the masking vectors are then zeroed out, replacing
the hidden layers h with Qh before passing them on to the next layer.
Work by Wager et al. [2013] establishes a strong connection between the dropout method
and L2 regularization. Another view links dropout to model averaging and ensemble techniques
[Srivastava et al., 2014].
e dropout technique is one of the key factors contributing to very strong results of neural-
network methods on image classiﬁcation tasks [Krizhevsky et al., 2012], especially when com-
bined with ReLU activation units [Dahl et al., 2013]. e dropout technique is eﬀective also in
NLP applications of neural networks.
4.7
SIMILARITY AND DISTANCE LAYERS
We sometimes wish to calculate a scalar value based on two vectors, such that the value reﬂects
the similarity, compatibility or distance between the two vectors. For example, vectors v1 2 Rd
and v2 2 Rd may be the output layers of two MLPs, and we would like to train the network to
produce similar vectors for some training examples, and dissimilar vectors for others.
In what follows we describe common functions that take two vectors u 2 Rd and v 2 Rd,
and return a scalar. ese functions can (and often are) integrated in feed-forward neural net-
works.
Dot Product
A very common options is to use the dot-product:
simdot.u; v/ Du � v D
d
X
iD1
uŒi�vŒi�
(4.8)
Euclidean Distance
Another popular options is the Euclidean Distance:
disteuclidean.u; v/ D
v
u
u
t
d
X
iD1
.uŒi� � vŒi�/2 D
p
.u � v/ � .u � v/ D jju � vjj2
(4.9)
Note that this is a distance metric and not a similarity: here, small (near zero) values indicate
similar vectors and large values dissimilar ones. e square-root is often omitted.
Trainable Forms
e dot-product and the euclidean distance above are ﬁxed functions. We
sometimes want to use a parameterized function, that can be trained to produce desired similarity
(or dissimilarity) values by focusing on speciﬁc dimensions of the vectors. A common trainable
similarity function is the bilinear form:

4.8. EMBEDDING LAYERS
49
simbilinear.u; v/ D uMv
(4.10)
M 2 Rd�d
where the matrix M is a parameter that needs to be trained.
Similarly, for a trainable distance function we can use:
dist.u; v/ D .u � v/M.u � v/
(4.11)
Finally, a multi-layer perceptron with a single output neuron can also be used for producing
a scalar from two vectors, by feeding it the concatenation of the two vectors.
4.8
EMBEDDING LAYERS
As will be further discussed in Chapter 8, when the input to the neural network contains sym-
bolic categorical features (e.g., features that take one of k distinct symbols, such as words from
a closed vocabulary), it is common to associate each possible feature value (i.e., each word in
the vocabulary) with a d-dimensional vector for some d. ese vectors are then considered pa-
rameters of the model, and are trained jointly with the other parameters. e mapping from a
symbolic feature values such as “word number 1249” to d-dimensional vectors is performed by
an embedding layer (also called a lookup layer). e parameters in an embedding layer are simply
a matrix E 2 Rjvocabj�d where each row corresponds to a diﬀerent word in the vocabulary. e
lookup operation is then simply indexing: v1249 D E Œ1249;W�. If the symbolic feature is encoded as
a one-hot vector x, the lookup operation can be implemented as the multiplication xE.
e word vectors are often concatenated to each other before being passed on to the next
layer. Embeddings are discussed in more depth in Chapter 8 when discussing dense representa-
tions of categorical features, and in Chapter 10 when discussing pre-trained word representations.

51
C H A P T E R
5
Neural Network Training
Similar to linear models, neural network are diﬀerentiable parameterized functions, and are
trained using gradient-based optimization (see Section 2.8). e objective function for nonlinear
neural networks is not convex, and gradient-based methods may get stuck in a local minima. Still,
gradient-based methods produce good results in practice.
Gradient calculation is central to the approach. e mathematics of gradient computation
for neural networks are the same as those of linear models, simply following the chain-rule of
diﬀerentiation. However, for complex networks this process can be laborious and error-prone.
Fortunately, gradients can be eﬃciently and automatically computed using the backpropagation
algorithm [LeCun et al., 1998b, Rumelhart et al., 1986]. e backpropagation algorithm is a
fancy name for methodically computing the derivatives of a complex expression using the chain-
rule, while caching intermediary results. More generally, the backpropagation algorithm is a spe-
cial case of the reverse-mode automatic diﬀerentiation algorithm [Neidinger, 2010, Section 7],
[Baydin et al., 2015, Bengio, 2012]. e following section describes reverse mode automatic dif-
ferentiation in the context of the computation graph abstraction. e rest of the chapter is devoted
to practical tips for training neural networks in practice.
5.1
THE COMPUTATION GRAPH ABSTRACTION
While one can compute the gradients of the various parameters of a network by hand and im-
plement them in code, this procedure is cumbersome and error prone. For most purposes, it is
preferable to use automatic tools for gradient computation [Bengio, 2012]. e computation-
graph abstraction allows us to easily construct arbitrary networks, evaluate their predictions for
given inputs (forward pass), and compute gradients for their parameters with respect to arbitrary
scalar losses (backward pass).
A computation graph is a representation of an arbitrary mathematical computation as
a graph. It is a directed acyclic graph (DAG) in which nodes correspond to mathematical
operations or (bound) variables and edges correspond to the ﬂow of intermediary values between
the nodes. e graph structure deﬁnes the order of the computation in terms of the dependencies
between the diﬀerent components. e graph is a DAG and not a tree, as the result of one
operation can be the input of several continuations. Consider for example a graph for the
computation of .a � b C 1/ � .a � b C 2/:

52
5. NEURAL NETWORK TRAINING
1
a
b
2
*
*
+
+
e computation of a � b is shared. We restrict ourselves to the case where the computation graph
is connected (in a disconnected graph, each connected component is an independent function that
can be evaluated and diﬀerentiated independently of the other connected components).
1 × 17
1 × 17
1 × 17
1 × 17
20 × 17
150 × 20
1 × 20
1 × 17
20 × 17
150 × 20
1 × 20
1 × 17
20 × 17
150 × 20
1 × 20
1 × 20
1 × 20
1 × 20
1 × 150
x
1 × 17
1 × 17
1 × 17
1 × 20
1 × 20
1 × 20
1 × 150
1 × 17
1 × 1
1 × 1
1 × 1
1 × 17
1 × 17
1 × 20
1 × 20
1 × 20
1 × 150
1 × 50
1 × 50
1 × 50
1 × 50
1 × 50
1 × 50
|V| × 50
“the”
“black”
“dog”
|V| × 50
“the”
“black”
“dog”
5
(a)
(b)
(c)
softmax
ADD
ADD
MUL
W 2
b2
W 1
b1
W 2
b2
W 1
E
E
b1
W 2
b2
W 1
b1
MUL
tanh
concat
concat
softmax
ADD
ADD
MUL
MUL
tanh
softmax
pick
log
neg
ADD
ADD
MUL
MUL
tanh
lookup
lookup
lookup
lookup
lookup
lookup
Figure 5.1: (a) Graph with unbound input. (b) Graph with concrete input. (c) Graph with concrete
input, expected output, and a ﬁnal loss node.
Since a neural network is essentially a mathematical expression, it can be represented as a
computation graph. For example, Figure 5.1a presents the computation graph for an MLP with
one hidden-layer and a softmax output transformation. In our notation, oval nodes represent

5.1. THE COMPUTATION GRAPH ABSTRACTION
53
mathematical operations or functions, and shaded rectangle nodes represent parameters (bound
variables). Network inputs are treated as constants, and drawn without a surrounding node. Input
and parameter nodes have no incoming arcs, and output nodes have no outgoing arcs. e output
of each node is a matrix, the dimensionality of which is indicated above the node.
is graph is incomplete: without specifying the inputs, we cannot compute an output.
Figure 5.1b shows a complete graph for an MLP that takes three words as inputs, and predicts the
distribution over part-of-speech tags for the third word. is graph can be used for prediction, but
not for training, as the output is a vector (not a scalar) and the graph does not take into account
the correct answer or the loss term. Finally, the graph in Figure 5.1c shows the computation
graph for a speciﬁc training example, in which the inputs are the (embeddings of) the words
“the,” “black,” “dog,” and the expected output is “NOUN” (whose index is 5). e pick node
implements an indexing operation, receiving a vector and an index (in this case, 5) and returning
the corresponding entry in the vector.
Once the graph is built, it is straightforward to run either a forward computation (com-
pute the result of the computation) or a backward computation (computing the gradients), as we
show below. Constructing the graphs may look daunting, but is actually very easy using dedicated
software libraries and APIs.
5.1.1
FORWARD COMPUTATION
e forward pass computes the outputs of the nodes in the graph. Since each node’s output de-
pends only on itself and on its incoming edges, it is trivial to compute the outputs of all nodes
by traversing the nodes in a topological order and computing the output of each node given the
already computed outputs of its predecessors.
More formally, in a graph of N nodes, we associate each node with an index i according
to their topological ordering. Let fi be the function computed by node i (e.g., multiplication.
addition, etc.). Let �.i/ be the parent nodes of node i, and ��1.i/ D fj j i 2 �.j/g the children
nodes of node i (these are the arguments of fi). Denote by v.i/ the output of node i, that is, the
application of fi to the output values of its arguments ��1.i/. For variable and input nodes, fi
is a constant function and ��1.i/ is empty. e computation-graph forward pass computes the
values v.i/ for all i 2 Œ1; N �.
Algorithm 5.3 Computation graph forward pass.
1: for i = 1 to N do
2:
Let a1; : : : ; am D ��1.i/
3:
v.i/  fi.v.a1/; : : : ; v.am//

54
5. NEURAL NETWORK TRAINING
5.1.2
BACKWARD COMPUTATION (DERIVATIVES, BACKPROP)
e backward pass begins by designating a node N with scalar (1 � 1) output as a loss-node,
and running forward computation up to that node. e backward computation computes the
gradients of the parameters with respect to that node’s value. Denote by d.i/ the quantity @N
@i .
e backpropagation algorithm is used to compute the values d.i/ for all nodes i.
e backward pass ﬁlls a table of values d.1/; : : : ; d.N / as in Algorithm 5.4.
Algorithm 5.4 Computation graph backward pass (backpropagation).
1: d.N /  1
F @N
@N D 1
2: for i = N-1 to 1 do
3:
d.i/  P
j 2�.i/ d.j/ � @fj
@i
F @N
@i D
X
j 2�.i/
@N
@j
@j
@i
e backpropagation algorithm (Algorithm 5.4) is essentially following the chain-rule of diﬀer-
entiation. e quantity @fj
@i is the partial derivative of fj .��1.j // w.r.t the argument i 2 ��1.j /.
is value depends on the function fj and the values v.a1/; : : : ; v.am/ (where a1; : : : ; am D
��1.j/) of its arguments, which were computed in the forward pass.
us, in order to deﬁne a new kind of node, one needs to deﬁne two methods: one for
calculating the forward value v.i/ based on the node’s inputs, and the another for calculating @fi
@x
for each x 2 ��1.i/.
..
Derivatives of “non-mathematical” functions
While deﬁning @fi
@x for mathematical func-
tions such is as log or C is straightforward, some ﬁnd it challenging to think about the
derivative of operations as as pick.x; 5/ that selects the ﬁfth element of a vector. e answer
is to think in terms of the contribution to the computation. After picking the ith element of
a vector, only that element participates in the remainder of the computation. us, the gradi-
ent of pick.x; 5/ is a vector g with the dimensionality of x where gŒ5� D 1 and gŒi¤5� D 0.
Similarly, for the function max.0; x/ the value of the gradient is 1 for x > 0 and 0 otherwise.
For further information on automatic diﬀerentiation, see Neidinger [2010, Section 7] and
Baydin et al. [2015]. For more in depth discussion of the backpropagation algorithm and compu-
tation graphs (also called ﬂow graphs), see Bengio et al. [2016, Section 6.5] and Bengio [2012],
LeCun et al. [1998b]. For a popular yet technical presentation, see Chris Olah’s description at
http://colah.github.io/posts/2015-08-Backprop/.

5.1. THE COMPUTATION GRAPH ABSTRACTION
55
5.1.3
SOFTWARE
Several software packages implement the computation-graph model, including eano,¹
[Bergstra et al., 2010], TensorFlow² [Abadi et al., 2015], Chainer,³ and DyNet⁴ [Neubig et al.,
2017]. All these packages support all the essential components (node types) for deﬁning a wide
range of neural network architectures, covering the structures described in this book and more.
Graph creation is made almost transparent by use of operator overloading. e framework de-
ﬁnes a type for representing graph nodes (commonly called expressions), methods for constructing
nodes for inputs and parameters, and a set of functions and mathematical operations that take
expressions as input and result in more complex expressions. For example, the python code for
creating the computation graph from Figure 5.1c using the DyNet framework is:
import dynet as dy
# model
i n i t i a l i z a t i o n .
model = dy . Model()
mW1 = model . add_parameters ((20 ,150) )
mb1 = model . add_parameters (20)
mW2 = model . add_parameters ((17 ,20) )
mb2 = model . add_parameters (17)
lookup = model . add_lookup_parameters ((100 , 50) )
trainer = dy . SimpleSGDTrainer(model)
def get_index (x) :
pass # Logic omitted .
Maps words to numeric IDs .
# The following
builds and executes the computation graph ,
# and updates model parameters .
# Only one data point
i s shown ,
in
practice
the
following
# should run in a data - feeding
loop .
# Building the computation graph :
dy . renew_cg () # create a new graph .
# Wrap the model parameters as graph - nodes .
W1 = dy . parameter (mW1)
b1 = dy . parameter (mb1)
W2 = dy . parameter (mW2)
b2 = dy . parameter (mb2)
# Generate the embeddings layer .
vthe
= dy . lookup [ get_index (”the”) ]
vblack = dy . lookup [ get_index (”black”) ]
vdog
= dy . lookup [ get_index (”dog”) ]
# Connect the
l e a f
nodes into a complete graph .
x = dy . concatenate ( [ vthe ,
vblack , vdog ] )
output = dy . softmax (W2*(dy . tanh(W1*x+b1) )+b2)
loss = -dy . log (dy . pick ( output ,
5) )
¹http://deeplearning.net/software/theano/
²https://www.tensorflow.org/
³http://chainer.org
⁴https://github.com/clab/dynet

56
5. NEURAL NETWORK TRAINING
loss_value = loss . forward ()
loss . backward () # the gradient
i s computed
# and stored
in the corresponding
# parameters .
trainer . update () # update the parameters according to the gradients .
Most of the code involves various initializations: the ﬁrst block deﬁnes model parameters that are
be shared between diﬀerent computation graphs (recall that each graph corresponds to a speciﬁc
training example). e second block turns the model parameters into the graph-node (Expression)
types. e third block retrieves the Expressions for the embeddings of the input words. Finally,
the fourth block is where the graph is created. Note how transparent the graph creation is—
there is an almost a one-to-one correspondence between creating the graph and describing it
mathematically. e last block shows a forward and backward pass. e equivalent code in the
TensorFlow package is:⁵
import
tensorflow as
t f
W1 = t f . get_variable (”W1” ,
[20 ,
150])
b1 = t f . get_variable (”b1” ,
[ 2 0 ] )
W2 = t f . get_variable (”W2” ,
[17 ,
20])
b2 = t f . get_variable (”b2” ,
[ 1 7 ] )
lookup = t f . get_variable (”W” ,
[100 ,
50])
def get_index (x) :
pass # Logic omitted
p1 = t f . placeholder ( t f . int32 ,
[ ] )
p2 = t f . placeholder ( t f . int32 ,
[ ] )
p3 = t f . placeholder ( t f . int32 ,
[ ] )
target = t f . placeholder ( t f . int32 ,
[ ] )
v_w1 = t f . nn . embedding_lookup( lookup , p1)
v_w2 = t f . nn . embedding_lookup( lookup , p2)
v_w3 = t f . nn . embedding_lookup( lookup , p3)
x = t f . concat ( [v_w1, v_w2, v_w3] ,
0)
output = t f . nn . softmax (
t f . einsum(” ij , j -> i ” , W2,
t f . tanh(
t f . einsum(” ij , j -> i ” , W1, x) + b1) ) + b2)
loss = - t f . log ( output [ target ] )
trainer = t f . train . GradientDescentOptimizer (0.1) . minimize ( loss )
# Graph definition done ,
compile
i t and feed
concrete data .
# Only one data - point
i s shown ,
in
practice we will
use
# a data - feeding
loop .
with
t f . Session ()
as
sess :
sess . run( t f . global_variables_initializer () )
feed_dict = {
p1 :
get_index (”the”) ,
p2 :
get_index (”black”) ,
p3 :
get_index (”dog”) ,
⁵TensorFlow code provided by Tim Rocktäschel. anks Tim!

5.1. THE COMPUTATION GRAPH ABSTRACTION
57
target : 5
}
loss_value = sess . run( loss ,
feed_dict )
# update , no c a l l
of backward necessary
sess . run( trainer ,
feed_dict )
e main diﬀerence between DyNet (and Chainer) to TensorFlow (and eano) is that the
formers use dynamic graph construction while the latters use static graph construction. In dynamic
graph construction, a diﬀerent computation graph is created from scratch for each training sam-
ple, using code in the host language. Forward and backward propagation are then applied to this
graph. In contrast, in the static graph construction approach, the shape of the computation graph
is deﬁned once in the beginning of the computation, using an API for specifying graph shapes,
with place-holder variables indicating input and output values. en, an optimizing graph com-
piler produces an optimized computation graph, and each training example is fed into the (same)
optimized graph. e graph compilation step in the static toolkits (TensorFlow and eano) is
both a blessing and a curse. On the one hand, once compiled, large graphs can be run eﬃciently
on either the CPU or a GPU, making it ideal for large graphs with a ﬁxed structure, where only
the inputs change between instances. However, the compilation step itself can be costly, and it
makes the interface more cumbersome to work with. In contrast, the dynamic packages focus
on building large and dynamic computation graphs and executing them “on the ﬂy” without a
compilation step. While the execution speed may suﬀer compared to the static toolkits, in prac-
tice the computation speeds of the dynamic toolkits are very competitive. e dynamic packages
are especially convenient when working with the recurrent and recursive networks described in
Chapters 14 and 18 as well as in structured prediction settings as described in Chapter 19, in
which the graphs of diﬀerent data-points have diﬀerent shapes. See Neubig et al. [2017] for
further discussion on the dynamic-vs.-static approaches, and speed benchmarks for the diﬀerent
toolkits. Finally, packages such as Keras⁶ provide a higher level interface on top of packages such
as eano and TensorFlow, allowing the deﬁnition and training of complex neural networks with
even fewer lines of code, provided that the architectures are well established, and hence supported
in the higher-level interface.
5.1.4
IMPLEMENTATION RECIPE
Using the computation graph abstraction and dynamic graph construction, the pseudo-code for
a network training algorithm is given in Algorithm 5.5.
Here, build_computation_graph is a user-deﬁned function that builds the computation
graph for the given input, output, and network structure, returning a single loss node. up-
date_parameters is an optimizer speciﬁc update rule. e recipe speciﬁes that a new graph is cre-
ated for each training example. is accommodates cases in which the network structure varies
between training examples, such as recurrent and recursive neural networks, to be discussed in
⁶https://keras.io

58
5. NEURAL NETWORK TRAINING
Algorithm 5.5 Neural network training with computation graph abstraction (using minibatches
of size 1).
1: Deﬁne network parameters.
2: for iteration = 1 to T do
3:
for Training example xi; yi in dataset do
4:
loss_node  build_computation_graph(xi, yi, parameters)
5:
loss_node.forward()
6:
gradients  loss_node().backward()
7:
parameters  update_parameters(parameters, gradients)
8: return parameters.
Chapters 14–18. For networks with ﬁxed structures, such as an MLPs, it may be more eﬃcient
to create one base computation graph and vary only the inputs and expected outputs between
examples.
5.1.5
NETWORK COMPOSITION
As long as the network’s output is a vector (1 � k matrix), it is trivial to compose networks by
making the output of one network the input of another, creating arbitrary networks. e compu-
tation graph abstractions makes this ability explicit: a node in the computation graph can itself
be a computation graph with a designated output node. One can then design arbitrarily deep and
complex networks, and be able to easily evaluate and train them thanks to automatic forward and
gradient computation. is makes it easy to deﬁne and train elaborate recurrent and recursive
networks, as discussed in Chapters 14–16 and 18, as well as networks for structured outputs and
multi-objective training, as we discuss in Chapters 19 and 20.
5.2
PRACTICALITIES
Once the gradient computation is taken care of, the network is trained using SGD or another
gradient-based optimization algorithm. e function being optimized is not convex, and for a
long time training of neural networks was considered a “black art” which can only be done by
selected few. Indeed, many parameters aﬀect the optimization process, and care has to be taken
to tune these parameters. While this book is not intended as a comprehensive guide to successfully
training neural networks, we do list here a few of the prominent issues. For further discussion on
optimization techniques and algorithms for neural networks, refer to Bengio et al. [2016, Chapter
8]. For some theoretical discussion and analysis, refer to Glorot and Bengio [2010]. For various
practical tips and recommendations, see Bottou [2012], LeCun et al. [1998a].

5.2. PRACTICALITIES
59
5.2.1
CHOICE OF OPTIMIZATION ALGORITHM
While the SGD algorithm works well, it may be slow to converge. Section 2.8.3 lists some alter-
native, more advanced stochastic-gradient algorithms. As most neural network software frame-
works provide implementations of these algorithms, it is easy and often worthwhile to try out
diﬀerent variants. In my research group, we found that when training larger networks, using the
Adam algorithm [Kingma and Ba, 2014] is very eﬀective and relatively robust to the choice of
the learning rate.
5.2.2
INITIALIZATION
e non-convexity of the objective function means the optimization procedure may get stuck in
a local minimum or a saddle point, and that starting from diﬀerent initial points (e.g., diﬀerent
random values for the parameters) may result in diﬀerent results. us, it is advised to run several
restarts of the training starting at diﬀerent random initializations, and choosing the best one
based on a development set.⁷ e amount of variance in the results due to diﬀerent random seed
selections is diﬀerent for diﬀerent network formulations and datasets, and cannot be predicted in
advance.
e magnitude of the random values has a very important eﬀect on the success of training.
An eﬀective scheme due to Glorot and Bengio [2010], called xavier initialization after Glorot’s
ﬁrst name, suggests initializing a weight matrix W 2 Rdin�dout as:
W � U
"
�
p
6
pdin C dout
; C
p
6
pdin C dout
#
;
(5.1)
where U Œa; b� is a uniformly sampled random value in the range Œa; b�. e suggestion is based
on properties of the tanh activation function, works well in many situations, and is the preferred
default initialization method by many.
Analysis by He et al. [2015] suggests that when using ReLU nonlinearities, the weights
should be initialized by sampling from a zero-mean Gaussian distribution whose standard devi-
ation is
q
2
din . is initialization was found by He et al. [2015] to work better than xavier initial-
ization in an image classiﬁcation task, especially when deep networks were involved.
5.2.3
RESTARTS AND ENSEMBLES
When training complex networks, diﬀerent random initializations are likely to end up with dif-
ferent ﬁnal solutions, exhibiting diﬀerent accuracies. us, if your computational resources allow,
it is advisable to run the training process several times, each with a diﬀerent random initializa-
tion, and choose the best one on the development set. is technique is called random restarts.
e average model accuracy across random seeds is also interesting, as it gives a hint as to the
stability of the process.
⁷When debugging, and for reproducibility of results, it is advised to used a ﬁxed random seed.

60
5. NEURAL NETWORK TRAINING
While the need to “tune” the random seed used to initialize models can be annoying, it also
provides a simple way to get diﬀerent models for performing the same task, facilitating the use
model ensembles. Once several models are available, one can base the prediction on the ensemble
of models rather than on a single one (for example by taking the majority vote across the diﬀerent
models, or by averaging their output vectors and considering the result as the output vector of the
ensembled model). Using ensembles often increases the prediction accuracy, at the cost of having
to run the prediction step several times (once for each model).
5.2.4
VANISHING AND EXPLODING GRADIENTS
In deep networks, it is common for the error gradients to either vanish (become exceedingly close
to 0) or explode (become exceedingly high) as they propagate back through the computation
graph. e problem becomes more severe in deeper networks, and especially so in recursive and
recurrent networks [Pascanu et al., 2012]. Dealing with the vanishing gradients problem is still
an open research question. Solutions include making the networks shallower, step-wise training
(ﬁrst train the ﬁrst layers based on some auxiliary output signal, then ﬁx them and train the upper
layers of the complete network based on the real task signal), performing batch-normalization
[Ioﬀe and Szegedy, 2015] (for every minibatch, normalizing the inputs to each of the network
layers to have zero mean and unit variance) or using specialized architectures that are designed to
assist in gradient ﬂow (e.g., the LSTM and GRU architectures for recurrent networks, discussed
in Chapter 15). Dealing with the exploding gradients has a simple but very eﬀective solution: clip-
ping the gradients if their norm exceeds a given threshold. Let Og be the gradients of all parameters
in the network, and k Ogk be their L2 norm. Pascanu et al. [2012] suggest to set: Og  threshold
k Ogk
Og if
k Ogk > threshold.
5.2.5
SATURATION AND DEAD NEURONS
Layers with tanh and sigmoid activations can become saturated—resulting in output values for
that layer that are all close to one, the upper-limit of the activation function. Saturated neurons
have very small gradients, and should be avoided. Layers with the ReLU activation cannot be
saturated, but can “die”—most or all values are negative and thus clipped at zero for all inputs,
resulting in a gradient of zero for that layer. If your network does not train well, it is advisable to
monitor the network for layers with many saturated or dead neurons. Saturated neurons are caused
by too large values entering the layer. is may be controlled for by changing the initialization,
scaling the range of the input values, or changing the learning rate. Dead neurons are caused by
all signals entering the layer being negative (for example this can happen after a large gradient
update). Reducing the learning rate will help in this situation. For saturated layers, another option
is to normalize the values in the saturated layer after the activation, i.e., instead of g.h/ D tanh.h/
using g.h/ D
tanh.h/
k tanh.h/k. Layer normalization is an eﬀective measure for countering saturation, but
is also expensive in terms of gradient computation. A related technique is batch normalization, due

5.2. PRACTICALITIES
61
to Ioﬀe and Szegedy [2015], in which the activations at each layer are normalized so that they
have mean 0 and variance 1 across each mini-batch. e batch-normalization techniques became
a key component for eﬀective training of deep networks in computer vision. As of this writing, it
is less popular in natural language applications.
5.2.6
SHUFFLING
e order in which the training examples are presented to the network is important. e SGD
formulation above speciﬁes selecting a random example in each turn. In practice, most implemen-
tations go over the training example in random order, essentially performing random sampling
without replacement. It is advised to shuﬄe the training examples before each pass through the
data.
5.2.7
LEARNING RATE
Selection of the learning rate is important. Too large learning rates will prevent the network from
converging on an eﬀective solution. Too small learning rates will take a very long time to converge.
As a rule of thumb, one should experiment with a range of initial learning rates in range Œ0; 1�,
e.g., 0:001, 0:01, 0:1, 1. Monitor the network’s loss over time, and decrease the learning rate once
the loss stops improving on a held-out development set. Learning rate scheduling decreases the
rate as a function of the number of observed minibatches. A common schedule is dividing the
initial learning rate by the iteration number. Léon Bottou [2012] recommends using a learning
rate of the form �t D �0.1 C �0�t/�1 where �0 is the initial learning rate, �t is the learning rate to
use on the tth training example, and � is an additional hyperparameter. He further recommends
determining a good value of �0 based on a small sample of the data prior to running on the entire
dataset.
5.2.8
MINIBATCHES
Parameter updates occur either every training example (minibatches of size 1) or every k train-
ing examples. Some problems beneﬁt from training with larger minibatch sizes. In terms of the
computation graph abstraction, one can create a computation graph for each of the k training
examples, and then connecting the k loss nodes under an averaging node, whose output will be
the loss of the minibatch. Large minibatched training can also be beneﬁcial in terms of computa-
tion eﬃciency on specialized computing architectures such as GPUs, and replacing vector-matrix
operations by matrix-matrix operations. is is beyond the scope of this book.

PART II
Working with Natural Language Data

65
C H A P T E R
6
Features for Textual Data
In the previous chapters we discussed the general learning problem, and saw some machine learn-
ing models and algorithms for training them. All of these models take as input vectors x and
produce predictions. Up until now we assumed the vectors x are given. In language processing,
the vectors x are derived from textual data, in order to reﬂect various linguistic properties of the
text. e mapping from textual data to real valued vectors is called feature extraction or feature
representation, and is done by a feature function. Deciding on the right features is an integral part
of a successful machine learning project. While deep neural networks alleviate a lot of the need in
feature engineering, a good set of core features still needs to be deﬁned. is is especially true for
language data, which comes in the form of a sequence of discrete symbols. is sequence needs
to be converted somehow to a numerical vector, in a non-obvious way.
We now diverge from the training machinery in order to discuss the feature functions that
are used for language data, which will be the topic of the next few chapters.
is chapter provides an overview of the common kinds of information sources that are
available for use as features when dealing with textual language data. Chapter 7 discusses feature
choices for some concrete NLP problems. Chapter 8 deals with encoding the features as input
vectors that can be fed to a neural network.
6.1
TYPOLOGY OF NLP CLASSIFICATION PROBLEMS
Generally speaking, classiﬁcation problems in natural language can be categorized into several
broad categories, depending on the item being classiﬁed (some problems in natural language
processing do not fall neatly into the classiﬁcation framework. For example, problems in which we
are required to produce sentences or longer texts—i.e., in document summarization and machine
translation. ese will be discussed in Chapter 17).
Word In these problems, we are faced with a word, such as “dog,” “magniﬁcent,” “magniﬃcant,”
or “parlez” and need to say something about it: Does it denote a living thing? What language
is it in? How common is it? What other words are similar to it? Is it a mis-spelling of another
word? And so on. ese kind of problems are actually quite rare, as words seldom appear
in isolation, and for many words their interpretation depends on the context in which they
are used.
Texts In these problems we are faced with a piece of text, be it a phrase, a sentence, a paragraph
or a document, and need to say something about it. Is it spam or not? Is it about politics or

66
6. FEATURES FOR TEXTUAL DATA
sports? Is it sarcastic? Is it positive, negative or neutral (toward some issue)? Who wrote it?
Is it reliable? Which of a ﬁxed set of intents does this text reﬂect (or none)? Will this text
be liked by 16–18 years old males? And so on. ese types of problems are very common,
and we’ll refer to them collectively as document classiﬁcation problems.
Paired Texts In these problems we are given a pair of words or longer texts, and need to say
something about the pair. Are words A and B synonyms? Is word A a valid translation for
word B? Are documents A and B written by the same author? Can the meaning of sentence
A be inferred from sentence B?
Word in Context Here, we are given a piece of text, and a particular word (or phrase, or letter,
etc.) within it, and we need to classify the word in the context of the text. For example, is
the word book in I want to book a ﬂight a noun, a verb or an adjective? Is the word apple
in a given context referring to a company or a fruit? Is on the right preposition to use in I
read a book on London? Does a given period denote a sentence boundary or an abbreviation?
Is the given word part of a name of a person, location, or organization? And so on. ese
types of questions often arise in the context of larger goals, such as annotating a sentence
for parts-of-speech, splitting a document into sentences, ﬁnding all the named entities in a
text, ﬁnding all documents mentioning a given entity, and so on.
Relation between two words Here we are given two words or phrases within the context of a
larger document, and need to say something about the relations between them. Is word A
the subject of verb B in a given sentence? Does the “purchase” relation hold between words
A and B in a given text? And so on.
Many of these classiﬁcation cases can be extended to structured problems in which we are in-
terested in performing several related classiﬁcation decisions, such that the answer to one decision
can inﬂuence others. ese are discussed in Chapter 19.
..
What is a word?
We are using the term word rather loosely. e question “what is a word?”
is a matter of debate among linguists, and the answer is not always clear.
One deﬁnition (which is the one being loosely followed in this book) is that words are
sequences of letters that are separated by whitespace. is deﬁnition is very simplistic. First,
punctuation in English is not separated by whitespace, so according to our deﬁnition dog,
dog?, dog. and dog) are all diﬀerent words. Our corrected deﬁnition is then words separated
by whitespace or punctuation. A process called tokenization is in charge of splitting text into
tokens (what we call here words) based on whitespace and punctuation. In English, the job
of the tokenizer is quite simple, although it does need to consider cases such as abbreviations
(I.B.M) and titles (Mr.) that needn’t be split. In other languages, things can become much
tricker: in Hebrew and Arabic some words attach to the next one without whitespace, and
in Chinese there are no whitespaces at all. ese are just a few examples.

6.2. FEATURES FOR NLP PROBLEMS
67
.
When working in English or a similar language (as this book assumes), tokenizing
on whitespace and punctuation (while handling a few corner cases) can provide a good ap-
proximation of words. However, our deﬁnition of word is still quite technical: it is derived
from the way things are written. Another common (and better) deﬁnition take a word to be
“the smallest unit of meaning.” By following this deﬁnition, we see that our whitespace-based
deﬁnition is problematic. After splitting by whitespace and punctuation, we still remain with
sequences such as don’t, that are actually two words, do not, that got merged into one sym-
bol. It is common for English tokenizers to handle these cases as well. e symbols cat and
Cat have the same meaning, but are they the same word? More interestingly, take something
like New York, is it two words, or one? What about ice cream? Is it the same as ice-cream
or icecream? And what about idioms such as kick the bucket?
In general, we distinguish between words and tokens. We refer to the output of a tok-
enizer as a token, and to the meaning-bearing units as words. A token may be composed of
multiple words, multiple tokens can be a single word, and sometimes diﬀerent tokens denote
the same underlying word.
Having said that, in this book, we use the term word very loosely, and take it to be
interchangeable with token. It is important to keep in mind, however, that the story is more
complex than that.
6.2
FEATURES FOR NLP PROBLEMS
In what follows, we describe the common features that are used for the above problems. As words
and letters are discrete items, our features often take the form of indicators or counts. An indicator
feature takes a value of 0 or 1, depending on the existence of a condition (e.g., a feature taking
the value of 1 if the word dog appeared at least once in the document, and 0 otherwise). A count
takes a value depending on the number of times some event occurred, e.g., a feature indicating
the number of times the word dog appears in the text.
6.2.1
DIRECTLY OBSERVABLE PROPERTIES
Features for Single Words
When our focus entity is a word outside of a context, our main source
of information is the letters comprising the word and their order, as well as properties derived
from these such as the length of the word, the orthographic shape of the word (Is the ﬁrst letter
capitalized? Are all letters capitalized? Does the word include a hyphen? Does it include a digit?
And so on), and preﬁxes and suﬃxes of the word (Does it start with un? Does it end with ing?).
We may also look at the word with relation to external sources of information: How many
times does the word appear in a large collection of text? Does the word appear in a list of common
person names in the U.S.? And so on.
Lemmas and Stems We often look at the lemma (the dictionary entry) of the word, mapping
forms such as booking, booked, books to their common lemma book. is mapping is usually per-

68
6. FEATURES FOR TEXTUAL DATA
formed using lemma lexicons or morphological analyzers, that are available for many languages.
e lemma of a word can be ambiguous, and lemmatizing is more accurate when the word is given
in context. Lemmatization is a linguistically deﬁned process, and may not work well for forms that
are not in the lemmatization lexicon, or for mis-spelling. A coarser process than lemmatization,
that can work on any sequence of letters, is called stemming. A stemmer maps sequences of words
to shorter sequences, based on some language-speciﬁc heuristics, such that diﬀerent inﬂections
will map to the same sequence. Note that the result of stemming need not be a valid word: picture
and pictures and pictured will all be stemmed to pictur. Various stemmers exist, with diﬀerent
levels of aggressiveness.
Lexical Resources An additional source of information about word forms are lexical resources.
ese are essentially dictionaries that are meant to be accessed programmatically by machines
rather than read by humans. A lexical resource will typically contain information about words,
linking them to other words and/or providing additional information.
For example, for many languages there are lexicons that map inﬂected word forms to their
possible morphological analyses (i.e., telling you that a certain word may be either a plural femi-
nine noun or a past-perfect verb). Such lexicons will typically also include lemma information.
A very well-known lexical resource in English is WordNet [Fellbaum, 1998]. WordNet is a
very large manually curated dataset attempting to capture conceptual semantic knowledge about
words. Each word belongs to one or several synsets, where each synsets describes a cognitive con-
cept. For example, the word star as a noun belongs to the synsets astronomical celestial body, someone
who is dazzlingly skilled, any celestial body visible from earth and an actor who plays a principle role,
among others. e second synset of star contains also the words ace, adept, champion, sensation,
maven, virtuoso, among others. Synsets are linked to each other by means of semantic relations
such as hypernymy and hyponymy (more speciﬁc or less speciﬁc words). For example, for the ﬁrst
synset of star these would include sun and nova (hyponyms) and celestial body (hypernym). Other
semantic relations in WordNet contain antonyms (opposite words) and holonyms and meronyms
(part-whole and whole-part relations). WordNet contains information about nouns, verbs, adjec-
tives, and adverbs.
FrameNet [Fillmore et al., 2004] and VerbNet [Kipper et al., 2000] are manually curated
lexical resources that focus around verbs, listing for many verbs the kinds of argument they take
(i.e., that giving involves the core arguments D, R, and T (the thing that is
being given), and may have non-core arguments such as T, P, P, and M,
among others.
e Paraphrase Database (PPDB) [Ganitkevitch et al., 2013, Pavlick et al., 2015] is a large,
automatically created dataset of paraphrases. It lists words and phrases, and for each one provides
a list of words and phrases that can be used to mean roughly the same thing.
Lexical resources such as these contain a lot of information, and can serve a good source of
features. However, the means of using such symbolic information eﬀectively is task dependent,

6.2. FEATURES FOR NLP PROBLEMS
69
and often requires non-trivial engineering eﬀorts and/or ingenuity. ey are currently not often
used in neural network models, but this may change.
Distributional Information Another important source of information about words is distribu-
tional—which other words behave similar to it in the text? ese deserve their own separate
treatment, and are discussed in Section 6.2.5 below. In Section 11.8, we discuss how lexical re-
sources can be used to inject knowledge into distributional word vectors that are derived from
neural network algorithms.
Features for Text
When we consider a sentence, a paragraph, or a document, the observable
features are the counts and the order of the letters and the words within the text.
Bag of words A very common feature extraction procedures for sentences and documents is the
bag-of-words approach (BOW). In this approach, we look at the histogram of the words within
the text, i.e., considering each word count as a feature. By generalizing from words to “basic ele-
ments,” the bag-of-letter-bigrams we used in the language identiﬁcation example in Section 2.3.1
is an example of the bag-of-words approach.
We can also compute quantities that are directly derived from the words and the letters, such
as the length of the sentence in terms of number of letters or number of words. When considering
individual words, we may of course use the word-based features from above, counting for example
the number of words in the document that have a speciﬁc preﬁx or suﬃx, or compute the ratio of
short words (with length below a given length) to long words in a document.
Weighting As before, we can also integrate statistics based on external information, focusing for
example on words that appear many times in the given document, yet appear relatively few times in
an external set of documents (this will distinguish words that have high counts in the documents
because they are generally common, like a and for from words that have a high count because
they relate to the document’s topic). When using the bag-of-words approach, it is common to
use TF-IDF weighting [Manning et al., 2008, Chapter 6]. Consider a document d which is
part of a larger corpus D. Rather than representing each word w in d by its normalized count
in the document
#d .w/
P
w02d #d .w0/ (the Term Frequency), TF-IDF weighting represent it instead by
#d .w/
P
w02d #d .w0/ � log
jDj
jfd2DWw2dgj. e second term is the Inverse Document Frequency: the inverse
of the number of distinct documents in the corpus in which this word occurred. is highlights
words that are distinctive of the current text.
Besides words, one may also look at consecutive pairs or triplets of words. ese are called
ngrams. Ngram features are discussed in depth in Section 6.2.4.
Features of Words in Context
When considering a word within a sentence or a document, the
directly observable features of the word are its position within the sentence, as well as the words or
letters surrounding it. Words that are closer to the target word are often more informative about
it than words that are further apart.¹
¹However, note that this is a gross generalization, and in many cases language exhibit a long-range dependencies between
words: a word at the end of a text may well be inﬂuenced by a word at the beginning.

70
6. FEATURES FOR TEXTUAL DATA
Windows For this reason, it is often common to focus on the immediate context of a word by
considering a window surrounding it (i.e., k words to each side, with typical values of k being
2, 5, and 10), and take the features to be the identities of the words within the window (e.g., a
feature will be “word X appeared within a window of ﬁve words surrounding the target word”). For
example, consider the sentence the brown fox jumped over the lazy dog, with the target word jumped.
A window of 2 words to each side will produce the set of features { word=brown, word=fox,
word=over, word=the }. e window approach is a version of the bag-of-words approach, but
restricted to items within the small window.
e ﬁxed size of the window gives the opportunity to relax the bag-of-word assumption
that order does not matter, and take the relative positions of the words in the window into account.
is results in relative-positional features such as “word X appeared two words to the left of the
target word.” For example, in the example above the positional window approach will result in
the set of features { word-2=brown, word-1=fox, word+1=over, word+2=the }.
Encoding of window-based features as vectors is discussed in Section 8.2.1. In Chapters 14
and 16 we will introduce the biRNN architecture, that generalizes window features by providing
a ﬂexible, adjustable, and trainable window.
Position Besides the context of the word, we may be interested in its absolute position within a
sentence. We could have features such as “the target word is the 5th word in the sentence,” or a
binned version indicating more coarse grained categories: does it appear within the ﬁrst 10 words,
between word 10 and 20, and so on.
Features for Word Relations
When considering two words in context, besides the position of
each one and the words surrounding them, we can also look at the distance between the words
and the identities of the words that appear between them.
6.2.2
INFERRED LINGUISTIC PROPERTIES
Sentences in natural language have structures beyond the linear order of their words. e structure
follows an intricate set of rules that are not directly observable to us. ese rules are collectively
referred to as syntax, and the study of the nature of these rules and regularities in natural language
is the study-object of linguistics.² While the exact structure of language is still a mystery, and rules
governing many of the more intricate patterns are either unexplored or still open for debate among
linguists, a subset of phenomena governing language are well documented and well understood.
ese include concepts such as word classes (part-of-speech tags), morphology, syntax, and even
parts of semantics.
While the linguistic properties of a text are not observable directly from the surface forms
of words in sentences and their order, they can be inferred from the sentence string with vary-
²is last sentence, is, of course, a gross simpliﬁcation. Linguistics has much wider breadth than syntax, and there are other
systems that regulate the human linguistic behavior besides the syntactic one. But for the purpose of this introductory book,
this simplistic view will be suﬃcient. For a more in depth overview, see the further reading recommendations at the end of
this section.

6.2. FEATURES FOR NLP PROBLEMS
71
ing degrees of accuracy. Specialized systems exist for the prediction of parts of speech, syntactic
trees, semantic roles, discourse relations, and other linguistic properties with various degrees of
accuracy,³ and these predictions often serve as good features for further classiﬁcation problems.
.
Linguistic Annotation
Let’s explore some forms of linguistic annotations. Consider the
sentence the boy with the black shirt opened the door with a key. One level of annotation assigns
to each word its part of speech:
the
boy
with
the
black
shirt
opened
the
door
with
a
key
D
N
P
D
A
N
V
D
N
P
D
N
Going further up the chain, we mark syntactic chunk boundaries, indicating the the boy
is a noun phrase.
[NP the boy ] [PP with ] [NP the black shirt ] [VP opened ] [NP the door ] [PP with ] [NP a key ]
Note that the word opened is marked as a verbal-chunk (VP). is may not seem very
useful because we already know its a verb. However, VP chunks may contain more elements,
covering also cases such as will opened and did not open.
e chunking information is local. A more global syntactic structure is a constituency
tree, also called a phrase-structure tree:
S
NP
DT
the
boy
opened
IN
IN
NN
NP
NP
DT
DT
with
with
the
the
black
shirt
a
key
door
JJ
NN
NN
DT
NN
NP
PP
PP
VBD
VP
Constituency trees are nested, labeled bracketing over the sentence, indicating the hi-
erarchy of syntactic units: the noun phrase the boy with the black shirt is made of the noun
³Indeed, for many researchers, improving the prediction of these linguistic properties is the natural language processing problem
they are trying to solve.

72
6. FEATURES FOR TEXTUAL DATA
..
phrase the boy and the preposition phrase (PP) with the black shirt. e latter itself contains
the noun phrase the black shirt. Having with a key nested under the VP and not under the
NP the door signals that with a key modiﬁes the verb opened (opened with a key) rather than
the NP (a door with a key).
A diﬀerent kind of syntactic annotation is a dependency tree. Under dependency
syntax, each word in the sentence is a modiﬁer of another word, which is called its head.
Each word in the sentence is headed by another sentence word, except for the main word,
usually a verb, which is the root of the sentence and is headed by a special “root” node.
the   boy  with   the  black   shirt   opened  the  door   with   a  key
det
prep
prep
amod
det
pobj
nsubj
root
dobj
pobj
det
det
While constituency trees make explicit the grouping of words into phrases, dependency
trees make explicit the modiﬁcation relations and connections between words. Words that are
far apart in the surface form of the sentence may be close in its dependency tree. For example,
boy and opened have four words between them in the surface form, but have a direct nsubj
edge connecting them in the dependency tree.
e dependency relations are syntactic: they are concerned with the structure of the
sentence. Other kinds of relations are more semantic. For example, consider the modiﬁers of
the verb open, also called the arguments of the verb. e syntactic tree clearly marks the boy
(with the black shirt), the door, and with a key as arguments, and also tells us that with a key
is an argument of open rather than a modiﬁer of door. It does not tell us, however, what are
the semantic-roles of the arguments with respect to the verb, i.e., that the boy is the A
performing the action, and that a key is an I (compare that to the boy opened the
door with a smile. Here, the sentence will have the same syntactic structure, but, unless we
are in a magical-world, a smile is a M rather than an I. e semantic role
labeling annotations reveal these structures:

6.2. FEATURES FOR NLP PROBLEMS
73
.
the   boy  with  the  black  shirt   opened    the  door  with  a  smile
the  boy  with   the  black  shirt   opened    the  door  with  a  key
Agent
Instrument
Patient
Manner
Patient
Agent
Besides the observable properties (letters, words, counts, lengths, linear distances, frequen-
cies, etc.), we can also look such inferred linguistic properties of words, sentences, and documents.
For example, we could look at the part-of-speech tag (POS) of a word within a document (Is it a
noun, a verb, adjective, or a determiner?), the syntactic role of a word (Does it serve as a subject or
an object of a verb? Is it the main verb of the sentence? Is it used as an adverbial modiﬁer?), or the
semantic role of it (e.g., in “the key opened the door,” key acts as an I, while in “the
boy opened the door” boy is an A). When given two words in a sentence, we can consider
the syntactic dependency tree of the sentence, and the subtree or paths that connect the two words
within the this tree, as well as properties of that path. Words that are far apart in the sentence in
terms of the number of words separating them can be close to each other in the syntactic structure.
When moving beyond the sentence, we may want to look at the discourse relations that
connect sentences together, such as E, C, CE, and so on.
ese relations are often expressed by discourse-connective words such as moreover, however, and
and, but are also expressed with less direct cues.
Another important phenomena is that of anaphora—consider the sentence sequence the
boy opened the door with a key. It1 wasn’t locked and he1 entered the room. He2 saw a man. He3 was
smiling. Anaphora resolution (also called coreference resolution) will tell us that It1 refers to the
door (and not the key or the boy), he2 refers to the boy and he3 is likely to refer to the man.
Part of speech tags, syntactic roles, discourse relations, anaphora, and so on are concepts that
are based on linguistic theories that were developed by linguists over a long period of time, with
the aim of capturing the rules and regularities in the very messy system of the human language.
While many aspects of the rules governing language are still open for debate, and others may
seem overly rigid or simplistic, the concepts explored here (and others) do indeed capture a wide
and important array of generalizations and regularities in language.
Are linguistic concepts needed? Some proponents of deep-learning argue that such inferred,
manually designed, linguistic properties are not needed, and that the neural network will learn
these intermediate representations (or equivalent, or better ones) on its own. e jury is still out on
this. My current personal belief is that many of these linguistic concepts can indeed be inferred by

74
6. FEATURES FOR TEXTUAL DATA
the network on its own if given enough data and perhaps a push in the right direction.⁴ However,
for many other cases we do not have enough training data available for the task we care about, and
in these cases providing the network with the more explicit general concepts can be very valuable.
Even if we do have enough data, we may want to focus the network on certain aspects of the text
and hint to it that it should ignore others, by providing the generalized concepts in addition to,
or even instead of, the surface forms of the words. Finally, even if we do not use these linguistic
properties as input features, we may want to help guide the network by using them as additional
supervision in a multi-task learning setup (see Chapter 20) or by designing network architecture
or training paradigms that are more suitable for learning certain linguistic phenomena. Overall,
we see enough evidence that the use of linguistic concepts help improve language understanding
and production systems.
Further Reading
When dealing with natural language text, it is well advised to be aware of the
linguistic concepts beyond letters and words, as well as of the current computational tools and re-
sources that are available. is book barely scratches the surface on this topic. e book of Bender
[2013] provides a good and concise overview of linguistic concepts directed at computational-
minded people. For a discussion on current NLP methods, tools, and resources see the book by
Jurafsky and Martin [2008] as well as the various specialized titles in this series.⁵
6.2.3
CORE FEATURES VS. COMBINATION FEATURES
In many cases, we are interested in a conjunction of features occurring together. For example,
knowing that the two indicators “the word book appeared in a window” and “the part-of-speech
V appeared in a window” is strictly less informative than knowing “the word book with the
assigned part of speech V appeared in a window.” Similarly, if we assign a distinct parameter
weight for each indicator feature (as is the case in linear models), then knowing that the two
distinct features “word in position �1 is like,” “word in position �2 is not” occur is almost useless
compared to the very indicative combined indicator “word in position �1 is like and word in
position �2 is not.” Similarly, knowing that a document contains the word Paris is an indication
toward the document being in the T category, and the same holds for the word Hilton.
However, if the document contains both words, it is an indication away from the T category
and toward the C or G categories.
Linear models cannot assign a score to a conjunction of events (X occurred and Y occurred
and …) that is not a sum of their individual scores, unless the conjunction itself is modeled as
its own feature. us, when designing features for a linear model, we must deﬁne not only the
core features but also many combination features.⁶ e set of possible combination is very large, and
⁴See, for example, the experiment in Section 16.1.2 in which a neural networks learns the concept of subject-verb agreement
in English, inferring the concepts of nouns, verbs, grammatical number and some hierarchical linguistics structures.
⁵Syntactic dependency structures are discussed in Kübler et al. [2008] and semantic roles in Palmer et al. [2010].
⁶is is a direct manifestation of the XOR problem discussed in Chapter 3, and the manually deﬁned combination-features are
the mapping function � that maps the nonlinearly separable vectors of core-features to a higher dimensional space in which
the data is more likely to be separable by a linear model.

6.2. FEATURES FOR NLP PROBLEMS
75
human expertise, coupled with trial and error, is needed in order to construct a set of combina-
tions that is both informative and relatively compact. Indeed, a lot of eﬀort has gone into design
decisions such as “include features of the form word at position -1 is X and at position +1 is
Y but do not include features of the form word at position -3 is X and at position -1 is Y.”
Neural networks provide nonlinear models, and do not suﬀer from this problem. When
using a neural network such as a multi-layer perceptron (Chapter 4), the model designer can
specify only the set of core features, and rely on the network training procedure to pick up on
the important combinations on its own. is greatly simpliﬁes the work of the model designer.
In practice, neural networks indeed manage to learn good classiﬁers based on core features only,
sometimes surpassing the best linear classiﬁer with human-designed feature combinations. How-
ever, in many other cases a linear classiﬁer with a good hand-crafted feature-set is hard to beat,
with the neural network models with core features getting close to but not surpassing the linear
models.
6.2.4
NGRAM FEATURES
A special case of feature combinations is that of ngrams—consecutive word sequences of a given
length. We already saw letter-bigram features in the language classiﬁcation case (Chapter 2).
Word-bigrams, as well as trigrams (sequences of three items) of letters or words are also com-
mon. Beyond that, 4-grams and 5-grams are sometimes used for letters, but rarely for words
due to sparsity issues. It should be intuitively clear why word-bigrams are more informative than
individual words: it captures structures such as New York, not good, and Paris Hilton. Indeed, a
bag-of-bigrams representation is much more powerful than bag-of-words, and in many cases
proves very hard to beat. Of course, not all bigrams are equally informative, bigrams such as of
the, on a, the boy, etc. are very common and, for most tasks, not more informative than their indi-
vidual components. However, it is very hard to know a-priori which ngrams will be useful for a
given task. e common solution is to include all ngrams up to a given length, and let the model
regularization discard of the less interesting ones by assigning them very low weights.
Note that vanilla neural network architectures such as the MLP cannot infer ngram fea-
tures from a document on their own in the general case: a multi-layer perceptron fed with a
bag-of-words feature vector of a document could learn combinations such as “word X appear in
the document and word Y appears in the document” but not “the bigram X Y appears in the
document.” us, ngram features are useful also in the context of nonlinear classiﬁcation.
Multi-layer perceptrons can infer ngrams when applied to a ﬁxed size windows with posi-
tional information—the combination of “word at position �1 is X” and “word at position �2 is Y”
is in eﬀect the bigram XY. More specialized neural network architectures such as convolutional
networks (Chapter 13) are designed to ﬁnd informative ngram features for a given task based on
a sequence of words of varying lengths. Bidirectional RNNs (Chapters 14 and 16) generalize the
ngram concept even further, and can be sensitive to informative ngrams of varying lengths, as
well as ngrams with gaps in them.

76
6. FEATURES FOR TEXTUAL DATA
6.2.5
DISTRIBUTIONAL FEATURES
Up until now our treatment of words was as discrete and unrelated symbols: the words pizza,
burger, and chair are all equally similar (and equally dis-similar) to each other as far as the algo-
rithm is concerned.
We did achieve some form of generalization across word types by mapping them to coarser-
grained categories such as parts-of-speech or syntactic roles (“the, a, an, some are all determiners”);
generalizing from inﬂected words forms to their lemmas (“book, booking, booked all share the
lemma book”); looking at membership in lists or dictionaries (“John, Jack, and Ralph appear in a list
of common U.S. ﬁrst names”); or looking at their relation to other words using lexical resources
such as WordNet. However, these solutions are quite limited: they either provide very coarse
grained distinctions, or otherwise rely on speciﬁc, manually compiled dictionaries. Unless we
have a specialized list of foods we will not learn that pizza is more similar to burger than it is to
chair, and it will be even harder to learn that pizza is more similar to burger than it is to icecream.
e distributional hypothesis of language, set forth by Firth [1957] and Harris [1954], states
that the meaning of a word can be inferred from the contexts in which it is used. By observing co-
occurrence patterns of words across a large body of text, we can discover that the contexts in which
burger occur are quite similar to those in which pizza occurs, less similar to those in which icecream
occurs, and very diﬀerent from those in which chair occurs. Many algorithms were derived over
the years to make use of this property, and learn generalizations of words based on the contexts
in which they occur. ese can be broadly categorized into clustering-based methods, which assign
similar words to the same cluster and represent each word by its cluster membership [Brown et al.,
1992, Miller et al., 2004], and to embedding-based methods which represent each word as a vector
such that similar words (words having a similar distribution) have similar vectors [Collobert and
Weston, 2008, Mikolov et al., 2013b]. Turian et al. [2010] discuss and compare these approaches.
ese algorithms uncover many facets of similarity between words, and can be used to derive
good word features: for example, one could replace words by their cluster ID (e.g., replacing both
the words June and aug by cluster732), replace rare or unseen words with the common word most
similar to them, or just use the word vector itself as the representation of the word.
However, care must be taken when using such word similarity information, as it can have
unintended consequences. For example, in some applications it is very useful to treat London and
Berlin as similar, while for others (for example when booking a ﬂight or translating a document)
the distinction is crucial.
We will discuss word embeddings methods and the use of word vectors in more detail in
Chapters 10 and 11.

77
C H A P T E R
7
Case Studies of NLP Features
After discussing the diﬀerent sources of information available for us for deriving features from
natural language text, we will now explore examples of concrete NLP classiﬁcation tasks, and
suitable features for them. While the promise of neural networks is to alleviate the need for manual
feature engineering, we still need to take these sources of information into consideration when
designing our models: we want to make sure that the network we design can make eﬀective use
of the available signals, either by giving it direct access to them by use of feature-engineering;
by designing the network architecture to expose the needed signals; or by adding them as an
additional loss signals when training the models.¹
7.1
DOCUMENT CLASSIFICATION: LANGUAGE
IDENTIFICATION
In the language identiﬁcation task, we are given a document or a sentence, and want to classify it
into one of a ﬁxed set of languages. As we saw in Chapter 2, a bag of letter-bigrams is a very strong
feature representation for this task. Concretely, each possible letter-bigram (or each letter bigram
appearing at least k times in at least one language) is a core feature, and the value of a core feature
for a given document is the count of that feature in the document.
A similar task is the one of encoding detection. Here, a good feature representation is a bag-of
byte-bigrams.
7.2
DOCUMENT CLASSIFICATION: TOPIC
CLASSIFICATION
In the Topic Classiﬁcation task, we are given a document and need to classify it into one of a
predeﬁned set of topics (e.g., Economy, Politics, Sports, Leisure, Gossip, Lifestyle, Other).
Here, the letter level is not very informative, and our basic units will be words. Word order
is not very informative for this task (except maybe for consecutive word pairs such as bigrams).
us, a good set of features will be the bag-of-words in the document, perhaps accompanied by a
bag-of-word-bigrams (each word and each word-bigram is a core feature).
¹Additionally, linear or log-linear models with manually designed features are still very eﬀective for many tasks. ey can be
very competitive in terms of accuracy, as well as being very easy to train and deploy at scale, and easier to reason about and
debug than neural networks. If nothing else, such models should be considered as strong baselines for whatever networks you
are designing.

78
7. CASE STUDIES OF NLP FEATURES
If we do not have many training examples, we may beneﬁt from pre-processing the doc-
ument by replacing each word with its lemma. We may also replace or supplement words by
distributional features such as word clusters or word-embedding vectors.
When using a linear classiﬁer, we may want to also consider word pairs, i.e., consider each
pair of words (not necessarily consecutive) that appear in the same document as a core feature. is
will result in a huge number of potential core features, and the number will need to be trimmed
down by designing some heuristic, such as considering only word pairs which appear in a speciﬁed
number of documents. Nonlinear classiﬁers alleviate this need.
When using a bag-of-words, it is sometimes useful to weight each word with proportion to
its informativeness, for example using TF-IDF weighting (Section 6.2.1). However, the learning
algorithm is often capable of coming up with the weighting on its own. Another option is to use
word indicators rather than word counts: each word in the document (or each word above a given
count) will be represented once, regardless of its number of occurrences in the document.
7.3
DOCUMENT CLASSIFICATION: AUTHORSHIP
ATTRIBUTION
In the authorship attribution task [Koppel et al., 2009] we are given a text and need to infer the
identify of its author (from a ﬁxed set of possible authors), or other characteristics of the author
of the text, such as their gender, their age or their native language.
e kind of information used to solve this task is very diﬀerent than that of topic
classiﬁcation—the clues are subtle, and involve stylistic properties of the text rather than con-
tent words.
us, our choice of features should shy away from content words and focus on more stylistic
properties.² A good set for such tasks focus on parts of speech (POS) tags and function words. ese
are words like on, of, the, and, before and so on that do not carry much content on their own but
rather serve to connect to content-bearing words and assign meanings to their compositions, as
well as pronouns (he, she, I, they, etc.) A good approximation of function words is the list of top-
300 or so most frequent words in a large corpus. By focusing on such features, we can learn to
capture subtle stylistic variations in writing, that are unique to an author and very hard to fake.
A good feature set for authorship attribution task include a bag-of-function-words-and-
pronouns, bag-of-POS-tags, and bags of POS bigrams, trigrams, and 4grams. Additionally, we
may want to consider the density of function words (i.e., the ratio between the number of function
words and content words in a window of text), a bag of bigrams of function words after removing
the content words, and the distributions of the distances between consecutive function words.
²One could argue that for age or gender identiﬁcation, we may as well observe also the content-words, as there are strong
correlation between age and gender of a person and the topics they write about and the language register they use. is is
generally true, but if we are interested in a forensic or adversarial setting in which the author has an incentive to hide their age
or gender, we better not rely on content-based features, as these are rather easy to fake, compared to the more subtle stylistic
cues.

7.4. WORD-IN-CONTEXT: PART OF SPEECH TAGGING
79
7.4
WORD-IN-CONTEXT: PART OF SPEECH TAGGING
In the parts-of-speech tagging task, we are given a sentence, and need to assign the correct part-
of-speech to each word in the sentence. e parts-of-speech come from a pre-deﬁned set, for this
example assume we will be using the tagset of the Universal Treebank Project [McDonald et al.,
2013, Nivre et al., 2015], containing 17 tags.³
Part-of-speech tagging is usually modeled as a structured task—the tag of the ﬁrst word
may depend on the tag of the third one—but it can be approximated quite well by classifying each
word in isolation into a POS-tag based on a window of two words to each side of the word. If we
tag the words in a ﬁxed order, for example from left to right, we can also condition each tagging
prediction on tag predictions made on previous tags. Our feature function when classifying a
word wi has access to all the words in the sentence (and their letters) as well as all the previous
tagging decisions (i.e., the assigned tags for words w1; : : : ; wi�1). Here, we discuss features as if
they are used in an isolated classiﬁcation task. In Chapter 19 we discuss the structured learning
case—using the same set of features.
e sources of information for the POS-tagging task can be divided into intrinsic cues
(based on the word itself) and extrinsic cues (based on its context). Intrinsic cues include the
identify of the word (some words are more likely than others to be nouns, for example), preﬁxes,
suﬃxes, and orthographic shape of the word (in English, words ending in -ed are likely past-tense
verbs, words starting with un- are likely to be adjectives, and words starting with a capital letter
are likely to be proper names), and the frequency of the word in a large corpus (for example,
rare words are more likely to be nouns). Extrinsic cues include the word identities, preﬁxes, and
suﬃxes of words surrounding the current word, as well as the part-of-speech prediction for the
previous words.
..
Overlapping features
If we have the word form as a feature, why do we need the preﬁxes
and suﬃxes? After all they are deterministic functions of the word. e reason is that if we
encounter a word that we have not seen in training (out of vocabulary or OOV word) or a word
we’ve seen only a handful of times in training (a rare word), we may not have robust enough
information to base a decision on. In such cases, it is good to back-oﬀ to the preﬁxes and
suﬃxes, which can provide useful hints. By including the preﬁx and suﬃx features also for
words that are observed many times in training, we allow the learning algorithms to better
adjust their weights, and hopefully use them properly when encountering OOV words.
³adjective, adposition, adverb, auxiliary verb, coordinating conjunction, determiner, interjection, noun, numeral, particle, pro-
noun, proper noun, punctuation, subordinating conjunction, symbol, verb, other.

80
7. CASE STUDIES OF NLP FEATURES
An example of a good set of core features for POS tagging is:
• word=X
• 2-letter-suﬃx=X
• 3-letter-suﬃx=X
• 2-letter-preﬁx=X
• 3-letter-preﬁx=X
• word-is-capitalized
• word-contains-hyphen
• word-contains-digit
• for P in Œ�2; �1; C1; C2�:
– Word at position P=X
– 2-letter-suﬃx of word at position P=X
– 3-letter-suﬃx of word at position P=X
– 2-letter-preﬁx of word at position P=X
– 3-letter-preﬁx of word at position P=X
– word at position P=X is capitalized
– word at position P=X contains hyphen
– word at position P=X contains digit
• Predicted POS of word at position -1=X
• Predicted POS of word at position -2=X
In addition to these, distributional information such as word clusters or word-embedding vectors
of the word and of surrounding words can also be useful, especially for words not seen in the
training corpus, as words with similar POS-tags tend to occur in more similar contexts to each
other than words of diﬀerent POS-tags.

7.5. WORD-IN-CONTEXT: NAMED ENTITY RECOGNITION
81
7.5
WORD-IN-CONTEXT: NAMED ENTITY
RECOGNITION
In the named-entity recognition (NER) task we are given a document and need to ﬁnd named
entities such as Milan, John Smith, McCormik Industries, and Paris, as well as to categorize them
into a pre-deﬁned set of categories such as L, O, P, or O. Note
that this task is context dependent, as Milan can be a location (the city) or an organization (a sports
team, “Milan played against Barsa Wednesday evening”), and Paris can be the name of a city or
a person.
A typical input to the problem would be a sentence such as:
John Smith , president of McCormik Industries visited his niece Paris in Milan , reporters
say .
and the expected output would be:
[PER John Smith ] , president of [ORG McCormik Industries ] visited his niece [PER Paris ]
in [LOC Milan ], reporters say .
While NER is a sequence segmentation task—it assigns labeled brackets over non-
overlapping sentence spans—it is often modeled as a sequence tagging task, like POS-tagging.
e use of tagging to solve segmentation tasks is performed using BIO encoded tags.⁴ Each word
is assigned one of the following tags, as seen in Table 7.1:
Table 7.1: BIO tags for named entity recognition
Tag
Meaning
O
Not part of a named entity
B-PER
I-PER
First word of a person name
Continuation of a person name
B-LOC
I-LOC
First word of a location name
Continuation of a location name
B-ORG
I-ORG
First word of an organization name
Continuation of an organization name
B-MISC
I-MISC
First word of another kind of named entity
Continuation of another kind of named entity
⁴Variants on the BIO tagging scheme are explored in the literature, and some perform somewhat better than it. See Lample
et al. [2016], Ratinov and Roth [2009].

82
7. CASE STUDIES OF NLP FEATURES
e sentence above would be tagged as:
John/B-PER Smith/I-PER ,/O president/O of/O McCormik/B-ORG Industries/I-ORG
visited/O his/O niece/O Paris/B-PER in/O Milan/B-LOC ,/O reporters/O say/O ./O
e translation from non-overlapping segments to BIO tags and back is straightforward.
Like POS-tagging, the NER task is a structured one, as tagging decisions for diﬀerent
words interact with each other (it is more likely to remain within the same entity type than to
switch, it is more likely to tag “John Smith Inc.” as B-ORG I-ORG I-ORG than as B-PER I-PER
B-ORG). However, we again assume it can be approximated reasonably well using independent
classiﬁcation decisions.
e core feature set for the NER task is similar to that of the POS-tagging task, and relies
on words within a 2-words window to each side of the focus word. In addition to the features of
the POS-tagging task which are useful for NER as well (e.g., -ville is a suﬃx indicating a location,
Mc- is a preﬁx indicating a person), we may want to consider also the identities of the words that
surround other occurrences of the same word in the text, as well as indicator functions that check
if the word occurs in pre-compiled lists of persons, locations and organizations. Distributional
features such word clusters or word vectors are also extremely useful for the NER task. For a
comprehensive discussion on features for NER, see Ratinov and Roth [2009].
7.6
WORD IN CONTEXT, LINGUISTIC FEATURES:
PREPOSITION SENSE DISAMBIGUATION
Prepositions, words like on, in, with, and for, serve for connecting predicates with their argu-
ments and nouns with their prepositional modiﬁers. Preposions are very common, and also very
ambiguous. Consider, for example, the word for in the following sentences.
(1)
a.
We went there for lunch.
b.
He paid for me.
c.
We ate for two hours.
d.
He would have left for home, but it started raining.
e word for plays a diﬀerent role in each of them: in (a) it indicates a P in (b) a B-
, in (c) a D and in (d) a L.
In order to fully understand the meaning of a sentence, one should arguablly know the
correct senses of the prepositions within it. e preposition-sense disambiguation task deals with
assigning the correct sense to a preposition in context, from a ﬁnite inventory of senses. Schneider
et al. [2015, 2016] discuss the task, present a uniﬁed sense inventory that covers many preposi-

7.6. WORDINCONTEXT,LINGUISTICFEATURES:PREPOSITIONSENSEDISAMBIGUATION
83
tions, and provide a small annotated corpus of sentences from online reviews, covering 4,250
preposition mentions, each annotated with its sense.⁵
Which are a good set of features for the preposition sense disambiguation task? We follow
here the feature set inspired by the work of Hovy et al. [2010].
Obviously, the preposition itself is a useful feature (the distribution of possible senses for
in is very diﬀerent from the distribution of senses for with or about, for example). Besides that,
we will look in the context in which the word occurs. A ﬁxed window around the preposition
may not be ideal in terms of information content, thought. Consider, for example, the following
sentences.
(2)
a.
He liked the round object from the very ﬁrst time he saw it.
b.
He saved the round object from him the very ﬁrst time they saw it.
e two instances of from have diﬀerent senses, but most of the words in a window around the
word are either not informative or even misleading. We need a better mechanism for selecting
informative contexts. One option would be to use a heuristic, such as “the ﬁrst verb on the
left” and “the ﬁrst noun on the right.” ese will capture the triplets hliked,from,timei and
hsaved,from,himi, which indeed contain the essence of the preposition sense. In linguistic terms,
we say that this heuristic helps us capture the governor and the and object of the preposition.
By knowing the identify of the preposition, as well as its governor and objects, humans can in
many cases infer the sense of the preposition, using reasoning processes about the ﬁne-grained
semantics of the words. e heuristic for extracting the object and governor requires the use of
a POS-tagger in order to identify the nouns and verbs. It is also somewhat brittle—it is not
hard to imagine cases in which it fails. We could reﬁne the heuristic with more rules, but a more
robust approach would be to use a dependency parser: the governor and object information is
easily readable from the syntactic tree, reducing the need for complex heuristics:
he  liked   the round  object  from  the  very  frst time   he saw  it
nsubj
nsubj dobj
det
amod
amod amod
det
rcmod
dobj
prep
pobj
root
Of course, the parser used for producing the tree may be wrong too. For robustness, we may look
at both the governor and object extracted from the parser and the governor and object extracted
using the heuristic, and use all four as sources for features (i.e., parse_gov=X, parse_obj=Y,
⁵Earlier sense inventories and annotated corpora for the task are also available. See, for example, Litkowski and Hargraves
[2005, 2007], Srikumar and Roth [2013a].

84
7. CASE STUDIES OF NLP FEATURES
heur_gov=Z, heur_obj=W), letting the learning process decide which of the sources is more
reliable and how to balance between them.
After extracting the governor and the object (and perhaps also words adjacent to the gov-
ernor and the object), we can use them as the basis for further feature extraction. For each of the
items, we could extract the following pieces of information:
• the actual surface form of the word;
• the lemma of the word;
• the part-of-speech of the word;
• preﬁxes and suﬃxes of the word (indicating adjectives of degree, number, order, etc such as
ultra-, poly-, post-, as well as some distinctions between agentive and non-agentive verbs);
and
• word cluster or distributional vector of the word.
If we allow the use of external lexical resources and don’t mind greatly enlarging the feature space,
Hovy et al. [2010] found the use of WordNet-based features to be helpful as well. For each of the
governor and the object, we could extract many WordNet indicators, such as:
• does the word have a WordNet entry?;
• hypernyms of the ﬁrst synset of the word;
• hypernyms of all synsets of the word;
• synonyms for ﬁrst synset of the word;
• synonyms for all synsets of the word;
• all terms in the deﬁnition of the word;
• the super-sense of the word (super-senses, also called lexicographer-ﬁles in the WordNet
jargon, are relatively high levels in the WordNet hierarchy, indicating concepts such as being
an animal, being a body part, being an emotion, being food, etc.); and
• various other indicators.
is process may result in tens or over a hundred of core features for each preposition
instance, i.e., hyper_1st_syn_gov=a, hyper_all_syn_gov=a, hyper_all_syn_gov=b,
hyper_all_syn_gov=c,
...,
hyper_1st_syn_obj=x,
hyper_all_syn_obj=y,
...,
term_in_def_gov=q, term_in_def_gov=w, etc.
See the work of Hovy et al. [2010] for the ﬁner details.

7.7. RELATION BETWEEN WORDS IN CONTEXT: ARC-FACTORED PARSING
85
e preposition-sense disambiguation task is an example of a high-level semantic classiﬁ-
cation problem, for which we need a set of features that cannot be readily inferred from the surface
forms, and can beneﬁt from linguistic pre-processing (i.e., POS-tagging and syntactic parsing)
as well as from selected pieces of information from manually curated semantic lexicons.
7.7
RELATION BETWEEN WORDS IN CONTEXT:
ARC-FACTORED PARSING
In the dependency parsing task, we are given a sentence and need to return a syntactic dependency
tree over it, such as the tree in Figure 7.1. Each word is assigned a parent word, except for the
main word of the sentence whose parent is a special *ROOT* symbol.
the   boy  with   the  black   shirt   opened  the  door   with   a  key
det
prep
prep
amod
det
pobj
nsubj
root
dobj
pobj
det
det
Figure 7.1: Dependency tree.
For more information on the dependency parsing task, its linguistic foundations and ap-
proaches to its solution, see the book by Kübler et al. [2008].
One approach to modeling the task is the arc-factored approach [McDonald et al., 2005],
where each of the possible n2 word-word relations (arcs) is assigned a score independent of the
others, and then we search for the valid tree with the maximal overall score. e score assignment
is made by a trained scoring function AS.h; m; sent/, receiving a sentence as well as the
indices h and m of two words within it that are considered as candidates for attachment (h is
the index of the candidate head-word and m is the index of the candidate modiﬁer). Training the
scoring function such that it works well with the search procedure will be discussed in Chapter 19.
Here, we focus on the features used in the scoring function.
Assume a sentence of n words w1Wn and their corresponding parts-of-speech p1Wn,
sent D .w1; w2; : : : ; wn; p1; p2; : : : ; pn/ When looking at an arc between words wh and wm,
we can make use of the following pieces of information.
We begin with the usual suspects:

86
7. CASE STUDIES OF NLP FEATURES
• e word form (and POS-tag) of the head word.
• e word form (and POS-tag) of the modiﬁer word. (Some words are less likely to be heads
or modiﬁers, regardless to who they are connected to. For example, determiners (“the,” “a”)
are often modiﬁers, and are never heads.)
• Words (POS-tags) in a window of two words to each side of the head word, including the
relative positions.
• Words (POS-tags) in a window of two words to each side of the modiﬁer word, including
the relative positions. (e window information is needed to give some context to the word.
Words behave diﬀerently in diﬀerent contexts.)
We use the parts-of-speech as well as the word forms themselves. Word-forms give us very speciﬁc
information (for example, that cake is a good candidate object for ate), while the parts-of-speech
provide lower level syntactic information that is more generalizable (for example, that determiners
and adjectives are good modiﬁers for nouns, and that nouns are good modiﬁers for verbs). As the
training corpora for dependency-trees are usually rather limited in size, it could be a good idea
to supplement or replace the words using distributional information, in the form of word clusters
or pre-trained word embeddings, that will capture generalizations across similar words, also for
words that may not have a good coverage in the training data.
We do not look at preﬁxes and suﬃxes of words, because these are not directly relevant
to the parsing task. While the aﬃxes of words indeed carry important syntactic information (is
the word likely to be a noun? a past verb?), this information is already available to us in through
the POS-tags. If we were parsing without access to POS-tag features (for example, if the parser
was in charge for both parsing and POS-tag assignments), it would be wise to include the suﬃx
information as well.
Of course, if we use a linear classiﬁer, we need to take care also of feature combinations,
with features such as “head candidate word is X and modiﬁer word candidate is Y and head
part-of-speech is Z and the word before the modiﬁer word is W.” Indeed, it is common for
dependency parsers based on linear models to have hundreds of such feature combinations.
In addition to these usual suspects, it is also informative to consider the following.
• e distance between words wh and wm in the sentence, dist D jh � mj. Some distances are
more likely to stand in a dependency relation than others.
• e direction between the words. In English, assume wm is a determiner (“the”) and wh is
a noun (“boy”), it is quite likely that there will be an arc between them if m < h and very
unlikely if m > h.
• All the words (POS-tags) of words that appear between the head and the modiﬁer words
in the sentence. is information is useful as it hints at possible competing attachments.

7.7. RELATION BETWEEN WORDS IN CONTEXT: ARC-FACTORED PARSING
87
For example, a determiner at wm is likely to modify a noun at wh>m, but not if a word wk
(m < k < h) between them is also a determiner. Note that the number of words between the
head and the modiﬁer is potentially unbounded (and also changes from instance to instance)
and so we need a way to encode a variable number of features, hinting at a bag-of-words
approach.

89
C H A P T E R
8
From Textual Features to Inputs
In Chapters 2 and 4 we discussed classiﬁers that accept feature vectors as input, without getting
into much details about the contents of these vectors. In Chapters 6 and 7 we discussed the
sources of information which can serve as the core features for various natural language tasks. In
this chapter, we discuss the details of going from a list of core-features to a feature-vector that
can serve as an input to a classiﬁer.
To recall, in Chapters 2 and 4 we presented machine-trainable models (either linear, log-
linear, or multi-layer perceptron). e models are parameterized functions f .x/ that take as input
a din dimensional vector x and produce a dout dimensional output vector. e function is often
used as a classiﬁer, assigning the input x a degree of membership in one or more of dout classes. e
function can be either simple (for a linear model) or more complex (for arbitrary neural networks).
In this chapter we focus on the input, x.
8.1
ENCODING CATEGORICAL FEATURES
When dealing with natural language, most of the features represent discrete, categorical features
such as words, letters, and part-of-speech tags. How do we encode such categorical data in a way
which is amenable for use by a statistical classiﬁer? We discuss two options, one-hot encodings and
dense embedding vectors, as well as the trade-oﬀs and the relations between them.
8.1.1
ONE-HOT ENCODINGS
In linear and log-linear models of the form f .x/ D xW C b, it is common to think in term of
indicator functions, and assign a unique dimension for each possible feature. For example, when
considering a bag-of-words representation over a vocabulary of 40,000 items, x will be a 40,000-
dimensional vector, where dimension number 23,227 (say) corresponds to the word dog, and
dimension number 12,425 corresponds to the word cat. A document of 20 words will be repre-
sented by a very sparse 40,000-dimensional vector in which at most 20 dimensions have non-zero
values. Correspondingly, the matrix W will have 40,000 rows, each corresponding to a particular
vocabulary word. When the core features are the words in a 5 words window surrounding and
including a target word (2 words to each side) with positional information, and a vocabulary of
40,000 words (that is, features of the form word-2=dog or word0=sofa), x will be a 200,000-
dimensional vector with 5 non-zero entries, with dimension number 19,234 corresponding to
(say) word-2=dog and dimension number 143,167 corresponding to word0=sofa. is is called
a one-hot encoding, as each dimension corresponds to a unique feature, and the resulting feature

90
8. FROM TEXTUAL FEATURES TO INPUTS
vector can be thought of as a combination of high-dimensional indicator vectors in which a single
dimension has a value of 1 and all others have a value of 0.
8.1.2
DENSE ENCODINGS (FEATURE EMBEDDINGS)
Perhaps the biggest conceptual jump when moving from sparse-input linear models to deeper
nonlinear models is to stop representing each feature as a unique dimension in a one-hot repre-
sentation, and representing them instead as dense vectors. at is, each core feature is embedded
into a d dimensional space, and represented as a vector in that space.¹ e dimension d is usu-
ally much smaller than the number of features, i.e., each item in a vocabulary of 40,000 items
(encoded as 40,000-dimensional one-hot vectors) can be represented as 100 or 200 dimensional
vector. e embeddings (the vector representation of each core feature) are treated as parameters
of the network, and are trained like the other parameters of the function f . Figure 8.1 shows the
two approaches to feature representation.
e general structure for an NLP classiﬁcation system based on a feed-forward neural net-
work is thus.
1. Extract a set of core linguistic features f1; : : : ; fk that are relevant for predicting the output
class.
2. For each feature fi of interest, retrieve the corresponding vector v.fi/.
3. Combine the vectors (either by concatenation, summation, or a combination of both) into
an input vector x.
4. Feed x into a nonlinear classiﬁer (feed-forward neural network).
e biggest change in the input when moving from linear to deeper classiﬁer is, then,
the move from sparse representations in which each feature is its own dimension, to a dense
representation in which each feature is mapped to a vector. Another diﬀerence is that we mostly
need to extract only core features and not feature combinations. We will elaborate on both these
changes brieﬂy.
8.1.3
DENSE VECTORS VS. ONE-HOT REPRESENTATIONS
What are the beneﬁts of representing our features as vectors instead of as unique IDs? Should we
always represent features as dense vectors? Let’s consider the two kinds of representations.
¹Diﬀerent feature types may be embedded into diﬀerent spaces. For example, one may represent word features using 100
dimensions, and part-of-speech features using 20 dimensions.

8.1. ENCODING CATEGORICAL FEATURES
91
x = (0, ...., 0, 1, 0, ...., 0, 1, 0 ..... 0, 1, 0, .... , 0 , 1, 0 ,0, 1, 0, .... , 0, 0, 0 , .... , 0)
w=dog
w=chair&pt=DET
w=dog&pw=the
w=dog&pt=DET
pt=DET
pt=NOUN
pw=the
x = (0.26, 0.25, -0.39, -0.07, 0.13, -0.17) (-0.43, -0.37, -0.12, 0.13, -0.11, 0.34) (-0.04, 0.50, 0.04, 0.44)
(-0.37, -0.23, 0.33, 0.38, -0.02, -0.37)
(-0.21, -0.11, -0.10, 0.07, 0.37, 0.15)
(0.26, 0.25, -0.39, -0.07, 0.13, -0.17)
 
          …
 
          …
(-0.43, -0.37, -0.12, 0.13, -0.11, 0.34)
 
          …
 
          …
(-0.32, 0.43, -0.14, 0.50, -0.13, -0.42)
 
          …
 
          …
(0.06, -0.21, -0.38, -0.28, -0.16, -0.44)
 
          …
chair
on
dog
the
mouth
gone
NOUN
VERB
DET
ADJ
PREP
ADV
(0.16, 0.03, -0.17, -0.13)
(0.41, 0.08, 0.44, 0.02)
 
…
 
…
(-0.04, 0.50, 0.04, 0.44)
(-0.01, -0.35, -0.27, 0.20)
(-0.26, 0.28, -0.34, -0.02)
 
…
 
…
(0.02, -0.17, 0.46, -0.08)
 
…
Word Embeddings
POS Embeddings
(a)
(b)
Figure8.1: Sparsevs.densefeaturerepresentations. Two encodings of the information: current word
is “dog;” previous word is “the;” previous pos-tag is “DET.” (a) Sparse feature vector. Each dimension
represents a feature. Feature combinations receive their own dimensions. Feature values are binary.
Dimensionality is very high. (b) Dense, embeddings-based feature vector. Each core feature is repre-
sented as a vector. Each feature corresponds to several input vector entries. No explicit encoding of
feature combinations. Dimensionality is low. e feature-to-vector mappings come from an embed-
ding table.
One Hot Each feature is its own dimension.
• Dimensionality of one-hot vector is same as number of distinct features.
• Features are completely independent from one another. e feature “word is ‘dog’ ” is
as dissimilar to “word is ‘thinking’ ” than it is to “word is ‘cat’ ”.

92
8. FROM TEXTUAL FEATURES TO INPUTS
Dense Each feature is a d-dimensional vector.
• Dimensionality of vector is d.
• Model training will cause similar features to have similar vectors—information is
shared between similar features.
One beneﬁt of using dense and low-dimensional vectors is computational: the majority of neural
network toolkits do not play well with very high-dimensional, sparse vectors. However, this is
just a technical obstacle, which can be resolved with some engineering eﬀort.
e main beneﬁt of the dense representations is in generalization power: if we believe some
features may provide similar clues, it is worthwhile to provide a representation that is able to cap-
ture these similarities. For example, assume we have observed the word dog many times during
training, but only observed the word cat a handful of times, or not at all. If each of the words
is associated with its own dimension, occurrences of dog will not tell us anything about the oc-
currences of cat. However, in the dense vectors representation the learned vector for dog may be
similar to the learned vector for cat, allowing the model to share statistical strength between the
two events. is argument assumes that we saw enough occurrences of the word cat such that
its vector will be similar to that of dog, or otherwise that “good” vectors are somehow given to
us. Such “good” word-vectors (also called pre-trained embeddings) can be obtained from a large
corpus of text through algorithms that make use of the distributional hypothesis. Such algorithms
are discussed in more depth in Chapter 10.
In cases where we have relatively few distinct features in the category, and we believe there
are no correlations between the diﬀerent features, we may use the one-hot representation. How-
ever, if we believe there are going to be correlations between the diﬀerent features in the group
(for example, for part-of-speech tags, we may believe that the diﬀerent verb inﬂections VB and
VBZ may behave similarly as far as our task is concerned) it may be worthwhile to let the net-
work ﬁgure out the correlations and gain some statistical strength by sharing the parameters. It
may be the case that under some circumstances, when the feature space is relatively small and
the training data is plentiful, or when we do not wish to share statistical information between
distinct words, there are gains to be made from using the one-hot representations. However, this
is still an open research question, and there is no strong evidence to either side. e majority of
work (pioneered by Chen and Manning [2014], Collobert and Weston [2008], Collobert et al.
[2011]) advocate the use of dense, trainable embedding vectors for all features. For work using
neural network architecture with sparse vector encodings, see Johnson and Zhang [2015].
8.2
COMBINING DENSE VECTORS
Each feature corresponds to a dense vector, and the diﬀerent vectors need to be combined some-
how. e prominent options are concatenation, summation (or averaging), and combinations of
the two.

8.2. COMBINING DENSE VECTORS
93
8.2.1
WINDOW-BASED FEATURES
Consider the case of encoding a window of size k words to each side of a focus word at position
i. Assume k D 2; we need to encode the words at positions i � 2, i � 1, i C 1 and i C 2. As-
sume the window items are the words a, b, c, and d, and let a,b,c and d be the corresponding
word vectors. If we do not care about the relative positions of the words within the window, we
will encode the window as a sum: a C b C c C d. If we do care about the relative positions, we
rather use concatenation: ŒaI bI cI d�. Here, even though a word will have the same vector regard-
less of its position within the window, the word’s position is reﬂected by its position within the
concatenation.²
We may not care much about the order, but would want to consider words further away
from the context word less than words that are closer to it. is can be encoded as a weighted
sum, i.e., 1
2a C b C c C 1
2d.
ese encodings can be mixed and matched. Assume we care if the feature occurs before
or after the focus word, but do not care about the distance as long as it is within the window. is
can be encoded using a combination of summation and concatenation: Œ.a C b/I .c C d/�.
A note on notation
When describing network layers that get concatenated vectors x, y, and z as
input, some authors use explicit concatenation (ŒxI yI z�W C b) while others use an aﬃne trans-
formation (xU C yV C zW C b/. If the weight matrices U , V , W in the aﬃne transformation
are diﬀerent³ than one another, the two notations are equivalent.
8.2.2
VARIABLE NUMBER OF FEATURES: CONTINUOUS BAG OF WORDS
Feed-forward networks assume a ﬁxed dimensional input. is can easily accommodate the case
of a feature-extraction function that extracts a ﬁxed number of features: each feature is represented
as a vector, and the vectors are concatenated. is way, each region of the resulting input vector
corresponds to a diﬀerent feature. However, in some cases the number of features is not known in
advance (for example, in document classiﬁcation it is common that each word in the sentence is
a feature). We thus need to represent an unbounded number of features using a ﬁxed size vector.
One way of achieving this is through a so-called continuous bag of words (CBOW) representation
[Mikolov et al., 2013b]. e CBOW is very similar to the traditional bag-of-words representation
in which we discard order information, and works by either summing or averaging the embedding
²Alternatively, we could have a separate embedding for each word/position pair, i.e., a1 and a�2 will represent the word a
when it appears in relative positions C1 and �2, respectively. Following this approach, we could then use a sum and still be
sensitive to position information: a�2 C b�1 C cC1 C dC2. is approach will not share information between instances
of words when they appear in diﬀerent positions, and may be harder to use with externally trained word vectors.
³e matrices should be diﬀerent in the sense that a change to one will not be reﬂected in the others. It is OK for the matrices
to happen to share the same values, of course.

94
8. FROM TEXTUAL FEATURES TO INPUTS
vectors of the corresponding features:⁴
CBOW.f1; : : : ; fk/ D 1
k
k
X
iD1
v.fi/:
(8.1)
A simple variation on the CBOW representation is weighted CBOW, in which diﬀerent vectors
receive diﬀerent weights:
WCBOW.f1; : : : ; fk/ D
1
Pk
iD1 ai
k
X
iD1
aiv.fi/:
(8.2)
Here, each feature fi has an associated weight ai, indicating the relative importance of the feature.
For example, in a document classiﬁcation task, a feature fi may correspond to a word in the
document, and the associated weight ai could be the word’s TF-IDF score.
8.3
RELATION BETWEEN ONE-HOT AND DENSE
VECTORS
Representing features as dense vectors is an integral part of the neural network framework, and,
consequentially, the diﬀerences between using sparse and dense feature representations are subtler
than they may appear at ﬁrst. In fact, using sparse, one-hot vectors as input when training a neural
network amounts to dedicating the ﬁrst layer of the network to learning a dense embedding vector
for each feature based on the training data.
When using dense vectors, each categorical feature value fi is mapped to a dense, d-
dimensional vector v.fi/. is mapping is performed through the use of an embedding layer or a
lookup layer. Consider a vocabulary of jV j words, each embedded as a d dimensional vector. e
collection of vectors can then be thought of as a jV j � d embedding matrix E in which each row
corresponds to an embedded feature. Let fi be the one-hot representation of feature fi, that is, a
jV j-dimensional vector, which is all zeros except for one index, corresponding to the value of the
ith feature, in which the value is 1. e multiplication fiE will then “select” the corresponding
row of E. us, v.fi/ can be deﬁned in terms of E and fi:
v.fi/ D fiE:
(8.3)
And, similarly,
CBOW.f1; : : : ; fk/ D
k
X
iD1
.fiE/ D
 k
X
iD1
fi
!
E:
(8.4)
⁴Note that if the v.fi/s were one-hot vectors rather than dense feature representations, the CBOW (Equation (8.1)) and
WCBOW (Equation (8.2)) would reduce to the traditional (weighted) bag-of-words representations, which is in turn equiv-
alent to a sparse feature-vector representation in which each binary indicator feature corresponds to a unique “word.”

8.4. ODDS AND ENDS
95
e input to the network is then considered to be a collection of one-hot vectors. While this is
elegant and well-deﬁned mathematically, an eﬃcient implementation typically involves a hash-
based data structure mapping features to their corresponding embedding vectors, without going
through the one-hot representation.
Consider a network which uses a “traditional” sparse representation for its input vectors,
and no embedding layer. Assuming the set of all available features is V and we have k “on” features
f1; : : : ; fk, fi 2 V , the network’s input is:
x D
k
X
iD1
fi
x 2 NjV j
C
(8.5)
and so the ﬁrst layer (ignoring the nonlinear activation) is:
xW C b D
 k
X
iD1
fi
!
W C b
(8.6)
W 2 RjV j�d;
b 2 Rd:
is layer selects rows of W corresponding to the input features in x and sums them, then adding
a bias term. is is very similar to an embedding layer that produces a CBOW representation
over the features, where the matrix W acts as the embedding matrix. e main diﬀerence is the
introduction of the bias vector b, and the fact that the embedding layer typically does not undergo
a nonlinear activation but rather is passed on directly to the ﬁrst layer. Another diﬀerence is that
this scenario forces each feature to receive a separate vector (row in W ) while the embedding layer
provides more ﬂexibility, allowing for example for the features “next word is dog” and “previous
word is dog” to share the same vector. However, these diﬀerences are small and subtle. When it
comes to multi-layer feed-forward networks, the diﬀerence between dense and sparse inputs is
smaller than it may seem at ﬁrst sight.
8.4
ODDS AND ENDS
8.4.1
DISTANCE AND POSITION FEATURES
e linear distance in between two words in a sentence may serve as an informative feature. For
example, in an event extraction task⁵ we may be given a trigger word and a candidate argument
word, and asked to predict if the argument word is indeed an argument of the trigger. Similarly,
in a coreference-resolution task (deciding if which of the previously mentioned entities, if at all,
a pronoun such as he or she refers to), we may be given a pair of (pronoun, candidate word) and
asked to predict if they co-refer or not. e distance (or relative position) between the trigger
⁵e event extraction task involves identiﬁcation of events from a predeﬁned set of event types. For example, identiﬁcation of
“purchase” events or “terror-attack” events. Each event type can be triggered by various triggering words (commonly verbs),
and has several slots (arguments) that needs to be ﬁlled (i.e., Who purchased? What was purchased? At what amount?).

96
8. FROM TEXTUAL FEATURES TO INPUTS
and the argument is a strong signal for these prediction tasks. In the “traditional” NLP setup,
distances are usually encoded by binning the distances into several groups (i.e., 1, 2, 3, 4, 5–10,
10+) and associating each bin with a one-hot vector. In a neural architecture, where the input
vector is not composed of binary indicator features, it may seem natural to allocate a single input
entry to the distance feature, where the numeric value of that entry is the distance. However, this
approach is not taken in practice. Instead, distance features are encoded similarly to the other
feature types: each bin is associated with a d-dimensional vector, and these distance-embedding
vectors are then trained as regular parameters in the network [dos Santos et al., 2015, Nguyen
and Grishman, 2015, Zeng et al., 2014, Zhu et al., 2015a].
8.4.2
PADDING, UNKNOWN WORDS, AND WORD DROPOUT
Padding
In some cases your feature extractor will look for things that do not exist. For example,
when working with parse trees, you may have a feature looking for the left-most dependant of
a given word, but the word may not have any dependents to its left. Perhaps you are looking
at the word to positions to the right of the current one, but you are at the end of the sequence
and two positions to the right is past the end. What should be done in such situations? When
using a bag-of-features approach (i.e., summing) you could just leave the feature out of the sum.
When using a concatenation, you may provide a zero-vector in the place. ese two approaches
work ﬁne technically, but could be sub-optimal for your problem domain. Maybe knowing that
there is no left-modiﬁer is informative? e suggested solution would be to add a special symbol
(padding symbol) to your embedding vocabulary, and use the associated padding vector in these
cases. Depending on the problem at hand, you may want to use diﬀerent padding vectors for
diﬀerent situations (i.e., no-left-modiﬁer may be a diﬀerent vector than no-right-modiﬁer). Such
paddings are important for good prediction performance, and are commonly used. Unfortunately,
their use is not often reported, or quickly glossed over, in many research papers.
Unknown Words
Another case where a requested feature vector will not be available is for out-
of-vocabulary (OOV ) items. You are looking for the word on the left, observe the value variational,
but this word was not a part of your training vocabulary, so you don’t have an embedding vector
for it. is case is diﬀerent from the padding case, because the item is there, but you just don’t
know it. e solution is similar, however, reserve a special symbol, U, representing an unknown
token, for use in such cases. Again, you may or may not want to use diﬀerent unknown symbols
for diﬀerent vocabularies. In any case, it is advised to not share the padding and the unknown
vectors, as they reﬂect two very diﬀerent conditions.
Word Signatures
Another technique for dealing with unknown words is backing-oﬀ from the
word forms to word signatures. Using the U symbol for unknown words is essentially backing-
oﬀ from all unknown words to the same signature. But, depending on the task one is trying to
solve, one may come up with more ﬁne-grained strategies. For example, we may replace unknown
words that end with ing with an *__ing* symbol, words that end with ed with an *__ed* sym-

8.4. ODDS AND ENDS
97
bol, words that start with un with an *un__* symbol, numbers with a *NUM* symbol, and so
on. e list of mappings is hand-crafted to reﬂect informative backing-oﬀ patterns. is approach
is often used in practice, but rarely reported in deep-learning papers. While there are approaches
that allow to automatically learn such backing-oﬀ behavior as part of the model training without
needing to manually deﬁne the backing-oﬀ patterns (see discussion on sub-word units in Sec-
tion 10.5.5), they are in many cases an overkill and hard-coding the patterns is as eﬀective and
more computationally eﬃcient.
Word Dropout
Reserving a special embedding vector for unknown words is not enough—if all
the features in the training set have their own embedding vectors, the unknown-word condition
will not be observed in training: the associated vector will not receive any updates, and the model
will not be tuned to handle the unknown condition. is is equivalent to just using a random
vector when an unknown word is encountered at test time. e model needs to be exposed to the
unknown-word condition during training. A possible solution would be to replace all (or some) of
the features with a low frequency in the training with the unknown symbol (i.e., pre-process the
data, replacing words with a frequency below a threshold with *unknown*). is solution works,
but has the disadvantage of losing some training data—these rare words will not receive any signal.
A better solution is the use of word-dropout: when extracting features in training, randomly replace
words with the unknown symbol. e replacement should be based on the word’s frequency: less
frequent words will be more likely to be replaced by the unknown symbol than frequent ones.
e random replacement should be decided on runtime—a word that was dropped once may or
may not be dropped when it is encountered again (say, in diﬀerent iterations over the training
data). ere is no established formula for deciding on the word dropout rate. Works in my group
use
˛
#.w/C˛, where ˛ is a parameter for controlling the aggressiveness of the dropout [Kiperwasser
and Goldberg, 2016b].
Word Dropout as Regularization
Besides better adaptation to unknown words, word dropout
may also be beneﬁcial for preventing overﬁtting and improving robustness by not letting the model
rely too much on any single word being present [Iyyer et al., 2015]. When used this way, word
dropout should be applied frequently also to frequent words. Indeed, the suggestion of Iyyer et al.
[2015] is to drop word instances according to a Bernoulli trial with probability p, regardless of
their frequency. When word dropout is a applied as a regularizer, you may not want to replace
the dropped words with the unknown symbol in some circumstances. For example, when the
feature representation is a bag-of-words over the document and more than a quarter of the words
are dropped, replacing the dropped words with the unknown word symbol will create a feature
representation that is not likely to occur at test time, where such a large concentration of unknown
words is unlikely.

98
8. FROM TEXTUAL FEATURES TO INPUTS
8.4.3
FEATURE COMBINATIONS
Note that the feature extraction stage in the neural network settings deals only with extraction of
core features. is is in contrast to the traditional linear-model-based NLP systems in which the
feature designer had to manually specify not only the core features of interest but also interactions
between them (e.g., introducing not only a feature stating “word is X” and a feature stating “tag
is Y” but also combined feature stating “word is X and tag is Y” or sometimes even “word is X,
tag is Y and previous word is Z”). e combination features are crucial in linear models because
they introduce more dimensions to the input, transforming it into a space where the data-points
are closer to being linearly separable. On the other hand, the space of possible combinations is
very large, and the feature designer has to spend a lot of time coming up with an eﬀective set of
feature combinations. One of the promises of the nonlinear neural network models is that one
needs to deﬁne only the core features. e nonlinearity of the classiﬁer, as deﬁned by the network
structure, is expected to take care of ﬁnding the indicative feature combinations, alleviating the
need for feature combination engineering.
As discussed in Section 3.3, kernel methods [Shawe-Taylor and Cristianini, 2004], and in
particular polynomial kernels [Kudo and Matsumoto, 2003], also allow the feature designer to
specify only core features, leaving the feature combination aspect to the learning algorithm. In
contrast to neural network models, kernels methods are convex, admitting exact solutions to the
optimization problem. However, the computational complexity of classiﬁcation in kernel methods
scales linearly with the size of the training data, making them too slow for most practical purposes,
and not suitable for training with large datasets. On the other hand, the computational complexity
of classiﬁcation using neural networks scales linearly with the size of the network, regardless of
the training data size.⁶
8.4.4
VECTOR SHARING
Consider a case where you have a few features that share the same vocabulary. For example, when
assigning a part-of-speech to a given word, we may have a set of features considering the previous
word, and a set of features considering the next word. When building the input to the classiﬁer,
we will concatenate the vector representation of the previous word to the vector representation
of the next word. e classiﬁer will then be able to distinguish the two diﬀerent indicators, and
treat them diﬀerently. But should the two features share the same vectors? Should the vector for
“dog:previous-word” be the same as the vector of “dog:next-word”? Or should we assign them
two distinct vectors? is, again, is mostly an empirical question. If you believe words behave
diﬀerently when they appear in diﬀerent positions (e.g., word X behaves like word Y when in the
previous position, but X behaves like Z when in the next position) then it may be a good idea to
use two diﬀerent vocabularies and assign a diﬀerent set of vectors for each feature type. However,
⁶Of course, one still needs to go over the entire dataset when training, and sometimes go over the dataset several times. is
makes training time scale linearly with the dataset size. However, each example, in either training or test time, is processed
in a constant time (for a given network). is is in contrast to a kernel classiﬁer, in which each example is processed in a time
that scales linearly with the dataset size.

8.4. ODDS AND ENDS
99
if you believe the words behave similarly in both locations, then something may be gained by
using a shared vocabulary for both feature types.
8.4.5
DIMENSIONALITY
How many dimensions should we allocate for each feature? Unfortunately, there are no theoret-
ical bounds or even established best-practices in this space. Clearly, the dimensionality should
grow with the number of the members in the class (you probably want to assign more dimensions
to word embeddings than to part-of-speech embeddings) but how much is enough? In current re-
search, the dimensionality of word-embedding vectors range between about 50 to a few hundreds,
and, in some extreme cases, thousands. Since the dimensionality of the vectors has a direct eﬀect
on memory requirements and processing time, a good rule of thumb would be to experiment with
a few diﬀerent sizes, and choose a good trade-oﬀ between speed and task accuracy.
8.4.6
EMBEDDINGS VOCABULARY
What does it mean to associate an embedding vector for every word? Clearly, we cannot associate
one with all possible values, and need to restrict ourselves to every value from a ﬁnite vocabulary.
is vocabulary is usually based on the training set, or, if we use pre-trained embeddings, on the
training on which the pre-trained embeddings were trained. It is recommended that the vocab-
ulary will also include a designated U symbol, associating a special vector to all words that are
not in the vocabulary.
8.4.7
NETWORK’S OUTPUT
For multi-class classiﬁcation problems with k classes, the network’s output is a k-dimensional
vector in which every dimension represents the strength of a particular output class. at is,
the output remains as in the traditional linear models—scalar scores to items in a discrete set.
However, as we saw in Chapter 4, there is a d � k matrix associated with the output layer. e
columns of this matrix can be thought of as d dimensional embeddings of the output classes. e
vector similarities between the vector representations of the k classes indicate the model’s learned
similarities between the output classes.
Historical Note
Representing words as dense vectors for input to a neural network was popu-
larized by Bengio et al. [2003] in the context of neural language modeling. It was introduced to
NLP tasks in the pioneering work of Collobert, Weston, and colleagues [Collobert and Weston,
2008, Collobert et al., 2011].⁷ Using embeddings for representing not only words but arbitrary
features was popularized following Chen and Manning [2014].
⁷While the work by Bengio, Collobert, Weston, and colleagues popularized the approaches, they were not the ﬁrst to use them.
Earlier authors that use dense continuous-space vectors for representing word inputs to neural networks include Lee et al.
[1992] and Forcada and Ñeco [1997]. Similarly, continuous-space language models were used for machine-translation already
by Schwenk et al. [2006].

100
8. FROM TEXTUAL FEATURES TO INPUTS
8.5
EXAMPLE: PART-OF-SPEECH TAGGING
e POS-tagging task (Section 7.4) we are given a sentence of n words w1; w2; : : : ; wn, and a
word position i, and need to predict the tag of wi. Assuming we tag the words from left to right,
we can also look at the previous tag predictions, Op1; : : : ; Opi�1. A list of concrete core features is
given in Section 7.4, here we discuss encoding them as an input vector. We need a feature function
x D �.s; i/, getting a sentence s comprised of words and previous tagging decisions and an input
position i, and returning a feature vector x. We assume a function suf .w; k/ that returns the
k-letter suﬃx of word w, and similarly pref .w; k/ that returns the preﬁx.
We begin with the three boolean questions: word-is-capitalized, word-contains-hyphen and
word-contains-digit. e most natural way to encode these is to associate each of them with its
own dimension, with a value of 1 if the condition holds for word wi and 0 otherwise.⁸ We will
put these in a 3-dimensional vector associated with word i, ci.
Next, we need to encode words, preﬁxes, suﬃxes, and part-of-speech tags in various po-
sitions in the window. We associate each word wi with an embedding vector vw.wi/ 2 Rdw.
Similarly, we associate each two-letter suﬃx suf .wi; 2/ with an embedding vector vs.suf .wi; 2//
and similarly for three-letter suﬃxes vs.suf .wi; 3//, vs.�/ 2 Rds. Preﬁxes get the same treatment,
with embeddings vp.�/ 2 Rdp. Finally, each POS-tag receives an embedding vt.pi/ 2 Rdt. Each
position i can be associated with a vector vi of the relevant word information (word form, preﬁxes,
suﬃxes, Boolean features):
vi D ŒciI vw.wi/I vs.suf .wi; 2//I vs.suf .wi; 3//I vp.pref .wi; 2//I vp.pref .wi; 3//�
vi 2 R3CdwC2dsC2dp:
Our input vector x is then a concatenation of the following vectors:
x D �.s; i/ D Œvi�2I vi�1I viI viC1I viC2I vt.pi�1/I vt.pi�2/�
x 2 R5.3CdwC2dsC2dp/C2dt :
Discussion
Note that the words in each position share the same embedding vectors—when
creating vi and vi�1 we read from the same embedding tables—and that a vector vi does not
“know” its relative position. However, because of the vector concatenation, the vector x “knows”
that which relative position each v is associated with because of its relative position within x. is
allows us to share some information between the words in the diﬀerent positions (the vector of
the word dog will receive updates when the word is at relative position �2 as well as when it is in
relative position C1), but will also be treated diﬀerently by the model when it appears in diﬀerent
relative positions, because it will be multiplied by a diﬀerent part of the matrix in the ﬁrst layer
of the network.
⁸A value of -1 for the negative condition is also a possible choice.

8.6. EXAMPLE: ARC-FACTORED PARSING
101
An alternative approach would be to associate each word-and-position pair with its
own embedding, i.e., instead of a single table vw we will have ﬁve embedding tables
vw�2; vw�1; vw0; vwC1; vwC2, and use appropriate one for each relative word position. is ap-
proach will substantially increase the number of parameters in the model (we will need to learn
ﬁve-times as many embedding vectors), and will not allow sharing between the diﬀerent words.
It will also be somewhat more wasteful in terms of computation, as in the previous approach we
could compute the vector vi for each word in the sentence once, and then re-use them when
looking at diﬀerent positions i, while in the alternative approach the vectors vi will need to be
re-computed for each position i we are looking at. Finally, it will be harder to use pre-trained
word vectors, because the pre-trained vectors do not have location information attached to them.
However, this alternative approach would allow us to treat each word position completely inde-
pendently from the others, if we wanted to.⁹
Another point to consider is capitalization. Should the words Dog and dog receive diﬀerent
embeddings? While capitalization is an important clue for tagging, in our case the capitalization
status of word wi is already encoded in the boolean features ci. It is thus advisable to lower-case
all words in the vocabulary before creating or querying the embedding table.
Finally, the preﬁx-2 and preﬁx-3 features are redundant with each other (one contains the
other) and similarly for the suﬃxes. Do we really need both? Can we make them share informa-
tion? Indeed, we could use letter embeddings instead of suﬃx embeddings, and replace the two
suﬃx embeddings with a vector composed of the concatenation of the three last letters in the
word. In Section 16.2.1, we will see an alternative approach, that uses character-level recurrent
neural networks (RNNs) to capture preﬁx, suﬃx and various other properties of the word form.
8.6
EXAMPLE: ARC-FACTORED PARSING
In the Arc-Factored parsing task (Section 7.7) we are given a sentence of n words w1Wn and their
parts-of-speech p1Wn, and need to predict a parse tree. Here, we are concerned with the features
for scoring a single attachment decision between words wh and wm, where wh is the candidate
head-word and wm is the candidate modiﬁer word.
A list of concrete core features was given in Section 7.7, and here we discuss encoding them
as an input vector. We deﬁne a feature function x D �.h; m; sent/ receiving a sentence comprised
of word and POS-tags, and the positions of a head-word (h) and a modiﬁer-word (m).
First, we need to consider the head word, its POS-tag, and the words and POS-tags in a
ﬁve-word window around the head word (two words to each side). We associate each word w in
our vocabulary with an embedding vector vw.w/ 2 Rdw and similarly each part-of-speech tag p
with an embedding vector vt.p/ 2 Rdt. We then deﬁne the vector representation of a word at
position i to be vi D Œvw.wi/I vt.pi� 2 RdwCdt, the concatenation of the word and POS vector.
⁹Note that, mathematically, even that last beneﬁt is not a real beneﬁt—when used as an input to a neural network, the ﬁrst
layer could potentially learn to look only at certain dimensions of the embeddings when they are used at position �1 and at
diﬀerent dimensions when they are used at position C2, achieving the same separation as in the alternative approach. us,
the original approach is just as expressive as the alternative one, at least in theory.

102
8. FROM TEXTUAL FEATURES TO INPUTS
en, we associate the head word with a vector h representing the word within its context,
and associate the modiﬁer word with a similar vector m:
h D Œvh�2I vh�1I vhI vhC1I vhC2�
m D Œvm�2I vm�1I vmI vmC1I vmC2�:
is takes care of the elements in the ﬁrst block of features. Note that, like with the part-
of-speech tagging features, this encoding cares about the relative position of each of the context
words. If we didn’t care about the positions, we could have instead represented the head word
as h0 D ŒvhI .vh�2 C vh�1 C vhC1 C vhC2/�. is sums the context words into a bag-of-words,
losing their positional information, yet, concatenates the context and the focus words, retaining
the distinction between them.
We now turn to the distance and direction features. While we could assign the distance a
single dimension with the numeric distance value, it is common to bin the distance into k discrete
bins (say 1, 2, 3, 4–7, 8–10, 11+), and associate each bin with a dd-dimensional embedding. e
direction is a Boolean feature, and we represent it as its own dimension.¹⁰ We denote the vector
containing the binned distance embedding concatenated with the Boolean direction feature as d.
Finally, we need to represent the words and POS-tags between the head and the modiﬁer.
eir number is unbounded and varies between diﬀerent instances, so we cannot use concatena-
tion. Fortunately, we do not care about the relative positions of the intervening items, so we can
use a bag-of-words encoding. Concretely, we represent the between-context words as a vector c
deﬁned as the average of the words and POS between vectors:
c D
m
X
iDh
vi:
Note that this sum potentially captures also the number of elements between the head and
modiﬁer words, making the distance feature potentially redundant.
Our ﬁnal representation of an attachment decision to be scored, x is then encoded as the
concatenation of the various elements:
¹⁰While this encoding of the dimension is very natural, Pei et al. [2015] follow a diﬀerent approach in their parser. Perhaps
motivated by the importance of distance information, they chose to not mark it as an input feature, but instead to use two
diﬀerent scoring functions, one for scoring left-to-right arcs and another for scoring right-to-left arcs. is gives a lot of power
to the direction information, while substantially increasing the number of parameters in the model.

8.6. EXAMPLE: ARC-FACTORED PARSING
103
x D �.h; m; sent/ D ŒhI mI cI d�;
where:
vi D Œvw.wi/I vt.pi�
h D Œvh�2I vh�1I vhI vhC1I vhC2�
m D Œvm�2I vm�1I vmI vmC1I vmC2�
c D
m
X
iDh
vi
d D binned distance embeddings; direction indicator.
Note how we combine positional window-based features with bag-of-word features by simple
concatenation. e neural network layers on top of x can then infer transformation and feature
combinations between the elements in the diﬀerent windows, as well as between the diﬀerent
elements in the bag-of-words representation. e process of creating the representation x—the
embedding tables for the words, POS-tags and binned distances, as well as the diﬀerent concate-
nations and summations, is also part of the neural network. It is reﬂected in the computation-
graph construction, and its parameters are trained jointly with the network.
e features creation part of the network could be even more complex. For example, if we
had reasons to believe that the interactions between a word and its POS-tag, and the interactions
within a context window, are more important than the interactions across elements of diﬀerent
entities, we could have reﬂected that in the input encoding by creating further nonlinear transfor-
mations in the feature-encoding process, i.e., replacing vi with v0
i D g.viW v C bv/ and h with
h0 D g.Œv0
h�2I v0
h�1I v0
hI v0
hC1I v0
hC2�W h C bh/, and setting: x D Œh0I m0I cI d�.

105
C H A P T E R
9
Language Modeling
9.1
THE LANGUAGE MODELING TASK
Language modeling is the task of assigning a probability to sentences in a language (“what is the
probability of seeing the sentence the lazy dog barked loudly?”). Besides assigning a probability to
each sequence of words, the language models also assigns a probability for the likelihood of a
given word (or a sequence of words) to follow a sequence of words (“what is the probability of
seeing the word barked after the seeing sequence the lazy dog?”).¹
Perfect performance at the language modeling task, namely predicting the next word in a
sequence with a number of guesses that is the same as or lower than the number of guesses required
by a human participant, is an indication of human level intelligence,² and is unlikely to be achieved
in the near future. Even without achieving human-level performance, language modeling is a
crucial components in real-world applications such as machine-translation and automatic speech
recognition, where the system produces several translation or transcription hypotheses, which are
then scored by a language model. For these reasons, language modeling plays a central role in
natural-language processing, AI, and machine-learning research.
Formally, the task of language modeling is to assign a probability to any sequence of words
w1Wn, i.e., to estimate P.w1Wn/. Using the chain-rule of probability, this can be rewritten as:
P.w1Wn/ D P.w1/P.w2 j w1/P.w3 j w1W2/P.w4 j w1W3/ : : : P.wn j w1Wn�1/:
(9.1)
at is, a sequence of word-prediction tasks where each word is predicted conditioned on the
preceding words. While the task of modeling a single word based on its left context seem more
manageable than assigning a probability score to an entire sentence, the last term in the equation
¹Note that the ability to assign a probability for a word following a sequence of words p.wi1jw1; w2; : : : ; wi�1/ and the
ability to assign a probabilities to arbitrary sequences of words p.w1; w2; : : : ; wk/ are equivalent, as one can be derived from
the other. Assume we can model probabilities of sequences. en the conditional probability of a word can be expressed as a
fraction of two sequences:
p.wijw1; w2; : : : ; wi�1/ D p.w1; w2; : : : ; wi�1; wi/
p.w1; w2; : : : ; wi�1/
:
Alternatively, if we could model the conditional probability of a word following a sequence of words, we could use the chain
rule to express the probability of sequences as a product of conditional probabilities:
p.w1; : : : ; wk/ D p.w1j<s>/ � p.w2j<s>; w1/ � p.w3j<s>; w1; w2/ � � � � � p.wkj<s>; w1; : : : ; wk�1/;
where <s> is a special start-of-sequence symbol.
²Indeed, any question can be posed as a next-word-guessing task, e.g., the answer to question X is ___. Even without such
pathological cases, predicting the next word in the text requires knowledge of syntactic and semantic rules of the language, as
well as vast amounts of world knowledge.

106
9. LANGUAGE MODELING
still requires conditioning on n � 1 words, which is as hard as modeling an entire sentence. For
this reason, language models make use of the markov-assumption, stating that the future is inde-
pendent of the past given the present. More formally, a kth order markov-assumption assumes
that the next word in a sequence depends only on the last k words:
P.wiC1 j w1Wi/ � P.wiC1 j wi�kWi/:
Estimating the probability of the sentence then becomes
P.w1Wn/ �
n
Y
iD1
P.wi j wi�kWi�1/;
(9.2)
where w�kC1; : : : ; w0 are deﬁned to be special padding symbols.
Our task is then to accurately estimate P.wiC1 j wi�kWi/ given large amounts of text.
While the kth order markov assumption is clearly wrong for any k (sentences can have
arbitrarily long dependencies, as a simple example consider the strong dependence between the
ﬁrst word of the sentence being what and the last one being ?), it still produces strong language
modeling results for relatively small values of k, and was the dominant approach for language
modeling for decades. is chapter discusses kth order language models. In Chapter 14 we discuss
language modeling techniques that do not make the markov assumption.
9.2
EVALUATING LANGUAGE MODELS: PERPLEXITY
ere are several metrics for evaluating language modeling. e application-centric ones evaluate
them in the context of performing a higher-level task, for example by measuring the improvement
in translation quality when switching the language-modeling component in a translation system
from model A to model B.
A more intrinsic evaluation of language models is using perplexity over unseen sentences.
Perplexity is an information theoretic measurement of how well a probability model predicts a
sample. Low perplexity values indicate a better ﬁt. Given a text corpus of n words w1; : : : ; wn
(n can be in the millions) and a language model function LM assigning a probability to a word
based on its history, the perplexity of LM with respect to the corpus is:
2� 1
n
Pn
iD1 log2 LM.wijw1Wi�1/:
Good language models (i.e., reﬂective of real language usage) will assign high probabilities
to the events in the corpus, resulting in lower perplexity values.
e perplexity measure is a good indicator of the quality of a language model.³ Perplexities
are corpus speciﬁc—perplexities of two language models are only comparable with respect to the
same evaluation corpus.
³It is important to note, however, that in many cases improvement in perplexity scores do not transfer to improvement in
extrinsic, task-quality scores. In that sense, the perplexity measure is good for comparing diﬀerent language models in terms
of their ability to pick-up regularities in sequences, but is not a good measure for assessing progress in language understanding
or language-processing tasks.

9.3. TRADITIONAL APPROACHES TO LANGUAGE MODELING
107
9.3
TRADITIONAL APPROACHES TO LANGUAGE
MODELING
e traditional approach to language models assumes a k-order markov property, and model
P.wiC1 D mjw1Wi/ � P.wiC1 D mjwi�kWi/. e role of the language model then is to provide
good estimates Op.wiC1 D mjwi�kWi/. e estimates are usually derived from corpus counts. Let
#.wiWj / be the count of the sequence of words wiWj in a corpus. e maximum likelihood estimate
(MLE) of Op.wiC1 D mjwi�kWi/ is then:
OpMLE.wiC1 D mjwi�kWi/ D #.wi�kWiC1/
#.wi�kWi/ :
While eﬀective, this baseline approach has a big shortcoming: if the event wi�kWiC1 was never
observed in the corpus (#.wi�kWiC1/ D 0), the probability assigned to it will be 0, and this in turn
will result in a 0-probability assignment to the entire corpus because of the multiplicative nature
of the sentence probability calculation. A zero probability translates to an inﬁnite perplexity—
which is very bad. Zero events are quite common: consider a trigram language model, which
only conditions on 2 words, and a vocabulary of 10,000 words (which is rather small). ere are
10,0003 D 1012 possible word triplets—it is clear that many of them won’t be observed in training
corpora of, say, 1010 words. While many of these events don’t occur because they do not make
sense, many others just did not occur in the corpus.
One way of avoiding zero-probability events is using smoothing techniques, ensuring an al-
location of a (possibly small) probability mass to every possible event. e simplest example is
probably additive smoothing, also called add-˛ smoothing [Chen and Goodman, 1999, Goodman,
2001, Lidstone, 1920]. It assumes each event occurred at least ˛ times in addition to its observa-
tions in the corpus. e estimate then becomes
Opadd-˛.wiC1 D mjwi�kWi/ D #.wi�kWiC1/ C ˛
#.wi�kWi/ C ˛jV j;
where jV j is the vocabulary size and 0 < ˛ � 1. Many more elaborate smoothing techniques exist.
Another popular family of approaches is using back-oﬀ : if the kgram was not observed,
compute an estimate based on a .k � 1/gram. A representative sample of this family is Jelinek
Mercer interpolated smoothing [Chen and Goodman, 1999, Jelinek and Mercer, 1980]:
Opint.wiC1 D mjwi�kWi/ D �wi�kWi
#.wi�kWiC1/
#.wi�kWi/
C .1 � �wi�kWi / Opint.wiC1 D mjwi�.k�1/Wi/:
For optimal performance, the values �wi�kWi should depend on the content of the condi-
tioning context wi�kWi: rare contexts should receive diﬀerent treatments than frequent ones.
e current state-of-the-art non-neural language modeling technique uses modiﬁed Kneser
Ney smoothing [Chen and Goodman, 1996], which is a variant of the technique proposed by
Kneser and Ney [1995]. For details, see Chen and Goodman [1996] and Goodman [2001].

108
9. LANGUAGE MODELING
9.3.1
FURTHER READING
Language modeling is a very vast topic, with decades of research. A good, formal overview of
the task, as well as motivations behind the perplexity measure can be found in the class notes
by Michael Collins.⁴ A good overview and empirical evaluation of smoothing techniques can be
found in the works of Chen and Goodman [1999] and Goodman [2001]. Another review of
traditional language modeling techniques can be found in the background chapters of the Ph.D.
thesis of Mikolov [2012]. For a recent advance in non-neural language modeling, see Pelemans
et al. [2016].
9.3.2
LIMITATIONS OF TRADITIONAL LANGUAGE MODELS
Language modeling approaches based on smoothed MLE estimates (“traditional”) are easy to
train, scale to large corpora, and work well in practice. ey do, however, have several important
shortcomings.
e smoothing techniques are intricate and based on back oﬀ to lower-order events. is
assumes a ﬁxed backing-up order, that needs to be designed by hand, and makes it hard to add
more “creative” conditioning contexts (i.e., if one wants to condition on the k previous words and
on the genre of the text, should the backoﬀ ﬁrst discard of the kth previous word, or of the genre
variable?). e sequential nature of the backoﬀ also makes it hard to scale toward larger ngrams
in order to capture long-range dependencies: in order to capture a dependency between the next
word and the word 10 positions in the past, one needs to see a relevant 11-gram in the text. In
practice, this very rarely happens, and the model backs oﬀ from the long history. It could be that a
better option would be to back oﬀ from the intervening words, i.e., allow for ngrams with “holes”
in them. However, these are tricky to deﬁne while retaining a proper generative probabilistic
framework.⁵
Scaling to larger ngrams is an inherent problem for MLE-based language models. e na-
ture of natural language and the large number of words in the vocabulary means that statistics
for larger ngrams will be sparse. Moreover, scaling to larger ngrams is very expensive in terms of
memory requirements. e number of possible ngrams over a vocabulary V is jV jn: increasing
the order by one will result in a jV j-fold increase to that number. While not all of the theoret-
ical ngrams are valid or will occur in the text, the number of observed events does grow at least
multiplicatively when increasing the ngram size by 1. is makes it very taxing to work larger
conditioning contexts.
Finally, MLE-based language models suﬀer from lack of generalization across contexts.
Having observed black car and blue car does not inﬂuence our estimates of the event red car if we
haven’t see it before.⁶
⁴http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf
⁵Although see lines of work on factored language models (i.e., A. Bilmes and Kirchhoﬀ [2003]) and on maximum-entropy
(log-linear) language models, starting with Rosenfeld [1996], as well as recent work by Pelemans et al. [2016].
⁶Class-based language models [Brown et al., 1992] try to tackle this by clustering the words using distributional algorithms,
and conditioning on the induced word-classes instead of or in addition to the words.

9.4. NEURAL LANGUAGE MODELS
109
9.4
NEURAL LANGUAGE MODELS
Nonlinear neural network models solve some of the shortcomings of traditional language mod-
els: they allow conditioning on increasingly large context sizes with only a linear increase in the
number of parameters, they alleviate the need for manually designing backoﬀ orders, and they
support generalization across diﬀerent contexts.
A model of the form presented in this chapter was popularized by Bengio et al. [2003].⁷
e input to the neural network is a kgram of words w1Wk, and the output is a probability
distribution over the next word. e k context words w1Wk are treated as a word window: each word
w is associated with an embedding vector v.w/ 2 Rdw, and the input vector x a concatenation
of the k words:
x D Œv.w1/I v.w2/I : : : I v.wk/�:
e input x is then fed to an MLP with one or more hidden layers:
Oy D P.wijw1Wk/ D LM.w1Wk/ D softmax.hW 2 C b2/
h D g.xW 1 C b1/
x D Œv.w1/I v.w2/I : : : I v.wk/�
v.w/ D E Œw�
(9.3)
wi 2 V
E 2 RjV j�dw
W 1 2 Rk�dw�dhid
b1 2 Rdhid
W 2 2 Rdhid�jV j
b2 2 RjV j:
V is a ﬁnite vocabulary, including the unique symbols U for unknown words, <s> for sentence
initial padding, and </s> for end-of-sequence marking. e vocabulary size, jV j, ranges between
10,000–1,000,000 words, with the common sizes revolving around 70,000.
Training
e training examples are simply word kgrams from the corpus, where the identities
of the ﬁrst k � 1 words are used as features, and the last word is used as the target label for the
classiﬁcation. Conceptually, the model is trained using cross-entropy loss. Working with cross
entropy loss works very well, but requires the use of a costly softmax operation which can be pro-
hibitive for very large vocabularies, prompting the use of alternative losses and/or approximations
(see below).
Memory and Computational Eﬃciency
Each of the k input words contributes dw dimensions
to x, and moving from k to k C 1 words will increase the dimensions of the weight matrix W 1
from k � dw � dhid to .k C 1/ � dw � dhid, a small linear increase in the number of parameters, in
contrast to a polynomial increase in the case of the traditional, count-based language models.
is is possible because the feature combinations are computed in the hidden layer. Increasing
the order k will likely require enlarging the dimension of dhid as well, but this is still a very
⁷A similar model was presented as early as 1988 by Nakamura and Shikano [1988] in their work on word class prediction with
neural networks.

110
9. LANGUAGE MODELING
modest increase in the number of parameters compared to the traditional modeling case. Adding
additional nonlinear layers to capture more complex interactions is also relatively cheap.
Each of the vocabulary words is associated with one dw dimensional vector (a row in E)
and one dhid dimensional vector (a column in W 2). us, a new vocabulary items will result in a
linear increase in the number of parameters, again much better than the traditional case. However,
while the input vocabulary (the matrix E) requires only lookup operations and can grow without
aﬀecting the computation speed, the size of the output vocabulary greatly aﬀects the computation
time: the softmax at the output layer requires an expensive matrix-vector multiplication with
the matrix W 2 2 Rdhid�jV j, followed by jV j exponentiations. is computation dominates the
runtime, and makes language modeling with large vocabulary sizes prohibitive.
Large output spaces
Working with neural probabilistic language models with large output
spaces (i.e., eﬃciently computing the softmax over the vocabulary) can be prohibitive both at
training time and at test time. Dealing with large output spaces eﬃciently is an active research
question. Some of the existing solutions are as follows.
Hierarchical softmax [Morin and Bengio, 2005] allows to compute the probability of a single word
in O.log jV j/ time rather than O.jV j/. is is achieved by structuring the softmax computation
as tree traversal, and the probability of each word as the product of branch selection decisions.
Assuming one is interested in the probability of a single word (rather than getting the distribution
over all words) this approach provides clear beneﬁts in both training and testing time.
Self-normalizing aproaches, such as noise-contrastive estimation (NCE) [Mnih and Teh, 2012,
Vaswani et al., 2013] or adding normalizing term to the training objective [Devlin et al., 2014].
e NCE approach improves training time performance by replacing the cross-entropy objective
with a collection of binary classiﬁcation problems, requiring the evaluation of the assigned scores
for k random words rather than the entire vocabulary. It also improves test-time prediction by
pushing the model toward producing “approximately normalized” exponentiated scores, making
the model score for a word a good substitute for its probability. e normalization term approach
of Devlin et al. [2014] similarly improves test time eﬃciency by adding a term to the training
objective that encourages the exponentiated model scores to sum to one, making the explicit
summation at test time unnecessary (the approach does not improve training time eﬃciency).
Sampling Approaches approximate the training-time softmax over a smaller subset of the vocabu-
lary [Jean et al., 2015].
A good review and comparison of these and other techniques for dealing with large output
vocabularies is available in Chen et al. [2016].
An orthogonal line of work is attempting to sidestep the problem by working at the char-
acters level rather than words level.
Desirable Properties
Putting aside the prohibitive cost of using the large output vocabulary, the
model has very appealing properties. It achieves better perplexities than state-of-the-art tradi-
tional language models such as Kneser-Ney smoothed models, and can scale to much larger orders

9.4. NEURAL LANGUAGE MODELS
111
than is possible with the traditional models. is is achievable because parameters are associated
only with individual words, and not with kgrams. Moreover, the words in diﬀerent positions
share parameters, making them share statistical strength. e hidden layers of the models are in
charge of ﬁnding informative word combinations, and can, at least in theory, learn that for some
words only sub-parts of the kgram are informative: it can learn to back-up to smaller kgrams if
needed, like a traditional language model, and do it in a context-dependent way. It can also learn
skip-grams, i.e., that for some combinations it should look at words 1, 2, and 5, skipping words
3 and 4.⁸
Another appealing property of the model, besides the ﬂexibility of the kgram orders, is the
ability to generalize across contexts. For example, by observing that the words blue, green, red,
black, etc. appear in similar contexts, the model will be able to assign a reasonable score to the
event green car even though it never observed this combination in training, because it did observe
blue car and red car.
e combination of these properties—the ﬂexibility of considering only parts of the con-
ditioning context and the ability to generalize to unseen contexts—together with the only linear
dependence on the conditioning context size in terms of both memory and computation, make
it very easy to increase the size of the conditioning context without suﬀering much from data
sparsity and computation eﬃciency.
e ease in which the neural language model can incorporate large and ﬂexible condition-
ing contexts allow for creative deﬁnitions of contexts. For example, Devlin et al. [2014] propose
a machine translation model in which the probability of the next word is conditioned on the pre-
vious k words in the generated translation, as well as on m words in the source language that the
given position in the translation is based on. is allows the model to be sensitive to topic-speciﬁc
jargon and multi-word expressions in the source language, and indeed results in much improved
translation scores.
Limitations
Neural language models of the form presented here do have some limitations: pre-
dicting the probability of a word in context is much more expensive than using a traditional
language model, and working with large vocabulary sizes and training corpora can become pro-
hibitive. However, they do make better use of the data, and can get very competitive perplexities
even with relatively small training set sizes.
When applied in the context of a machine-translation system, neural language models do
not always improve the translation quality over traditional Kneser-Ney smoothed language mod-
els. However, translation quality does improve when the probabilities from a traditional language
model and a neural one are interpolated. It seems that the models complement each other: the
neural language models generalize better to unseen events, but sometimes this generalization can
hurt the performance, and the rigidity of the traditional models is preferred. As an example, con-
sider the opposite of the colors example above: a model is asked to assign a probability to the
sentence red horse. A traditional model will assign it a very low score, as it likely observed such
⁸Such skip-grams were explored also for non-neural language models; see Pelemans et al. [2016] and the references therein.

112
9. LANGUAGE MODELING
an event only a few times, if at all. On the other hand, a neural language model may have seen
brown horse, black horse, and white horse, and also learned independently that black, white, brown,
and red can appear in similar contexts. Such a model will assign a much higher probability to the
event red horse, which is undesired.
9.5
USING LANGUAGE MODELS FOR GENERATION
Language models can also be used for generating sentences. After training a language model
on a given collection of text, one can generate (“sample”) random sentences from the model
according to their probability using the following process: predict a probability distribution over
the ﬁrst word conditioned on the start symbol, and draw a random word according to the predicted
distribution. en, predict a probability distribution over the second word conditioned on the ﬁrst,
and so on, until predicting the end-of-sequence </s> symbol. Already with k D 3 this produces
very passable text, and the quality improves with higher orders.
When decoding (generating a sentence) from a trained language-model in this way, one
can either choose the highest scoring prediction (word) at each step, or sample a random word
according to the predicted distribution. Another option is to use beam search in order to ﬁnd
a sequence with a globally high probability (following the highest-prediction at each step may
result in sub-optimal overall probability, as the process may “trap itself into a corner,” leading to
preﬁxes that are followed by low-probability events. is is called the label-bias problem, discussed
in depth by Andor et al. [2016] and Laﬀerty et al. [2001].
..
Sampling from a multinomial distribution
A multinomial distribution over jV j elements
associates a probability value pi � 0 for each item 0 < i � jV j, such that PjV j
iD1 pi D 1. In
order to sample a random item from a multinomial distribution according to its probability,
the following algorithm can be used:
1: i  0
2: s � U Œ0; 1�
F a uniform random number between 0 and 1
3: while s � 0 do
4:
i  i C 1
5:
s  s � pi
6: return i
is is a naive algorithm, with a computational complexity linear in the vocabulary
size O.jV j/. is can be prohibitively slow using large vocabularies. For peaked distributions
where the values are sorted by decreasing probability, the average time would be much faster.
e alias method [Kronmal and Peterson, Jr., 1979] is an eﬃcient algorithm for sampling
from arbitrary multinomial distributions with large vocabularies, allowing sampling in O.1/
after linear time pre-processing.

9.6. BYPRODUCT: WORD REPRESENTATIONS
113
9.6
BYPRODUCT: WORD REPRESENTATIONS
Language models can be trained on raw text: for training a k-order language model one just needs
to extract .k C 1/grams from running text, and treat the .k C 1/th word as the supervision signal.
us, we can generate practically unlimited training data for them.
Consider the matrix W 2 appearing just before the ﬁnal softmax. Each column in this ma-
trix is a dhid dimensional vector that is associated with a vocabulary item. During the ﬁnal score
computation, each column in W 2 is multiplied by the context representation h, and this produces
the score of the corresponding vocabulary item. Intuitively, this should cause words that are likely
to appear in similar contexts to have similar vectors. Following the distributional hypothesis ac-
cording to which words that appear in similar contexts have similar meanings, words with similar
meanings will have similar vectors. A similar argument can be made about the rows of the em-
bedding matrix E. As a byproduct of the language modeling process, we also learn useful word
representations in the rows and columns of the matrices E and W 2.
In the next chapter we further explore the topic of learning useful word representations
from raw text.

115
C H A P T E R
10
Pre-trained
Word Representations
A main component of the neural network approach is the use of embeddings—representing each
feature as a vector in a low dimensional space. But where do the vectors come from? is chapter
surveys the common approaches.
10.1
RANDOM INITIALIZATION
When enough supervised training data is available, one can just treat the feature embeddings the
same as the other model parameters: initialize the embedding vectors to random values, and let
the network-training procedure tune them into “good” vectors.
Some care has to be taken in the way the random initialization is performed. e method
used by the eﬀective W2V implementation [Mikolov et al., 2013b,a] is to initialize the word
vectors to uniformly sampled random numbers in the range Œ� 1
2d ; 1
2d � where d is the number of
dimensions. Another option is to use xavier initialization (see Section 5.2.2) and initialize with
uniformly sampled values from
h
�
p
6
p
d ;
p
6
p
d
i
.
In practice, one will often use the random initialization approach to initialize the embed-
ding vectors of commonly occurring features, such as part-of-speech tags or individual letters,
while using some form of supervised or unsupervised pre-training to initialize the potentially rare
features, such as features for individual words. e pre-trained vectors can then either be treated
as ﬁxed during the network training process, or, more commonly, treated like the randomly ini-
tialized vectors and further tuned to the task at hand.
10.2
SUPERVISED TASK-SPECIFIC PRE-TRAINING
If we are interested in task A, for which we only have a limited amount of labeled data (for
example, syntactic parsing), but there is an auxiliary task B (say, part-of-speech tagging) for which
we have much more labeled data, we may want to pre-train our word vectors so that they perform
well as predictors for task B, and then use the trained vectors for training task A. In this way, we
can utilize the larger amounts of labeled data we have for task B. When training task A we can
either treat the pre-trained vectors as ﬁxed, or tune them further for task A. Another option is to
train jointly for both objectives; see Chapter 20 for more details.

116
10. PRE-TRAINED WORD REPRESENTATIONS
10.3
UNSUPERVISED PRE-TRAINING
e common case is that we do not have an auxiliary task with large enough amounts of annotated
data (or maybe we want to help bootstrap the auxiliary task training with better vectors). In
such cases, we resort to “unsupervised” auxiliary tasks, which can be trained on huge amounts of
unannotated text.
e techniques for training the word vectors are essentially those of supervised learning,
but instead of supervision for the task that we care about, we instead create practically unlimited
number of supervised training instances from raw text, hoping that the tasks that we created will
match (or be close enough to) the ﬁnal task we care about.¹
e key idea behind the unsupervised approaches is that one would like the embedding
vectors of “similar” words to have similar vectors. While word similarity is hard to deﬁne and
is usually very task-dependent, the current approaches derive from the distributional hypothesis
[Harris, 1954], stating that words are similar if they appear in similar contexts. e diﬀerent meth-
ods all create supervised training instances in which the goal is to either predict the word from its
context, or predict the context from the word.
In the ﬁnal section of Chapter 9, we saw how language modeling creates word vectors as a
byproduct of training. Indeed, language modeling can be treated as an “unsupervised” approach
in which a word is predicted based on the context of the k preceding words. Historically, the
algorithm of Collobert and Weston [Collobert and Weston, 2008, Collobert et al., 2011] and
the W2V family of algorithms described below [Mikolov et al., 2013b,a] were inspired by
this property of language modeling. e W2V algorithms are designed to perform the same
side eﬀects as language modeling, using a more eﬃcient and more ﬂexible framework. e GV
algorithm by Pennington et al. [2014] follows a similar objective. ese algorithms are also deeply
connected to another family of algorithms which evolved in the NLP and IR communities, and
that are based on matrix factorization [Levy and Goldberg, 2014]. Word embeddings algorithms
are discussed in Section 10.4.
An important beneﬁt of training word embeddings on large amounts of unannotated data
is that it provides vector representations for words that do not appear in the supervised training
set. Ideally, the representations for these words will be similar to those of related words that do
appear in the training set, allowing the model to generalize better on unseen events. It is thus
desired that the similarity between word vectors learned by the unsupervised algorithm captures
the same aspects of similarity that are useful for performing the intended task of the network.
Arguably, the choice of auxiliary problem (what is being predicted, based on what kind of
context) aﬀects the resulting vectors much more than the learning method that is being used to
train them. Section 10.5 surveys diﬀerent choices of auxiliary problems.
Word embeddings derived by unsupervised training algorithms have applications in NLP
beyond using them for initializing the word-embeddings layer of neural network model. ese
are discussed in Chapter 11.
¹e interpretation of creating auxiliary problems from raw text is inspired by Ando and Zhang [2005a,b].

10.4. WORD EMBEDDING ALGORITHMS
117
10.3.1 USING PRE-TRAINED EMBEDDINGS
When using pre-trained word embeddings, there are some choices that should be taken. e ﬁrst
choice is about pre-processing: Should the pre-trained word vectors be used as is, or should each
vector be normalized to unit length? is is task dependent. For many word embedding algo-
rithms, the norm of the word vector correlates with the word’s frequency. Normalizing the words
to unit length removes the frequency information. is could either be a desirable uniﬁcation, or
an unfortunate information loss.
e second choice regards ﬁne-tuning the pre-trained vectors for the task. Consider an
embedding matrix E 2 RjV j�d associating words from vocabulary V with d-dimensional vectors.
A common approach is to treat E as model parameters, and change it with the rest of the network.
While this works well, it has the potential undesirable eﬀect of changing the representations for
words that appear in the training data, but not for other words that used to be close to them in the
original pre-trained vectors E. is may hurt the generalization properties we aim to get from
the pre-training procedure. An alternative is to leave the pre-trained vectors E ﬁxed. is keeps
the generalization, but prevents the model from adapting the representations for the given task.
A middle ground is to keep E ﬁxed, but use an additional matrix T 2 Rd�d. Instead of looking at
the rows of E, we look at rows of a transformed matrix E 0 D ET . e transformation matrix T is
tuned as part of the network, allowing to ﬁne-tune some aspects of the pre-trained vectors for the
task. However, the task-speciﬁc adaptations are in the form of linear transformations that apply to
all words, not just those seen in training. e downside of this approach is the inability to change
the representations of some words but not others (for example, if hot and cold received very similar
vectors, it could be very hard for a linear transformation T to separate them). Another option is
to keep E ﬁxed, but use an additional matrix � 2 RjV j�d and take the embedding matrix to be
E 0 D E C � or E 0 D ET C �. e � matrix is initialized to 0 and trained with the network,
allowing to learn additive changes to speciﬁc words. Adding a strong regularization penalty over
� will encourage the ﬁne-tuned representations to stay close to the original ones.²
10.4
WORD EMBEDDING ALGORITHMS
e neural networks community has a tradition of thinking in terms of distributed representations
[Hinton et al., 1987]. In contrast to local representations, in which entities are represented as
discrete symbols and the interactions between entities are encoded as a set of discrete relations
between symbols forming a graph, in distributed representations each entity is instead represented
as a vector of value (“a pattern of activations”), and the meaning of the entity and its relation to
other entities are captured by the activations in the vector, and the similarities between diﬀerent
vectors. In the context of language processing, it means that words (and sentences) should not
be mapped to discrete dimensions but rather mapped to a shared low dimensional space, where
²Note that all the updates during gradient-based training are additive, and so without regularization, updating E during
training and keeping E ﬁxed but updating � and looking at E C � will result in the same ﬁnal embeddings. e approaches
only diﬀer when regularization is applied.

118
10. PRE-TRAINED WORD REPRESENTATIONS
each word will be associated with a d-dimensional vector, and the meaning of the word will be
captured by its relation to other words and the activation values in its vector.
e natural language processing community has a tradition in thinking in terms of distribu-
tional semantics, in which a meaning of a word could be derived from its distribution in a corpus,
i.e., from the aggregate of the contexts in which it is being used. Words that tend to occur in
similar contexts tend to have similar meanings.
ese two approaches to representing words—in terms of patterns of activations that are
learned in the context of a larger algorithm and in terms of co-occurrence patterns with other
words or syntactic structures, give rise to seemingly very diﬀerent views of word representations,
leading to diﬀerent algorithmic families and lines of thinking.
In Section 10.4.1 we will explore the distributional approach to word representation, and
in Section 10.4.2 we’ll explore the distributed approaches. Section 10.4.3 will connect the two
worlds, and show that for the most part, current state-of-the-art distributed representations of
words are using distributional signals to do most of their heavy lifting, and that the two algorith-
mic families are deeply connected.
10.4.1 DISTRIBUTIONAL HYPOTHESIS AND WORD REPRESENTATIONS
e Distributional Hypothesis about language and word meaning states that words that occur in
the same contexts tend to have similar meanings [Harris, 1954]. e idea was popularized by Firth
[1957] through the saying “you shall know a word by the company it keeps.” Intuitively, when
people encounter a sentence with an unknown word such as the word wampimuk in Marco saw
a hairy little wampinuk crouching behind a tree, they infer the meaning of the word based on the
context in which it occurs. is idea has given rise to the ﬁeld of distributional semantics: a research
area interested in quantifying semantic similarities between linguistic items according to their
distributional properties in large text corpora. For a discussion of the linguistic and philosophical
basis of the distributional hypothesis, see Sahlgren [2008].
Word-context Matrices
In NLP, a long line of research³ captures the distributional properties of words using word-context
matrices, in which each row i represents a word, each column j represents a linguistic context in
which words can occur, and a matrix entry M Œi;j � quantiﬁes the strength of association between
a word and a context in a large corpus. In other words, each word is represented as a sparse vector
in high dimensional space, encoding the weighted bag of contexts in which it occurs. Diﬀerent
deﬁnitions of contexts and diﬀerent ways of measuring the association between a word and a
context give rise to diﬀerent word representations. Diﬀerent distance functions can be used to
measure the distances between word vectors, which are taken to represent the semantic distances
between the associated words.
³See the survey of Turney and Pantel [2010] and Baroni and Lenci [2010] for an overview.

10.4. WORD EMBEDDING ALGORITHMS
119
More formally, denote by VW the set of words (the words vocabulary) and by VC the set
of possible contexts. We assume each word and each context are indexed, such that wi is the ith
word in the words vocabulary and cj is the jth word in the context vocabulary. e matrix M f 2
RjVW j�jVC j is the word-context matrix, deﬁned as M f
Œi;j � D f .wi; cj /, where f is an association
measure of the strength between a word and a context.
Similarity Measures
Once words are represented as vectors, one can compute similarities between words by computing
the similarities between the corresponding vectors. A common and eﬀective measure is the cosine
similarity, measuring the cosine of the angle between the vectors:
simcos.u; v/ D
u � v
kuk2kvk2
D
P
i uŒi� � vŒi�
pP
i.uŒi�/2pP
i.vŒi�/2 :
(10.1)
Another popular measure is the generalized Jacaard similarity, deﬁned as:⁴
simJacaard.u; v/ D
P
Pi min.uŒi�; vŒi�/
i max.uŒi�; vŒi�/:
(10.2)
Word-context Weighting and PMI
e function f is usually based on counts from a large corpus. Denote by #.w; c/ the number of
times word w occurred in the context c in the corpus D, and let jDj be the corpus size (jDj D
P
w02VW ;c02VC #.w0; c0/). It is intuitive to deﬁne f .w; c/ to be the count f .w; c/ D #.w; c/ or the
normalized count f .w; c/ D P.w; c/ D #.w;c/
jDj . However, this has the undesired eﬀect of assign-
ing high weights to word-context pairs involving very common contexts (for example, consider
the context of a word to be the previous word. en for a word such as cat the events the cat
and a cat will receive much higher scores than cute cat and small cat even though the later are
much more informative). To counter this eﬀect, it is better to deﬁne f to favor informative con-
texts for a given word—contexts that co-occur more with the given word than with other words.
An eﬀective metric that captures this behavior is the pointwise mutual information (PMI): an
information-theoretic association measure between a pair of discrete outcomes x and y, deﬁned
as:
PMI.x; y/ D log
P.x; y/
P.x/P.y/:
(10.3)
In our case, PMI.w; c/ measures the association between a word w and a context c by
calculating the log of the ratio between their joint probability (the frequency in which they co-
occur together) and their marginal probabilities (the frequencies in which they occur individually).
PMI can be estimated empirically by considering the actual number of observations in a corpus:
f .w; c/ D PMI.w; c/ D log #.w; c/ � jDj
#.w/ � #.c/ ;
(10.4)
⁴When thinking of u and v as sets, the Jacaard similarity is deﬁned as ju\vj
ju[vj.

120
10. PRE-TRAINED WORD REPRESENTATIONS
where #.w/ D P
c02VC #.w; c0/ and #.c/ D P
w02VW #.w0; c/ are the corpus frequencies of w and
c respectively. e use of PMI as a measure of association in NLP was introduced by Church and
Hanks [1990] and widely adopted for word similarity and distributional semantic tasks [Dagan
et al., 1994, Turney, 2001, Turney and Pantel, 2010].
Working with the PMI matrix presents some computational challenges. e rows of M PMI
contain many entries of word-context pairs .w; c/ that were never observed in the corpus, for
which PMI.w; c/ D log 0 D �1. A common solution is to use the positive PMI (PPMI) metric,
in which all negative values are replaced by 0:⁵
PPMI.w; c/ D max .PMI .w; c/ ; 0/ :
(10.5)
Systematic comparisons of various weighting schemes for entries in the word-context sim-
ilarity matrix show that the PMI, and more so the positive-PMI (PPMI) metrics provide the
best results for a wide range of word-similarity tasks [Bullinaria and Levy, 2007, Kiela and Clark,
2014].
A deﬁciency of PMI is that it tends to assign high value to rare events. For example, if
two events occur only once, but occur together, they will receive a high PMI value. It is therefore
advisable to apply a count threshold before using the PMI metric, or to otherwise discount rare
events.
Dimensionality Reduction through Matrix Factorization
A potential obstacle of representing words as the explicit set of contexts in which they occur is
that of data sparsity—some entries in the matrix M may be incorrect because we did not observe
enough data points. Additionally, the explicit word vectors are of a very high dimensions (de-
pending on the deﬁnition of context, the number of possible contexts can be in the hundreds of
thousands, or even millions).
Both issues can be alleviated by considering a low-rank representation of the data using a
dimensionality reduction technique such as the singular value decomposition (SVD).
SVD works by factorizing the matrix M 2 RjVW j�jVC j into two narrow matrices: a W 2
RjVW j�d word matrix and a C 2 RjVC j�d context matrix, such that W C > D M 0 2 RjVW j�jVC j
is the best rank-d approximation of M in the sense that no other rank-d matrix has a closer L2
distance to M than M 0.
e low-rank representation M 0 can be seen as a “smoothed” version of M: based on robust
patterns in the data, some of the measurements are “ﬁxed.” is has the eﬀect, for example, of
adding words to contexts that they were not seen with, if other words in this context seem to
⁵When representing words, there is some intuition behind ignoring negative values: humans can easily think of positive associ-
ations (e.g., “Canada” and “snow”) but ﬁnd it much harder to invent negative ones (“Canada” and “desert”). is suggests that
the perceived similarity of two words is inﬂuenced more by the positive context they share than by the negative context they
share. It therefore makes some intuitive sense to discard the negatively associated contexts and mark them as “uninformative”
(0) instead. A notable exception would be in the case of syntactic similarity. For example, all verbs share a very strong negative
association with being preceded by determiners, and past tense verbs have a very strong negative association to be preceded by
“be” verbs and modals.

10.4. WORD EMBEDDING ALGORITHMS
121
co-locate with each other. Moreover, the matrix W allows to represent each word as a dense d-
dimensional vector instead of a sparse jVCj-dimensional one, where d � jVCj (typical choices
are 50 < d < 300), such that the d-dimensional vectors captures the most important directions
of variation in the original matrix. One can then compute similarities based on the dense d-dim
vectors instead of the sparse high-dimensional ones.
..
e mathematics of SVD
e Singular Value Decomposition (SVD) is an algebraic tech-
nique by which an m � n real or complex matrix M is factorized into three matrices:
M D U DV ;
where U is an m � m real or complex matrix, D is an m � n real or complex matrix, and
V is an n � n matrix. e matrices U and V > are orthonormal, meaning that their rows are
unit-length and orthogonal to each other. e matrix D is diagonal, where the elements on
the diagonal are the singular values of M, in decreasing order.
e factorization is exact. e SVD has many uses, in machine learning and elsewhere.
For our purposes, SVD is used for dimensionality reduction—ﬁnding low-dimensional rep-
resentations of high-dimensional data that preserve most of the information in the original
data.
Consider the multiplication U QDV where QD is a version of D in which all but the ﬁrst
k elements on the diagonal are replaced by zeros. We can now zero out all but the ﬁrst k rows
of U and columns of V , as they will be zeroed out by the multiplication anyhow. Deleting
the rows and columns leaves us with three matrices, QU (m � k), �D (k � k, diagonal) and
V (k � n). e product:
M 0 D QU QD QV
is a .m � n/ matrix of rank k.
e matrix M 0 is the product of thin matrices ( QU and QV , with k much smaller than m
and n), and can be thought of as a low rank approximation of M.
According to the Eckart-Young theorem [Eckart and Young, 1936], the matrix M 0 is
the best rank-k approximation of M under L2 loss. at is, M 0 is the minimizer of:
M 0 D argmin
X 2Rm�n
kX � Mk2
s:t: X is rank-k:
e matrix M 0 can be thought of as a smoothed version of M, in the sense that it uses only
the k most inﬂuential directions in the data.
Approximating row distances e low-dimensional rows of E D QU QD are low-rank approx-
imations of the high-dimensional rows of the original matrix M, in the sense that computing
the dot product between rows of E is equivalent to computing the dot-product between the
rows of the reconstructed matrix M 0. at is, E Œi� � E Œj� D M 0Œi� � M 0Œj�.

122
10. PRE-TRAINED WORD REPRESENTATIONS
..
To see why, consider the m � m matrix S E D EE >. An entry Œi; j� in this matrix is
equal to the dot product between rows i and j in E: S E Œi;j � D E Œi� � E Œj�. Similarly for the
matrix S M 0 D M 0M 0>.
We will show that S E D S M 0. Recall that QV QV
> D I because QV is orthonormal. Now:
S M 0 D M 0M 0> D . QU QD QV /. QU QD QV /> D . QU QD QV /. QV
> QD
> QU
>/ D
D . QU QD/. QV QV
>/. QD
> QU
>/ D . QU QD/. QU QD/> D EE > D S E :
We can thus use the rows of E instead of the high-dimensional rows of M 0 (and
instead of the high-dimensional rows of M. Using a similar argument, we can also use the
rows of . QD QV /> instead of the columns of M 0).
When using SVD for word similarity, the rows of M correspond to words, the columns
to contexts, and the vectors comprising the rows of E are low-dimensional word representa-
tions. In practice, it is often better to not use E D QU QD but instead to use the more “balanced”
version E D QU
p
QD, or even ignoring the singular values QD completely and taking E D QU .
10.4.2 FROM NEURAL LANGUAGE MODELS TO DISTRIBUTED
REPRESENTATIONS
In contrast to the so-called count-based methods described above, the neural networks community
advocates the use of distributed representations of word meanings. In a distributed representation,
each word is associated with a vector in Rd, where the “meaning” of the word with respect to some
task is captured in the diﬀerent dimensions of the vector, as well as in the dimensions of other
words. Unlike the explicit distributional representations in which each dimension corresponds to
a speciﬁc context the word occurs in, the dimensions in the distributed representation are not
interpretable, and speciﬁc dimensions do not necessarily correspond to speciﬁc concepts. e
distributed nature of the representation means that a given aspect of meaning may be captured by
(distributed over) a combination of many dimensions, and that a given dimension may contribute
to capturing several aspects of meaning.⁶
Consider the language modeling network in Equation (9.3) in Chapter 9. e context of a
word is the kgram of words preceding it. Each word is associated with a vector, and their concate-
nation is encoded into a dhid dimensional vector h using a nonlinear transformation. e vector
h is then multiplied by a matrix W 2 in which each column corresponds to a word, and interac-
tions between h and columns in W 2 determine the probabilities of the diﬀerent words given the
context. e columns of W 2 (as well as the rows of the embeddings matrix E) are distributed
⁶We note that in many ways the explicit distributional representations is also “distributed”: diﬀerent aspects of the meaning
of a word are captured by groups of contexts the word occurs in, and a given context can contribute to diﬀerent aspects of
meaning. Moreover, after performing dimensionality reduction over the word-context matrix, the dimensions are no longer
easily interpretable.

10.4. WORD EMBEDDING ALGORITHMS
123
representations of words: the training process determines good values to the embeddings such
that they produce correct probability estimates for a word in the context of a kgram, capturing
the “meaning” of the words in the columns of W 2 associated with them.
Collobert and Weston
e design of the network in Equation (9.3) is driven by the language modeling task, which poses
two important requirements: the need to produce a probability distributions over words, and the
need to condition on contexts that can be combined using the chain-rule of probability to produce
sentence-level probability estimates. e need to produce a probability distribution dictates the
need to compute an expensive normalization term involving all the words in the output vocabulary,
while the need to decompose according to the chain-rule restricts the conditioning context to
preceding kgrams.
If we only care about the resulting representations, both of the constraints can be relaxed,
as was done by Collobert and Weston [2008] in a model which was reﬁned and presented in
greater depth by Bengio et al. [2009]. e ﬁrst change introduced by Collobert and Weston was
changing the context of a word from the preceding kgram (the words to its left) to a word-
window surrounding it (i.e., computing P.w3jw1w2�w4w5/ instead of P.w5jw1w2w3w4�/).
e generalization to other kinds of ﬁxed-sized contexts c1Wk is straightforward.
e second change introduced by Collobert and Weston is to abandon the probabilistic
output requirement. Instead of computing a probability distribution over target words given a
context, their model only attempts to assign a score to each word, such that the correct word
scores above incorrect ones. is removes the need to perform the computationally expensive
normalization over the output vocabulary, making the computation time independent of the out-
put vocabulary size. is not only makes the network much faster to train and use, but also makes
it scalable to practically unlimited vocabularies (the only cost of increasing the vocabulary is a
linear increase in memory usage).
Let w be a target word, c1Wk be an ordered list of context items, and vw.w/ and vc.c/
embedding functions mapping word and context indices to demb dimensional vectors (from now
on we assume the word and context vectors have the same number of dimensions). e model of
Collobert and Weston computes a score s.w; c1Wk/ of a word-context pair by concatenating the
word and the context embeddings into a vector x, which is fed into an MLP with one hidden
layer whose single output is the score assigned to the word-context combination:
s.w; c1Wk/ D g.xU / � v
x D Œvc.c1/I : : : I vc.ck/I vw.w/�
(10.6)
U 2 R.kC1/demb�dh
v 2 Rdh:
e network is trained with a margin-based ranking loss to score correct word-context
pairs .w; c1Wk/ above incorrect word-context pairs .w0; c1Wk/ with a margin of at least 1. e loss

124
10. PRE-TRAINED WORD REPRESENTATIONS
L.w; c1Wk/ for a given word-context pair is given by:
L.w; c; w0/ D max.0; 1 � .s.w; c1Wk/ � s.w0; c1Wk///
(10.7)
where w0 is a random word from the vocabulary. e training procedure repeatedly goes over
word-context pairs from the corpus, and for each one samples a random word w0, computes the
loss L.w; c; w0/ using w0, and updates parameters U , v and the word and context embeddings to
minimize the loss.
e use of randomly sampled words to produce negative examples of incorrect word-context
to drive the optimization is also at the core of the W2V algorithm, to be described next.
Word2Vec
e widely popular W2V algorithm was developed by Tomáš Mikolov and colleagues
over a series of papers [Mikolov et al., 2013b,a]. Like the algorithm of Collobert and Weston,
W2V also starts with a neural language model and modiﬁes it to produce faster results.
textscWord2V is not a single algorithm: it is a software package implementing two diﬀer-
ent context representations (CBOW and Skip-Gram) and two diﬀerent optimization objectives
(Negative-Sampling and Hierarchical Softmax). Here, we focus on the Negative-Sampling ob-
jective (NS).
Like Collobert and Weston’s algorithm, the NS variant of W2V works by training
the network to distinguish “good” word-context pairs from “bad” ones. However, W2V
replaces the margin-based ranking objective with a probabilistic one. Consider a set D of correct
word-context pairs, and a set ND of incorrect word-context pairs. e goal of the algorithm is to
estimate the probability P.D D 1jw; c/ that the word-context pair came from the correct set D.
is should be high (1) for pairs from D and low (0) for pairs from ND. e probability con-
straint dictates that P.D D 1jw; c/ D 1 � P.D D 0jw; c/. e probability function is modeled
as a sigmoid over the score s.w; c/:
P.D D 1jw; c/ D
1
1 C e�s.w;c/ :
(10.8)
e corpus-wide objective of the algorithm is to maximize the log-likelihood of the data
D [ ND:
L.‚I D; ND/ D
X
.w;c/2D
log P.D D 1jw; c/ C
X
.w;c/2 ND
log P.D D 0jw; c/:
(10.9)
e positive examples D are generated from a corpus. e negative examples ND can be
generated in many ways. In W2V, they are generated by the following process: for each
good pair .w; c/ 2 D, sample k words w1Wk and add each of .wi; c/ as a negative example to ND.
is results in the negative samples data ND being k times larger than D. e number of negative
samples k is a parameter of the algorithm.

10.4. WORD EMBEDDING ALGORITHMS
125
e negative words w can be sampled according to their corpus-based frequency
P #.w/
w0 #.w0/,
or, as done in the W2V implementation, according to a smoothed version in which the
counts are raised to the power of 3
4 before normalizing:
#.w/0:75
P
w0 #.w0/0:75 . is second version gives
more relative weight to less frequent words, and results in better word similarities in practice.
CBOW
Other than changing the objective from margin-based to a probabilistic one,
W2V also considerably simplify the deﬁnition of the word-context scoring function,
s.w; c/. For a multi-word context c1Wk, the CBOW variant of W2V deﬁnes the context
vector c to be a sum of the embedding vectors of the context components: c D Pk
iD1 ci. It then
deﬁnes the score to be simply s.w; c/ D w � c, resulting in:
P.D D 1jw; c1Wk/ D
1
1 C e�.w�c1Cw�c2C:::Cw�ck/ :
e CBOW variant loses the order information between the context’s elements. In return,
it allows the use of variable-length contexts. However, note that for contexts with bound length,
the CBOW can still retain the order information by including the relative position as part of
the content element itself, i.e., by assigning diﬀerent embedding vector to context elements in
diﬀerent relative positions.
Skip-Gram
e skip-gram variant of W2V scoring decouples the dependence between
the context elements even further. For a k-elements context c1Wk, the skip-gram variant assumes
that the elements ci in the context are independent from each other, essentially treating them
as k diﬀerent contexts, i.e., a word-context pair .w; ciWk/ will be represented in D as k diﬀerent
contexts: .w; c1/; : : : ; .w; ck/. e scoring function s.w; c/ is deﬁned as in the CBOW version,
but now each context is single embedding vector:
P.D D 1jw; ci/ D
1
1 C e�w�ci
P.D D 1jw; c1Wk/ D
k
Y
iD1
P.D D 1jw; ci/ D
k
Y
1Di
1
1 C e�w�ci
log P.D D 1jw; c1Wk/ D
k
X
iD1
log
1
1 C e�w�ci :
(10.10)
While introducing strong independence assumptions between the elements of the context,
the skip-gram variant is very eﬀective in practice, and very commonly used.
10.4.3 CONNECTING THE WORLDS
Both the distributional “count-based” method and the distributed “neural” ones are based on the
distributional hypothesis, attempting capture the similarity between words based on the similarity

126
10. PRE-TRAINED WORD REPRESENTATIONS
between the contexts in which they occur. In fact, Levy and Goldberg [2014] show that the ties
between the two worlds are deeper than appear at ﬁrst sight.
e training of W2V models result in two embedding matrices, E W 2 RjVW j�demb
and E C 2 RjVC j�demb representing the words and the contexts, respectively. e context embed-
dings are discarded after training, and the word embeddings are kept. However, imagine keeping
the context embedding matrix E C and consider the product E W � E C > D M 0 2 RjVW j�jVC j.
Viewed this way, W2V is factorizing an implicit word-context matrix M 0. What are the
elements of matrix M 0? An entry M 0Œw;c� corresponds to the dot product of the word and con-
text embedding vectors w � c. Levy and Goldberg show that for the combination of skip-grams
contexts and the negative sampling objective with k negative samples, the global objective is min-
imized by setting w � c D M 0Œw;c� D PMI.w; c/ � log k. at is, W2V is implicitly factor-
izing a matrix which is closely related to the well-known word-context PMI matrix! Remarkably,
it does so without ever explicitly constructing the matrix M 0.⁷
e above analysis assumes that the negative samples are sampled according to the cor-
pus frequency of the words P.w/ D
P #.w/
w0 #.w0/. Recall that the W2V implementation sam-
ples instead from a modiﬁed distribution P 0:75.w/ D
#.w/0:75
P
w0 #.w0/0:75 . Under this sampling scheme,
the optimal value changes to PMI0:75.w; c/ � log k D log
P.w;c/
P 0:75.w/P.c/ � log k. Indeed, using this
modiﬁed version of PMI when constructing sparse and explicit distributional vectors improves
the similarity in that setup as well.
e W2V algorithms are very eﬀective in practice, and are highly scalable, allowing to
train word representations with very large vocabularies over billions of words of text in a matter
of hours, with very modest memory requirements. e connection between the SGNS variant
of W2V and word-context matrix-factorization approaches ties the neural methods and
the traditional “count-based” ones, suggesting that lessons learned in the study of “distributional”
representation can transfer to the “distributed” algorithms, and vice versa, and that in a deep sense
the two algorithmic families are equivalent.
10.4.4 OTHER ALGORITHMS
Many variants on the W2V algorithms exist, none of which convincingly produce qual-
itatively or quantitatively superior word representations. is sections list a few of the popular
ones.
NCE
e noise-contrastive estimation (NCE) approach of Mnih and Kavukcuoglu [2013] is
very similar to the SGNS variant of W2V, but instead of modeling P.D D 1 j w; ci/ as in
⁷If the optimal assignment was satisﬁable, the skip-grams with negative-sampling (SGNS) solution is the same as the SVD
over word-context matrix solution. Of course, the low dimensionality demb of w and c may make it impossible to satisfy
w � c D PMI.w; c/ � log k for all w and c pairs, and the optimization procedure will attempt to ﬁnd the best achievable
solution, while paying a price for each deviation from the optimal assignment. is is where the SGNS and the SVD objectives
diﬀer—SVD puts a quadratic penalty on each deviation, while SGNS uses a more complex penalty term.

10.5. THE CHOICE OF CONTEXTS
127
Equation (10.10), it is modeled as:
P.D D 1 j w; ci/ D
e�w�ci
e�w�ci C k � q.w/
(10.11)
P.D D 0 j w; ci/ D
k � q.w/
e�w�ci C k � q.w/;
(10.12)
where q.w/ D #.w/
jDj is the observed unigram frequency of w in the corpus. is algorithm is based
on the noise-contrastive estimation probability modeling technique [Gutmann and Hyvärinen,
2010]. According to Levy and Goldberg [2014], this objective is equivalent to factorizing the
word-context matrix whose entries are the log conditional probabilities log P.wjc/ � log k.
GloVe
e GV algorithm [Pennington et al., 2014] constructs an explicit word-context
matrix, and trains the word and context vectors w and c attempting to satisfy:
w � c C bŒw� C bŒc� D log #.w; c/
8.w; c/ 2 D;
(10.13)
where bŒw� and bŒc� are word-speciﬁc and context-speciﬁc trained biases. e optimization pro-
cedure looks at observed word context pairs while skipping zero count events. In terms of matrix
factorization, if we ﬁx bŒw� D log #.w/ and bŒc� D log #.c/ we’ll get an objective that is very sim-
ilar to factorizing the word-context PMI matrix, shifted by log.jDj/. However, in GloVe these
parameters are learned and not ﬁxed, giving it another degree of freedom. e optimization objec-
tive is weighted least-squares loss, assigning more weight to the correct reconstruction of frequent
items. Finally, when using the same word and context vocabularies, the GloVe model suggests
representing each word as the sum of its corresponding word and context embedding vectors.
10.5
THE CHOICE OF CONTEXTS
e choice of context by which a word is predicted has a profound eﬀect on the resulting word
vectors and the similarities they encode.
In most cases, the contexts of a word are taken to be other words that appear in its surround-
ing, either in a short window around it, or within the same sentence, paragraph or document. In
some cases the text is automatically parsed by a syntactic parser, and the contexts are derived from
the syntactic neighborhood induced by the automatic parse trees. Sometimes, the deﬁnitions of
words and context change to include also parts of words, such as preﬁxes or suﬃxes.
10.5.1 WINDOW APPROACH
e most common approach is a sliding window approach, in which auxiliary tasks are created by
looking at a sequence of 2m C 1 words. e middle word is called the focus word and the m words
to each side are the contexts. en, either a single task is created in which the goal is to predict the
focus word based on all of the context words (represented either using CBOW [Mikolov et al.,

128
10. PRE-TRAINED WORD REPRESENTATIONS
2013b] or vector concatenation [Collobert and Weston, 2008]), or 2m distinct tasks are created,
each pairing the focus word with a diﬀerent context word. e 2m tasks approach, popularized
by Mikolov et al. [2013a] is referred to as a skip-gram model. Skip-gram-based approaches are
shown to be robust and eﬃcient to train [Mikolov et al., 2013a, Pennington et al., 2014], and
often produce state of the art results.
Eﬀect of Window Size
e size of the sliding window has a strong eﬀect on the resulting vector
similarities. Larger windows tend to produce more topical similarities (i.e., “dog,” “bark” and
“leash” will be grouped together, as well as “walked,” “run” and “walking”), while smaller windows
tend to produce more functional and syntactic similarities (i.e., “Poodle,” “Pitbull,” “Rottweiler,”
or “walking,”“running,”“approaching”).
Positional Windows
When using the CBOW or skip-gram context representations, all the dif-
ferent context words within the window are treated equally. ere is no distinction between con-
text words that are close to the focus words and those that are farther from it, and likewise there
is no distinction between context words that appear before the focus words to context words that
appear after it. Such information can easily be factored in by using positional contexts: indicating
for each context word also its relative position to the focus words (i.e., instead of the context word
being “the” it becomes “the:+2,” indicating the word appears two positions to the right of the
focus word). e use of positional context together with smaller windows tend to produce sim-
ilarities that are more syntactic, with a strong tendency of grouping together words that share a
part of speech, as well as being functionally similar in terms of their semantics. Positional vectors
were shown by Ling et al. [2015a] to be more eﬀective than window-based vectors when used to
initialize networks for part-of-speech tagging and syntactic dependency parsing.
Variants
Many variants on the window approach are possible. One may lemmatize words before
learning, apply text normalization, ﬁlter too short or too long sentences, or remove capitalization
(see, e.g., the pre-processing steps described by dos Santos and Gatti [2014]). One may sub-
sample part of the corpus, skipping with some probability the creation of tasks from windows
that have too common or too rare focus words. e window size may be dynamic, using a diﬀer-
ent window size at each turn. One may weigh the diﬀerent positions in the window diﬀerently,
focusing more on trying to predict correctly close word-context pairs than further away ones.
Each of these choices is a hyperparameter to be manually set before training, and will eﬀect the
resulting vectors. Ideally, these will be tuned for the task at hand. Much of the strong performance
of the W2V implementation can be attributed to specifying good default values for these
hyperparameters. Some of these hyperparameters (and others) are discussed in detail in Levy et al.
[2015].

10.5. THE CHOICE OF CONTEXTS
129
10.5.2 SENTENCES, PARAGRAPHS, OR DOCUMENTS
Using a skip-gram (or CBOW) approach, one can consider the contexts of a word to be all the
other words that appear with it in the same sentence, paragraph, or document. is is equivalent
to using very large window sizes, and is expected to result in word vectors that capture topical
similarity (words from the same topic, i.e., words that one would expect to appear in the same
document, are likely to receive similar vectors).
10.5.3 SYNTACTIC WINDOW
Some work replace the linear context within a sentence with a syntactic one [Bansal et al., 2014,
Levy and Goldberg, 2014]. e text is automatically parsed using a dependency parser, and the
context of a word is taken to be the words that are in its proximity in the parse tree, together with
the syntactic relation by which they are connected. Such approaches produce highly functional
similarities, grouping together words than can ﬁll the same role in a sentence (e.g., colors, names
of schools, verbs of movement). e grouping is also syntactic, grouping together words that share
an inﬂection [Levy and Goldberg, 2014].
..
e eﬀect of context
e following table, taken from Levy and Goldberg [2014], shows
the top-5 most similar words to some seed words, when using bag-of-words windows of size
5 and 2 (BW5 and BW2), as well as dependency-based contexts (D), using the same
underlying corpora (Wikipedia), and the same embeddings algorithm (W2V).
Notice how for some words (e.g., batman) the induced word similarities are somewhat
agnostic to the contexts, while for others there is a clear trend: the larger window contexts
result in more topical similarities (hogwars is similar to other terms in the Harry Potter uni-
verse, turing is related to computability, dancing is similar to other inﬂections of the word)
while the syntactic-dependency contexts result in more functional similarities (hogwarts sim-
ilar to other ﬁctional or non-ﬁctional schools, turing is similar to other scientists, and dancing
to other gerunds of entrainment activities). e smaller context window is somewhere in be-
tween the two.
is re-aﬃrms that context choices strongly aﬀects the resulting word representations,
and stresses the need to take the choice of context into consideration when using “unsuper-
vised” word embeddings.

130
10. PRE-TRAINED WORD REPRESENTATIONS
.
Target Word
BoW5
BoW2
Deps
batman
nightwing
aquaman
catwoman
superman
manhunter
superman
superboy
aquaman
catwoman
batgirl
superman
superboy
supergirl
catwoman
aquaman
hogwarts
dumbledore
hallows
half-blood
malfoy
snape
evernight
sunnydale
garderobe
blandings
collinwood
sunnydale
collinwood
calarts
greendale
millf eld
turing
nondeterministic
non-deterministic
computability
deterministic
f nite-state
non-deterministic
f nite-state
nondeterministic
buchi
primality
pauling
hotelling
heting
lessing
hamming
f orida
gainesville
f a
jacksonville
tampa
lauderdale
f a
alabama
gainesville
tallahassee
texas
texas
louisiana
georgia
california
carolina
object-oriented
aspect-oriented
smalltalk
event-driven
prolog
domain-specif c
aspect-oriented
event-driven
objective-c
dataf ow
4gl
event-driven
domain-specif c
rule-based
data-driven
human-centered
dancing
singing
dance
dances
dancers
tap-dancing
singing
dance
dances
breakdancing
clowning
singing
rapping
breakdancing
miming
busking
10.5.4 MULTILINGUAL
Another option is using multilingual, translation-based contexts [Faruqui and Dyer, 2014, Her-
mann and Blunsom, 2014]. For example, given a large amount of sentence-aligned parallel text,

10.5. THE CHOICE OF CONTEXTS
131
one can run a bilingual alignment model such as the IBM model 1 or model 2 (i.e., using the
GIZA++ software), and then use the produced alignments to derive word contexts. Here, the
context of a word instance is the foreign language words that are aligned to it. Such alignments
tend to result in synonym words receiving similar vectors. Some authors work instead on the
sentence alignment level, without relying on word alignments [Gouws et al., 2015] or train an
end-to-end machine-translation neural network and use the resulting word embeddings [Hill
et al., 2014]. An appealing method is to mix a monolingual window-based approach with a mul-
tilingual approach, creating both kinds of auxiliary tasks. is is likely to produce vectors that
are similar to the window-based approach, but reducing the somewhat undesired eﬀect of the
window-based approach in which antonyms (e.g., hot and cold, high and low) tend to receive
similar vectors [Faruqui and Dyer, 2014]. For further discussion on multilingual word embed-
dings and a comparison of diﬀerent methods see Levy et al. [2017].
10.5.5 CHARACTER-BASED AND SUB-WORD REPRESENTATIONS
An interesting line of work attempts to derive the vector representation of a word from the char-
acters that compose it. Such approaches are likely to be particularly useful for tasks which are
syntactic in nature, as the character patterns within words are strongly related to their syntactic
function. ese approaches also have the beneﬁt of producing very small model sizes (only one
vector for each character in the alphabet together with a handful of small matrices needs to be
stored), and being able to provide an embedding vector for every word that may be encountered.
dos Santos and Gatti [2014], dos Santos and Zadrozny [2014], and Kim et al. [2015] model
the embedding of a word using a convolutional network (see Chapter 13) over the characters.
Ling et al. [2015b] model the embedding of a word using the concatenation of the ﬁnal states of
two RNN (LSTM) encoders (Chapter 14), one reading the characters from left to right, and the
other from right to left. Both produce very strong results for part-of-speech tagging. e work of
Ballesteros et al. [2015] show that the two-LSTMs encoding of Ling et al. [2015b] is beneﬁcial
also for representing words in dependency parsing of morphologically rich languages.
Deriving representations of words from the representations of their characters is motivated
by the unknown words problem—what do you do when you encounter a word for which you do not
have an embedding vector? Working on the level of characters alleviates this problem to a large
extent, as the vocabulary of possible characters is much smaller than the vocabulary of possible
words. However, working on the character level is very challenging, as the relationship between
form (characters) and function (syntax, semantics) in language is quite loose. Restricting oneself
to stay on the character level may be an unnecessarily hard constraint. Some researchers propose
a middle-ground, in which a word is represented as a combination of a vector for the word itself
with vectors of sub-word units that comprise it. e sub-word embeddings then help in sharing
information between diﬀerent words with similar forms, as well as allowing back-oﬀ to the sub-
word level when the word is not observed. At the same time, the models are not forced to rely
solely on form when enough observations of the word are available. Botha and Blunsom [2014]

132
10. PRE-TRAINED WORD REPRESENTATIONS
suggest to model the embedding vector of a word as a sum of the word-speciﬁc vector if such
vector is available, with vectors for the diﬀerent morphological components that comprise it (the
components are derived using Morfessor [Creutz and Lagus, 2007], an unsupervised morphologi-
cal segmentation method). Gao et al. [2014] suggest using as core features not only the word form
itself but also a unique feature (hence a unique embedding vector) for each of the letter-trigrams
in the word.
Another middle ground between characters and words is breaking up words into “mean-
ingful units” which are larger than characters and are automatically derived from the corpus. One
such approach is to use Byte-Pair Encoding (B) [Gage, 1994], which was introduced by Sen-
nrich et al. [2016a] in the context of Machine Translation and proved to be very eﬀective. In the
B approach, one decides on a vocabulary size (say 10,000), and then looks for 10,000 units that
can represent all the words in the corpus vocabulary according to the following algorithm, taken
from Sennrich et al. [2016a].
We initialize the symbol vocabulary with the character vocabulary, and represent each
word as a sequence of characters, plus a special end-of-word symbol ‘�’, which allows
us to restore the original tokenization after translation. We iteratively count all symbol
pairs and replace each occurrence of the most frequent pair (A, B) with a new sym-
bol AB. Each merge operation produces a new symbol which represents a character
n-gram. Frequent character n-grams (or whole words) are eventually merged into a
single symbol, thus B requires no shortlist. e ﬁnal symbol vocabulary size is equal
to the size of the initial vocabulary, plus the number of merge operations—the latter
is the only hyperparameter of the algorithm. For eﬃciency, we do not consider pairs
that cross word boundaries. e algorithm can thus be run on the dictionary extracted
from a text, with each word being weighted by its frequency.
10.6
DEALING WITH MULTI-WORD UNITS AND WORD
INFLECTIONS
Two issues that are still under-explored with respect to word representations have to do with the
deﬁnition of a word. e unsupervised word embedding algorithms assume words correspond
to tokens (consecutive characters without whitespace or punctuation, see the “What is a word?”
discussion in Section 6.1). is deﬁnition often breaks.
In English, we have many multi-token units such as New York and ice cream, as well as looser
cases such as Boston University or Volga River, that we may want to assign to single vectors.
In many languages other than English, rich morphological inﬂection systems make forms
that relate to the same underlying concept look diﬀerently. For example, in many languages adjec-
tives are inﬂected for number and gender, causing the word yellow describing a plural, masculine
noun to have a diﬀerent form from the word yellow describing a singular, feminine noun. Even
worse, as the inﬂection system also dictates the forms of the neighboring words (nouns near the

10.7. LIMITATIONS OF DISTRIBUTIONAL METHODS
133
singular feminine form of yellow are themselves in a singular feminine form), diﬀerent inﬂections
of the same word often do not end up similar to each other.
While there are no good solutions to either of these problems, they can both be addressed to
a reasonable degree by deterministically pre-processing the text such that it better ﬁts the desired
deﬁnitions of words.
In the multi-token units case, one can derive a list of such multi-token items, and replace
them in the text with single entities (i.e., replacing occurrences of New York with New_York.
Mikolov et al. [2013a] proposes a PMI-based method for automatically creating such a list, by
considering the PMI of a word pair, and merging pairs with PMI scores that pass some predeﬁned
thresholds. e process then iteratively repeats to merge pairs + words into triplets, and so on.
en, the embedding algorithm is run over the pre-processed corpus. is coarse but eﬀective
heuristic is implemented as part of the W2V package, allowing to derive embeddings also
for some prominent multi-token items.⁸
In the inﬂections case, one can mitigate the problem to a large extent by pre-processing the
corpus by lemmatizing some or all of the words, embedding the lemmas instead of the inﬂected
forms.
A related pre-processing is POS-tagging the corpus, and replacing words with (word,POS)
pairs, creating, for example, the two diﬀerent token types bookNOUN and bookVERB, that will each
receive a diﬀerent embedding vector. For further discussion on the interplay of morphological
inﬂections and word embeddings algorithms see Avraham and Goldberg [2017], Cotterell and
Schutze [2015].
10.7
LIMITATIONS OF DISTRIBUTIONAL METHODS
e distributional hypothesis oﬀers an appealing platform for deriving word similarities by repre-
senting words according to the contexts in which they occur. It does, however, have some inherent
limitations that should be considered when using the derived representations.
Deﬁnition of similarity e deﬁnition of similarity in distributional approaches is completely
operational: words are similar if used in similar contexts. But in practice, there are many facets of
similarity. For example, consider the words dog, cat, and tiger. On the one hand, cat is more similar
to dog than to tiger, as both are pets. On the other hand, cat can be considered more similar to tiger
than to dog as they are both felines. Some facets may be preferred over others in certain use cases,
and some may not be attested by the text as strongly as others. e distributional methods provide
very little control over the kind of similarities they induce. is could be controlled to some extent
by the choice of conditioning contexts (Section 10.5), but it is far from being a complete solution.
BlackSheeps When using texts as the conditioning contexts, many of the more “trivial” properties
of the word may not be reﬂected in the text, and thus not captured in the representation. is
happens because of a well-documented bias in people’s use of language, stemming from eﬃciency
constraints on communication: people are less likely to mention known information than they are
⁸For in-depth discussion of heuristics for ﬁnding informative word collocations, see Manning and Schütze [1999, Chapter 5].

134
10. PRE-TRAINED WORD REPRESENTATIONS
to mention novel one. us, when people talk of white sheep, they will likely refer to them as sheep,
while for black sheep they are much more likely to retain the color information and say black sheep.
A model trained on text data only can be greatly misled by this.
Antonyms Words that are the opposite of each other (good vs. bad, buy vs. sell, hot vs cold) tend to
appear in similar contexts (things that can be hot can also be cold, things that are bought are often
sold). As a consequence, models based on the distributional hypothesis tend to judge antonyms
as very similar to each other.
Corpus Biases For better or worse, the distributional methods reﬂect the usage patterns in the
corpora on which they are based, and the corpora in turn reﬂect human biases in the real world
(cultural or otherwise). Indeed, Caliskan-Islam et al. [2016] found that distributional word vec-
tors encode “every linguistic bias documented in psychology that we have looked for,” including racial
and gender stereotypes (i.e., European American names are closer to pleasant terms while African
American names are closer to unpleasant terms; female names are more associated with family
terms than with career terms; it is possible to predict the percentage of women in an occupa-
tion according to U.S. census based on the vector representation of the occupation name). Like
with the antonyms case, this behavior may or may not be desired, depending on the use case:
if our task is to guess the gender of a character, knowing that nurses are stereotypically females
while doctors are stereotypically males may be a desired property of the algorithm. In many other
cases, however, we would like to ignore such biases. In any case, these tendencies of the induced
word similarities should be taken into consideration when using distributional representations.
For further discussion, see Caliskan-Islam et al. [2016] and Bolukbasi et al. [2016].
Lack of Context e distributional approaches aggregate the contexts in which a term occurs in
a large corpus. e result is a word representation which is context independent. In reality, there
is no such thing as a context-independent meaning for a word. As argued by Firth [1935], “the
complete meaning of a word is always contextual, and no study of meaning apart from context can be
taken seriously”. An obvious manifestation of this is the case of polysemy: some words have obvious
multiple senses: a bank may refer to a ﬁnancial institution or to the side of a river, a star may an
abstract shape, a celebrity, an astronomical entity, and so on. Using a single vector for all forms
is problematic. In addition to the multiple senses problem, there are also much subtler context-
dependent variations in word meaning.

135
C H A P T E R
11
Using Word Embeddings
In Chapter 10 we discussed algorithms for deriving word vectors from large quantities of unan-
notated text. Such vectors can be very useful as initialization for the word embedding matrices
in dedicated neural networks. ey also have practical uses on their own, outside the context of
neural networks. is chapter discusses some of these uses.
Notation
In this chapter, we assume each word is assigned an integer index, and use symbols
such as w or wi to refer to both a word and its index. E Œw� is then the row in E corresponding
to word w. We sometimes use w, wi to denote the vectors corresponding to w and wi.
11.1
OBTAINING WORD VECTORS
Word-embedding vectors are easy to train from a corpus, and eﬃcient implementations of train-
ing algorithms are available. Moreover, one can also download pre-trained word vectors that were
trained on very large quantities of text (bearing in mind that diﬀerences in training regimes and
underlying corpora have a strong inﬂuence on the resulting representations, and that the available
pre-trained representations may not be the best choice for the particular use case).
As the time of this writing, eﬃcient implementations of the W2V algorithms are
available as a stand-alone binary¹ as well as in the GS python package.² A modiﬁcation of
the W2V binary that allows using arbitrary contexts is also available.³ An eﬃcient imple-
mentation of the GloVe model is available as well.⁴ Pre-trained word vectors for English can be
obtained from Google⁵ and Stanford⁶ as well as other sources. Pre-trained vectors in languages
other than English can be obtain from the Polyglot project.⁷
11.2
WORD SIMILARITY
Given pre-trained word embedding vectors, the major use aside from feeding them into a neural
network is to compute the similarity between two words using a similarity function over vectors
sim.u; v/. A common and eﬀective choice for similarity between vectors is the cosine similarity,
¹https://code.google.com/archive/p/word2vec/
²https://radimrehurek.com/gensim/
³https://bitbucket.org/yoavgo/word2vecf
⁴http://nlp.stanford.edu/projects/glove/
⁵https://code.google.com/archive/p/word2vec/
⁶http://nlp.stanford.edu/projects/glove/
⁷http://polyglot.readthedocs.org

136
11. USING WORD EMBEDDINGS
corresponding to the cosine of the angle between the vectors:
simcos.u; v/ D
u � v
kuk2kvk2
:
(11.1)
When the vectors u and v are of unit-length (kuk2 D kvk2 D 1) the cosine similarity re-
duces to a dot-product simcos.u; v/ D u � v D P
i uŒi�vŒi�. Working with dot-products is very con-
venient computationally, and it is common to normalize the embeddings matrix such that each
row has unit length. From now on, we assume the embeddings matrix E is normalized in this
way.
11.3
WORD CLUSTERING
e word vectors can be easily clustered using clustering algorithms such as KMeans that are
deﬁned over Euclidean spaces. e clusters can then be used as features in learning algorithms
that work with discrete features, or in other systems that require discrete symbols such as IR
indexing systems.
11.4
FINDING SIMILAR WORDS
With row-normalized embeddings matrix as described above, the cosine similarity between two
words w1 and w2 is given by:
simcos.w1; w2/ D E Œw1� � E Œw2�:
(11.2)
We are often interested in the k most similar words to a given word. Let w D E Œw� be the
vector corresponding to word w. e similarity to all other words can be computed by the matrix-
vector multiplication s D Ew. e result s is a vector of similarities, where sŒi� is the similarity of
w to the ith word in the vocabulary (the ith row in E). e k most similar words can be extracted
by ﬁnding the indices corresponding to the k highest values in s.
In a optimized modern scientiﬁc computing library such as numpy,⁸ such matrix-vector
multiplication is executed in milliseconds for embedding matrices with hundreds of thousands of
vectors, allowing rather rapid calculation of similarities.
Word similarities that result from distributional measures can be combined with other
forms of similarity. For example, we can deﬁne a similarity measure that is based on orthographic
similarity (words that share the same letters). By ﬁltering the list of top-k distributional-similar
words to contain words that are also orthographically similar to the target word, we can ﬁnd
spelling variants and common typos of the target word.
⁸http://www.numpy.org/

11.5. ODD-ONE OUT
137
11.4.1 SIMILARITY TO A GROUP OF WORDS
We may be interested in ﬁnding the most similar word to a group of words. is need arises when
we have a list of related words, and want to expand it (for example, we have a list of four countries
and want to extend it with names of more countries, or we have a list of gene names, and want
ﬁnd names of additional genes). Another use case is when we want to direct the similarity to be
to a given sense of a word. By creating a list of words that are related to that sense, we can direct
the similarity query toward that sense.
ere are many way of deﬁning similarity of an item to a group, here we take the deﬁnition
to be the average similarity to the items in the group, i.e., given a group of words w1Wk we deﬁne
its similarity to word w as: sim.w; w1Wk/ D 1
k
Pk
iD1 simcos.w; wi/.
anks to linearity, computing the average cosine similarity from a group of words to all
other words can be again done using a single matrix-vector multiplication, this time between the
embedding matrix and the average word vector of the words in the group. e vector s in which
sŒw� D sim.w; w1Wk/ is computed by:
s D E.w1 C w2 C : : : C wk/=k:
(11.3)
11.5
ODD-ONE OUT
Consider an odd-one-out question in which we are given a list of words and need to ﬁnd the one
that does not belong. is can be done by computing the similarity between each word to the
average word vector of the group, and returning the least similar word.
11.6
SHORT DOCUMENT SIMILARITY
Sometimes we are interested in computing a similarity between two documents. While the best
results are likely to be achieved using dedicated models solutions based on pre-trained word em-
beddings are often very competitive, especially when dealing with short documents as such web
queries, newspaper headlines, or tweets. e idea is to represent each document as a bag-of-
words, and deﬁne the similarity between the documents to be the sum of the pairwise similarities
between the words in the documents. Formally, consider two documents D1 D w1
1; w1
2; : : : ; w1
m
and D2 D w2
1; w2
2; : : : ; w2
n, and deﬁne the document similarity as:
simdoc.D1; D2/ D
m
X
iD1
n
X
j D1
cos.w1
i ; w2
j /:
Using basic linear algebra, it is straightforward to show that for normalized word vectors
this similarity function can be computed as the dot product between the continuous-bag-of-words

138
11. USING WORD EMBEDDINGS
representations of the documents:
simdoc.D1; D2/ D
 m
X
iD1
w1
i
!
�
0
@
n
X
j D1
w2
j
1
A :
Consider a document collection D1Wk, and let D be a matrix in which each row i is the
continuous bag-of-words representation of document Di. en the similarity between a new
document D0 D w0
1Wn and each of the documents in the collection can be computed using a single
matrix-vector product: s D D �
�Pn
iD1 w0
i
�.
11.7
WORD ANALOGIES
An interesting observation by Mikolov and colleagues [Mikolov et al., 2013a, Mikolov et al.,
2013] that greatly contributed to the popularity of word embeddings is that one can perform
“algebra” on the word vectors and get meaningful results. For example, for word embeddings
trained using W2V, one could take the vector of the word king, subtract the word man, add
the word woman and get that the closest vector to the result (when excluding the words king, man,
and woman) belongs to the word queen. at is, in vector space wking � wman C wwoman � wqueen.
Similar results are obtained for various other semantic relations, for example wFrance � wParis C
wLondon � wEngland, and the same holds for many other cities and countries.
is has given rise to the analogy solving task in which diﬀerent word embeddings are eval-
uated on their ability to answer analogy questions of the form man:woman ! king:? by solving:
analogy.m W w ! k W‹/ D
argmax
v2V nfm;w;kg
cos.v; k � m C w/:
(11.4)
Levy and Goldberg [2014] observe that for normalized vectors, solving the maximization
in Equation (11.4) is equivalent to solving Equation (11.5), that is, searching for a word that is
similar to king, similar to man, and dissimilar to woman:
analogy.m W w ! k W‹/ D
argmax
v2V nfm;w;kg
cos.v; k/ � cos.v; m/ C cos.v; w/:
(11.5)
Levy and Goldberg refer to this method as 3CA. e move from arithmetics between
words in vector space to arithmetics between word similarities helps to explain to some extent the
ability of the word embeddings to “solve” analogies, as well as suggest which kinds of analogies
can be recovered by this method. It also highlights a possible deﬁciency of the 3CA analogy
recovery method: because of the additive nature of the objective, one term in the summation may
dominate the expression, eﬀectively ignoring the others. As suggested by Levy and Goldberg, this
can be alleviated by changing to a multiplicative objective (3CM):
analogy.m W w ! k W‹/ D
argmax
v2V nfm;w;kg
cos.v; k/ cos.v; w/
cos.v; m/ C �
:
(11.6)

11.8. RETROFITTING AND PROJECTIONS
139
While the analogy-recovery task is somewhat popular for evaluating word embeddings,
it is not clear what success on a benchmark of analogy tasks says about the quality of the word
embeddings beyond their suitability for solving this speciﬁc task.
11.8
RETROFITTING AND PROJECTIONS
More often than not, the resulting similarities do not fully reﬂect the similarities one has in mind
for their application. Often, one can come up with or have access to a representative and relatively
large list of word pairs that reﬂects the desired similarity better than the word embeddings, but has
worse coverage. e retroﬁtting method of Faruqui et al. [2015] allows to use such data in order to
improve the quality of the word embeddings matrix. Faruqui et al. [2015] show the eﬀectiveness of
the approach by using information derived from WordNet and PPDB (Section 6.2.1) to improve
pre-trained embedding vectors.
e method assumes pre-trained word embedding matrix E as well as a graph G that en-
codes binary word to word similarities—nodes in the graph are words, and words are similar if
they are connected by an edge. Note that the graph representation is very general, and a list of
word pairs that are considered similar easily ﬁts within the framework. e method works by solv-
ing an optimization problems that searches for a new word embeddings matrix OE whose rows are
close both to the corresponding rows in E but also to the rows corresponding to their neighbors
in the graph G. Concretely, the optimization objective is:
argmin
OE
n
X
iD1
0
@˛ik OE Œwi� � E Œwi�k2 C
X
.wi;wj /2G
ˇij k OE Œwi� � OE Œwj �k2
1
A ;
(11.7)
where ˛i and ˇij reﬂect the importance of a word being similar to itself or to another word. In
practice, ˛i are typically set uniformly to 1, while ˇij is set to the inverse of the degree of wi in
the graph (if a word has many neighbors, it has smaller inﬂuence on each of them). e approach
works quite well in practice.
A related problem is when one has two embedding matrices: one with a small vocabulary
E S 2 RjVS j�demb and another one with a large vocabulary E L 2 RjVLj�demb that were trained sep-
arately, and are hence incompatible. Perhaps the smaller vocabulary matrix was trained using a
more expensive algorithm (possibly as part of a larger and more complex network), and the larger
one was downloaded from the web. ere is some overlap in the vocabularies, and one is inter-
ested in using word vectors from the larger matrix E L for representing words that are not available
in the smaller one E S. One can then bridge the gap between the two embedding spaces using a
linear projection⁹ [Kiros et al., 2015, Mikolov et al., 2013]. e training objective is searching for
a good projection matrix M 2 Rdemb�demb that will map rows in E L such that they are close to
⁹Of course, for this to work one needs to assume a linear relation between the two spaces. e linear projection method often
works well in practice.

140
11. USING WORD EMBEDDINGS
corresponding rows in E S by solving the following optimization problem:
argmin
M
X
w2VS \VL
kE L
Œw� � M � E S
Œw�k:
(11.8)
e learned matrix can then be used to project also the rows of E L that do not have corresponding
rows in E S. is approach was successfully used by Kiros et al. [2015] to increase the vocabulary
size of an LSTM-based sentence encoder (the sentence encoding model of Kiros et al. [2015] is
discussed in Section 17.3).
Another cute (if somewhat less robust) application of the projection approach was taken
by Mikolov et al. [2013] who learned matrices to project between embedding vectors trained on
language A (say English) to embedding vectors trained on language B (say Spanish) based on a
seed list of known word-word translation between the languages.
11.9
PRACTICALITIES AND PITFALLS
While oﬀ-the-shelf, pre-trained word embeddings can be downloaded and used, it is advised to
not just blindly download word embeddings and treat them as a black box. Choices such as the
source of the training corpus (but not necessarily its size: larger is not always better, and a smaller
but cleaner, or smaller but more domain-focused corpora, are often more eﬀective for a given use
case), the contexts that were used for deﬁning the distributional similarities, and many hyper-
parameters of the learning can greatly inﬂuence the results. In presence of an annotated test set
for the similarity task one cares about, it is best to experiment with several setting and choose the
setup that works best on a development set. For discussion on the possible hyper-parameters and
how they may aﬀect the resulting similarities, see the work of Levy et al. [2015].
When using oﬀ-the-shelf embedding vectors, it is better to use the same tokenization and
text normalization schemes that were used when deriving the corpus.
Finally, the similarities induced by word vectors are based on distributional signals, and
therefore susceptible to all the limitations of distributional similarity methods described in Sec-
tion 10.7. One should be aware of these limitations when using word vectors.

141
C H A P T E R
12
Case Study: A Feed-forward
Architecture for Sentence
Meaning Inference
In Section 11.6 we introduced the sum of pairwise word similarities as a strong baseline for the
short document similarity task. Given two sentences, the ﬁrst one with words w1
1; : : : ; w1
`1 and
the second one with words w2
1; : : : ; w2
`2, each word is associated with a corresponding pre-trained
word vector w1
1W`1, w2
1W`2, and the similarity between the documents is given by:
`1
X
iD1
`2
X
j D1
sim
�
w1
i ; w2
j
�
:
While this is a strong baseline, it is also completely unsupervised. Here, we show how a
document similarity score can be greatly improved if we have a source of training data. We will
follow the network presented by Parikh et al. [2016] for the Stanford Natural Language Inference
(SNLI) semantic inference task. Other than providing a strong model for the SNLI task, this
model also demonstrates how the basic network components described so far can be combined in
various layers, resulting in a complex and powerful network that is trained jointly for a task.
12.1
NATURAL LANGUAGE INFERENCE AND THE SNLI
DATASET
In the natural language inference task, also called recognizing textual entailment (RTE), you are
given two texts, s1 and s2, and need to decide if s1 entails s2 (that is, can you infer s2 from s1),
contradicts it (they cannot both be true), or if the texts are neutral (the second one neither entails
nor contradicts the ﬁrst). Example sentences for the diﬀerent conditions are given in Table 12.1.
e entailment task was introduced by Dagan and Glickman [2004], and subsequently es-
tablished through a series of benchmarks known as the PASCAL RTE Challenges [Dagan et al.,
2005]. e task is very challenging,¹ and solving it perfectly entails human level understanding of
¹e SNLI dataset described here focuses on descriptions of scenes that appear in images, and is easier than the general and
un-restricted RTE task, which may require rather complex inference steps in order to solve. An example of an entailing pair in
the un-restricted RTE task is About two weeks before the trial started, I was in Shapiro’s oﬃce in Century City ) Shapiro worked
in Century City.

142
12. CASE STUDY: A FEED-FORWARD ARCHITECTURE FOR SENTENCE
Table12.1: e Natural Language Inference (Textual Entailment) Task. e examples are taken from
the development set of the SNLI dataset.
Two men on bicycles competing in a race.
Entail
People are riding bikes.
Neutral
Men are riding bicycles on the street.
Contradict
A few people are catching f sh.
Two doctors perform surgery on patient.
Entail
Doctors are performing surgery.
Neutral
Two doctors are performing surgery on a man.
Contradict
Two surgeons are having lunch.
language. For in-depth discussion on the task and approaches to its solution that do not involve
neural networks, see the book by Dagan, Roth, Sammons, and Zanzotto in this series [Dagan
et al., 2013].
SNLI is a large dataset created by Bowman et al. [2015], containing 570k human-written
sentence pairs, each pair manually categorized as entailing, contradicting, or neutral. e sen-
tences were created by presenting image captions to annotators, and asking them, without seeing
the image, to write a caption that is deﬁnitely a true description of the image (entail), a caption
that is might be a true description of the image (neutral), and a caption that is a deﬁnitely false
description of the image (contradict). After collecting 570k sentence pairs this way, 10% of them
were further validated by presenting sentence pairs to diﬀerent annotators and asking them to
categorize the pair into entailing, neutral, or contradicting. e validated sentences are then used
for the test and validation sets. e examples in Table 12.1 are from the SNLI dataset.
While simpler than the previous RTE challenge datasets, it is also much larger, and still not
trivial to solve (in particular for distinguishing the entailing from the neutral events). e SNLI
dataset is a popular dataset for assessing meaning inference models. Notice that the task goes
beyond mere pairwise word similarity: for example, consider the second sentence in Table 12.1:
the neutral sentence is much more similar (in terms of average word similarity) to the original
one than the entailed sentence. We need the ability to highlight some similarities, degrade the
strength of others, and also to understand which kind of similarities are meaning preserving (i.e.,
going from man to patient in the context of a surgery), and which add new information (i.e., going
from patient to man). e network architecture is designed to facilitate this kind of reasoning.
12.2
A TEXTUAL SIMILARITY NETWORK
e network will work in several stages. In the ﬁrst stage, our goal is to compute pairwise word
similarities that are more suitable for the task. e similarity function for two word vectors is

12.2. A TEXTUAL SIMILARITY NETWORK
143
deﬁned to be:
sim.w1; w2/ D MLPtransform.w1/ � MLPtransform.w2/
(12.1)
MLPtransform.x/ 2 Rds
w1; w2 2 Rdemb:
at is, we ﬁrst transform each word vector by use of a trained nonlinear transformation,
and then take the dot-product of the transformed vectors.
Each word in sentence a can be similar to several words in sentence b, and vice versa. For
each word wa
i in sentence a we compute a `b-dimensional vector of its similarities to words in
sentence b, normalized via softmax so that all similarities are positive and sum to one. is is
called the alignment vector for the word:
˛a
i D softmax.sim.wa
i ; wb
1/; : : : ; sim.wa
i ; wb
`b//:
(12.2)
We similarly compute an alignment vector for each word in b
˛b
i D softmax.sim.wa
1; wb
i /; : : : ; sim.wa
`a; wb
i //
˛a
i 2 NC
`b
˛b
i 2 NC
`a:
For every word wa
i we compute a vector Nwb
i composed of a weighted-sum of the words in
b that are aligned to wa
i , and similarly for every word wb
j:
Nwb
i D
`b
X
jD1
˛a
i Œj�wb
j
Nwa
j D
`a
X
iD1
˛b
i Œj�wa
i :
(12.3)
A vector Nwb
i captures the weighted mixture of words in sentence b that are triggered by the ith
word in sentence a.
Such weighted sum representations of a sequence of vectors, where the weights are com-
puted by a softmax over scores such as the one in Equation (12.2), are often referred to as an
attention mechanism. e name comes from the fact that the weights reﬂect how important is
each item in the target sequence to the given source item—how much attention should be given
to each of the items in the target sequence with respect to the source item. We will discuss atten-
tion in more details in Chapter 17, when discussing conditioned-generation models.
e similarity between wa
i and the corresponding triggered mixture Nwb
i in sentence b is not
necessarily relevant for the NLI task. We attempt to transform each such pair into a representation

144
12. CASE STUDY: A FEED-FORWARD ARCHITECTURE FOR SENTENCE
vector va
i that focuses on the important information for the task. is is done using another feed-
forward network:
va
i D MLPpair.Œwa
i I Nwb
i �/
vb
j D MLPpair.Œwb
j I Nwa
j �/:
(12.4)
Note that unlike the similarity function in Equation (12.1) that considered each term individually,
here the function can handle both terms diﬀerently.
Finally, we sum the resulting vectors and pass them into a ﬁnal MLP classiﬁer for predicting
the relation between the two sentences (entail, contradict, or neutral):
va D
X
i
va
i
vb D
X
j
vb
j
Oy D MLPdecide.ŒvaI vb�/:
(12.5)
In the work of Parikh et al. [2016], all the MLPs have two hidden layers of size 200, and a
ReLU activation function. e entire process is captured in the same computation graph, and the
network is trained end-to-end using the cross-entropy loss. e pre-trained word embeddings
themselves were not changed with the rest of the network, relying on MLPtransform to do the
needed adaptation. As of the time of this writing, this architecture is the best performing network
on the SNLI dataset.
To summarize the architecture, the transform network learns a similarity function for word-
level alignment. It transforms each word into a space that preserves important word-level sim-
ilarities. After the transform network, each word vector is similar to other words that are likely
to refer to the same entity or the same event. e goal of this network is to ﬁnd words that may
contribute to the entailment. We get alignments in both directions: from each word in a to mul-
tiple words in b, and from each word in b to multiple words in a. e alignments are soft, and are
manifested by weighted group membership instead of by hard decisions, so a word can participate
in many pairs of similarities. is network is likely to put men and people next to each other, men
and two next to each other and man and patient next to each other, and likewise for inﬂected
forms perform and performing.
e pair network then looks at each aligned pair (word + group) using a weighted-CBOW
representation, and extracts information relevant to the pair. Is this pair useful for the entailment
prediction task? It also looks at sentence each component of the pair came from, and will likely
learn that patient and man are entailing in one direction and not the other.
Finally, the decide network looks at the aggregated data from the word pairs, and comes up
with a decision based on all the evidence. We have three stages of reasoning: ﬁrst one recovers weak
local evidence in terms of similarity alignment; the second one looks at weighted multi-word units
and also adds directionality; and the third integrates all the local evidence into a global decision.

12.2. A TEXTUAL SIMILARITY NETWORK
145
e details of the network are tuned for this particular task and dataset, and it is not clear
if they will generalize to other settings. e idea of this chapter was not to introduce a speciﬁc
network architecture, but rather to demonstrate that complex architectures can be designed, and
that it is sometimes worth the eﬀort to do so. A new component introduced in this chapter that
is worth noting is the use of the soft alignment weights ˛a
i (also sometimes called attention), in
order to compute a weighted sum of elements Nwb
i [Equation (12.3)]. We will encounter this idea
again when discussing attention-based conditioned generation with RNNs in Chapter 17.

PART III
Specialized Architectures

149
In the previous chapters, we’ve discussed supervised learning and feed-forward neural net-
works, and how they can be applied to language tasks. e feed-forward neural networks are for
the most part general-purpose classiﬁcation architectures—nothing in them is tailored speciﬁ-
cally for language data or sequences. Indeed, we mostly structured the language tasks to ﬁt into
the MLP framework.
In the following chapters, we will explore some neural architectures that are more special-
ized for dealing with language data. In particular, we will discuss 1D convolutional-and-pooling
architectures (CNNs), and recurrent neural networks (RNNs). CNNs are neural architectures
that are specialized at identifying informative ngrams and gappy-ngrams in a sequence of text,
regardless of their position, but while taking local ordering patterns into account. RNNs are neu-
ral architectures that are designed to capture subtle patterns and regularities in sequences, and
that allow modeling non-markovian dependencies looking at “inﬁnite windows” around a fo-
cus word, while zooming-in on informative sequential patterns in that window. Finally, we will
discuss sequence-generation models and conditioned generation.
Feature Extraction
e CNN and RNN architectures explored in this part of the book are pri-
marily used as feature extractors. A CNN or an RNN network are not a standalone component,
but rather each such network produces a vector (or a sequence of vectors) that are then fed into
further parts of the network that will eventually lead to predictions. e network is trained end-
to-end (the predicting part and the convolutional/recurrent architectures are trained jointly) such
that the vectors resulting from the convolutional or recurrent part of the network will capture the
aspects of the input that are useful for the given prediction task. In the following chapters, we
introduce feature extractors that are based on the CNN and the RNN architectures. As the time
of this writing, RNN-based feature extractors are more established than CNNs as feature extrac-
tors for text-based applications. However, the diﬀerent architectures have diﬀerent strengths and
weaknesses, and the balance between them may shift in the future. Both are worth knowing, and
hybrid approaches are also likely to become popular. Chapters 16 and 17 discuss the integration
of RNN-based feature extractors in diﬀerent NLP prediction and generation architectures. Large
parts of the discussion in these chapters are applicable also to convolutional networks.
CNNs and RNNs as Lego Bricks
When learning about the CNN and RNN architectures, it is
useful to think about them as “Lego Bricks,” that can be mixed and matched to create a desired
structure and to achieve a desired behavior.
is Lego-bricks-like mixing-and-matching is facilitated by the computation-graph mech-
anism and gradient-based optimization. It allows treating network architectures such as MLPs,
CNNs and RNNs as components, or blocks, that can be mixed and matched to create larger
and larger structures—one just needs to make sure that that input and output dimensions of the
diﬀerent components match—and the computation graph and gradient-based training will take
care of the rest.

150
is allows us to create large and elaborate network structures, with multiple layers of
MLPs, CNNs and RNNs feeding into each other, and training the entire network in an end-
to-end fashion. Several examples are explored in later chapters, but many others are possible, and
diﬀerent tasks may beneﬁt from diﬀerent architectures. When learning about a new architecture,
don’t think “which existing component does it replace?” or “how do I use it to solve a task?” but
rather “how can I integrate it into my arsenal of building blocks, and combine it with the other
components in order to achieve a desired result?”.

151
C H A P T E R
13
Ngram Detectors:
Convolutional Neural Networks
Sometimes we are interested in making predictions based on ordered sets of items (e.g., the se-
quence of words in a sentence, the sequence of sentences in a document, and so on). Consider,
for example, predicting the sentiment (positive, negative, or neutral) of sentences such as the
following.
• Part of the charm of Satin Rouge is that it avoids the obvious with humor and lightness.
• Still, this ﬂick is fun and host to some truly excellent sequences.
Some of the sentence words are very informative of the sentiment (charm, fun, excellent) other
words are less informative (Still, host, ﬂick, lightness, obvious, avoids) and to a good approximation,
an informative clue is informative regardless of its position in the sentence. We would like to feed
all of the sentence words into a learner, and let the training process ﬁgure out the important
clues. One possible solution is feeding a CBOW representation into a fully connected network
such as an MLP. However, a downside of the CBOW approach is that it ignores the ordering
information completely, assigning the sentences “it was not good, it was actually quite bad” and “it
was not bad, it was actually quite good” the exact same representation. While the global positions of
the indicators “not good” and “not bad” do not matter for the classiﬁcation task, the local ordering
of the words (that the word “not” appears right before the word “bad”) is very important. Similarly,
in the corpus-based example Montias pumps a lot of energy into his nuanced narative, and surrounds
himself with a cast of quirky—but not stereotyped—street characters, there is a big diﬀerence between
“not stereotyped” (positive indicator) and “not nuanced” (negative indicator). While the examples
above are simple cases of negation, some patterns are not as obvious, e.g., “avoids the obvious” vs.
“obvious” or vs. “avoids the charm” in the ﬁrst example. In short, looking at ngrams is much more
informative than looking at a bag-of-words.
A naive approach would suggest embedding word-pairs (bi-grams) or word-triplets (tri-
grams) rather than words, and building a CBOW over the embedded ngrams. While such an
architecture is indeed quite eﬀective, it will result huge embedding matrices, will not scale for
longer ngrams, and will suﬀer from data sparsity problems as it does not share statistical strength
between diﬀerent ngrams (the embedding of “quite good” and “very good” are completely inde-
pendent of one another, so if the learner saw only one of them during training, it will not be able
to deduce anything about the other based on its component words).

152
13. NGRAM DETECTORS: CONVOLUTIONAL NEURAL NETWORKS
e CNN architecture
is chapter introduces the convolution-and-pooling (also called con-
volutional neural networks, or CNNs) architecture, which is tailored to this modeling problem.
A convolutional neural network is designed to identify indicative local predictors in a large struc-
ture, and to combine them to produce a ﬁxed size vector representation of the structure, capturing
the local aspects that are most informative for the prediction task at hand. I.e., the convolutional
architecture will identify ngrams that are predictive for the task at hand, without the need to
pre-specify an embedding vector for each possible ngram. (In Section 13.2, we discuss an alter-
native method that allows working with unbounded ngram vocabularies while keeping a bounded
size embedding matrix). e convolutional architecture also allows to share predictive behavior
between ngrams that share similar components, even if the exact ngram was never seen at test
time.
e convolutional architecture could be expanded into a hierarchy of convolution layers,
each one eﬀectively looking at a longer range of ngrams in the sentence. is also allows the
model to be sensitive to some non-contiguous ngrams. is is discussed in Section 13.3.
As discussed in the opening section of this part of the book, the CNN is in essence a
feature-extracting architecture. It does not constitute a standalone, useful network on its own,
but rather is meant to be integrated into a larger network, and to be trained to work in tandem
with it in order to produce an end result. e CNN layer’s responsibility is to extract meaningful
sub-structures that are useful for the overall prediction task at hand.
History and Terminology
Convolution-and-pooling architectures [LeCun and Bengio, 1995]
evolved in the neural networks vision community, where they showed great success as object
detectors—recognizing an object from a predeﬁned category (“cat,” “bicycles”) regardless of its
position in the image [Krizhevsky et al., 2012]. When applied to images, the architecture is us-
ing 2D (grid) convolutions. When applied to text, we are mainly concerned with 1D (sequence)
convolutions. Convolutional networks were introduced to the NLP community in the pioneering
work of Collobert et al. [2011] who used them for semantic-role labeling, and later by Kalchbren-
ner et al. [2014] and Kim [2014] who used them for sentiment and question-type classiﬁcation.
Because of their origins in the computer-vision community, a lot of the terminology around
convolutional neural networks is borrowed from computer vision and signal processing, including
terms such as ﬁlter, channel, and receptive-ﬁeld which are often used also in the context of text
processing. We will mention these terms when introducing the corresponding concepts.
13.1
BASIC CONVOLUTION + POOLING
e main idea behind a convolution and pooling architecture for language tasks is to apply a non-
linear (learned) function over each instantiation of a k-word sliding window over the sentence.¹
is function (also called “ﬁlter”) transforms a window of k words into a scalar value. Several such
ﬁlters can be applied, resulting in ` dimensional vector (each dimension corresponding to a ﬁlter)
¹e window-size, k, is sometimes referred to as the receptive ﬁeld of the convolution.

13.1. BASIC CONVOLUTION + POOLING
153
that captures important properties of the words in the window. en, a “pooling” operation is used
to combine the vectors resulting from the diﬀerent windows into a single `-dimensional vector,
by taking the max or the average value observed in each of the ` dimensions over the diﬀerent
windows. e intention is to focus on the most important “features” in the sentence, regardless of
their location—each ﬁlter extracts a diﬀerent indicator from the window, and the pooling opera-
tion zooms in on the important indicators. e resulting `-dimensional vector is then fed further
into a network that is used for prediction. e gradients that are propagated back from the net-
work’s loss during the training process are used to tune the parameters of the ﬁlter function to
highlight the aspects of the data that are important for the task the network is trained for. In-
tuitively, when the sliding window of size k is run over a sequence, the ﬁlter function learns to
identify informative kgrams. Figure 13.2 illustrates an application of a convolution-and-pooling
network over a sentence.
13.1.1 1D CONVOLUTIONS OVER TEXT
We begin by focusing on the one-dimensional convolution operation.² e next section will focus
on pooling.
Consider a sequence of words w1Wn D w1; : : : ; wn, each with their corresponding demb
dimensional word embedding E Œwi� D wi. A 1D convolution of width-k works by moving a
sliding-window of size k over the sentence, and applying the same “ﬁlter” to each window
in the sequence, where a ﬁlter is a dot-product with a weight vector u, which is often fol-
lowed by a nonlinear activation function. Deﬁne the operator ˚.wiWiCk�1/ to be the con-
catenation of the vectors wi; : : : ; wiCk�1. e concatenated vector of the ith window is then
xi D ˚.wiWiCk�1/ D ŒwiI wiC1I : : : I wiCk�1�, xi 2 Rk�demb.
We then apply the ﬁlter to each window-vector, resulting scalar values pi:
pi Dg.xi � u/
(13.1)
xi D ˚ .wiWiCk�1/
(13.2)
pi 2 R
xi 2 Rk�demb
u 2 Rk�demb;
where g is a nonlinear activation.
It is customary to use ` diﬀerent ﬁlters, u1; : : : ; u`, which can be arranged into a matrix U ,
and a bias vector b is often added:
pi D g.xi � U C b/
(13.3)
pi 2 R`
xi 2 Rk�demb
U 2 Rk�demb�`
b 2 R`:
Each vector pi is a collection of ` values that represent (or summarize) the ith window. Ideally,
each dimension captures a diﬀerent kind of indicative information.
²1D here refers to a convolution operating over 1-dimensional inputs such as sequences, as opposed to 2D convolutions which
are applied to images.

154
13. NGRAM DETECTORS: CONVOLUTIONAL NEURAL NETWORKS
Narrow vs. Wide Convolutions
How many vectors pi do we have? For a sentence of length n
with a window of size k, there are n � k C 1 positions in which to start the sequence, and we
get n � k C 1 vectors p1Wn�kC1. is is called a narrow convolution. An alternative is to pad the
sentence with k � 1 padding-words to each side, resulting in n C k C 1 vectors p1WnCkC1. is is
called a wide convolution [Kalchbrenner et al., 2014]. We use m to denote the number of resulting
vectors.
An Alternative Formulation of Convolutions
In our description of convolutions over a sequence
of n items w1Wn each item is associated with a d-dimensional vector, and the vector are concate-
nated into a large 1 � d � n sentence vector. e convolution network with a window of size k
and ` output values is then based on a k � d � ` matrix. is matrix is applied to segments of the
1 � d � n sentence matrix that correspond to k-word windows. Each such multiplication results
in ` values. Each of these k values can be thought of as the result of a dot product between a
k � d � 1 vector (a row in the matrix) and a sentence segment.
Another (equivalent) formulation that is often used in the literature is one in which the n
vectors are stacked on top of each other, resulting in an n � d sentence matrix. e convolution
operation is then performed by sliding ` diﬀerent k � d matrices (called “kernels” or “ﬁlters”)
over the sentence matrix, and performing a matrix convolution between each kernel and the cor-
responding sentence-matrix segment. e matrix convolution operation between two matrices is
deﬁned as performing element-wise multiplication of the two matrices, and summing the results.
Each of the ` sentence-kernel convolution operations produces a single value, for a total of `
values. It is easy to convince oneself that the two approaches are indeed equivalent, by observing
that each kernel corresponds to a row in the k � d � ` matrix, and the convolution with a kernel
corresponds to a dot-product with a matrix row.
Figure 13.1 show narrow and wide convolutions in the two notations.
*PAD* the
the actual
actual service
service was
was not
not very
very good
good *PAD*
*PAD* 
the
actual
service
was
not
very
good
*PAD*
the       actual    service      was         not        very       good
the actual
actual service
service was
was not
not very
very good
(a)
(b)
Figure 13.1: e inputs and outputs of a narrow and a wide convolution in the vector-concatenation
and the vector-stacking notations. (a) A narrow convolution with a window of size k D 2 and 3-
dimensional output (` D 3), in the vector-concatenation notation. (b) A wide convolution with a win-
dow of size k D 2, a 3-dimensional output (` D 3), in the vector-stacking notation.

[Image: extracted_image_163_0.png]
[Image: extracted_image_163_1.png]
13.1. BASIC CONVOLUTION + POOLING
155
Channels
In computer vision, a picture is represented as a collection of pixels, each represent-
ing the color intensity of a particular point. When using an RGB color scheme, each pixel is a
combination of three intensity values—one for each of the Red, Green, and Blue components.
ese are then stored in three diﬀerent matrices. Each matrix provides a diﬀerent “view” of the
image, and is referred to as a Channel. When applying a convolution to an image in computer
vision, it is common to apply a diﬀerent set of ﬁlters to each channel, and then combine the three
resulting vectors into a single vector. Taking the diﬀerent-views-of-the-data metaphor, we can
have multiple channels in text processing as well. For example, one channel will be the sequence
of words, while another channel is the sequence of corresponding POS tags. Applying the convo-
lution over the words will result in m vectors pw
1Wm, and applying it over the POS-tags will result
in m vectors pt
1Wm. ese two views can then be combined either by summation pi D pw
i C pt
i
or by concatenation pi D Œpw
i I pt
i�.
To summarize
e main idea behind the convolution layer is to apply the same parameterized
function over all k-grams in the sequence. is creates a sequence of m vectors, each representing
a particular k-gram in the sequence. e representation is sensitive to the identity and order of
the words within a k-gram, but the same representation will be extracted for a k-gram regardless
of its position within the sequence.
13.1.2 VECTOR POOLING
Applying the convolution over the text results in m vectors p1Wm, each pi 2 R`. ese vectors are
then combined (pooled) into a single vector c 2 R` representing the entire sequence. Ideally, the
vector c will capture the essence of the important information in the sequence. e nature of the
important information that needs to be encoded in the vector c is task dependent. For example,
if we are performing sentiment classiﬁcation, the essence are informative ngrams that indicate
sentiment, and if we are performing topic-classiﬁcation, the essence are informative ngrams that
indicate a particular topic.
During training, the vector c is fed into downstream network layers (i.e., an MLP), cul-
minating in an output layer which is used for prediction.³ e training procedure of the network
calculates the loss with respect to the prediction task, and the error gradients are propagated all
the way back through the pooling and convolution layers, as well as the embedding layers. e
training process tunes the convolution matrix U , the bias vector b, the downstream network, and
potentially also the embeddings matrix E such that the vector c resulting from the convolution
and pooling process indeed encodes information relevant to the task at hand.⁴
³e input to the downstream network can be either c itself, or a combination of c and other vectors.
⁴Besides being useful for prediction, a by-product of the training procedure is a set of parameters W , B, and embeddings E
that can be used in a convolution and pooling architecture to encode arbitrary length sentences into ﬁxed-size vectors, such
that sentences that share the same kind of predictive information will be close to each other.

156
13. NGRAM DETECTORS: CONVOLUTIONAL NEURAL NETWORKS
Max-pooling
e most common pooling operation is max pooling, taking the maximum value
across each dimension.
cŒj� D max
1<i�m pi Œj �
8j 2 Œ1; `�;
(13.4)
pi Œj � denotes the jth component of pi. e eﬀect of the max-pooling operation is to get the
most salient information across window positions. Ideally, each dimension will “specialize” in a
particular sort of predictors, and max operation will pick on the most important predictor of each
type.
Figure 13.2 provides an illustration of the convolution and pooling process with a max-
pooling operation.
the quick brown fox jumped over the lazy dog
the quick brown
          quick brown fox
                  brown fox jumped
                            fox jumped over
 
 
  jumped over the
 
 
             over the lazy
 
 
 
      the lazy dog
6 × 3
W
max
MUL+tanh
MUL+tanh
MUL+tanh
MUL+tanh
MUL+tanh
MUL+tanh
MUL+tanh
convolution
pooling
Figure 13.2: 1D convolution+pooling over the sentence “the quick brown fox jumped over the lazy
dog.” is is a narrow convolution (no padding is added to the sentence) with a window size of 3.
Each word is translated to a 2-dim embedding vector (not shown). e embedding vectors are then
concatenated, resulting in 6-dim window representations. Each of the seven windows is transfered
through a 6 � 3 ﬁlter (linear transformation followed by element-wise tanh), resulting in seven 3-
dimensional ﬁltered representations. en, a max-pooling operation is applied, taking the max over
each dimension, resulting in a ﬁnal 3-dimensional pooled vector.
Average Pooling
e second most common pooling type being average-pooling—taking the
average value of each index instead of the max:
c D 1
m
m
X
iD1
pi:
(13.5)

[Image: extracted_image_165_0.png]
13.1. BASIC CONVOLUTION + POOLING
157
One view of average-pooling is that of taking a continuous bag-of-words (CBOW) of the k-gram
representations resulting from the convolutions rather than from the sentence words.
K-max Pooling
Another variation, introduced by Kalchbrenner et al. [2014] is k-max pooling
operation, in which the top k values in each dimension are retained instead of only the best one,
while preserving the order in which they appeared in the text.⁵ For example, consider the following
matrix:
2
66664
1
2
3
9
6
5
2
3
1
7
8
1
3
4
1
3
77775
:
A 1-max pooling over the column vectors will result in
�
9
8
5
�, while a 2-max pool-
ing will result in the following matrix:
�9
6
3
7
8
5
�
whose rows will then be concatenated to
�
9
6
3
7
8
5
�.
e k-max pooling operation makes it possible to pool the k most active indicators that
may be a number of positions apart; it preserves the order of the features, but is insensitive to
their speciﬁc positions. It can also discern more ﬁnely the number of times the feature is highly
activated [Kalchbrenner et al., 2014].
Dynamic Pooling
Rather than performing a single pooling operation over the entire sequence,
we may want to retain some positional information based on our domain understanding of the
prediction problem at hand. To this end, we can split the vectors pi into r distinct groups, apply
the pooling separately on each group, and then concatenate the r resulting `-dimensional vectors
c1; : : : ; cr. e division of the pis into groups is performed based on domain knowledge. For
example, we may conjecture that words appearing early in the sentence are more indicative than
words appearing late. We can then split the sequence into r equally sized regions, applying a
separate max-pooling to each region. For example, Johnson and Zhang [2015] found that when
classifying documents into topics, it is useful to have 20 average-pooling regions, clearly separating
the initial sentences (where the topic is usually introduced) from later ones, while for a sentiment
classiﬁcation task a single max-pooling operation over the entire sentence was optimal (suggesting
that one or two very strong signals are enough to determine the sentiment, regardless of the
position in the sentence).
Similarly, in a relation extraction kind of task we may be given two words and asked to
determine the relation between them. We could argue that the words before the ﬁrst word, the
words after the second word, and the words between them provide three diﬀerent kinds of infor-
⁵In this chapter, we use k to denote the window-size of the convolution. e k in k-max pooling is a diﬀerent, and unrelated,
value. We use the letter k for consistency with the literature.

158
13. NGRAM DETECTORS: CONVOLUTIONAL NEURAL NETWORKS
mation [Chen et al., 2015]. We can thus split the pi vectors accordingly, pooling separately the
windows resulting from each group.
13.1.3 VARIATIONS
Rather than a single convolutional layer, several convolutional layers may be applied in parallel.
For example, we may have four diﬀerent convolutional layers, each with a diﬀerent window size in
the range 2–5, capturing k-gram sequences of varying lengths. e result of each convolutional
layer will then be pooled, and the resulting vectors concatenated and fed to further processing
[Kim, 2014].
e convolutional architecture need not be restricted into the linear ordering of a sen-
tence. For example, Ma et al. [2015] generalize the convolution operation to work over syntactic
dependency trees. ere, each window is around a node in the syntactic tree, and the pooling is
performed over the diﬀerent nodes. Similarly, Liu et al. [2015] apply a convolutional architecture
on top of dependency paths extracted from dependency trees. Le and Zuidema [2015] propose
performing max pooling over vectors representing the diﬀerent derivations leading to the same
chart item in a chart parser.
13.2
ALTERNATIVE: FEATURE HASHING
Convolutional networks for text work as very eﬀective feature detectors for consecutive k-grams.
However, they require many matrix multiplications, resulting in non-negligible computation. A
more time-eﬃcient alternative would be to just use k-gram embeddings directly, and then pool
the k-grams using average pooling (resulting in a continuous-bag-of-ngrams representations) or
max pooling. A downside of the approach is that it requires allocating a dedicated embedding
vector for each possible k-gram, which can be prohibitive in terms of memory as the number of
k-grams in the training corpus can be very large.
A solution to the problems is the use of the feature hashing technique that originated in
linear models [Ganchev and Dredze, 2008, Shi et al., 2009, Weinberger et al., 2009] and recently
adopted to neural networks [Joulin et al., 2016]. e idea behind feature hashing is that we don’t
pre-compute vocabulary-to-index mapping. Instead, we allocate an embedding matrix E with N
rows (N should be suﬃciently large, but not prohibitive, say in the millions or tens of millions).
When a k-gram is seen in training, we assign it to a row in E by applying a hash function h that
will deterministically map it into a number in the range Œ1; N�, i D h.k-gram/ 2 Œ1; N �. We then
use the corresponding row E Œh.k-gram/� as the embedding vector. Every k-gram will be dynamically
assigned a row index this way, without the need to store an explicit kgram-to-index mapping or
to dedicate an embedding vector to each k-gram. Some k-grams may share the same embedding
vector due to hash collisions (indeed, with the space of possible k-grams being much larger than
the number of allocated embedding vectors such collisions are bound to happen), but as most k-
grams are not informative for the task the collisions will be smoothed out by the training process.
If one wants to be more careful, several distinct hash functions h1; : : : ; hr can be used, and each

13.3. HIERARCHICAL CONVOLUTIONS
159
k-gram represented as the sum of the rows corresponding to its hashes (Pr
iD1 E Œhi.k-gram/�). is
way, if an informative k-gram happens to collide with another informative k-gram using one
hash, it still likely to have a non-colliding representation from one of the other hashes.
is hashing trick (also called hash kernel) works very well in practice, resulting in very
eﬃcient bag-of-ngrams models. It is recommended as a go-to baseline before considering more
complex approaches or architectures.
13.3
HIERARCHICAL CONVOLUTIONS
e 1D convolution approach described so far can be thought of as an ngram detector. A convo-
lution layer with a window of size k is learning to identify indicative k-grams in the input.
e approach can be extended into a hierarchy of convolutional layers, in which a sequence
of convolution layers are applied one after the other. Let CONVk
‚.w1Wn/ be the result of applying
a convolution with window size k and parameters ‚ to each k-size window in the sequence w1Wn:
p1Wm DCONVk
U ;b.w1Wn/
pi Dg.˚.wiWiCk�1/ � U C b/
m D
(
n � k C 1
narrow convolution
n C k C 1
wide convolution:
(13.6)
We can now have a succession of r convolutional layers that feed into each other as follows:
p1
1Wm1 DCONVk1
U 1;b1.w1Wn/
p2
1Wm2 DCONVk2
U 2;b2.p1
1Wm1/
� � �
pr
1Wmr DCONVkr
U r;br .pr�1
1Wmr�1/:
(13.7)
e resulting vectors pr
1Wmr capture increasingly larger eﬀective windows (“receptive-ﬁelds”) of the
sentence. For r layers with a window of size k, each vector pr
i will be sensitive to a window
of r.k � 1/ C 1 words.⁶ Moreover, the vector pr
i can be sensitive to gappy-ngrams of k C r � 1
words, potentially capturing patterns such as “not
good” or “obvious
predictable
plot”
where
stands for a short sequence of words, as well more specialized patterns where the gaps
⁶To see why, consider that the ﬁrst convolution layer transforms each sequence of k neighboring word-vectors into vectors
representing k-grams. en, the second convolution layer will combine each k consecutive k-gram-vectors into vectors that
capture a window of k C .k � 1/ words, and so on, until the rth convolution will capture k C .r � 1/.k � 1/ D r.k �
1/ C 1 words.

160
13. NGRAM DETECTORS: CONVOLUTIONAL NEURAL NETWORKS
can be further specialized (i.e., “a sequence of words that do not contain not” or “a sequence of
words that are adverb-like”).⁷ Figure 13.3 shows a two-layer hierarchical convolution with k D 2.
the actual service
the actual 
actual service
service was
was not
not very
very good
actual service was
service was not
was not very
not very good
the 
        actual 
   service 
was 
        not 
  very 
         good
Figure 13.3: Two-layer hierarchical convolution with k=2.
Strides, Dilation and Pooling
So far, the convolution operation is applied to each k-word win-
dow in the sequence, i.e., windows starting at indices 1; 2; 3; : : :. is is said to have a stride of
size 1. Larger strides are also possible, i.e., with a stride of size 2 the convolution operation will
be applied to windows starting at indices 1; 3; 5; : : :. More generally, we deﬁne CONVk;s as:
p1Wm DCONVk;s
U ;b.w1Wn/
pi Dg.˚.w1C.i�1/sW.sCk/i/ � U C b/;
(13.8)
where s is the stride size. e result will be a shorter output sequence from the convolutional
layer.
In a dilated convolution architecture [Strubell et al., 2017, Yu and Koltun, 2016] the hi-
erarchy of convolution layers each has a stride size of k � 1 (i.e., CONVk;k�1). is allows an
exponential growth in the eﬀective window size as a function of the number of layers. Figure 13.4
shows convolution layers with diﬀerent stride lengths. Figure 13.5 shows a dilated convolution
architecture.
An alternative to the dilation approach is to keep the stride-size ﬁxed at 1, but shorten the
sequence length between each layer by applying local pooling, i.e, consecutive k0-gram of vectors
⁷To see why, consider a sequence of two convolution layer each with a window of size 2 over the sequence funny and appealing.
e ﬁrst convolution layer will encode funny and and and appealing as vectors, and may choose to retain the equivalent of
“funny
” and “
appealing” in the resulting vectors. e second convolution layer can then combine these into “funny
appealing,” “funny
” or “
appealing.”

[Image: extracted_image_169_0.png]
13.3. HIERARCHICAL CONVOLUTIONS
161
k = 3, s = 1
k = 3, s = 2
k = 3, s = 3
(a)
(b)
(c)
Figure 13.4: Strides. (a–c) Convolution layer with k=3 and stride sizes 1, 2, 3.
can be converted into a single vector using max pooling or averaged pooling. Even if we pool
just every two neighboring vectors, each convolutional-and-pooling layer in the hierarchy will
halve the length of the sequence. Similar to the dilation approach, we again gain an exponential
decrease in sequence length as a function of the number of layers.
Parameter Tying and Skip-connections
Another variation that can be applied to the hierarchical
convolution architecture is performing parameter-tying, using the same set of parameters U; b
in all the parameter layers. is results in more parameter sharing, as well as allowing to use an
unbounded number of convolution layers (as all the convolution layers share the same parameters,
the number of convolution layers need not be set in advance), which in turn allows to reduce
arbitrary length sequences into a single vector by using a sequence of narrow convolutions, each
resulting in a shorter sequence of vectors.
When using deep architectures, skip-connections are sometimes useful: these work by feed-
ing into the ith layer not only the vectors resulting from the i � 1th layer, but also vectors from

[Image: extracted_image_170_0.png]
162
13. NGRAM DETECTORS: CONVOLUTIONAL NEURAL NETWORKS
Figure 13.5: ree-layer dilated hierarchical convolution with k=3.
previous layers which are combined to the vectors of the i � 1th layer using either concatenation,
averaging, or summation.
Further Reading
e use of hierarchical and dilated convolution and pooling architectures is
very common in the computer-vision community, where various deep architectures—comprising
of arrangements of many convolutions and pooling layers with diﬀerent strides—have been pro-
posed, resulting in very strong image classiﬁcation and object recognition results [He et al., 2016,
Krizhevsky et al., 2012, Simonyan and Zisserman, 2015]. e use of such deep architectures for
NLP is still more preliminary. Zhang et al. [2015] provide initial experiments with text classi-
ﬁcation with hierarchical convolutions over characters, and Conneau et al. [2016] provide fur-
ther results, this time with very deep convolutional networks. e work of Strubell et al. [2017]
provides a good overview of hierarchical and dilated architectures for a sequence labeling task.
Kalchbrenner et al. [2016] use dilated convolutions as encoders in an encoder-decoder archi-
tecture (Section 17.2) for machine translation. e hierarchy of convolutions with local pooling
approach is used by Xiao and Cho [2016], who apply it to a sequence of character in a document-
classiﬁcation task, and then feed the resulting vectors into a recurrent neural network. We return
to this example in Section 16.2.2, after discussing recurrent-neural-networks.

[Image: extracted_image_171_0.png]
163
C H A P T E R
14
Recurrent Neural Networks:
Modeling Sequences and Stacks
When dealing with language data, it is very common to work with sequences, such as words (se-
quences of letters), sentences (sequences of words), and documents. We saw how feed-forward
networks can accommodate arbitrary feature functions over sequences through the use of vector
concatenation and vector addition (CBOW). In particular, the CBOW representations allows to
encode arbitrary length sequences as ﬁxed sized vectors. However, the CBOW representation is
quite limited, and forces one to disregard the order of features. e convolutional networks also
allow encoding a sequence into a ﬁxed size vector. While representations derived from convolu-
tional networks are an improvement over the CBOW representation as they oﬀer some sensitivity
to word order, their order sensitivity is restricted to mostly local patterns, and disregards the order
of patterns that are far apart in the sequence.¹
Recurrent neural networks (RNNs) [Elman, 1990] allow representing arbitrarily sized se-
quential inputs in ﬁxed-size vectors, while paying attention to the structured properties of the in-
puts. RNNs, particularly ones with gated architectures such as the LSTM and the GRU, are very
powerful at capturing statistical regularities in sequential inputs. ey are arguably the strongest
contribution of deep-learning to the statistical natural-language processing tool-set.
is chapter describes RNNs as an abstraction: an interface for translating a sequence of
inputs into a ﬁxed sized output, that can then be plugged as components in larger networks.
Various architectures that use RNNs as a component are discussed. In the next chapter, we deal
with concrete instantiations of the RNN abstraction, and describe the Elman RNN (also called
Simple RNN), the Long-short-term Memory (LSTM), and the Gated Recurrent Unit (GRU).
en, in Chapter 16 we consider examples of modeling NLP problems using with RNNs.
In Chapter 9, we discussed language modeling and the Markov assumption. RNNs allow
for language models that do not make the Markov assumption, and condition the next word on
the entire sentence history (all the words preceding it). is ability opens the way to conditioned
generation models, where a language model that is used as a generator is conditioned on some
other signal, such as a sentence in another language. Such models are described in more depth in
Chapter 17.
¹However, as discussed in Section 13.3, hierarchical and dilated convolutional architectures do have the potential of capturing
relatively long-range dependencies within a sequence.

164
14. RECURRENT NEURAL NETWORKS: MODELING SEQUENCES AND STACKS
14.1
THE RNN ABSTRACTION
We use xiWj to denote the sequence of vectors xi; : : : ; xj. On a high-level, the RNN is a function
that takes as input an arbitrary length ordered sequence of n din-dimensional vectors x1Wn D
x1; x2; : : : ; xn, (xi 2 Rdin) and returns as output a single dout dimensional vector yn 2 Rdout:
yn D RNN.x1Wn/
(14.1)
xi 2 Rdin
yn 2 Rdout:
is implicitly deﬁnes an output vector yi for each preﬁx x1Wi of the sequence x1Wn. We
denote by RNN? the function returning this sequence:
y1Wn D RNN?.x1Wn/
yi D RNN.x1Wi/
(14.2)
xi 2 Rdin
yi 2 Rdout:
e output vector yn is then used for further prediction. For example, a model for predict-
ing the conditional probability of an event e given the sequence x1Wn can be deﬁned as p.e D
j jx1Wn/ D softmax.RNN.x1Wn/ � W C b/Œj�, the jth element in the output vector resulting from
the softmax operation over a linear transformation of the RNN encoding yn D RNN.x1Wn/. e
RNN function provides a framework for conditioning on the entire history x1; : : : ; xi without
resorting to the Markov assumption which is traditionally used for modeling sequences, described
in Chapter 9. Indeed, RNN-based language models result in very good perplexity scores when
compared to ngram-based models.
Looking in a bit more detail, the RNN is deﬁned recursively, by means of a function R
taking as input a state vector si�1 and an input vector xi and returning a new state vector si.
e state vector si is then mapped to an output vector yi using a simple deterministic function
O.�/.² e base of the recursion is an initial state vector, s0, which is also an input to the RNN.
For brevity, we often omit the initial vector s0, or assume it is the zero vector.
When constructing an RNN, much like when constructing a feed-forward network, one
has to specify the dimension of the inputs xi as well as the dimensions of the outputs yi. e
dimensions of the states si are a function of the output dimension.³
²Using the O function is somewhat non-standard, and is introduced in order to unify the diﬀerent RNN models to to be
presented in the next chapter. For the Simple RNN (Elman RNN) and the GRU architectures, O is the identity mapping,
and for the LSTM architecture O selects a ﬁxed subset of the state.
³While RNN architectures in which the state dimension is independent of the output dimension are possible, the current
popular architectures, including the Simple RNN, the LSTM, and the GRU do not follow this ﬂexibility.

14.1. THE RNN ABSTRACTION
165
RNN?.x1WnI s0/ Dy1Wn
yi DO.si/
si DR.si�1; xi/
(14.3)
xi 2 Rdin; yi 2 Rdout; si 2 Rf .dout/:
e functions R and O are the same across the sequence positions, but the RNN keeps
track of the states of computation through the state vector si that is kept and being passed across
invocations of R.
Graphically, the RNN has been traditionally presented as in Figure 14.1.
yi
R, O
xi
si-1
si
θ
Figure 14.1: Graphical representation of an RNN (recursive).
is presentation follows the recursive deﬁnition, and is correct for arbitrarily long sequences.
However, for a ﬁnite sized input sequence (and all input sequences we deal with are ﬁnite) one
can unroll the recursion, resulting in the structure in Figure 14.2.
While not usually shown in the visualization, we include here the parameters � in order to high-
light the fact that the same parameters are shared across all time steps. Diﬀerent instantiations of
R and O will result in diﬀerent network structures, and will exhibit diﬀerent properties in terms
of their running times and their ability to be trained eﬀectively using gradient-based methods.
However, they all adhere to the same abstract interface. We will provide details of concrete in-
stantiations of R and O—the Simple RNN, the LSTM, and the GRU—in Chapter 15. Before
that, let’s consider working with the RNN abstraction.

166
14. RECURRENT NEURAL NETWORKS: MODELING SEQUENCES AND STACKS
y1
x1
s0
R, O
s1
y2
x2
R, O
s2
y3
x3
R, O
s3
y4
x4
R, O
s4
y5
x5
R, O
s5
θ
Figure 14.2: Graphical representation of an RNN (unrolled).
First, we note that the value of si (and hence yi) is based on the entire input x1; : : : ; xi.
For example, by expanding the recursion for i D 4 we get:
s4 DR.s3; x4/
DR.
s3
‚ …„ ƒ
R.s2; x3/; x4/
DR.R.
s2
‚ …„ ƒ
R.s1; x2/; x3/; x4/
DR.R.R.
s1
‚ …„ ƒ
R.s0; x1/; x2/; x3/; x4/:
(14.4)
us, sn and yn can be thought of as encoding the entire input sequence.⁴ Is the encoding
useful? is depends on our deﬁnition of usefulness. e job of the network training is to set the
parameters of R and O such that the state conveys useful information for the task we are tying to
solve.
14.2
RNN TRAINING
Viewed as in Figure 14.2 it is easy to see that an unrolled RNN is just a very deep neural network
(or rather, a very large computation graph with somewhat complex nodes), in which the same pa-
rameters are shared across many parts of the computation, and additional input is added at various
layers. To train an RNN network, then, all we need to do is to create the unrolled computation
graph for a given input sequence, add a loss node to the unrolled graph, and then use the backward
⁴Note that, unless R is speciﬁcally designed against this, it is likely that the later elements of the input sequence have stronger
eﬀect on sn than earlier ones.

14.3. COMMON RNN USAGE-PATTERNS
167
(backpropagation) algorithm to compute the gradients with respect to that loss. is procedure
is referred to in the RNN literature as backpropagation through time (BPTT) [Werbos, 1990].⁵
What is the objective of the training? It is important to understand that the RNN does not
do much on its own, but serves as a trainable component in a larger network. e ﬁnal prediction
and loss computation are performed by that larger network, and the error is back-propagated
through the RNN. is way, the RNN learns to encode properties of the input sequences that are
useful for the further prediction task. e supervision signal is not applied to the RNN directly,
but through the larger network.
Some common architectures of integrating the RNN within larger networks are given be-
low.
14.3
COMMON RNN USAGE-PATTERNS
14.3.1 ACCEPTOR
One option is to base the supervision signal only at the ﬁnal output vector, yn. Viewed this way,
the RNN is trained as an acceptor. We observe the ﬁnal state, and then decide on an outcome.⁶ For
example, consider training an RNN to read the characters of a word one by one and then use the
ﬁnal state to predict the part-of-speech of that word (this is inspired by Ling et al. [2015b]), an
RNN that reads in a sentence and, based on the ﬁnal state decides if it conveys positive or negative
sentiment (this is inspired by Wang et al. [2015b]) or an RNN that reads in a sequence of words
and decides whether it is a valid noun-phrase. e loss in such cases is deﬁned in terms of a
function of yn D O.sn/. Typically, the RNN’s output vector yn is fed into a fully connected layer
or an MLP, which produce a prediction. e error gradients are then backpropagated through the
rest of the sequence (see Figure 14.3).⁷ e loss can take any familiar form: cross entropy, hinge,
margin, etc.
14.3.2 ENCODER
Similar to the acceptor case, an encoder supervision uses only the ﬁnal output vector, yn. However,
unlike the acceptor, where a prediction is made solely on the basis of the ﬁnal vector, here the
⁵Variants of the BPTT algorithm include unrolling the RNN only for a ﬁxed number of input symbols at each time: ﬁrst
unroll the RNN for inputs x1Wk, resulting in s1Wk. Compute a loss, and backpropagate the error through the network (k
steps back). en, unroll the inputs xkC1W2k, this time using sk as the initial state, and again backpropagate the error for k
steps, and so on. is strategy is based on the observations that for the Simple RNN variant, the gradients after k steps tend
to vanish (for large enough k), and so omitting them is negligible. is procedure allows training of arbitrarily long sequences.
For RNN variants such as the LSTM or the GRU that are designed speciﬁcally to mitigate the vanishing gradients problem,
this ﬁxed size unrolling is less motivated, yet it is still being used, for example when doing language modeling over a book
without breaking it into sentences. A similar variant unrolls the network for the entire sequence in the forward step, but only
propagates the gradients back for k steps from each position.
⁶e terminology is borrowed from Finite-State Acceptors. However, the RNN has a potentially inﬁnite number of states,
making it necessary to rely on a function other than a lookup table for mapping states to decisions.
⁷is kind of supervision signal may be hard to train for long sequences, especially so with the Simple RNN, because of the
vanishing gradients problem. It is also a generally hard learning task, as we do not tell the process on which parts of the input
to focus. Yet, it does work very well in many cases.

168
14. RECURRENT NEURAL NETWORKS: MODELING SEQUENCES AND STACKS
x1
s0
R, O
s1
x2
R, O
s2
x3
R, O
s3
x4
R, O
s4
x5
R, O
y5
predict and
calculate loss
loss
Figure 14.3: Acceptor RNN training graph.
ﬁnal vector is treated as an encoding of the information in the sequence, and is used as additional
information together with other signals. For example, an extractive document summarization
system may ﬁrst run over the document with an RNN, resulting in a vector yn summarizing
the entire document. en, yn will be used together with other features in order to select the
sentences to be included in the summarization.
14.3.3 TRANSDUCER
Another option is to treat the RNN as a transducer, producing an output Oti for each input it
reads in. Modeled this way, we can compute a local loss signal Llocal. Oti; ti/ for each of the out-
puts Oti based on a true label ti. e loss for unrolled sequence will then be: L.Ot1Wn; t1Wn/ D
Pn
iD1 Llocal.Oti; ti/, or using another combination rather than a sum such as an average or a
weighted average (see Figure 14.4). One example for such a transducer is a sequence tagger, in
which we take xiWn to be feature representations for the n words of a sentence, and ti as an input
for predicting the tag assignment of word i based on words 1:i. A CCG super-tagger based on
such an architecture provides very strong CCG super-tagging results [Xu et al., 2015], although
in many cases a transducer based on a bi-directional RNN (biRNN, see Section 14.4 below) is a
better ﬁt for such tagging problems.
A very natural use-case of the transduction setup is for language modeling, in which the
sequence of words x1Wi is used to predict a distribution over the .i C 1/th word. RNN-based
language models are shown to provide vastly better perplexities than traditional language models
[Jozefowicz et al., 2016, Mikolov, 2012, Mikolov et al., 2010, Sundermeyer et al., 2012].
Using RNNs as transducers allows us to relax the Markov assumption that is traditionally
taken in language models and HMM taggers, and condition on the entire prediction history.

14.4. BIDIRECTIONAL RNNS (BIRNN)
169
x1
s0
R, O
s1
x2
R, O
s2
x3
R, O
s3
x4
R, O
s4
x5
R, O
y5
y4
y3
y2
y1
predict and
calculate loss
predict and
calculate loss
predict and
calculate loss
predict and
calculate loss
predict and
calculate loss
loss
sum
Figure 14.4: Transducer RNN training graph.
Special cases of the RNN transducer is the RNN generator, and the related conditioned-
generation (also called encoder-decoder) and the conditioned-generation with attention architectures.
ese will be discussed in Chapter 17.
14.4
BIDIRECTIONAL RNNS (BIRNN)
A useful elaboration of an RNN is a bidirectional-RNN (also commonly referred to as biRNN)
[Graves, 2008, Schuster and Paliwal, 1997].⁸ Consider the task of sequence tagging over a sen-
tence x1; : : : ; xn. An RNN allows us to compute a function of the ith word xi based on the
past—the words x1Wi up to and including it. However, the following words xiC1Wn may also be
useful for prediction, as is evident by the common sliding-window approach in which the focus
word is categorized based on a window of k words surrounding it. Much like the RNN relaxes the
Markov assumption and allows looking arbitrarily back into the past, the biRNN relaxes the ﬁxed
window size assumption, allowing to look arbitrarily far at both the past and the future within
the sequence.
Consider an input sequence x1Wn. e biRNN works by maintaining two separate states,
sf
i and sb
i for each input position i. e forward state sf
i is based on x1; x2; : : : ; xi, while the
backward state sb
i is based on xn; xn�1; : : : ; xi. e forward and backward states are generated
by two diﬀerent RNNs. e ﬁrst RNN (Rf , Of ) is fed the input sequence x1Wn as is, while
the second RNN (Rb, Ob) is fed the input sequence in reverse. e state representation si is
⁸When used with a speciﬁc RNN architecture such as an LSTM, the model is called biLSTM.

170
14. RECURRENT NEURAL NETWORKS: MODELING SEQUENCES AND STACKS
then composed of both the forward and backward states. e output at position i is based on the
concatenation of the two output vectors yi D Œyf
i I yb
i � D ŒOf .sf
i /I Ob.sb
i /�, taking into account
both the past and the future. In other words, yi, the biRNN encoding of the ith word in a
sequence is the concatenation of two RNNs, one reading the sequence from the beginning, and
the other reading it from the end.
We deﬁne biRNN.x1Wn; i/ to be the output vector corresponding to the ith sequence po-
sition:⁹
biRNN.x1Wn; i/ D yi D ŒRNNf .x1Wi/I RNNb.xnWi/�:
(14.6)
e vector yi can then be used directly for prediction, or fed as part of the input to a more
complex network. While the two RNNs are run independently of each other, the error gradients
at position i will ﬂow both forward and backward through the two RNNs. Feeding the vector yi
through an MLP prior to prediction will further mix the forward and backward signals. Visual
representation of the biRNN architecture is given in Figure 14.5.
xthe
1
s0
s0
R f,O f 
R f,O f 
R f,O f 
R f,O f 
Rb,Ob 
Rb,Ob
Rb,Ob 
Rb,Ob
xbrown
2
xfox
3
xjumped
4
xjumped
5
xover
6
xthe
7
xdog
8
y4
y4
concat
f
r
Figure 14.5: Computing the biRNN representation of the word jumped in the sentence “the brown
fox jumped over the dog.”
Note how the vector y4, corresponding to the word jumped, encodes an inﬁnite window
around (and including) the focus vector xjumped.
Similarly to the RNN case, we also deﬁne biRNN?.x1Wn/ as the sequence of vectors y1Wn:
biRNN?.x1Wn/ D yiWn D biRNN.x1Wn; 1/; : : : ; biRNN.x1Wn; n/:
(14.7)
⁹e biRNN vector can either a simple concatenation of the two RNN vectors as in Equation (14.6), or followed by another
linear-transformation to reduce its dimension, often back to the dimension of the single RNN input:
biRNN.x1Wn; i/ D yi D ŒRNNf .x1Wi /I RNNb.xnWi /�W :
(14.5)
is is variant is often used when stacking several biRNNs on top of each other as discussed in Section 14.5.

14.5. MULTI-LAYER (STACKED) RNNS
171
e n output vectors yiWn can be eﬃciently computed in linear time by ﬁrst running the
forward and backward RNNs, and then concatenating the relevant outputs. is architecture is
depicted in Figure 14.6.
xthe
Rb,Ob 
Rb,Ob 
Rb,Ob 
Rb,Ob 
Rb,Ob 
xbrown
xfox
xjumped
x*
ythe
ybrown
yfox
yjumped
y*
y4
y5
concat
concat
concat
concat
concat
b
b
b
y2
y3
b
b
y1
b
y2
y1f
f
y4
y3f
f
y5
f
s5
bs4
bs3
bs2
bs1
bs0
R f,O f 
R f,O f 
R f,O f 
R f,O f 
R f,O f 
fs0
fs1
fs2
fs3
fs4
fs5
Figure 14.6: Computing the biRNN? for the sentence “the brown fox jumped.”
e biRNN is very eﬀective for tagging tasks, in which each input vector corresponds to
one output vector. It is also useful as a general-purpose trainable feature-extracting component,
that can be used whenever a window around a given word is required. Concrete usage examples
are given in Chapter 16.
e use of biRNNs for sequence tagging was introduced to the NLP community by Irsoy
and Cardie [2014].
14.5
MULTI-LAYER (STACKED) RNNS
RNNs can be stacked in layers, forming a grid [Hihi and Bengio, 1996]. Consider k RNNs,
RNN1; : : : ; RNNk, where the jth RNN has states sj
1Wn and outputs yj
1Wn. e input for the ﬁrst
RNN are x1Wn, while the input of the jth RNN (j � 2) are the outputs of the RNN below
it, yj�1
1Wn . e output of the entire formation is the output of the last RNN, yk
1Wn. Such layered
architectures are often called deep RNNs. A visual representation of a three-layer RNN is given
in Figure 14.7. biRNNs can be stacked in a similar fashion.¹⁰
¹⁰e term deep-biRNN is used in the literature to describe to diﬀerent architecture: in the ﬁrst, the biRNN state is a concate-
nation of two deep RNNs. In the second, the output sequence of on biRNN is fed as input to another. My research group
found the second variant to often performs better.

172
14. RECURRENT NEURAL NETWORKS: MODELING SEQUENCES AND STACKS
R3,O3 
R3,O3 
R3,O3 
R3,O3 
R3,O3 
y1
y2
y3
y4
y5
x1
x2
x3
x4
x5
y2
y1
3
3
3
y4
y3
3
3
y5
3
y2
y1
2
2
y4
y3
2
2
y5
2
s0
3s1
3s2
3s3
3s4
3s5
R2,O2 
R2,O2 
R2,O2 
R2,O2 
R2,O2 
2
y2
y1
1
1
y4
y3
1
1
y5
1
s0
2s1
2s2
2s3
2s4
2s5
R1,O1 
R1,O1 
R1,O1 
R1,O1 
R1,O1 
1s0
1s1
1s2
1s3
1s4
1s5
Figure 14.7: A three-layer (“deep”) RNN architecture.
While it is not theoretically clear what is the additional power gained by the deeper archi-
tecture, it was observed empirically that deep RNNs work better than shallower ones on some
tasks. In particular, Sutskever et al. [2014] report that a four-layers deep architecture was crucial
in achieving good machine-translation performance in an encoder-decoder framework. Irsoy and
Cardie [2014] also report improved results from moving from a one-layer biRNN to an architec-
ture with several layers. Many other works report result using layered RNN architectures, but do
not explicitly compare to one-layer RNNs. In the experiment of my research group, using two or
more layers indeed often improves over using a single one.
14.6
RNNS FOR REPRESENTING STACKS
Some algorithms in language processing, including those for transition-based parsing [Nivre,
2008], require performing feature extraction over a stack. Instead of being conﬁned to looking at
the k top-most elements of the stack, the RNN framework can be used to provide a ﬁxed-sized
vector encoding of the entire stack.
e main intuition is that a stack is essentially a sequence, and so the stack state can be
represented by taking the stack elements and feeding them in order into an RNN, resulting in a
ﬁnal encoding of the entire stack. In order to do this computation eﬃciently (without performing
an O.n/ stack encoding operation each time the stack changes), the RNN state is maintained
together with the stack state. If the stack was push-only, this would be trivial: whenever a new

14.6. RNNS FOR REPRESENTING STACKS
173
element x is pushed into the stack, the corresponding vector x will be used together with the RNN
state si in order to obtain a new state siC1. Dealing with pop operation is more challenging, but
can be solved by using the persistent-stack data-structure [Goldberg et al., 2013, Okasaki, 1999].
Persistent, or immutable, data-structures keep old versions of themselves intact when modiﬁed.
e persistent stack construction represents a stack as a pointer to the head of a linked list. An
empty stack is the empty list. e push operation appends an element to the list, returning the
new head. e pop operation then returns the parent of the head, but keeping the original list
intact. From the point of view of someone who held a pointer to the previous head, the stack did
not change. A subsequent push operation will add a new child to the same node. Applying this
procedure throughout the lifetime of the stack results in a tree, where the root is an empty stack
and each path from a node to the root represents an intermediary stack state. Figure 14.8 provides
an example of such a tree. e same process can be applied in the computation graph construction,
creating an RNN with a tree structure instead of a chain structure. Backpropagating the error from
a given node will then aﬀect all the elements that participated in the stack when the node was
created, in order. Figure 14.9 shows the computation graph for the stack-RNN corresponding to
the last state in Figure 14.8. is modeling approach was proposed independently by Dyer et al.
[2015] and Watanabe and Sumita [2015] for transition-based dependency parsing.
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
a
a
b
a
b
c
a
b
c
a
b
c
d
a
b
c
d
e
a
b
c
d
a
b
c
d
a
b
c
d
e
f
head
head
head
head
head
head
head
head
head
(1) push a
(2) push b
(3) push c
(4) pop
(5) push d
(6) pop
(7) pop
(8) push e
(9) push f
Figure 14.8: An immutable stack construction for the sequence of operations push a; push b; push c;
pop; push d; pop; pop; push e; push f.

174
14. RECURRENT NEURAL NETWORKS: MODELING SEQUENCES AND STACKS
xa
s0
R, O
sa
sa
xb
R, O
sa,b
sa,b
sa,e
xc
R, O
sa,b,c
ya
ya,b
ya,b,c xd
R, O
sa,b,d
xe
R, O
ya,b,d
sa,e,f
ya,e,f
ya,e
xf
R, O
Figure 14.9: e stack-RNN corresponding to the ﬁnal state in Figure 14.8.
14.7
A NOTE ON READING THE LITERATURE
Unfortunately, it is often the case that inferring the exact model form from reading its description
in a research paper can be quite challenging. Many aspects of the models are not yet standardized,
and diﬀerent researchers use the same terms to refer to slightly diﬀerent things. To list a few
examples, the inputs to the RNN can be either one-hot vectors (in which case the embedding
matrix is internal to the RNN) or embedded representations; the input sequence can be padded
with start-of-sequence and/or end-of-sequence symbols, or not; while the output of an RNN
is usually assumed to be a vector which is expected to be fed to additional layers followed by a
softmax for prediction (as is the case in the presentation in this tutorial), some papers assume
the softmax to be part of the RNN itself; in multi-layer RNN, the “state vector” can be either
the output of the top-most layer, or a concatenation of the outputs from all layers; when using
the encoder-decoder framework, conditioning on the output of the encoder can be interpreted in
various diﬀerent ways; and so on. On top of that, the LSTM architecture described in the next

14.7. A NOTE ON READING THE LITERATURE
175
section has many small variants, which are all referred to under the common name LSTM. Some
of these choices are made explicit in the papers, other require careful reading, and others still are
not even mentioned, or are hidden behind ambiguous ﬁgures or phrasing.
As a reader, be aware of these issues when reading and interpret model descriptions. As a
writer, be aware of these issues as well: either fully specify your model in mathematical notation,
or refer to a diﬀerent source in which the model is fully speciﬁed, if such a source is available.
If using the default implementation from a software package without knowing the details, be
explicit of that fact and specify the software package you use. In any case, don’t rely solely on
ﬁgures or natural language text when describing your model, as these are often ambiguous.

177
C H A P T E R
15
Concrete Recurrent Neural
Network Architectures
After describing the RNN abstraction, we are now in place to discuss speciﬁc instantiations of it.
Recall that we are interested in a recursive function si D R.xi; si�1/ such that si encodes the
sequence x1Wn. We will present several concrete instantiations of the abstract RNN architecture,
providing concrete deﬁnitions of the functions R and O. ese include the Simple RNN (S-
RNN), the Long Short-Term Memory (LSTM) and the Gated Recurrent Unit (GRU).
15.1
CBOW AS AN RNN
On particularly simple choice of R is the addition function:
si DR.xi; si�1/ D si�1 C xi
yi DO.si/ D si
(15.1)
si; yi 2 Rds; xi 2 Rds:
Following the deﬁnition in Equation (15.1), we get the continuous-bag-of-words model:
the state resulting from inputs x1Wn is the sum of these inputs. While simple, this instantiation
of the RNN ignores the sequential nature of the data. e Elman RNN, described next, adds
dependence on the sequential ordering of the elements.¹
15.2
SIMPLE RNN
e simplest RNN formulation that is sensitive to the ordering of elements in the sequence is
known as an Elman Network or Simple-RNN (S-RNN). e S-RNN was proposed by Elman
[1990] and explored for use in language modeling by Mikolov [2012]. e S-RNN takes the
following form:
si DR.xi; si�1/ D g.si�1W s C xiW x C b/
yi DO.si/ D si
(15.2)
si; yi 2 Rds; xi 2 Rdx; W x 2 Rdx�ds; W s 2 Rds�ds; b 2 Rds:
¹e view of the CBOW representation as an RNN is not a common one in the literature. However, we ﬁnd it to be a good
stepping stone into the Elman RNN deﬁnition. It is also useful to have the simple CBOW encoder in the same framework as
the RNNs as it can also serve the role of an encoder in a conditioned generation network such as those described in Chapter 17.

178
15. CONCRETE RECURRENT NEURAL NETWORK ARCHITECTURES
at is, the state si�1 and the input xi are each linearly transformed, the results are added
(together with a bias term) and then passed through a nonlinear activation function g (commonly
tanh or ReLU). e output at position i is the same as the hidden state in that position.²
An equivalent way of writing Equation (15.2) is Equation (15.3), both are used in the
literature:
si DR.xi; si�1/ D g.Œsi�1I xi�W C b/
yi DO.si/ D si
(15.3)
si; yi 2 Rds; xi 2 Rdx; W 2 R.dxCds/�ds; b 2 Rds:
e S-RNN is only slightly more complex than the CBOW, with the major diﬀerence
being the nonlinear activation function g. However, this diﬀerence is a crucial one, as adding the
linear transformation followed by the nonlinearity makes the network sensitive to the order of the
inputs. Indeed, the Simple RNN provides strong results for sequence tagging [Xu et al., 2015]
as well as language modeling. For comprehensive discussion on using Simple RNNs for language
modeling, see the Ph.D. thesis by Mikolov [2012].
15.3
GATED ARCHITECTURES
e S-RNN is hard to train eﬀectively because of the vanishing gradients problem [Pascanu
et al., 2012]. Error signals (gradients) in later steps in the sequence diminish quickly in the back-
propagation process, and do not reach earlier input signals, making it hard for the S-RNN to
capture long-range dependencies. Gating-based architectures, such as the LSTM [Hochreiter
and Schmidhuber, 1997] and the GRU [Cho et al., 2014b] are designed to solve this deﬁciency.
Consider the RNN as a general purpose computing device, where the state si represents a
ﬁnite memory. Each application of the function R reads in an input xiC1, reads in the current
memory si, operates on them in some way, and writes the result into memory, resulting in a new
memory state siC1. Viewed this way, an apparent problem with the S-RNN architecture is that
the memory access is not controlled. At each step of the computation, the entire memory state is
read, and the entire memory state is written.
How does one provide more controlled memory access? Consider a binary vector g 2
f0; 1gn. Such a vector can act as a gate for controlling access to n-dimensional vectors, using
the hadamard-product operation x ˇ g:³ Consider a memory s 2 Rd, an input x 2 Rd and a
gate g 2 0; 1d. e computation s0  g ˇ x C .1 � g/ ˇ .s/ “reads” the entries in x that cor-
respond to the 1 values in g, and writes them to the new memory s0. en, locations that weren’t
²Some authors treat the output at position i as a more complicated function of the state, e.g., a linear transformation, or an
MLP. In our presentation, such further transformation of the output are not considered part of the RNN, but as separate
computations that are applied to the RNNs output.
³e hadamard-product is a fancy name for element-wise multiplication of two vectors: the hadamard product x D u ˇ v
results in xŒi� D uŒi� � vŒi�.

15.3. GATED ARCHITECTURES
179
read to are copied from the memory s to the new memory s0 through the use of the gate (1 � g).
Figure 15.1 shows this process for updating the memory with positions 2 and 5 from the input.
8
11
3
7
5
15
s
0
1
0
0
0
1
10
11
12
13
14
15
1
0
1
1
1
0
8
9
3
7
5
8
g
(1 – g)
x
s
⨀
⨀
+
Figure 15.1: Using binary gate vector g to control access to memory s0.
e gating mechanism described above can serve as a building block in our RNN: gate
vectors can be used to control access to the memory state si. However, we are still missing two
important (and related) components: the gates should not be static, but be controlled by the
current memory state and the input, and their behavior should be learned. is introduced an
obstacle, as learning in our framework entails being diﬀerentiable (because of the backpropagation
algorithm) and the binary 0-1 values used in the gates are not diﬀerentiable.⁴
A solution to the above problem is to approximate the hard gating mechanism with a
soft—but diﬀerentiable—gating mechanism. To achieve these diﬀerentiable gates, we replace the
requirement that g 2 f0; 1gn and allow arbitrary real numbers, g0 2 Rn, which are then pass
through a sigmoid function �.g0/. is bounds the value in the range .0; 1/, with most values
near the borders. When using the gate �.g0/ ˇ x, indices in x corresponding to near-one val-
ues in �.g0/ are allowed to pass, while those corresponding to near-zero values are blocked. e
gate values can then be conditioned on the input and the current memory, and trained using a
gradient-based method to perform a desired behavior.
is controllable gating mechanism is the basis of the LSTM and the GRU architectures,
to be deﬁned next: at each time step, diﬀerentiable gating mechanisms decide which parts of the
inputs will be written to memory, and which parts of memory will be overwritten (forgotten).
is rather abstract description will be made concrete in the next sections.
15.3.1 LSTM
e Long Short-Term Memory (LSTM) architecture [Hochreiter and Schmidhuber, 1997] was
designed to solve the vanishing gradients problem, and is the ﬁrst to introduce the gating mech-
anism. e LSTM architecture explicitly splits the state vector si into two halves, where one half
⁴It is in principle possible to learn also models with non-diﬀerentiable components such as binary gates using reinforcement-
learning techniques. However, as the time of this writing such techniques are brittle to train. Reinforcement learning tech-
niques are beyond the scope of this book.

180
15. CONCRETE RECURRENT NEURAL NETWORK ARCHITECTURES
is treated as “memory cells” and the other is working memory. e memory cells are designed to
preserve the memory, and also the error gradients, across time, and are controlled through diﬀer-
entiable gating components—smooth mathematical functions that simulate logical gates. At each
input state, a gate is used to decide how much of the new input should be written to the memory
cell, and how much of the current content of the memory cell should be forgotten. Mathemati-
cally, the LSTM architecture is deﬁned as:⁵
sj D R.sj�1; xj / DŒcj I hj �
cj Df ˇ cj�1 C i ˇ z
hj Do ˇ tanh.cj /
i D�.xj W xi C hj�1W hi/
f D�.xj W xf C hj�1W hf /
o D�.xj W xo C hj�1W ho/
z D tanh.xj W xz C hj�1W hz/
yj D O.sj / Dhj
(15.4)
sj 2 R2�dh; xi 2 Rdx; cj ; hj ; i; f ; o; z 2 Rdh; W xı 2 Rdx�dh; W hı 2 Rdh�dh:
e state at time j is composed of two vectors, cj and hj, where cj is the memory com-
ponent and hj is the hidden state component. ere are three gates, i, f , and o, controlling
for input, forget, and output. e gate values are computed based on linear combinations of the
current input xj and the previous state hj�1, passed through a sigmoid activation function. An
update candidate z is computed as a linear combination of xj and hj�1, passed through a tanh
activation function. e memory cj is then updated: the forget gate controls how much of the
previous memory to keep (f ˇ cj �1), and the input gate controls how much of the proposed
update to keep (i ˇ z). Finally, the value of hj (which is also the output yj) is determined based
on the content of the memory cj, passed through a tanh nonlinearity and controlled by the output
gate. e gating mechanisms allow for gradients related to the memory part cj to stay high across
very long time ranges.
For further discussion on the LSTM architecture see the Ph.D. thesis by Alex Graves
[2008], as well as Chris Olah’s description.⁶ For an analysis of the behavior of an LSTM when
used as a character-level language model, see Karpathy et al. [2015].
⁵ere are many variants on the LSTM architecture presented here. For example, forget gates were not part of the original
proposal in Hochreiter and Schmidhuber [1997], but are shown to be an important part of the architecture. Other variants
include peephole connections and gate-tying. For an overview and comprehensive empirical comparison of various LSTM
architectures, see Greﬀ et al. [2015].
⁶http://colah.github.io/posts/2015-08-Understanding-LSTMs/

15.3. GATED ARCHITECTURES
181
.
e vanishing gradients problem in Recurrent Neural Networks and its Solution
Intu-
itively, recurrent neural networks can be thought of as very deep feed-forward networks,
with shared parameters across diﬀerent layers. For the Simple-RNN [Equation (15.3)], the
gradients then include repeated multiplication of the matrix W , making it very likely for the
values to vanish or explode. e gating mechanism mitigate this problem to a large extent by
getting rid of this repeated multiplication of a single matrix.
For further discussion of the exploding and vanishing gradient problem in RNNs,
see Section 10.7 in Bengio et al. [2016]. For further explanation of the motivation behind
the gating mechanism in the LSTM (and the GRU) and its relation to solving the vanishing
gradient problem in recurrent neural networks, see Sections 4.2 and 4.3 in the detailed course
notes of Cho [2015].
LSTMs are currently the most successful type of RNN architecture, and they are respon-
sible for many state-of-the-art sequence modeling results. e main competitor of the LSTM-
RNN is the GRU, to be discussed next.
Practical Considerations
When training LSTM networks, Jozefowicz et al. [2015] strongly
recommend to always initialize the bias term of the forget gate to be close to one.
15.3.2 GRU
e LSTM architecture is very eﬀective, but also quite complicated. e complexity of the system
makes it hard to analyze, and also computationally expensive to work with. e gated recurrent
unit (GRU) was recently introduced by Cho et al. [2014b] as an alternative to the LSTM. It was
subsequently shown by Chung et al. [2014] to perform comparably to the LSTM on several (non
textual) datasets.
Like the LSTM, the GRU is also based on a gating mechanism, but with substantially
fewer gates and without a separate memory component.
sj D RGRU.sj �1; xj / D.1 � z/ ˇ sj�1 C z ˇ Qsj
z D�.xj W xz C sj�1W sz/
r D�.xj W xr C sj�1W sr/
Qsj D tanh.xj W xs C .r ˇ sj �1/W sg/
yj D OGRU.sj / Dsj
(15.5)
sj ; Qsj 2 Rds; xi 2 Rdx; z; r 2 Rds; W xı 2 Rdx�ds; W sı 2 Rds�ds:

182
15. CONCRETE RECURRENT NEURAL NETWORK ARCHITECTURES
One gate (r) is used to control access to the previous state sj �1 and compute a proposed up-
date Qsj. e updated state sj (which also serves as the output yj) is then determined based on
an interpolation of the previous state sj �1 and the proposal Qsj, where the proportions of the
interpolation are controlled using the gate z.⁷
e GRU was shown to be eﬀective in language modeling and machine translation. How-
ever, the jury is still out between the GRU, the LSTM and possible alternative RNN architectures,
and the subject is actively researched. For an empirical exploration of the GRU and the LSTM
architectures, see Jozefowicz et al. [2015].
15.4
OTHER VARIANTS
Improvements to non-gated architectures
e gated architectures of the LSTM and the GRU
help in alleviating the vanishing gradients problem of the Simple RNN, and allow these RNNs to
capture dependencies that span long time ranges. Some researchers explore simpler architectures
than the LSTM and the GRU for achieving similar beneﬁts.
Mikolov et al. [2014] observed that the matrix multiplication si�1W s coupled with the
nonlinearity g in the update rule R of the Simple RNN causes the state vector si to undergo large
changes at each time step, prohibiting it from remembering information over long time periods.
ey propose to split the state vector si into a slow changing component ci (“context units”) and
a fast changing component hi.⁸ e slow changing component ci is updated according to a linear
interpolation of the input and the previous component: ci D .1 � ˛/xiW x1 C ˛ci�1, where ˛ 2
.0; 1/. is update allows ci to accumulate the previous inputs. e fast changing component hi
is updated similarly to the Simple RNN update rule, but changed to take ci into account as
well:⁹ hi D �.xiW x2 C hi�1W h C ciW c/. Finally, the output yi is the concatenation of the
slow and the fast changing parts of the state: yi D ŒciI hi�. Mikolov et al. demonstrate that this
architecture provides competitive perplexities to the much more complex LSTM on language
modeling tasks.
e approach of Mikolov et al. can be interpreted as constraining the block of the matrix
W s in the S-RNN corresponding to ci to be a multiple of the identity matrix (see Mikolov et al.
[2014] for the details). Le et al. [2015] propose an even simpler approach: set the activation
function of the S-RNN to a ReLU, and initialize the biases b as zeroes and the matrix W s as the
identify matrix. is causes an untrained RNN to copy the previous state to the current state, add
the eﬀect of the current input xi and set the negative values to zero. After setting this initial bias
toward state copying, the training procedure allows W s to change freely. Le et al. demonstrate
that this simple modiﬁcation makes the S-RNN comparable to an LSTM with the same number
of parameters on several tasks, including language modeling.
⁷e states s are often called h in the GRU literature.
⁸We depart from the notation in Mikolov et al. [2014] and reuse the symbols used in the LSTM description.
⁹e update rule diverges from the S-RNN update rule also by ﬁxing the nonlinearity to be a sigmoid function, and by not
using a bias term. However, these changes are not discussed as central to the proposal.

15.5. DROPOUT IN RNNS
183
Beyond diﬀerential gates
e gating mechanism is an example of adapting concepts from the
theory of computation (memory access, logical gates) into diﬀerentiable—and hence gradient-
trainable—systems. ere is considerable research interest in creating neural network architec-
tures to simulate and implement further computational mechanisms, allowing better and more
ﬁne grained control. One such example is the work on a diﬀerentiable stack [Grefenstette et al.,
2015] in which a stack structure with push and pop operations is controlled using an end-to-end
diﬀerentiable network, and the neural turing machine [Graves et al., 2014] which allows read and
write access to content-addressable memory, again, in a diﬀerentiable system. While these eﬀorts
are yet to result in robust and general-purpose architectures that can be used in non-toy language
processing applications, they are well worth keeping an eye on.
15.5
DROPOUT IN RNNS
Applying dropout to RNNs can be a bit tricky, as dropping diﬀerent dimensions at diﬀerent time
steps harms the ability of the RNN to carry informative signals across time. is prompted Pham
et al. [2013], Zaremba et al. [2014] to suggest applying dropout only on the non-recurrent con-
nection, i.e., only to apply it between layers in deep-RNNs and not between sequence positions.
More recently, following a variational analysis of the RNN architecture, Gal [2015] sug-
gests applying dropout to all the components of the RNN (both recurrent and non-recurrent),
but crucially retain the same dropout mask across time steps. at is, the dropout masks are sam-
pled once per sequence, and not once per time step. Figure 15.2 contrasts this form of dropout
(“variational RNN”) with the architecture proposed by Pham et al. [2013], Zaremba et al. [2014].
e variational RNN dropout method of Gal is the current best-practice for applying
dropout in RNNs.

184
15. CONCRETE RECURRENT NEURAL NETWORK ARCHITECTURES
yt–1
yt+1
yt
yt–1
yt+1
yt
xt–1
xt+1
xt
xt–1
xt+1
xt
(a) Naive dropout RNN
(b) Variational RNN
Figure 15.2: Gal’s proposal for RNN dropout (b), vs. the previous suggestion by Pham et al. [2013],
Zaremba et al. [2014] (a). Figure from Gal [2015], used with permission. Each square represents
an RNN unit, with horizontal arrows representing time dependence (recurrent connections). Vertical
arrows represent the input and output to each RNN unit. Colored connections represent dropped-out
inputs, with diﬀerent colors corresponding to diﬀerent dropout masks. Dashed lines correspond to
standard connections with no dropout. Previous techniques (naive dropout, left) use diﬀerent masks
at diﬀerent time steps, with no dropout on the recurrent layers. Gal’s proposed technique (Variational
RNN, right) uses the same dropout mask at each time step, including the recurrent layers.

[Image: extracted_image_192_0.png]
185
C H A P T E R
16
Modeling
with Recurrent Networks
After enumerating common usage patterns in Chapter 14 and learning the details of concrete
RNN architectures in Chapter 15, we now explore the use of RNNs in NLP applications through
some concrete examples. While we use the generic term RNN, we usually mean gated architec-
tures such as the LSTM or the GRU. e Simple RNN consistently results in lower accuracies.
16.1
ACCEPTORS
e simplest use of RNNs is as acceptors: read in an input sequence, and produce a binary or
multi-class answer at the end. RNNs are very strong sequence learners, and can pick-up on very
intricate patterns in the data.
is power is often not needed for many natural language classiﬁcation tasks: the word-
order and sentence structure turn out to not be very important in many cases, and bag-of-words
or bag-of-ngrams classiﬁer often works just as well or even better than RNN-acceptors.
is section presents two examples of acceptor usages for language problems. e ﬁrst is
a canonical one: sentiment classiﬁcation. e approach works well, but less powerful approaches
can also prove competitive. e second is a somewhat contrived example: it does not solve any
“useful” task on its own, but demonstrates the power of RNNs and the kind of patterns they are
capable of learning.
16.1.1 SENTIMENT CLASSIFICATION
Sentence-Level Sentiment Classiﬁcation
In the sentence-level sentiment classiﬁcation task, we are given a sentence, often as part of a
review, and need to assign it one of two values: P or N.¹ is is a somewhat
simplistic view of the sentiment detection task—but one which is often used nonetheless. is is
also the task that motivated our discussion of convolutional neural networks, in Chapter 13. An
example of naturally occurring P and N sentences in the movie-reviews domain
would be the following:²
¹In a more challenging variant, the goal is a three-way classiﬁcation into P, N, and N.
²ese examples are taken from the Stanford Sentiment Treebank [Socher et al., 2013b].

186
16. MODELING WITH RECURRENT NETWORKS
P: It’s not life-aﬃrming—it’s vulgar and mean, but I liked it.
N: It’s a disappointing that it only manages to be decent instead of dead brilliant.
Note that the positive example contains some negative phrases (not life aﬃrming, vulgar, and
mean), while the negative examples contains some positive ones (dead brilliant). Correctly pre-
dicting the sentiment requires understanding not only the individual phrases but also the context
in which they occur, linguistic constructs such as negation, and the overall structure of the sen-
tence. Sentiment classiﬁcation is a tricky and challenging task, and properly solving it involves
handling such issues as sarcasm and metaphor. e deﬁnition of sentiment is also not straight-
forward. For a good overview of the challenges in sentiment classiﬁcation and its deﬁnition, see
the comprehensive review by Pang and Lee [2008]. For our current purpose, however, we will
ignore the complexities in deﬁnition and treat it as a data-driven, binary classiﬁcation task.
e task is straightforward to model using an RNN-acceptor: after tokenization, the RNN
reads in the words of the sentence one at a time. e ﬁnal RNN state is then fed into an MLP
followed by a softmax-layer with two outputs. e network is trained with cross-entropy loss
based on the gold sentiment labels. For a ﬁner-grained classiﬁcation task, where one needs to
assign a sentiment on scale of 1–5 or 1–10 (a “star rating”), it is straightforward to change the
MLP to produce 5 outputs instead of 2. To summarize the architecture:
p.label D k j w1Wn/ D OyŒk�
Oy D softmax.MLP.RNN.x1Wn///
x1Wn D E Œw1�; : : : ; E Œwn�:
(16.1)
e word embeddings matrix E is initialized using pre-trained embeddings learned over a large
external corpus using an algorithm such as W2V or GV with a relatively wide window.
It is often helpful to extend the model in Equation (16.1) by considering two RNNs, one
reading the sentence in its given order and the other one reading it in reverse. e end states of
the two RNNs are then concatenated and fed into the MLP for classiﬁcation:
p.label D k j w1Wn/ D OyŒk�
Oy D softmax.MLP.ŒRNNf.x1Wn/I RNNb.xnW1/�//
x1Wn D E Œw1�; : : : ; E Œwn�:
(16.2)
ese bidirectional models produce strong results for the task [Li et al., 2015].
For longer sentences, Li et al. [2015] found it useful to use a hierarchical architecture, in
which the sentence is split into smaller spans based on punctuation. en, each span is fed into a
forward and a backward RNN as described in Equation (16.2). Sequence of resulting vectors (one
for each span) are then fed into an RNN acceptor such as the one in Equation (16.1). Formally,

16.1. ACCEPTORS
187
given a sentence w1Wn which is split into m spans, w1
1W`1; : : : ; wm
1W`m, the architecture is given by:
p.label D k j w1Wn/ D OyŒk�
Oy D softmax.MLP.RNN.z1Wm///
zi D ŒRNNf.xi
1W`i /I RNNb.xi
`i W1/�
xi
1W`i D E Œwi
1�; : : : ; E Œwi
`i �:
(16.3)
Each of the m diﬀerent spans may convey a diﬀerent sentiment. e higher-level acceptor reads
the summary z1Wm produced by the lower level encoders, and decides on the overall sentiment.
Sentiment classiﬁcation is also used as a test-bed for hierarchical, tree-structured recursive
neural networks, as described in Chapter 18.
Document Level Sentiment Classiﬁcation
e document level sentiment classiﬁcation is similar to the sentence level one, but the input text
is much longer—consisting of multiple sentences—and the supervision signal (sentiment label)
is given only at the end, not for the individual sentences. e task is harder than sentence-level
classiﬁcation, as the individual sentences may convey diﬀerent sentiments than the overall one
conveyed by the document.
Tang et al. [2015] found it useful to use a hierarchical architecture for this task, similar to
the one used by Li et al. [2015]. [Equation (16.3)]: each sentence si is encoded using a gated RNN
producing a vector zi, and the vectors z1Wn are then fed into a second gated RNN, producing a
vector h D RNN.z1Wn/ which is then used for prediction: Oy D softmax.MLP.h//.
e authors experimented also with a variant in which all the intermediate vectors from
the document-level RNN are kept, and their average is fed into the MLP (h1Wn D RNN?.z1Wn/,
Oy D softmax.MLP. 1
n
Pn
iD1 hi//. is produced slightly higher results in some cases.
16.1.2 SUBJECT-VERB AGREEMENT GRAMMATICALITY DETECTION
Grammatical English sentences obey the constraint that the head of the subject of a present-tense
verb must agree with in on the number inﬂection (* denote ungrammatical sentences):
(1)
a.
e key is on the table.
b.
*e key are on the table.
c.
*e keys is on the table.
d.
e keys are on the table.
is relationship is non-trivial to infer from the sequence alone, as the two elements can be
separated by arbitrary long sentential material, which may include nouns of the opposite number:

188
16. MODELING WITH RECURRENT NETWORKS
(2)
a.
e keys to the cabinet in the corner of the room are on the table.
b.
*e keys to the cabinet in the corner of the room is on the table.
Given the diﬃculty in identifying the subject from the linear sequence of the sentence,
dependencies such as subject-verb agreement serve as an argument for structured syntactic
representations in humans [Everaert et al., 2015]. Indeed, given a correct syntactic parse tree of
the sentence, the relation between the verb and its subject becomes trivial to extract:
Te   keys   to   the   cabinet   in   the   corner   of   the   room   are   on   the   table
det
det
pobj
pobj
nsubj
pobj
root
pobj
det
det
det
prep
prep
prep
prep
In work with Linzen and Dupoux [Linzen et al., 2016], we set out to ﬁnd if RNNs,
which are sequential learners, can pick up on this rather syntactic regularity, by learning from
word sequences alone. We set up several prediction tasks based on naturally occurring text from
Wikipedia to test this. One of the tasks was grammaticality detection: the RNN is tasked with
reading a sentence, and at the end deciding if it is grammatical or not. In our setup, grammatical
sentences were Wikipedia sentences that contain a present-tense verb, while ungrammatical ones
are Wikipedia sentences with a present-tense verb in which we picked up one of the present-tense
verbs at random and ﬂipped its form from singular to plural or the other way around.³ Note that
a bag-of-words or bag-of-ngrams model is likely to have a very hard time solving this particular
problem, as the dependency between the verb and the subject relies on the structure of the sen-
tence which is lost when moving to a bag-of-words representation, and can also span more than
any number of words n.
e model was trained as a straightforward acceptor:
Oy D softmax.MLP.RNN.E Œw1�; : : : ; E Œwn�///
using cross-entropy loss. We had tens of thousands training sentences, and hundreds of thousands
test sentences (many of the agreement cases are not hard, and we wanted the test set to contain a
substantial amount of hard cases).
is is a hard task, with very indirect supervision: the supervision signal did not include any
clue as to where the grammaticality clue is. e RNN had to learn the concept of number (that
plural and singular words belong to diﬀerent groups), the concept of agreement (that the form of
the verb should match the form of the subject) and the concept of subjecthood (to identify which
³Some ﬁne details: we identiﬁed verbs using automatically assigned POS-tags. We used a vocabulary of the most frequent
10,000 words in the corpus, and words not in the vocabulary were replaced with their automatically assigned POS tag.

16.2. RNNS AS FEATURE EXTRACTORS
189
of the nouns preceding the verb determines the verb’s form). Identiﬁcation of the correct subject
requires learning to identify syntactic markers of nested structures, in order to be able to skip
over distracting nouns in nested clauses. e RNN handled the learning task remarkably well,
and managed to solve the vast majority (> 99% accuracy) of the test set cases. When focusing on
the really hard cases, in which the verb and its subject were separated by 4 nouns of the opposite
number, the RNN still managed to get accuracy of over 80%. Note that if it were to learn a
heuristic of predicting the number of the last noun, its accuracy would have been 0% on these
cases, and for a heuristic of choosing a random preceding noun the accuracy would have been
20%.
To summarize, this experiment demonstrates the learning power of gated RNNs, and the
kinds of subtle patterns and regularities in the data they can pick up on.
16.2
RNNS AS FEATURE EXTRACTORS
A major use case of RNNs is as ﬂexible, trainable feature extractors, that can replace parts of the
more traditional feature extraction pipelines when working with sequences. In particular, RNNs
are good replacements for window-based extractors.
16.2.1 PART-OF-SPEECH TAGGING
Let’s re-consider the part-of-speech tagging problem under the RNN setup.
e skeleton: deep biRNN
POS-tagging is a special case of the sequence tagging task, assigning
an output tag to each of the n input words. is makes a biRNN an ideal candidate for the basic
structure.
Given a sentence with words s D w1Wn, we will translate them into input vectors x1Wn using
a feature function xi D �.s; i/. e input vectors will be fed into a deep biRNN, producing output
vectors y1Wn D biRNN?.x1Wn/. Each of the vectors yi will then be fed into an MLP which will
predict one of k possible output tags for the word. Each vector yi is focused on position i in
the sequence, but also has information regarding the entire sequence surrounding that position
(an “inﬁnite window”). rough the training procedure, the biRNN will learn to focus on the
sequential aspects that are informative for predicting the label for wi, and encode them in the
vector yi.
From words to inputs with character-level RNNs
How do we map a word wi to an input vector
xi? One possibility is to use an embedding matrix, which can be either randomly initialized or
pre-trained using a technique such as W2V with positional window contexts. Such map-
ping will be performed through an embedding matrix E, mapping words to embedding vectors
ei D E Œwi�. While this works well, it can also suﬀer from coverage problems for vocabulary items
not seen during training or pre-training. Words are made of characters, and certain suﬃxes and
preﬁxes, as well as other orthographic cues such as the presence of capitalization, hyphens, or dig-

190
16. MODELING WITH RECURRENT NETWORKS
its can provide strong hints regarding the word’s ambiguity class. In Chapters 7 and 8 we discussed
integrating such information using designated features. Here, we will replace these manually de-
signed feature extractors with RNNs. Speciﬁcally, we will use two character-level RNNs. For a
word w made of characters c1; : : : ; c`, we will map each character into a corresponding embed-
ding vector ci. e word will then be encoded using a forward RNN and reverse RNN over the
characters. ese RNNs can then either replace the word embedding vector, or, better yet, be
concatenated to it:
xi D �.s; i/ D ŒE Œwi�I RNNf .c1W`/I RNNb.c`W1/�:
Note that the forward-running RNN focuses on capturing suﬃxes, the backward-running
RNN focuses on preﬁxes, and both RNNs can be sensitive to capitalization, hyphens, and even
word length.
e ﬁnal model
e tagging models then becomes:
p.ti D j jw1; : : : ; wn/ D softmax.MLP.biRNN.x1Wn; i///Œj�
xi D �.s; i/ D ŒE Œwi�I RNNf .c1W`/I RNNb.c`W1/�:
(16.4)
e model is trained using cross-entropy loss. Making use of word dropout (Section 8.4.2) for
the word embeddings is beneﬁcial. An illustration of the architecture is given in Figure 16.1.
A similar tagging model is described in the work of Plank et al. [2016], in which it was
shown to produce very competitive results for a wide range of languages.
Character-level Convolution and Pooling
In the architecture above, words are mapped to vec-
tors using forward-moving and backward-moving RNNs over the word’s characters. An alterna-
tive is to represent words using character-level convolution and pooling neural networks (CNN,
Chapter 13). Ma and Hovy [2016] demonstrate that using a one-layer convolutional-and-pooling
layer with a window-size of k D 3 over each word’s characters is indeed eﬀective for part-of-
speech tagging and named-entity recognition tasks.
Structured models
In the above model, the tagging prediction for word i is performed inde-
pendently of the other tags. is may work well, but one could also condition the ith tag on
the previous model predictions. e conditioning can be either the previous k tags (following a
markov assumption), in which case we use tag embeddings E Œt�, resulting in:
p.ti D jjw1; : : : ; wn; ti�1; : : : ; ti�k/ D softmax.MLP.ŒbiRNN.x1Wn; i/I E Œti�1�I : : : I E Œti�k��//Œj �;
or on the entire sequence of previous predictions t1Wi�1, in which case an RNN is used for encoding
the tag sequence:
p.ti D jjw1; : : : ; wn; t1Wi�1/ D softmax.MLP.ŒbiRNN.x1Wn; i/I RNNt.t1Wi�1/�//Œj �:

16.2. RNNS AS FEATURE EXTRACTORS
191
pred
DET
ADJ
NN
VB
IN
BI
pred
concat
BI
pred
BI
pred
BI
pred
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
ø(the)
ø(brown)
ø(fox)
ø(jumped)
ø(over)
E[brown]
…
…
…
…
Rf
c*S*
cb
Rf
cr
Rf
co
Rf
cw
Rf
cn
Rf
c*E*
c*E*
Rf
Rb
cn
Rb
cw
Rb
co
Rb
cr
Rb
cb
Rb
c*S*
Rb
Figure 16.1: Illustration of the RNN tagging architecture. Each word wi is converted into a vector
�.wi/ which is a concatenation of an embedding vector and the end states of forward- and backward-
moving character level RNNs. e word vectors are then fed into a deep biRNN. e output of each
of the outer layer biRNN states is then fed into a predicting network (MLP followed by softmax)
resulting in a tag prediction. Note that each tagging prediction can conditions on the entire input
sentence.
In both cases, the model can be run in greedy mode, predicting the tags ti in sequence, or
using dynamic programming search (in the markov case) or beam-search (in both cases) to ﬁnd
a high-scoring tagging sequence. Such a model was used for CCG-supertagging (assigning each
word one of a large number of tags encoding a rich syntactic structure) by Vaswani et al. [2016].
Structured prediction training for such models is discussed in Chapter 19.
16.2.2 RNN–CNN DOCUMENT CLASSIFICATION
In the sentiment classiﬁcation examples in Section 16.1.1, we had embedding vectors feeding
into a forward-moving RNN and a backward-moving RNN, followed by a classiﬁcation layer
[Equation (16.2)]. In the tagger example in Section 16.2.1, we saw that the word embeddings
can be supplemented (or replaced) with character-level models such as RNNs or CNNs over

192
16. MODELING WITH RECURRENT NETWORKS
the characters, in order to improve the model’s coverage and help it deal with unseen words,
inﬂections, and typos.
e same approach can be eﬀective also for document classiﬁcation: instead of feeding
word-embeddings into the two RNNs, we feed vectors that result either from character-level
RNNs over each word, or from a convolutional-and-pooling layer applied over each word.
Another alternative is to apply a hierarchical convolution-and-pooling network (Sec-
tion 13.3) on the characters, in order to get a shorter sequence of vectors that represent units
that are beyond characters but are not necessarily words (the captured information may capture
either more or less than a single word), and then feed the resulting sequence of vectors into the
two RNNs and the classiﬁcation layer. Such an approach is explored by Xiao and Cho [2016] on
several document classiﬁcation tasks. More speciﬁcally, their hierarchical architecture includes
a series of convolutional and pooling layers. At each layer, a convolution with window size k
is applied to the sequence of input vectors, and then max-pooling is applied between each two
neighboring resulting vectors, halving the sequence length. After several such layers (with win-
dow sizes varying between 5 and 3 as a function of the layer, i.e., widths of 5, 5, 3), the resulting
vectors are fed into forward-running and backward-running GRU RNNs, which are then fed into
a classiﬁcation component (a fully connected layer followed by softmax). ey also apply dropout
between the last convolutional layer and the RNNs, and between the RNN and the classiﬁcation
component. is approach is eﬀective for several document classiﬁcation tasks.
16.2.3 ARC-FACTORED DEPENDENCY PARSING
We revisit the arc-factored dependency-parsing task from Section 7.7. Recall that we are given
a sentence sent with words w1Wn and corresponding POS-tags t1Wn, and need to assign, for each
word pair .wi; wj / a score indicating the strength assigned to word wi being the head of word wj.
In Section 8.6 we derived an intricate feature function for the task, based on windows surrounding
the head and modiﬁer words, the words between the head and modiﬁer words, and their POS
tags. is intricate feature function can be replaced by a concatenation of two biRNN vectors,
corresponding to the head and the modiﬁer words.
Speciﬁcally, given words and POS-tags w1Wn and t1Wn with the corresponding embedding
vectors w1Wn and t1Wn, we create a biRNN encoding vi for each sentence position by concatenating
the word and POS vectors, and feeding them into a deep-biRNN:
v1Wn D biRNN?.x1Wn/
xi D ŒwiI ti�:
(16.5)
We then score a head-modiﬁer candidate by passing the concatenation of the biRNN vectors
through an MLP:
AS.h; m; w1Wn; t1Wn/ D MLP.�.h; m; s// D MLP.ŒvhI vm�/:
(16.6)

16.2. RNNS AS FEATURE EXTRACTORS
193
Illustration of the architecture is given in Figure 16.2. Notice that the biRNN vectors vi
encode the words in context, essentially forming an inﬁnite window to each side of the word wi,
which is sensitive to both the POS-tag sequence and the word sequence. Moreover, the con-
catenation ŒvhI vm� include RNNs running up to each word in each direction, and in particular
it covers the sequence of positions between wh and wm, and the distance between them. e
biRNN is trained as part of the larger network, and learns to focus on the important aspects of
the sequence the syntactic parsing task (structured-training of the arc-factored parser is explained
in Section 19.4.1).
BI
concat
BI
BI
concat
BI
BI
concat
BI
BI
concat
concat
BI
BI
concat
BI
BI
concat
BI
BI
concat
BI
BI
concat
BI
BI
concat
BI
ø(over, fox, s)
E[the]
E[D]
E[fox]
E[N]
E[who]
E[P]
E[likes]
E[V]
E[apples]
E[N]
E[jumped]
E[V]
E[over]
E[P]
E[a]
E[D]
E[dog]
E[N]
the/D
fox/N
who/P
likes/V
apples/N jumped/V
over/P
a/D
dog/N
Figure 16.2: Illustration of the arc-factored parser feature extractor for the arc between fox and over.
Such a feature extractor was used in the work of Kiperwasser and Goldberg [2016b], in
which it was shown to produce state-of-the-art parsing results for the arc-factored approach,
rivaling the scores of much more complex parsing models. A similar approach was taken also by
Zhang et al. [2016], achieving similar results with a diﬀerent training regime.
In general, whenever one is using words as features in a task that is sensitive to word order or
sentence structure, the words can be replaced by their trained biLSTM vectors. Such an approach
was taken by Kiperwasser and Goldberg [2016b] and Cross and Huang [2016a,b] in the context
of transition-based syntactic parsing, with impressive results.

195
C H A P T E R
17
Conditioned Generation
As discussed in Chapter 14, RNNs can act as non-markovian language models, conditioning
on the entire history. is ability makes them suitable for use as generators (generating natural
language sequences) and conditioned generators, in which the generated output is conditioned on
a complex input. is chapter discusses these architectures.
17.1
RNN GENERATORS
A special case of using the RNN-transducer architecture for language modeling (Section 14.3.3)
is sequence generation. Any language model can be used for generation, as described in Section 9.5.
For the RNN-transducer, generation works by tying the output of the transducer at time i with its
input at time i C 1: after predicting a distribution over the next output symbols p.ti D kjt1Wi�1/,
a token ti is chosen and its corresponding embedding vector is fed as the input to the next step.
e process stops when generating a special end-of-sequence symbol, often denoted as </s>.
e process is depicted in Figure 17.1.
E[<s>]
<s>
E[the]
the
E[black]
black
E[fox]
fox
E[jumped]
jumped
s0
R, O
s1
y1
y2
y3
y4
y5
R, O
s2
R, O
s3
R, O
s4
R, O
predict
predict
predict
predict
predict
the
black
fox
jumped
</s>
Figure 17.1: Transducer RNN used as a generator.
Similar to the case of generation from an ngram language model (Section 9.5), when gen-
erating from a trained RNN transducer one can either choose the highest probability item at

196
17. CONDITIONED GENERATION
each step, sample an item according to the model’s predicted distribution, or use beam-search for
ﬁnding a globally high-probability output.
An impressive demonstration of the ability of gated RNN to condition on arbitrarily long
histories is through a RNN-based language model that is trained on characters rather than on
words. When used as a generator, the trained RNN language model is tasked with generat-
ing random sentences character by character, each character conditioning on the previous ones
[Sutskever et al., 2011]. Working on the character level forces the model to look further back into
the sequence in order to connect letters to words and words to sentences, and to form meaningful
patterns. e generated texts not only resemble ﬂuent English, but also show sensitivity to prop-
erties that are not captured by ngram language models, including line lengths and nested paren-
thesis balancing. When trained on C source code, the generated sequences adhere to indentation
patterns, and the general syntactic constraints the C language. For an interesting demonstration
and analysis of the properties of RNN-based character level language models, see Karpathy et al.
[2015].
17.1.1 TRAINING GENERATORS
When training the generator, the common approach is to simply train it as a transducer that aims
to put a large probability mass on the next token in the observed sequence based on the previously
observed tokens (i.e., training as a language model).
More concretely, for every n words sentence w1; : : : ; wn in the training corpus, we produce
an RNN transducer with n C 1 inputs and n C 1 corresponding outputs, where the ﬁrst input is
the start-of-sentence symbol, followed by the n words of the sentence. e ﬁrst expected output
is then w1, the second expected output is w2, and so on, and the n C 1th expected output is the
end-of-sentence symbol.
is training approach is often called teacher-forcing, as the generator is fed the observed
word even if its own prediction put a small probability mass on it, and in test time it would have
generated a diﬀerent word at this state.
While this works, it does not handle well deviations from the gold sequences. Indeed, when
applied as a generator, feeding on its own predictions rather than on gold sequences, the gener-
ator will be required to assign probabilities given states not observed in training. Searching for
a high-probability output sequence using beam-search may also beneﬁt from a dedicated train-
ing procedure. As of this writing, coping with these situations is still an open research question,
which is beyond the scope of this book. We brieﬂy touch upon this when discussing structured
prediction in Chapter 19.3.
17.2
CONDITIONED GENERATION (ENCODER-DECODER)
While using the RNN as a generator is a cute exercise for demonstrating its strength, the power
of the RNN transducer is really revealed when moving to a conditioned generation framework.

17.2. CONDITIONED GENERATION (ENCODER-DECODER)
197
e generation framework generates the next token tj C1 based on the previously generated
tokens Ot1Wj:
Otj C1 � p.tj C1 D k j Ot1Wj /:
(17.1)
is is modeled in the RNN framework as:
p.tj C1 D k j Ot1Wj / D f .RNN.Ot1Wj //
Otj � p.tj j Ot1Wj �1/;
(17.2)
or, if using the more detailed recursive deﬁnition:
p.tj C1 D k j Ot1Wj / D f .O.sj C1//
sjC1 D R.Otj ; sj /
Otj � p.tj j Ot1Wj �1/;
(17.3)
where f is a parameterized function that maps the RNN state to a distribution over words, for
example f .x/ D softmax.xW C b/ or f .x/ D softmax.MLP.x//.
In the conditioned generation framework, the next token is generated based on the previ-
ously generated tokens, and an additional conditioning context c.
Otj C1 � p.tj C1 D k j Ot1Wj ; c/:
(17.4)
When using the RNN framework, the context c is represented as a vector c:
p.tj C1 D k j Ot1Wj ; c/ D f .RNN.v1Wj //
vi D ŒOtiI c�
Otj � p.tj j Ot1Wj �1; c/;
(17.5)
or, using the recursive deﬁnition:
p.tj C1 D k j Ot1Wj ; c/ D f .O.sjC1//
sj C1 D R.sj ; ŒOtj I c�/
Otj � p.ti j Ot1Wj �1; c/:
(17.6)
At each stage of the generation process the context vector c is concatenated to the input Otj,
and the concatenation is fed into the RNN, resulting in the next prediction. Figure 17.2 illustrates
the architecture.

198
17. CONDITIONED GENERATION
E[<s>]
<s>
E[the]
the
E[black]
black
E[fox]
fox
E[jumped]
jumped
s0
R, O
s1
y1
y2
y3
y4
y5
R, O
s2
R, O
s3
R, O
s4
R, O
predict
predict
predict
predict
predict
concat
concat
concat
concat
concat
the
black
fox
jumped
</s>
c
c
c
c
c
Figure 17.2: Conditioned RNN generator.
What kind of information can be encoded in the context c? Pretty much any data we can
put our hands on during training, and that we ﬁnd useful. For example, if we have a large corpus of
news items categorized into diﬀerent topics, we can treat the topic as a conditioning context. Our
language model will then be able to generate texts conditioned on the topic. If we are interested
in movie reviews, we can condition the generation on the genre of the movie, the rating of the
review, and perhaps the geographic region of the author. We can then control these aspects when
generating text. We can also condition on inferred properties, that we automatically extract from
the text. For example, we can derive heuristics to tell us if a given sentence is written in ﬁrst
person, if it contains a passive-voice construction, and the level of vocabulary used in it. We can
then use these aspects as conditioning context for training, and, later, for text generation.
17.2.1 SEQUENCE TO SEQUENCE MODELS
e context c can have many forms. In the previous subsection, we described some ﬁxed-length,
set-like examples of conditioning contexts. Another popular approach takes c to be itself a se-
quence, most commonly a piece of text. is gives rise to the sequence to sequence conditioned
generation framework, also called the encoder-decoder framework [Cho et al., 2014a, Sutskever
et al., 2014].

17.2. CONDITIONED GENERATION (ENCODER-DECODER)
199
In sequence to sequence conditioned generation, we have a source sequence x1Wn (for ex-
ample reﬂecting a sentence in French) and we are interested in generating a target output se-
quence t1Wm (for example the translation of the sentence into English). is works by encoding
the source sentence x1Wn into a vector using an encoder function c D E.x1Wn/, commonly an
RNN: c D RNNenc.x1Wn/. A conditioned generator RNN (decoder) is then used to generate the
desired output t1Wm according to Equation (17.5). e architecture is illustrated in Figure 17.3.
E[<s>]
<s>
E[a]
a
E[conditioning]
conditioning
E[sequence]
sequence
E[</s>]
</s>
s0
RD, OD
RD, OD
RD, OD
RD, OD
RD, OD
s1
y1
y2
y3
y4
y5
s2
s3
s4
RE, OE
RE, OE
RE, OE
RE, OE
RE, OE
predict
predict
predict
predict
predict
concat
concat
concat
concat
concat
the
black
fox
jumped
</s>
c
c
c
c
c
E[<s>]
<s>
E[the]
the
E[black]
black
E[fox]
fox
E[jumped]
jumped
Figure 17.3: Sequence-to-sequence RNN generator.
is setup is useful for mapping sequences of length n to sequences of length m. e encoder
summarizes the source sentence as a vector c, and the decoder RNN is then used to predict (using
a language modeling objective) the target sequence words conditioned on the previously predicted
words as well as the encoded sentence c. e encoder and decoder RNNs are trained jointly. e

200
17. CONDITIONED GENERATION
supervision happens only for the decoder RNN, but the gradients are propagated all the way back
to the encoder RNN (see Figure 17.4).
s0
RD, OD
RD, OD
RD, OD
RD, OD
RD, OD
s1
t1
t1
x1
x2
x3
x4
x5
t2
t3
t4
<s>
t2
t3
t4
t5
s2
s3
s4
s0
s1
s2
s3
s4
RE, OE
RE, OE
RE, OE
RE, OE
RE, OE
predict and
calculate
loss
predict and
calculate
loss
predict and
calculate
loss
predict and
calculate
loss
predict and
calculate
loss
d
e
e
e
e
e
s5
e
d
d
d
d
sum
loss
Figure 17.4: Sequence-to-sequence RNN training graph.
17.2.2 APPLICATIONS
e sequence-to-sequence approach is very general, and can potentially ﬁt any case where a map-
ping from an input sequence to an output sequence is needed. We list some example use cases
from the literature.

17.2. CONDITIONED GENERATION (ENCODER-DECODER)
201
Machine Translation
e sequence-to-sequence approach was shown to be surprisingly eﬀec-
tive for Machine Translation [Sutskever et al., 2014] using deep LSTM RNNs. In order for the
technique to work, Sutskever et al. found it eﬀective to input the source sentence in reverse, such
that xn corresponds to the ﬁrst word of the sentence. In this way, it is easier for the second RNN
to establish the relation between the ﬁrst word of the source sentence to the ﬁrst word of the
target sentence.
While the success of the sequence-to-sequence approach in French-to-English translation
is impressive, it is worth noting that the approach of Sutskever et al. [2014] required eight layers
of high-dimensional LSTMs, is very computationally expensive, and is non-trivial to train well.
Later in this chapter (Section 17.4) we describe attention-based architectures, an elaboration on
the sequence-to-sequence architecture that is much more useful for machine translation.
Email Auto-response
Here, the task is to map an email, that can be potentially long, into a
short answer such as Yes, I’ll do it, Great, see you on Wednesday or It won’t work out. Kannan et al.
[2016] describe an implementation of the auto-response feature for the Google Inbox product.
e core of the solution is a straightforward sequence to sequence conditioned generation model
based on an LSTM encoder that reads in the email, and an LSTM decoder that generates an
appropriate response. is component is trained on many email-response pairs. Of course, in
order to successfully integrate the response generation component into a product, it needs to be
supplemented by additional modules, to schedule the triggering of the response component, to
ensure diversity of responses and balance negative and positive responses, maintain user privacy,
and so on. For details, see Kannan et al. [2016].
Morphological Inﬂection
In the morphological inﬂection task, the input is a base word and a
desired inﬂection request, and the output is an inﬂected form of the word. For example, for the
Finnish word bruttoarvo and the desired inﬂection pos=N,case=IN+ABL,num=PL the desired
output is bruttoarvoista. While the task has traditionally been approached using hand-crafted lex-
icons and ﬁnite-state transducers, it is also a very good ﬁt for character level sequence-to-sequence
conditioned generation models [Faruqui et al., 2016]. Results of the SIGMORPHON 2016
shared task on inﬂection generation indicate that recurrent neural network approaches outper-
form all other participating approaches [Cotterell et al., 2016]. e second-place system [Aharoni
et al., 2016] used a sequence-to-sequence model with a few enhancements for the task, while the
winning system [Kann and Schütze, 2016] used an ensemble of attentive sequence-to-sequence
models, such as the ones described in Section 17.4.
Other Uses
Mapping a sequence of n items to a sequence of m items is very general, and almost
any task can be formulated in an encode-and-generate solution. However, the fact that a task
can be formulated this way, does not mean that it should be—perhaps better architectures are
more suitable for it, or are easier to learn. We now describe several applications that seem to
be needlessly hard to learn under the encoder-decoder framework, and for which other, better-

202
17. CONDITIONED GENERATION
suited architectures exist. e fact that the authors managed to get decent accuracies with the
encoder-decoder framework attests to the power of the framework.
Filippova et al. [2015] use the architecture for performing sentence compression by deletion.
In this task, we are given a sentence such as “Alan Turing, known as the father of computer science, the
codebreaker that helped win World War 2, and the man tortured by the state for being gay, is to receive a
pardon nearly 60 years after his death” and are required to produce a shorter (“compressed”) version
containing the main information in the sentence by deleting words from the original sentence.
An example compression would be “Alan Turing is to receive a pardon.” Filippova et al. [2015]
model the problem as a sequence-to-sequence mapping in which the input sequence is the input
sentence (possibly coupled with syntactic information derived from an automatically produced
parse-tree), and the output is a sequence of K, D, and S decisions. e model was
trained on a corpus of about 2 million sentence-and-compression pairs extracted automatically
from news articles [Filippova and Altun, 2013], producing state-of-the-art results.¹
Gillick et al. [2016] perform part-of-speech tagging and named-entity recognition by treat-
ing it as a sequence-to-sequence problem mapping a sequence of unicode bytes to a sequence of
spans predictions of the form S12,L13,PER,S40,L11,LOC indicating a 13-bytes long P
entity starting at oﬀset 12, and an 11-bytes long L entity starting at oﬀset 40.²
Vinyals et al. [2014] perform syntactic parsing as a sequence-to-sequence task mapping a
sentence to a set of constituency bracketing decisions.
17.2.3 OTHER CONDITIONING CONTEXTS
e conditioned-generation approach is very ﬂexible—the encoder needn’t be an RNN. Indeed,
the conditioning context vector can be based on a single word, a CBOW encoding, be generated
by a convolutional network, or based on some other complex computation.
Furthermore, the conditioning context need not even be text-based. In a dialog setting (in
which the RNN is trained to produce responses to messages in a dialog) Li et al. [2016] use as con-
text a trainable embedding vector which is associated with the user who wrote the response. e
intuition is that diﬀerent users have diﬀerent communication styles, based on their age, gender,
social role, background knowledge, personality traits and many other latent factors. By condition-
ing on the user when generating the response, the network can learn to adapt its predictions while
still using an underlying language model as a backbone. Moreover, as a side eﬀect of training the
generator, the network also learns user embeddings, producing similar vectors to users who have
similar communication styles. At test time, one can inﬂuence the style of the generated response
by feeding in a particular user (or average user vector) as a conditioning context.
¹While impressive, the sequence-to-sequence approach is arguably an overkill for this task, in which we map a sequence of n
words into a sequence of n decisions, where the ith decision relates directly to the ith word. is is in essence a sequence
tagging task, and a biLSTM transducer, such as those described in the previous chapter, could be a better ﬁt. Indeed, the work
of Klerke et al. [2016] shows that similar (though a bit lower) accuracies can be obtained using a biRNN transducer trained
on several orders of magnitude less data.
²is is, again, a sequence tagging task which can be performed well using a biLSTM transducer, or a structured biLSTM
transducer (biLSTM-CRF), as described in Section 19.4.2.

17.3. UNSUPERVISED SENTENCE SIMILARITY
203
Departing further away from language, a popular use-case is in image captioning: an input
image is encoded as a vector (usually using a multi-layer convolutional network³) and this vector is
used as a conditioning context for an RNN generator that is trained to predict image descriptions
[Karpathy and Li, 2015, Mao et al., 2014, Vinyals et al., 2015].
Work by Huang et al. [2016] extend the captioning task to the more elaborate one of
visual story telling, in which the input is a series of images, and the output is a story describing
the progression in the images. Here, the encoder is an RNN that reads in a sequence of image
vectors.
17.3
UNSUPERVISED SENTENCE SIMILARITY
It is often desired to have vector representations of sentences such that similar sentences have
similar vectors. is problem is somewhat ill deﬁned (what does it mean for sentences to be simi-
lar?), and is still an open research question, but some approaches produce reasonable results. Here,
we focus on unsupervised approaches, in the sense that they can be trained from un-annotated
data. e result of the training is an encoder function E.w1Wn/ such that similar sentences are
encoded to similar vectors.
Most approaches are based on the sequence-to-sequence framework: an encoder RNN is
trained to produced context vectors c that will then be used by an RNN decoder to perform a
task. As a consequence, the important information from the sentence with respect to the task must
be captured in c. en, the decoder RNN is thrown away, and the encoder is used to generate
sentence representations c, under the premise that similar sentences will have similar vectors. e
resulting similarity function across sentences, then, crucially relies on the task of the decoder was
trained to perform.
Auto Encoding
e auto-encoding approach is a conditioned generation model in which a
sentence is encoded using an RNN, and then the decoder attempts to reconstruct the input sen-
tence. is way, the model is trained to encode the information that is needed to reconstruct the
sentence, again, hopefully resulting in similar sentences having similar vectors. e sentence re-
construction objective may not be ideal for general sentence similarity, however, as it is likely to
push apart representations of sentences that convey similar meanings but use diﬀerent words.
Machine Translation
Here, a sequence-to-sequence network is trained to translate sentences
from English to another language. Intuitively, the vectors produced by the encoder are useful for
translation, and so they encode the essence of the sentence that is needed to translate it properly,
resulting in sentences that will be translated similarly to have similar vectors. is method requires
a large corpus for the conditioned generation task, such as a parallel corpus used in machine
translation.
³Mapping images to vectors using neural architectures is a well-studied topic with established best practices and many success
stories. It also falls outside the scope of this book.

204
17. CONDITIONED GENERATION
Skip-thoughts
e model of Kiros et al. [2015], assigned the name skip-thought vectors by its
authors, presents an interesting objective to the sentence similarity problem. e model extend
the distributional hypothesis from words to sentences, arguing that sentences are similar if they
appear in similar contexts, where a context of a sentence are the sentences surrounding it. e
skip-thoughts model is thus a conditioned generation model where an RNN encoder maps a
sentence to a vector, and then one decoder is trained to reconstruct the previous sentence based on
the encoded representation, and a second decoder is trained to reconstruct the following sentence.
e trained skip-thought encoder produces impressive results in practice, mapping sentences such
as:
(a) he ran his hand inside his coat, double-checking that the unopened letter was still there; and
(b) he slipped his hand between his coat and his shirt, where the folded copies lay in a brown envelope.
to similar vectors.
Syntactic Similarity
e work of Vinyals et al. [2014] demonstrate that an encoder-decoder can
produce decent results for phrase-based syntactic parsing, by encoding the sentence and requiring
the decoder to reconstruct a linearized parse tree as a stream of bracketing decisions, i.e., mapping
from:
the boy opened the door
to:
(S (NP DT NN ) (VP VBD (NP DT NN ) ) )
e encoded sentence representations under such training are likely to capture the syntactic struc-
ture of the sentence.
17.4
CONDITIONED GENERATION WITH ATTENTION
In the encoder-decoder networks described in Section 17.2 the input sentence is encoded into
a single vector, which is then used as a conditioning context for an RNN-generator. is archi-
tectures forces the encoded vector c D RNNenc.x1Wn/ to contain all the information required for
generation, and requires the generator to be able to extract this information from the ﬁxed-length
vector. Given these rather strong requirements, the architecture works surprisingly well. How-
ever, in many cases it can be substantially improved by the addition of an attention mechanism.
e conditioned generation with attention architecture [Bahdanau et al., 2014] relaxes the condi-
tion that the entire source sentence be encoded as a single vector. Instead, the input sentence
is encoded as a sequence of vectors, and the decoder uses a soft attention mechanism in order to
decide on which parts of the encoding input it should focus. e encoder, decoder, and attention
mechanism are all trained jointly in order to play well with each other.

17.4. CONDITIONED GENERATION WITH ATTENTION
205
More concretely, the encoder-decoder with attention architecture encodes a length n input
sequence x1Wn using a biRNN, producing n vectors c1Wn:
c1Wn D E.x1Wn/ D biRNN?.x1Wn/:
e generator (decoder) can then use these vectors as a read-only memory representing the con-
ditioning sentence: at any stage j of the generation process, it chooses which of the vectors c1Wn
it should attend to, resulting in a focused context vector cj D attend.c1Wn; Ot1Wj /.
e focused context vector cj is then used for conditioning the generation at step j:
p.tj C1 D k j Ot1Wj ; x1Wn/ D f .O.sjC1//
sjC1 D R.sj ; ŒOtj I cj �/
cj D attend.c1Wn; Ot1Wj /
Otj � p.tj j Ot1Wj �1; x1Wn/:
(17.7)
In terms of representation power, this architectures subsumes the previous encoder-decoder ar-
chitecture: by setting attend.c1Wn; Ot1Wj / D cn, we get Equation (17.6).
How does the function attend.�; �/ look like? As you may have guessed by this point, it
is a trainable, parameterized function. is text follows the attention mechanism described by
Bahdanau et al. [2014], who were the ﬁrst to introduce attention in the context of sequence to
sequence generation.⁴ While this particular attention mechanism is popular and works well, many
variants are possible. e work of Luong et al. [2015] explores some of them in the context of
machine translation.
e implemented attention mechanism is soft, meaning that at each stage the decoder sees
a weighted average of the vectors c1Wn, where the weights are chosen by the attention mechanism.
More formally, at stage j the soft attention produces a mixture vector cj:
cj D
n
X
iD1
˛j
Œi� � ci:
˛j 2 Rn
C is the vector of attention weights for stage j, whose elements ˛j
Œi� are all positive and
sum to one.
e values ˛j
Œi� are produced in a two stage process: ﬁrst, unnormalized attention weights
N˛j
Œi� are produced using a feed-forward network MLPatt taking into account the decoder state at
time j and each of the vectors ci:
N˛j D N˛j
Œ1�; : : : ; N˛j
Œn� D
D MLPatt.Œsj I c1�/; : : : ; MLPatt.Œsj I cn�/:
(17.8)
⁴e description of the decoder part of the model diﬀers in some small aspects from that of Bahdanau et al. [2014], and is
more similar to that of Luong et al. [2015].

206
17. CONDITIONED GENERATION
e unnormalized weights N˛j are then normalized into a probability distribution using the soft-
max function:
˛j D softmax. N˛j
Œ1�; : : : ; N˛j
Œn�/:
In the context of machine translation, one can think of MLPatt as computing a soft alignment
between the current decoder state sj (capturing the recently produced foreign words) and each
of the source sentence components ci.
e complete attend function is then:
attend.c1Wn; Ot1Wj / D cj
cj D
n
X
iD1
˛j
Œi� � ci
˛j D softmax. N˛j
Œ1�; : : : ; N˛j
Œn�/
N˛j
Œi� D MLPatt.Œsj I ci�/;
(17.9)
and the entire sequence-to-sequence generation with attention is given by:
p.tj C1 D k j Ot1Wj ; x1Wn/ D f .Odec.sj C1//
sj C1 D Rdec.sj ; ŒOtj I cj �/
cj D
n
X
iD1
˛j
Œi� � ci
c1Wn D biRNN?
enc.x1Wn/
˛j D softmax. N˛j
Œ1�; : : : ; N˛j
Œn�/
N˛j
Œi� D MLPatt.Œsj I ci�/
Otj � p.tj j Ot1Wj �1; x1Wn/
f .z/ D softmax.MLPout.z//
MLPatt.Œsj I ci�/ D v tanh.Œsj I ci�U C b/:
(17.10)
A sketch of the architecture is given in Figure 17.5.
Why use the biRNN encoder to translate the conditioning sequence x1Wn into the context
vectors c1Wn instead of letting the attention mechanism look directly at x1Wn? Couldn’t we just use
cj D Pn
iD1 ˛j
Œi� � xi and N˛j
Œi� D MLPatt.Œsj I xi�/? We could, but we get important beneﬁts from
the encoding process. First, the biRNN vectors ci represent the items xi in their sentential context,

17.4. CONDITIONED GENERATION WITH ATTENTION
207
s0
s1
y1
y2
y3
y4
y5
s2
s3
s4
predict
predict
predict
predict
predict
concat
concat
concat
concat
concat
attend
attend
attend
attend
attend
the
black
fox
jumped
</s>
c0
c1
c4
c2
c3
RD, OD
RD, OD
RD, OD
RD, OD
RD, OD
BIE
BIE
BIE
BIE
BIE
E[<s>]
<s>
E[a]
a
E[conditioning]
conditioning
E[sequence]
sequence
E[</s>]
</s>
E[<s>]
<s>
E[the]
the
E[black]
black
E[fox]
fox
E[jumped]
jumped
Figure 17.5: Sequence-to-sequence RNN generator with attention.
that is, they represent a window focused around the input item xi and not the item itself. Second,
by having a trainable encoding component that is trained jointly with the decoder, the encoder
and decoder evolve together and the network can learn to encode relevant properties of the input
that are useful for decoding, and that may not be present at the source sequence x1Wn directly. For
example, the biRNN encoder may learn to encode the position of xi within the sequence, and the
decoder could use this information to access the elements in order, or learn to pay more attention
to elements in the beginning of the sequence then to elements at its end.

208
17. CONDITIONED GENERATION
Attentive conditioned generation models are very powerful, and work very well on many
sequence to sequence generation tasks.
17.4.1 COMPUTATIONAL COMPLEXITY
e conditioned generation without attention is relatively cheap: the encoding is performed in
linear time in the input length (O.n/), and the decoding is performed in linear time in the output
length (O.m/). While generating a distribution over words from a large vocabulary is in itself ex-
pensive, this is an orthogonal issue to this analysis, in which we consider the vocabulary scoring as
a constant time operation. e overall complexity of the sequence to sequence generation process
is then O.m C n/.⁵
What is the cost of adding the attention mechanism? e encoding of the input sequence
remains an O.n/ linear time operation. However, each step of the decoding process now needs
to compute cj. is entails n evaluations of MLPatt followed by a normalization step and a sum-
mation of n vectors. e complexity of a decoding step grew from a constant time operation to
linear in the length of the conditioning sentence (O.n/), resulting in a total runtime of O.m � n/.
17.4.2 INTERPRETABILITY
Non-attentive encoder-decoder networks (much like most other neural architectures) are ex-
tremely opaque: we do not have a clear understanding on what exactly is encoded in the encoded
vector, how this information is exploited in the decoder, and what prompted a particular decoder
behavior. An important beneﬁt of the attentive architecture is that it provides a simple way of
peeking inside some of the reasoning in the decoder and what it learned. At each stage of the
decoding process, one can look at the produced attention weights ˛j and see which areas of the
source sequence the decoder found relevant when producing the given output. While this is still a
weak form of interpretability, it is leaps and bounds beyond the opaqueness of the non-attentive
models.
17.5
ATTENTION-BASED MODELS IN NLP
Conditioned-generation with attention is a very powerful architecture. It is the main algorithm
driving state-of-the-art machine translation, and provides strong results on many other NLP
tasks. is section provides a few examples of its usage.
⁵While the output length m is in principle not bounded, in practice the trained decoders do learn to produce outputs with
a length distribution similar to lengths in the training dataset, and in the worst case one can always put a hard limit on the
length of generated sentences.

17.5. ATTENTION-BASED MODELS IN NLP
209
17.5.1 MACHINE TRANSLATION
While we initially described machine translation in the context of plain sequence to sequence gen-
eration, current state-of-the-art machine translation systems are powered by models that employ
attention.
e ﬁrst results with attentive sequence-to-sequence models for machine translation are
due to Bahdanau et al. [2014], who essentially used the architecture described in the previous
section as is (using a GRU-ﬂavored RNN), employing beam-search when generating from the
decoder at test time. While Luong et al. [2015] explored variations on the attention mechanism
leading to some gains, most progress in neural machine translation use the attentive sequence-
to-sequence architecture as is (either with LSTMs or GRUs), while changing its inputs.
While we cannot expect to cover neural machine translation in this rather short section,
we list some improvements due to Sennrich and colleagues that push the boundaries of the state-
of-the-art.
Sub-word Units
In order to deal with vocabularies of highly inﬂected languages (as well as to
restrict the vocabulary size in general), Sennrich et al. [2016a] propose moving to working with
sub-word units that are smaller than a token. eir algorithm processes the source and target side
texts using an algorithm called B in search for prominent subword units (the algorithm itself
is described at the end of Section 10.5.5). When run on English, this stage is likely to ﬁnd units
such as er, est, un, low and wid. e source and target sentences are then processed to split words
according to the induced segmentation (i.e., converting the widest network into the wid_ _est
net_ _work). is processed corpus is then fed into an attentive sequence-to-sequence training.
After decoding test sentences, the output is processed once more to un-split the sub-word units
back into words. is process reduces the number of unknown tokens, makes it easier to generalize
to new vocabulary items, and improves translation quality. Related research eﬀort attempt to work
directly on the character level (encoding and decoding characters instead of words), with notable
success [Chung et al., 2016].
Incorporating monolingual data
e sequence-to-sequence models are trained on parallel cor-
pora of aligned sentences in the source and target languages. Such corpora exist, but are naturally
much smaller than available monolingual data, which is essentially inﬁnite. Indeed, the previous
generation of statistical machine translation systems⁶ train a translation model on the parallel data,
and a separate language model on much larger monolingual data. e sequence-to-sequence ar-
chitecture does not currently allow such a separation, training the language model (decoder) and
translation model (encoder-decoder interaction) jointly.
How can we make use of target-side monolingual data in a sequence-to-sequence frame-
work? Sennrich et al. [2016b] propose the following training protocol: when attempting to trans-
late from source to target, ﬁrst train a translation model from target to source, and use it to
⁶For an overview, see the book of Koehn [2010] as well as the book on syntax-based machine translation in this series [Williams
et al., 2016].

210
17. CONDITIONED GENERATION
translate a large monolingual corpus of target sentences. en, add the resulting (target,source)
pairs to the parallel corpus as (source,target) examples. Train a source to target MT system on the
combined corpus. Note that while the system now trains on automatically produced examples,
all of the target side sentences it sees are original, so the language modeling component is never
trained on automatically produced text. While somewhat of a hack, this training protocol brings
substantial improvements in translation quality. Further research will likely yield cleaner solutions
for integrating monolingual data.
Linguistic Annotations
Finally, Sennrich and Haddow [2016] show that the attentive
sequence-to-sequence architecture can learn better translation model if its input is supplemented
with linguistic annotations. at is, given a source sentence w1; : : : ; wn, rather than creating the
input vectors x1Wn by simply assigning an embedding vector to each word (xi D E Œwi�), the sen-
tence is run through a linguistic annotation pipeline that includes part-of-speech tagging, syn-
tactic dependency parsing and lemmatization. Each word is then supplemented with an encoding
vector of its part of speech tag (pi), it’s dependency label with respect to its head (ri), its lemma
(li), and morphological features (mi). e input vectors x1Wn is then deﬁned as concatenation
of these features: xi D ŒwiI piI riI liI mi�. ese additional features consistently improve trans-
lation quality, indicating that linguistic information is helpful even in the presence of powerful
models than can in theory learn the linguistic concepts on their own. Similarly, Aharoni and
Goldberg [2017] show that by training the decoder in a German to English translation system
to produce linearized syntactic trees instead of a sequence of words, the resulting translations
exhibit more consistent reordering behavior, and better translation quality. ese works barely
scratch the surface with respect to integrating linguistic information. Further research may come
up with additional linguistic cues that could be integrated, or improved ways of integrating the
linguistic information.
Open issues
As of the time of this writing, major open issues in neural machine translation
include scaling up the size of the output vocabulary (or removing the dependence on it by moving
to character-based outputs), training while taking the beam-search decoding into account, and
speeding up training and decoding. Another topic that becomes popular is the move to models
that make use of syntactic information. at said, the ﬁeld is moving extremely fast, and this
paragraph may not be relevant by the time the book gets to press.
17.5.2 MORPHOLOGICAL INFLECTION
e morphological inﬂection task discussed above in the context of sequence to sequence models
also work better when used with an attentive sequence-to-sequence architecture, as evident by
the architecture of the winning system in the SIGMORPHON shared task on morphological
reinﬂection [Cotterell et al., 2016]. e winning system [Kann and Schütze, 2016] essentially
use an oﬀ-the-shelf attentive sequence to sequence model. e input to the shared task is a word
form and a desired inﬂection, given as a list of target part-of-speech tags and morphological

17.5. ATTENTION-BASED MODELS IN NLP
211
features, e.g., NOUN Gender=Male Number=Plural, and the desired output as an inﬂected
form. is is translated to a sequence to sequence model by creating an input sequence that is
the list of inﬂection information, followed by the list of characters of the input word. e desired
output is then the list of characters in the target word.
17.5.3 SYNTACTIC PARSING
While more suitable architectures exist, the work of Vinyals et al. [2014] show that attentive
sequence to sequence models can produce competitive syntactic parsing results, by reading in a
sentence (a word at a time) and outputting a sequence of bracketing decisions. is may not seem
like an ideal architecture for parsing—indeed, one can get superior results with better tailored
architectures, as evident by the work of Cross and Huang [2016a]. However, considering the
generality of the architecture, the system works surprisingly well, and produces impressive parsing
results. In order to get fully competitive results, some extra steps must be taken: the architecture
needs a lot of training data. It is trained on parse-trees produced by running two treebank-trained
parsers on a large text corpus, and selecting trees on which the two parsers agree (high-conﬁdence
parses). In addition, for the ﬁnal parser, an ensemble (Section 5.2.3) of several attention networks
is used.

PART IV
Additional Topics

215
C H A P T E R
18
Modeling Trees
with Recursive Neural Networks
e RNN is very useful for modeling sequences. In language processing, it is often natural and
desirable to work with tree structures. e trees can be syntactic trees, discourse trees, or even trees
representing the sentiment expressed by various parts of a sentence [Socher et al., 2013b]. We
may want to predict values based on speciﬁc tree nodes, predict values based on the root nodes,
or assign a quality score to a complete tree or part of a tree. In other cases, we may not care about
the tree structure directly but rather reason about spans in the sentence. In such cases, the tree is
merely used as a backbone structure which helps guide the encoding process of the sequence into
a ﬁxed size vector.
e recursive neural network abstraction (RecNN) [Pollack, 1990], popularized in NLP by
Richard Socher and colleagues [Socher, 2014, Socher et al., 2010, 2011, 2013a] is a generalization
of the RNN from sequences to (binary) trees.¹
Much like the RNN encodes each sentence preﬁx as a state vector, the RecNN encodes
each tree-node as a state vector in Rd. We can then use these state vectors either to predict values
of the corresponding nodes, assign quality values to each node, or as a semantic representation of
the spans rooted at the nodes.
e main intuition behind the recursive neural networks is that each subtree is represented
as a d-dimensional vector, and the representation of a node p with children c1 and c2 is a func-
tion of the representation of the nodes: vec.p/ D f .vec.c1/; vec.c2//, where f is a composition
function taking two d-dimensional vectors and returning a single d-dimensional vector. Much
like the RNN state si is used to encode the entire sequence x1Wi, the RecNN state associated
with a tree node p encodes the entire subtree rooted at p. See Figure 18.1 for an illustration.
18.1
FORMAL DEFINITION
Consider a binary parse tree T over an n-word sentence. As a reminder, an ordered, unlabeled
tree over a string x1; : : : ; xn can be represented as a unique set of triplets .i; k; j /, s.t. i � k � j.
Each such triplet indicates that a node spanning words xiWj is parent of the nodes spanning xiWk
and xkC1Wj. Triplets of the form .i; i; i/ correspond to terminal symbols at the tree leaves (the
words xi). Moving from the unlabeled case to the labeled one, we can represent a tree as a set of
¹While presented in terms of binary parse trees, the concepts easily transfer to general recursively deﬁned data structures, with
the major technical challenge is the deﬁnition of an eﬀective form for R, the combination function.

216
18. MODELING TREES WITH RECURSIVE NEURAL NETWORKS
S = 
NP2 = 
NP1 = 
V = 
VP = 
combine
combine
Figure 18.1: Illustration of a recursive neural network. e representations of V and NP1 are com-
bined to form the representation of VP. e representations of VP and NP2 are then combined to
form the representation of S.
6-tuples .A ! B; C; i; k; j/, whereas i, k, and j indicate the spans as before, and A, B, and C
are the node labels of of the nodes spanning xiWj, xiWk, and xkC1Wj, respectively. Here, leaf nodes
have the form .A ! A; A; i; i; i/, where A is a pre-terminal symbol. We refer to such tuples as
production rules. For an example, consider the syntactic tree for the sentence “the boy saw her
duck.”
S
NP
Det
Noun
boy
Verb
saw
NP
VP
Det
the
Noun
her
duck
Its corresponding unlabeled and labeled representations are as shown in Table 18.1.

18.1. FORMAL DEFINITION
217
Table 18.1: Unlabeled and labeled representations
Unlabeled
Labeled
Corresponding Span
(1,1,1)
(Det, Det, Det, 1, 1, 1)
x1:1   the
(2.2.2)
(Nound, Noun, Noun, 2, 2, 2)
x2:2   boy
(3,3,3)
(Verb, Verb, Verb, 3, 3, 3)
x3:3   saw
(4, 4, 4)
(Det, Det, Det, 4, 4, 4)
x4:4   her
(5, 5, 5)
(Noun, Noun, Noun, 5, 5, 5)
x5:5   duck
(4, 4, 5)
(NP, Det, Noun, 4, 4, 5)
x4:5   her duck
(3, 3, 5)
(VP, Verb, NP, 3, 3, 5)
x3:5   saw her duck
(1, 1, 2)
(NP, Det, Nound, 1, 1, 2)
x1:2   the boy
(1, 2, 5)
(S, NP, VP, 1 2, 5)
x1:5   the boy saw her duck
e set of production rules above can be uniquely converted to a set of tree nodes qA
iWj
(indicating a node with symbol A over the span xiWj) by simply ignoring the elements .B; C; k/
in each production rule. We are now in position to deﬁne the recursive neural network.
A recursive neural network (RecNN) is a function that takes as input a parse tree over an n-
word sentence x1; : : : ; xn. Each of the sentence’s words is represented as a d-dimensional vector
xi, and the tree is represented as a set T of production rules .A ! B; C; i; j; k/. Denote the nodes
of T by qA
iWj. e RecNN returns as output a corresponding set of inside state vectors sA
iWj, where
each inside state vector sA
iWj 2 Rd represents the corresponding tree node qA
iWj, and encodes the
entire structure rooted at that node. Like the sequence RNN, the tree-shaped RecNN is deﬁned
recursively using a function R, where the inside vector of a given node is deﬁned as a function of
the inside vectors of its direct children.² Formally:
RecNN.x1; : : : ; xn; T / DfsA
iWj 2 Rd j qA
iWj 2 T g
sA
iWi Dv.xi/
sA
iWj DR.A; B; C; sB
iWk; sC
kC1Wj /
qB
iWk 2 T ; qC
kC1Wj 2 T :
(18.1)
e function R usually takes the form of a simple linear transformation, which may or may
not be followed by a nonlinear activation function g:
R.A; B; C; sB
iWk; sC
kC1Wj / D g.ŒsB
iWkI sC
kC1Wj �W /:
(18.2)
²Le and Zuidema [2014] extend the RecNN deﬁnition such that each node has, in addition to its inside state vector, also an
outside state vector representing the entire structure around the subtree rooted at that node. eir formulation is based on the
recursive computation of the classic inside-outside algorithm, and can be thought of as the biRNN counterpart of the tree
RecNN. For details, see Le and Zuidema [2014].

218
18. MODELING TREES WITH RECURSIVE NEURAL NETWORKS
is formulation of R ignores the tree labels, using the same matrix W 2 R2d�d for all combina-
tions. is may be a useful formulation in case the node labels do not exist (e.g., when the tree does
not represent a syntactic structure with clearly deﬁned labels) or when they are unreliable. How-
ever, if the labels are available, it is generally useful to include them in the composition function.
One approach would be to introduce label embeddings v.A/ mapping each non-terminal symbol
to a dnt dimensional vector, and change R to include the embedded symbols in the combination
function:
R.A; B; C; sB
iWk; sC
kC1Wj / D g.ŒsB
iWkI sC
kC1Wj I v.A/I v.B/�W /
(18.3)
(here, W 2 R2dC2dnt�d). Such approach is taken by Qian et al. [2015]. An alternative approach,
due to Socher et al. [2013a] is to untie the weights according to the non-terminals, using a dif-
ferent composition matrix for each B; C pair of symbols:³
R.A; B; C; sB
iWk; sC
kC1Wj / D g.ŒsB
iWkI sC
kC1Wj �W BC /:
(18.4)
is formulation is useful when the number of non-terminal symbols (or the number of possible
symbol combinations) is relatively small, as is usually the case with phrase-structure parse trees. A
similar model was also used by Hashimoto et al. [2013] to encode subtrees in semantic-relation
classiﬁcation task.
18.2
EXTENSIONS AND VARIATIONS
As all of the deﬁnitions of R above suﬀer from the vanishing gradients problem of the Simple
RNN, several authors sought to replace it with functions inspired by the LSTM gated archi-
tecture, resulting in Tree-shaped LSTMs [Tai et al., 2015, Zhu et al., 2015b]. e question of
optimal tree representation is still very much an open research question, and the vast space of pos-
sible combination functions R is yet to be explored. Other proposed variants on tree-structured
RNNs includes a recursive matrix-vector model [Socher et al., 2012] and recursive neural tensor
network [Socher et al., 2013b]. In the ﬁrst variant, each word is represented as a combination
of a vector and a matrix, where the vector deﬁnes the word’s static semantic content as before,
while the matrix acts as a learned “operator” for the word, allowing more subtle semantic com-
positions than the addition and weighted averaging implied by the concatenation followed by
linear transformation function. In the second variant, words are associated with vectors as usual,
but the composition function becomes more expressive by basing it on tensor instead of matrix
operations.
In our own work [Kiperwasser and Goldberg, 2016a], we propose a tree encoder that is
not restricted to binary trees but instead can work with arbitrary branching trees. e encoding
is based on RNNs (speciﬁcally LSTMs), where each subtree encoding is recursively deﬁned as
the merging of two RNN states, one running over the encodings of the left subtrees (from left to
³While not explored in the literature, a trivial extension would condition the transformation matrix also on A.

18.3. TRAINING RECURSIVE NEURAL NETWORKS
219
right) and ending in the root node, and the other running over the encodings of the right subtrees
(from right to left), and ending in the root node.
18.3
TRAINING RECURSIVE NEURAL NETWORKS
e training procedure for a recursive neural network follows the same recipe as training other
forms of networks: deﬁne a loss, spell out the computation graph, compute gradients using back-
propagation,⁴ and train the parameters using SGD.
With regard to the loss function, similar to the sequence RNN one can associate a loss
either with the root of the tree, with any given node, or with a set of nodes, in which case the
individual node’s losses are combined, usually by summation. e loss function is based on the
labeled training data which associates a label or other quantity with diﬀerent tree nodes.
Additionally, one can treat the RecNN as an Encoder, whereas the inside-vector associated
with a node is taken to be an encoding of the tree rooted at that node. e encoding can potentially
be sensitive to arbitrary properties of the structure. e vector is then passed as input to another
network.
For further discussion on recursive neural networks and their use in natural language tasks,
refer to the Ph.D. thesis of Socher [2014].
18.4
A SIMPLE ALTERNATIVE–LINEARIZED TREES
e RecNN abstraction provides a ﬂexible mechanism for encoding trees as vectors, using a re-
cursive, compositional approach. e RecNN encodes not only the given tree, but also all of its
subtrees. If this recursiveness of the encoding is not needed, and all we need is a vector represen-
tation of an entire tree, that is sensitive to the tree structure, simpler alternatives may work well.
In particular, linearizing trees into linear sequence that is then fed into a gated RNN acceptor (or
a biRNN encoder) has proven to be very eﬀective in several works [Choe and Charniak, 2016,
Luong et al., 2016, Vinyals et al., 2014]. Concretely, the tree for the sentence the boy saw her duck,
presented above, will be translated into the linear string:
(S (NP (Det the Det) (Noun boy Noun) NP) (VP (Verb saw Verb) (NP (Det her Det)
(Noun duck Noun) NP) VP) S)
which will then be fed into a gated RNN such as an LSTM. e ﬁnal state of the RNN can then
be used as the vector representation of the tree. Alternatively, the tree structure can be scored by
training an RNN language model over such linearized parse-trees, and taking the language-model
probability of the linearized parse tree to stand for its quality.
⁴Before the introduction of the computation graph abstraction, the speciﬁc backpropagation procedure for computing the
gradients in a RecNN as deﬁned above was referred to as the back-propagation through structure (BPTS) algorithm [Goller
and Küchler, 1996].

220
18. MODELING TREES WITH RECURSIVE NEURAL NETWORKS
18.5
OUTLOOK
e concept of recursive, tree-structured networks is powerful, intriguing, and seems very suited
for dealing with the recursive nature of language. However, as of the end of 2016, it is safe to
say that they don’t yet show any real and consistent beneﬁts over simpler architectures. Indeed,
in many cases sequence-level models such as RNNs capture the desired regularities just as well.
Either we have not yet found the killer-application for tree-structured networks, or we have not
yet found the correct architecture or training regimes. Some comparison and analysis of the use
of tree-structured vs. sequence-structured networks for language tasks can be found in the work
of Li et al. [2015]. As it stands, the use of tree-structured networks for processing language data
is still an open research area. Finding the killer-app for such networks, providing better training
regimes, or showing that tree-like architectures are not needed are all exciting research directions.

221
C H A P T E R
19
Structured Output Prediction
Many problems in NLP involve structured outputs: cases where the desired output is not a class
label or distribution over class labels, but a structured object such as a sequence, a tree, or a graph.
Canonical examples are sequence tagging (e.g., part-of-speech tagging) sequence segmentation
(chunking, NER), syntactic parsing, and machine translation. In this chapter, we discuss the
application of neural network models for structured tasks.
19.1
SEARCH-BASED STRUCTURED PREDICTION
e common approach to structured data prediction is search based. For in-depth discussion of
search-based structure prediction in pre-deep-learning NLP, see the book by Smith [2011]. e
techniques can easily be adapted to use a neural network. In the neural networks literature, such
models were discussed under the framework of energy-based learning [LeCun et al., 2006, Section
7]. ey are presented here using setup and terminology familiar to the NLP community.
Search-based structured prediction is formulated as a search problem over possible struc-
tures:
predict.x/ D argmax
y2Y.x/
scoreglobal.x; y/;
(19.1)
where x is an input structure, y is an output over x (in a typical example x is a sentence and y is
a tag-assignment or a parse-tree over the sentence), Y.x/ is the set of all valid structures over x,
and we are looking for an output y that will maximize the score of the x; y pair.
19.1.1 STRUCTURED PREDICTION WITH LINEAR MODELS
In the rich literature on structure prediction with linear and log-linear models, the scoring func-
tion is modeled as a linear function:
scoreglobal.x; y/ D w � ˆ.x; y/;
(19.2)
where ˆ is a feature extraction function and w is a weight vector.
In order to make the search for the optimal y tractable, the structure y is decomposed into
parts, and the feature function is deﬁned in terms of the parts, where �.p/ is a part-local feature
extraction function:
ˆ.x; y/ D
X
p2parts.x;y/
�.p/:
(19.3)

222
19. STRUCTURED OUTPUT PREDICTION
Each part is scored separately, and the structure score is the sum of the component parts
scores:
scoreglobal.x; y/ Dw � ˆ.x; y/ D w �
X
p2y
�.p/ D
X
p2y
w � �.p/ D
X
p2y
scorelocal.p/;
(19.4)
where p 2 y is a shorthand for p 2 parts.x; y/. e decomposition of y into parts is such that
there exists an inference algorithm that allows for eﬃcient search for the best scoring structure
given the scores of the individual parts.
19.1.2 NONLINEAR STRUCTURED PREDICTION
One can now trivially replace the linear scoring function over parts with a neural network:
scoreglobal.x; y/ D
X
p2y
scorelocal.p/ D
X
p2y
NN.�.p//;
(19.5)
where �.p/ maps the part p into a din dimensional vector.
In case of a one hidden-layer feed-forward network:
scoreglobal.x; y/ D
X
p2y
MLP1.�.p// D
X
p2y
.g.�.p/W 1 C b1//w
(19.6)
�.p/ 2 Rdin, W 1 2 Rdin�d1, b1 2 Rd1, w 2 Rd1. A common objective in structured prediction
is making the gold structure y score higher than any other structure y0, leading to the following
(generalized perceptron [Collins, 2002]) loss:
max
y0 scoreglobal.x; y0/ � scoreglobal.x; y/:
(19.7)
e maximization is performed using a dedicated search algorithm, which is often based
on dynamic programming or a similar search technique.
In terms of implementation, this means: create a computation graph CGp for each of the
possible parts, and calculate its score. en, run inference (i.e., search) to ﬁnd the best scoring
structure y0 according to the scores of its parts. Connect the output nodes of the computation
graphs corresponding to parts in the gold (predicted) structure y (y0) into a summing node CGy
(CG0
y). Connect CGy and CG0
y using a “minus” node, CGl, and compute the gradients.
As argued in LeCun et al. [2006, Section 5], the generalized perceptron loss may not be
a good loss function when training structured prediction neural networks as it does not have a
margin, and a margin-based hinge loss is preferred:
max.0; m C max
y0¤y scoreglobal.x; y0/ � scoreglobal.x; y//:
(19.8)

19.1. SEARCH-BASED STRUCTURED PREDICTION
223
It is trivial to modify the implementation above to work with the hinge loss.
Note that in both cases we lose the nice properties of the linear model. In particular, the
model is no longer convex. is is to be expected, as even the simplest nonlinear neural network is
already non-convex. Nonetheless, we could still use standard neural network optimization tech-
niques to train the structured model.
Training and inference is slower, as we have to evaluate the neural network (and take gra-
dients) once for each part, a total of jparts.x; y/j times.
Cost Augmented Training
Structured prediction is a vast ﬁeld, and this book does not attempt
to cover it fully. For the most part, the loss functions, regularizers and methods described in, e.g.,
Smith [2011], are easily transferable to the neural network framework, although losing convexity
and many of the associated theoretical guarantees. One technique that is worth mentioning specif-
ically is cost augmented training, also called loss augmented inference. While it brings modest gains
when used in linear structured prediction, my research group found it essential to successfully
training neural network-based structured-prediction models using the generalized perceptron or
the margin based losses, especially when using strong feature extractors such as RNNs.
e maximization term in Equations (19.7) and (19.8) is looking for a structure y0 that
receives a high score according to the current model, and is also wrong. en the loss reﬂects
the diﬀerence in scores between y0 and the gold structure y. Once the model is suﬃciently well
trained, the incorrect structure y0 and the correct one y are likely to be similar to each other
(because the model learned to assign high scores to structures that are reasonably good). Recall
that the global score function is in fact composed of a sum of local part scores. Parts that appear
in both scoring terms (of y0 and of y) will cancel each other out, and will result in gradients of 0
for the associated network parameters. If y and y0 are similar to each other, then most parts will
overlap and cancel out this way, leading to an overall very small update for the example.
e idea behind cost-augmented training is to change the maximization to ﬁnd structures
y0 that score well under the model and are also relatively wrong in the sense that they have many
incorrect parts. Formally, the hinge objective changes to:
max
�
0; m C max
y0¤y
�scoreglobal.x; y0/ C ��.y; y0/
�
� scoreglobal.x; y/
�
;
(19.9)
where � is a scalar hyperparameter indicating the relative importance of � vs. the model score,
and �.y; y0/ is a function counting the number of incorrect parts in y0 with respect to y:
�.y; y0/ D jfp W p 2 y0; p 62 ygj:
(19.10)
Practically, the new maximization can be implemented by increasing the local score of each in-
correct part by � before calling the maximization procedure.
e use of cost augmented inference surfaces highly incorrect examples and result in more
loss terms that do not cancel out, causing more eﬀective gradient updates.

224
19. STRUCTURED OUTPUT PREDICTION
19.1.3 PROBABILISTIC OBJECTIVE (CRF)
e error-based and margin-based losses above attempt to score the correct structure above in-
correct ones, but does not tell anything about the ordering of the structures below the highest
scoring one, or the score distances between them.
In contrast, a discriminative probabilistic loss attempts to assign a probability to each pos-
sible structure given the input, such that the probability of the correct structure is maximized. e
probabilistic losses are concerned with the scores of all possible structures, not just the highest
scoring one.
In a probabilistic framework (also known as conditional random ﬁelds, or CRF), each of
the parts scores is treated as a clique potential (see Laﬀerty et al. [2001], Smith [2011]) and the
score of each structure y is deﬁned to be:
score.x; y/ D P.yjx/ D
escoreglobal.x;y/
P
y02Y.x/ escoreglobal.x;y0/
D
exp.P
p2y scorelocal.p//
P
y02Y.x/ exp.P
p2y0 scorelocal.p//
D
exp.P
p2y NN.�.p///
P
y02Y.x/ exp.P
p2y0 NN.�.p///:
(19.11)
e scoring function deﬁnes a conditional distribution P.yjx/, and we wish to set the param-
eters of the network such that corpus conditional log likelihood P
.xi;yi/2training log P.yijxi/ is
maximized.
e loss for a given training example .x; y/ is then:
LCRF.y0; y/ D � log score.x; y/:
(19.12)
at is, the loss is related to the distance of the probability of the correct structure from 1. e
CRF loss can be seen as an extension of the hard-classiﬁcation cross-entropy loss to the structured
case.
Taking the gradient with respect to the loss in Equation (19.12) is as involved as building
the associated computation graph. e tricky part is the denominator (the partition function)
which requires summing over the potentially exponentially many structures in Y. However, for
some problems, a dynamic programming algorithm exists for eﬃciently solving the summation
in polynomial time (i.e., the forward-backward viterbi recurrences for sequences and the CKY
inside-outside recurrences for tree structures). When such an algorithm exists, it can be adapted
to also create a polynomial-size computation graph.
19.1.4 APPROXIMATE SEARCH
Sometimes, eﬃcient search algorithms for the prediction problem are not available. We may not
have an eﬃcient way of ﬁnding the best scoring structure (solving the maximization) in Equa-

19.1. SEARCH-BASED STRUCTURED PREDICTION
225
tions (19.7), (19.8), or (19.9), or not have an eﬃcient algorithm for computing the partition
function (denominator) in Equation (19.11). In such cases, one can resort to approximate infer-
ence algorithms, such as beam search. When using beam search, the maximization and summation
are with respect to the items in the beam. For example, one may use beam search for looking for
a structure with an approximately high score, and for the partition function sum over the struc-
tures remaining in the beam instead of over the exponentially large Y.x/. A related technique
when working with inexact search is early-update: instead of computing the loss over complete
structures, compute it over partial structures as soon as the gold items falls oﬀ the beam. For an
analysis of the early update techniques and alternative loss-computation and update strategies
when learning under approximate search, see Huang et al. [2012].
19.1.5 RERANKING
When searching over all possible structures is intractable, ineﬃcient, or hard to integrate into a
model, another alternative to beam search is the use of reranking. In the reranking framework
[Charniak and Johnson, 2005, Collins and Koo, 2005] a base model is used to produce a list of
the k-best scoring structures. A more complex model is then trained to score the candidates in the
k-best list such that the best structure with respect to the gold one is scored highest. As the search
is now performed over k items rather than over an exponential space, the complex model can con-
dition on (extract features from) arbitrary aspects of the scored structure. e base model that is
used for predicting the k-best structures can be based on a simpler model, with stronger inde-
pendence assumptions, which can produce reasonable, but not great, results. Reranking methods
are natural candidates for structured prediction using neural network models, as they allow the
modeler to focus on the feature extraction and network structure, while removing the need to in-
tegrate the neural network scoring into a decoder. Indeed, reranking methods are often used for
experimenting with neural models that are not straightforward to integrate into a decoder, such
as convolutional, recurrent, and recursive networks. Works using the reranking approach include
Auli et al. [2013], Le and Zuidema [2014], Schwenk et al. [2006], Socher et al. [2013a], Zhu
et al. [2015a], and Choe and Charniak [2016].
19.1.6 SEE ALSO
Beyond the examples in Section 19.4, sequence-level CRFs with neural network clique potentials
are discussed in Peng et al. [2009] and Do et al. [2010], where they are applied to sequence la-
beling of biological data, OCR data, and speech signals, and by Wang and Manning [2013] who
apply them on traditional natural language tagging tasks (chunking and NER). Similar sequence
tagging architecture is also described in Collobert and Weston [2008], Collobert et al. [2011]. A
hinge-based approach was used by Pei et al. [2015] for arc-factored dependency parsing with a
manually deﬁned feature extractor, and by Kiperwasser and Goldberg [2016b] using a biLSTM
feature extractor. e probabilistic approach was used by Durrett and Klein [2015] for a CRF
constituency parser. e approximate beam-based partition function (approximate CRF) was ef-

226
19. STRUCTURED OUTPUT PREDICTION
fectively used by Zhou et al. [2015] in a transition-based parser, and later by Andor et al. [2016]
for various tasks.
19.2
GREEDY STRUCTURED PREDICTION
In contrast to the search-based structured prediction approaches, there are greedy approaches
that decompose the structured problem into a sequence of local prediction problems and train-
ing a classiﬁer to perform each local decision well. At test time, the trained classiﬁer is used
in a greedy manner. Examples of this approach are left-to-right tagging models [Giménez and
Màrquez, 2004] and greedy transition-based parsing [Nivre, 2008].¹ Because they do not assume
search, greedy approaches are not restricted in the kind of features that are available to them, and
can use rich conditioning structures. is make greedy approaches quite competitive in terms of
prediction accuracies for many problems.
However, the greedy approaches are heuristic by deﬁnition, and have the potential of suf-
fering from error-propagation: prediction errors that are made early in the sequence cannot be
ﬁxed, and can propagate into larger errors later on. e problem is especially severe when using
a method with a limited horizon into the sentence, such as common with window-based feature
extractors. Such methods process the sentence tokens in a ﬁxed order, and only see a local window
around the prediction point. ey have no way of knowing what the future of the sequence hold,
and are likely to be misled by the local context into incorrect decisions.
Fortunately, the use of RNNs (and especially biRNNs) mitigate the eﬀect considerably. A
feature extractor which is based on a biRNN can essentially see through the end of the input,
and be trained to extract useful information from arbitrarily far sequence positions. is abil-
ity turn greedy local models that are trained with biRNN extractor into greedy global models:
each decision can condition on the entire sentence, making the process less susceptible to being
“surprised” later on by an unexpected output. As each prediction can become more accurate, the
overall accuracy grows considerably.
Indeed, works in syntactic parsing show that greedy prediction models that are trained with
global biRNN feature extractors rival the accuracy of search-based methods that combine global
search with local feature extractors [Cross and Huang, 2016a, Dyer et al., 2015, Kiperwasser and
Goldberg, 2016b, Lewis et al., 2016, Vaswani et al., 2016].
In addition to global feature extractors, the greedy methods can beneﬁt from training tech-
niques that aim to mitigate the error propagation problem by either attempting to take easier
predictions before harder ones (the easy-ﬁrst approach [Goldberg and Elhadad, 2010]) or mak-
ing training conditions more similar to testing conditions by exposing the training procedure to
inputs that result from likely mistakes [Hal Daumé III et al., 2009, Goldberg and Nivre, 2013].
ese are eﬀective also for training greedy neural network models, as demonstrated by Ma et al.
¹Transition-based parsers are beyond the scope of this book, but see Kübler et al. [2008], Nivre [2008], and Goldberg and
Nivre [2013] for an overview.

19.3. CONDITIONAL GENERATION AS STRUCTURED OUTPUT PREDICTION
227
[2014] (easy-ﬁrst tagger) and Ballesteros et al. [2016], Kiperwasser and Goldberg [2016b] (dy-
namic oracle training for greedy dependency parsing).
19.3
CONDITIONAL GENERATION AS STRUCTURED
OUTPUT PREDICTION
Finally, RNN generators, especially in the conditioned generator setup (Chapter 17), can also be
seen as an instance of structured-prediction. e series of predictions made by the generator pro-
duces a structured output Ot1Wn. Each individual prediction has an associated score (or probability)
score.Oti j Ot1Wi�1/ and we are interested in output sequence with maximal score (or maximal prob-
ability), i.e., such that Pn
iD1 score.OtijOt1Wi�1/ is maximized. Unfortunately, the non-markovian
nature of the RNN means that the scoring function cannot be decomposed into factors that allow
for exact search using standard dynamic programming techniques, and approximate search must
be used.
One popular approximate technique is using greedy prediction, taking the highest scoring
item at each stage. While this approach is often eﬀective, it is obviously non-optimal. Indeed,
using beam search as an approximate search often works far better than the greedy approach.
At this stage, it is important to consider how conditioned generators are trained. As de-
scribed in Section 17.1.1, generators are trained using a teacher-forcing technique: they are trained
using a probabilistic objective that attempts to assign high probability mass to gold observed se-
quences. Given a gold sequence t1Wn, at each stage i the model is trained to assign a high probability
mass to the gold event Oti D ti conditioned on the gold history t1Wi�1.
ere are two shortcomings with this approach: ﬁrst, it is based on the gold history t1Wi�1
while in practice the generator will be tasked with assigning scores based on its predicted history
Ot1Wi�1. Second, it is a locally normalized model: the model assigns a probability distribution over
each event, and thus susceptible to the label bias problem,² which can hurt the quality of solutions
returned by beam search. Both of these problems were tackled in the NLP and machine-learning
communities, but are not yet fully explored in the RNN generation setting.
e ﬁrst problem can be mitigated using training protocols such SEARN [Hal Daumé III
et al., 2009], DAGGER [Ross and Bagnell, 2010, Ross et al., 2011], and exploration-training
with dynamic oracles [Goldberg and Nivre, 2013]. Application of these techniques in the context
of RNN generators is proposed by Bengio et al. [2015] under the term scheduled sampling.
e second problem can be treated by discarding of the locally normalized objective and
moving to global, sequence-level objectives that are more suitable for beam decoding. Such ob-
jectives include the beam approximations of the structured hinge loss [Equation (19.8)] and the
CRF loss [Equation (19.11)] discussed in Section 19.1.4 above. Wiseman and Rush [2016] dis-
cuss global sequence-level scoring objectives for RNN generators.
²For a discussion of the label bias problem, see Section 3 of Andor et al. [2016] and the references therein.

228
19. STRUCTURED OUTPUT PREDICTION
19.4
EXAMPLES
19.4.1 SEARCH-BASED STRUCTURED PREDICTION: FIRST-ORDER
DEPENDENCY PARSING
Consider the dependency-parsing task, described in Section 7.7. e input is an n-words sen-
tence s D w1; : : : ; wn, and we are interested in ﬁnding a dependency parse tree y over the sentence
(Figure 7.1). A dependency parse tree is a rooted directed tree over the words in the sentence.
Every word in the tree is assigned a single parent (its head), that can be either another word in
the sentence or special ROOT element. e parent word is called a head and its daughter words
are called modiﬁers.
Dependency parsing ﬁts nicely in the search-based structured prediction framework de-
scribed in Section 19.1. Speciﬁcally, Equation (19.5) states that we should assign scores to trees
by decomposing them into parts and scoring each part individually. e parsing literature de-
scribes many possible factorizations [Koo and Collins, 2010, Zhang and McDonald, 2012], here
we focus on the simplest one, due to McDonald et al. [2005]: the arc-factored decomposition.
Each part will be an arc in the tree (i.e., pair of head word wh and modiﬁer word wm). Each arc
.wh; wm/ will be scored individually based on a local scoring function that will asses the quality
of the attachment. After assigning a score to each of the possible n2 arcs, we can run an inference
alogorithm such as the Eisner algorithm [Eisner and Satta, 1999, Kübler et al., 2008, McDonald
et al., 2005] to ﬁnd the valid projective tree³ whose sum of arc scores is maximal.
Equation (19.5) then becomes:
scoreglobal.x; y/ D
X
.wh;wm/2y
scorelocal.wh; wm/ D
X
.wh;wm/2y
NN.�.h; m; s//;
(19.13)
where �.h; m; s/ is a feature function translating the sentence indices h and m into real-valued
vectors. We discussed feature extractors for the parsing task in Sections 7.7 and 8.6 (using man-
ually designed features) and in Section 16.2.3 (using a biRNN feature extractor). Here, assume
the feature extractor is given and focus on the training procedure.
Once we decide on a particular form for the NN component (say an MLP, NN.x/ D
.tanh.xU C b// � v), we can easily compute the score aŒh;m� of each possible arch (assuming the
index of ROOT is 0):
aŒh;m� D .tanh.�.h; m; s//U C b/ � v
8h 2 0; : : : ; n
8m 2 1; : : : ; n:
(19.14)
³Parsing people talk about projective and non-projective trees. Projective trees pose additional constraints on the form of the
tree: that it can be drawn over a linearization of the words in the sentence in their original order, without crossing arcs. While
the distinction is an important one in the parsing world, it is beyond the scope of this book. For more details, see Kübler et al.
[2008] and Nivre [2008].

19.4. EXAMPLES
229
We then run the Eisner algorithm, resulting in a predicted tree y0 with maximal score:
y0 D max
y2Y
X
.h;m/2y
aŒh;m� D Eisner.n; a/:
If we were to use cost-augmented inference, we would have used instead the scores Na:
NaŒh;m� D aŒh;m� C
(
0
if .h; m/ 2 y
�
otherwise:
Once we have the predicted tree y0 and gold tree y, we can create a computation graph for the
structured hinge loss of the trees, according to:
max.0; 1 C
X
.h0;m0/2y0
tanh.�.h0; m0; s//U C b/ � v
„
ƒ‚
…
maxy0¤y scoreglobal.s;y0/
�
X
.h;m/2y
tanh.�.h; m; s//U C b/ � v
„
ƒ‚
…
scoreglobal.s;y/
/:
(19.15)
We then compute the gradients with respect to the loss using backprop, update the parameters
accordingly, and move to the next tree in the training set.
is parsing approach is described in Pei et al. [2015] (using the manually designed feature
function from Section 8.6) and Kiperwasser and Goldberg [2016b] (using the biRNN feature
extractor from Section 16.2.3).
19.4.2 NEURAL-CRF FOR NAMED ENTITY RECOGNITION
Independent Classiﬁcation
Consider the named entity recognition task described in Sec-
tion 7.5. It is a sequence segmentation task which is often modeled as sequence tagging: each word
in the sentence is assigned one of K BIO-tags described in Table 7.1, and the tagging decisions are
then deterministically translated into spans. In Section 7.5 we treated NER as a word-in-context
classiﬁcation problem, assuming each tagging decision for each word is performed independently
of the others.
Under the independent classiﬁcation framework, we are given a sentence s D w1; : : : ; wn,
and use a feature function �.i; s/ to create a feature vector representing the word wi in the context
of the sentence. en, a classiﬁer such as an MLP is used to predict a score (or a probability) to
each tag:
Oti D softmax.MLP.�.i; s///
8i 2 1; : : : ; nI
(19.16)
here, Oti is a vector of predicted tag scores, and Oti Œk� is the score of tagging word i with tag k.
e predicted tagging Oy1; : : : ; Oyn for the sentence is then obtained by independently choosing the
highest scoring tag for each sentence position:
Oyi D argmax
k
Oti Œk�
8i 2 1; : : : ; n;
(19.17)

230
19. STRUCTURED OUTPUT PREDICTION
and the score of the assignment Oy D Oy1; : : : ; Oyn is:
score.s; Oy/ D
n
X
iD1
ti Œ Oyi�:
(19.18)
Structured Tagging by Coupling Tag-Pair Decisions
e independent classiﬁcation approach
may work reasonably well in many cases, but is sub-optimal because neighboring decisions inﬂu-
ence each other. Consider a sequence such as Paris Hilton: the ﬁrst word can be either a location
or a person, and the second word can be either an organization or a person, but if we chose one
of them to be a person, the second one should be tagged person with certainty. We would like to
have the diﬀerent tagging decisions inﬂuence each other, and have this reﬂected in the score. A
common way to do this is by introducing tag-tag factors: compatibility scores for pairs of neigh-
boring tags. Intuitively, a pair such as B-PER I-PER should receive a high score, while a pair
B-PER I-ORG should receive a very low, or even negative score. For a tagset of K possible tags,
we introduce a scoring matrix A 2 RK�K in which AŒg;h� is the compatibility score of the tag
sequence g h.
e scoring function for a tagging assignment is updated to take the tagging factors into
account:
score.s; Oy/ D
n
X
iD1
ti Œ Oyi� C
nC1
X
iD1
AŒ Oyi�1; Oyi�;
(19.19)
where the tags at locations 0 and n C 1 are special *START* and *END* symbols. Given tagging
scores for individual words t1Wn and the values in A, one can ﬁnd the sequence Oy maximizing
Equation (19.19) using the Viterbi dynamic-programming algorithm.
As we do not need the tag scores in each position to be positive and sum to one, we remove
the softmax when computing the scores ti:
Oti D MLP.�.i; s//
8i 2 1; : : : ; n:
(19.20)
e tagging scores ti are determined by a neural network according to Equation (19.20),
and the matrix A can be considered as additional model parameters. We can now proceed to
train a structured model using the structured hinge-loss [Equation (19.8)] or the cost-augmented
structured hinge loss [Equation (19.9)].
Instead, we will follow Lample et al. [2016] and use the probabilistic CRF objective.
Structured CRF Training
Under the CRF objective, our goal is to assign a probability to each
possible tag sequence y D y1; : : : ; yn over a sentence s. is is modeled by taking a softmax over

19.4. EXAMPLES
231
all the possible taggings:
score.s; y/ D P.y j s/ D
escore.s;y/
P
y02Y.s/ escore.s;y0/
D
exp.Pn
iD1 ti Œyi� C Pn
iD1 AŒyi;yiC1�/
P
y02Y.s/ exp.Pn
iD1 ti Œy0
i� C Pn
iD1 AŒy0
i;y0
iC1�/:
(19.21)
e denominator is the same for all possible taggings y, so ﬁnding the best sequence (without its
probability) amounts to ﬁnding the sequence that maximizes score.s; y/, and can be done using
Viterbi as above.
e loss is then deﬁned as the negative log likelihood of the correct structure y:
� log P.yjs/ D �
 nC1
X
iD1
ti Œyi� C
nC1
X
iD1
AŒyi�1;yi�
!
C log
X
y02Y.s/
exp
 nC1
X
iD1
ti Œy0
i� C
nC1
X
iD1
AŒy0
i�1;y0
i�
!
D �
 nC1
X
iD1
ti Œyi� C
nC1
X
iD1
AŒyi�1;yi�
!
„
ƒ‚
…
score of gold
C
M
y02Y.s/
 nC1
X
iD1
ti Œy0
i� C
nC1
X
iD1
AŒy0
i�1;y0
i�
!
„
ƒ‚
…
using dynamic program
;
(19.22)
where L denotes addition in log-space (logadd) and L.a; b; c; d/ D log.ea C eb C ec C ed/.
e ﬁrst term can be easily constructed as a computation graph, but the second is a bit less trivial
to construct, as it requires summing over the nk diﬀerent sequences in Y.s/. Fortunately, it can
be solved using a variant of the Viterbi algorithm⁴ which we describe below.
..
Properties of Log-addition
e log-add operation performs addition in log-space. It has
the following properties that we use in constructing the dynamic program. ey are trivial
to prove with basic mathematic manipulation, and the reader is encouraged to do so.
M
.a; b/ D
M
.b; a/
Commutativity
(19.23)
M
.a;
M
.b; c// D
M
.a; b; c/
Associativity
(19.24)
M
.a C c; b C c/ D
M
.a C b/ C c
Distributivity
(19.25)
⁴is algorithm is known as the forward algorithm, which is diﬀerent than the algorithm for computing the forward pass in
the computation graph.

232
19. STRUCTURED OUTPUT PREDICTION
Denote by Y.s; r; k/ the set of sequences of length r that end with symbol k. e set of all
possible sequences over jsj is then Y.s/ D Y.s; n C 1; *END*/. Further denote by Y.s; r; `; k/
the sequences of length r where the last symbol is k and the second to last symbol is `. Let
�Œr; k� D L
y02Y.s;r;k/
Pr
iD1.ti Œy0
i� C AŒy0
i�1;y0
i�/. Our goal is computing �Œn C 1; *END*�. As
a shorthand, deﬁne f .i; y0
i�1; y0
i/ D ti Œy0
i� C AŒy0
i�1;y0
i�. We now get:
�Œr; k� D
M
y02Y.s;r;k/
r
X
iD1
f .i; y0
i�1; y0
i/
�Œr C 1; k� D
M
`
M
y02Y.s;rC1;`;k/
 rC1
X
iD1
f .i; y0
i�1; y0
i/
!
D
M
`
M
y02Y.s;rC1;`;k/
 r
X
iD1
�
f .i; y0
i�1; y0
i/
�
C f .r C 1; y0
r�1 D `; y0
r D k/
!
D
M
`
0
@
M
y02Y.s;rC1;`;k/
 r
X
iD1
f .i; y0
i�1; y0
i/
!
C f .r C 1; y0
r�1 D `; y0
r D k/
1
A
D
M
`
�
�Œr; `� C f .r C 1; y0
r�1 D `; y0
r D k/
�
D
M
`
�
�Œr; `� C trC1Œk� C AŒ`;k�
�
:
We obtained the recurrence:
�Œr C 1; k� D
M
`
�
�Œr; l� C trC1Œk� C AŒ`;k�
�
(19.26)
which we can use to construct the computation graph for computing the denominator, �Œn C
1; *END*�.⁵ After building the computation graph, we can compute the gradients using back-
propagation.
19.4.3 APPROXIMATE NER-CRF WITH BEAM-SEARCH
In the previous section, we transformed the NER prediction into a structured task by coupling the
output tags at positions i and i � 1 using a score matrix A assigning a score to each consecutive
tag pair. is is akin to using a ﬁrst-order markov assumption in which the tag in position i is
independent of the tags at positions < i � 1 given the tag at i � 1. is independence assumption
allowed us to decompose the sequence scoring and derive eﬃcient algorithms for ﬁnding the
highest scoring sequence as well as the sum over all possible tag sequences.
⁵Observe that this recursion is the same as the one for the best-path Viterbi algorithm, with L replaced with a max.

19.4. EXAMPLES
233
We may want to relax this markov independence assumption and instead condition the
tag yi at all previous tags y1Wi�1. is can be incorporated into the tagging model by adding an
additional RNN over the tag history. We now score a tag sequence y D y1; : : : ; yn as:
score.s; Oy/ D
nC1
X
iD1
f .Œ�.s; i/I RNN. Oy1Wi/�/;
(19.27)
where f is a parametric function such as a linear transformation or an MLP, and � is a feature
function mapping the word as position i in the sentence s to a vector.⁶ In words, we compute the
local score of tagging position i with tag k by considering features of sentence position i, as well
as an RNN encoding of the tag sequence y1; y2; yi�1; k. We then compute the global score as a
sum of local scores.
Unfortunately, the RNN component ties the diﬀerent local scores over all previous tagging
decisions, preventing us from using eﬃcient dynamic programming algorithms for ﬁnding the
exact best tagging sequence or the sum of all possible tag sequences under the model. Instead,
we must resort to approximation such as beam search. Using a beam of size r, we can develop r
diﬀerent tag sequences Oy1; : : : ; Oyr.⁷ e approximate best tag sequence is then the highest scoring
of the r beam sequences:
argmax
i21;:::;r
score.s; Oyi/:
For training, we can use the approximate CRF objective:
scoreAC.s; y/ D QP .yjs/ D
escore.s;y/
P
y02 QY.s;r/ escore.s;y0/
(19.28)
LCRF.y0; y/ D � log QP .yjs/
D � score.s; y/ C log
X
y02 QY.s;r/
escore.s;y0/
(19.29)
QY.s; r/ Dfy1; : : : ; yrg [ fyg:
Instead of normalizing by summing over the entire set of sequences Y.s/, we sum over QY.s; r/: the
union of the gold sequence and the r beam sequences. r is a small number, making the summation
is trivial. As r approaches nK we approach the true CRF objective.
⁶e feature function � can be based on a word window or a biLSTM, similar to the feature functions for POS-tagging in
Sections 8.5 and 16.2.1.
⁷e beam search algorithm works in stages. After obtaining r possible tag sequences of length i ( Oy1
1Wi ; : : : ; Oyr
1Wi ) corre-
sponding to the ﬁrst i words of the sentence, we extend each sequence with all possible tags, score each of the resulting r � K
sequences, and retain the top scoring r sequences. e process continues until we have r tag sequences over the entire sentence.

235
C H A P T E R
20
Cascaded, Multi-task and
Semi-supervised Learning
When processing natural language, it is often the case that we have several tasks that feed into
each other. For example, the syntactic parser we discussed in Sections 7.7, 16.2.3, and 19.4.1
takes as input parts of speech tags, that are in themselves automatically predicted by a statistical
model. Feeding the predictions of one model as the input of another, when the two models are
independent, is called a pipeline system. An alternative approach is model cascading. In model
cascading, rather than feeding the predictions of model A (the tagger) into model B (the parser),
we instead feed into the parser the intermediate representations that are informative for predicting
the tags. at is, rather than committing to a particular tagging decision, we pass on the tagging
uncertainty to the parser. Model cascading is very easy to implement in deep learning system, by
simply passing the vector before the argmax, or even one of the hidden vectors.
A related technique is multi-task learning [Caruana, 1997], in which we have several re-
lated predictions tasks (that may or may not feed into each other). We would like to leverage
the information in one of the tasks in order to improve the accuracy on the other tasks. In deep
learning, the idea is to have diﬀerent networks for the diﬀerent tasks, but let the networks share
some of their structure and parameters. is way, a common predictive core (the shared structure)
is inﬂuenced by all the tasks, and training data for one task may help improve the predictions of
the other ones.
A cascading approach lends itself naturally to the multi-task learning framework: instead of
just passing in the intermediate output of the tagger to the parser, we can instead plug in the sub-
graph of the computation graph that is responsible for the intermediate tagging representation as
input to the parser’s computation graph, and backpropagate the parser’s error all the way back to
the (now shared) base of the tagging component.
Another related and similar case is that of semi-supervised learning, in which we have su-
pervised training data for task A, and what to use annotated or unannotated data for other tasks
in order to improve the performance on task A.
is chapter deals with these three techniques.
20.1
MODEL CASCADING
In model-cascading, large networks are built by composing them out of smaller component net-
works. For example, in Section 16.2.1 we describe an RNN-based neural network for predicting

236
20. CASCADED, MULTI-TASK AND SEMI-SUPERVISED LEARNING
the part of speech of a word based on its sentential context and the characters that compose it. In
a pipeline approach, we would use this network for predicting parts of speech, and then feed the
predictions as input features to a neural network that does syntactic chunking or parsing.
Instead, we could think of the hidden layers of this network as an encoding that captures
the relevant information for predicting the part of speech. In a cascading approach, we take the
hidden layers of this network and connect them (and not the part of speech prediction themselves)
as the inputs for the syntactic network. We now have a larger network that takes as input sequences
of words and characters, and outputs a syntactic structure.
As a concrete example, consider the tagging and parsing networks described in Sec-
tions 16.2.1 and 16.2.3. e tagging network [Equation (16.4)], reproduced here, predicts the
tag of the ith word according to:
ti D argmax
j
softmax.MLP.biRNN.x1Wn; i///Œj�
xi D �.s; i/ D ŒE Œwi�I RNNf .c1W`/I RNNb.c`W1/�
(20.1)
while the parsing network [Equation (16.6)] assigns arc-scores according to:
AS.h; m; w1Wn; t1Wn/ D MLP.�.h; m; s// D MLP.ŒvhI vm�/
v1Wn D biRNN?.x1Wn/
xi D ŒwiI ti�:
(20.2)
e important thing to note here is that the parser takes as input words w1Wn and tags t1Wn, and
then converts the words and tags into embedding vectors, and concatenates them to form its
corresponding input representations x1Wn.
In the cascading approach, we’ll feed the tagger’s pre-prediction state directly into the
parser, in one joint network. Concretely, denote by zi the tagger’s pre-prediction for word i:
zi D MLP.biRNN.x1Wn; i//. We can now use zi as the input representation of the ith word in
the parser, resulting in:
AS.h; m; w1Wn/ D MLPparser.�.h; m; s// D MLPparser.ŒvhI vm�/
v1Wn D biRNN?
parser.z1Wn/
zi D MLPtagger.biRNNtagger.x1Wn; i//
xi D �tagger.s; i/ D
h
E Œwi�I RNNf
tagger.c1W`/I RNNb
tagger.c`W1/
i
:
(20.3)
e computation graph abstraction allows us to easily propagate the error gradients from the
syntactic task loss all the way back to the characters.¹
Figure 20.1 presents a sketch of the entire network.
¹Depending on the situation, we may or may not want to backpropagate the error all the way back.

20.1. MODEL CASCADING
237
BIparse
BIparse
BIparse
BIparse
BIparse
BIparse
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BIparse
BIparse
BIparse
BIparse
concat
ø(the)
ø(brown)
ø(fox)
ø(jumped)
ø(over)
E[brown]
…
…
…
…
Rf
c*S*
cb
Rf
cr
Rf
co
Rf
cw
Rf
cn
Rf
c*E*
c*E*
Rf
Rb
cn
Rb
cw
Rb
co
Rb
cr
Rb
cb
Rb
c*S*
Rb
concat
DET
ADJ
NN
VB
IN
pred
pred
pred
pred
pred
ø(jumped, brown, s)
Figure 20.1: Tagging-parsing cascade network [Equation (20.3)].

238
20. CASCADED, MULTI-TASK AND SEMI-SUPERVISED LEARNING
While the parser has access to the word identities, they may be diluted by the time they pass
through all the tagger RNN layers. To remedy this, we may use a skip-connection, and pass the
word embeddings E Œwi� directly to the parser, in addition to the tagger’s output:
AS.h; m; w1Wn/ D MLPparser.�.h; m; s// D MLPparser.ŒvhI vm�/
v1Wn D biRNN?
parser.z1Wn/
zi D ŒE Œwi�I z0
i�
z0
i D MLPtagger.biRNNtagger.x1Wn; i//
xi D �tagger.s; i/ D
h
E Œwi�I RNNf
tagger.c1W`/I RNNb
tagger.c`W1/
i
:
(20.4)
is architecture is depicted in Figure 20.2.
To combat the vanishing gradient problem of deep networks, as well as to make better use of
available training material, the individual component network’s parameters can be bootstrapped
by training them separately on a relevant task, before plugging them in to the larger network for
further tuning. For example, the part-of-speech predicting network can be trained to accurately
predict parts-of-speech on a relatively large annotated corpus, before plugging its hidden layer
into the syntactic parsing network for which less training data is available. In case the training
data provide direct supervision for both tasks, we can make use of it during training by creating a
network with two outputs, one for each task, computing a separate loss for each output, and then
summing the losses into a single node from which we backpropagate the error gradients.
Model cascading is very common when using convolutional, recursive, and recurrent neural
networks, where, for example, a recurrent network is used to encode a sentence into a ﬁxed sized
vector, which is then used as the input of another network. e supervision signal of the recurrent
network comes primarily from the upper network that consumes the recurrent network’s output
as it inputs.
In our example, both the tagger and the parser were based on a biRNN backbone. is is
not necessary—either or both of the networks could just as well be a feed-forward network that
gets a word-window as input, a convolutional network, or any other architecture that produces
vectors and that can pass gradients.
20.2
MULTI-TASK LEARNING
Multi-task learning (MTL) is a related technique, in which we have several related tasks that we
assume are correlated, in the sense that learning to solve one is likely to provide “intuitions” about
solving the other. For example, consider the syntactic chunking task (see Linguistic Annotation
frame in Section 6.2.2), in which we annotate a sentence with chunk boundaries, producing
output such as:
[NP the boy ] [PP with ] [NP the black shirt ] [VP opened ] [NP the door ] [PP with ] [NP a key ]

20.2. MULTI-TASK LEARNING
239
BIparse
BIparse
BIparse
BIparse
BIparse
BIparse
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BItag
BIparse
BIparse
BIparse
BIparse
concat
ø(the)
ø(brown)
ø(fox)
ø(jumped)
ø(over)
E[brown]
E[the]
E[brown]
E[fox]
E[jumped]
E[over]
…
…
…
…
Rf
c*S*
cb
Rf
cr
Rf
co
Rf
cw
Rf
cn
Rf
c*E*
c*E*
Rf
Rb
cn
Rb
cw
Rb
co
Rb
cr
Rb
cb
Rb
c*S*
Rb
concat
pred
concat
concat
concat
concat
concat
pred
pred
pred
pred
ø(jumped, brown, s)
DET
ADJ
NN
VB
IN
Figure 20.2: Tagging-parsing cascade with skip-connections for the word embeddings [Equa-
tion (20.4)].

240
20. CASCADED, MULTI-TASK AND SEMI-SUPERVISED LEARNING
Like named-entity recognition, chunking is a sequence-segmentation task, and can be reduced
to a tagging task using the BIO encoding scheme (see Section 7.5). A network for chunking then
may be modeled as a deep biRNN, followed by an MLP for individual tag predictions:
p.chunkTagi D j / D softmax.MLPchunk.biRNNchunk.x1Wn; i///Œj�
xi D �.s; i/ D E cnkŒwi�
(20.5)
(for brevity, we removed the character-level RNNs from the input, but they can be trivially added.)
Note that this is very similar to a POS-tagging network:
p.posTagi D j/ D softmax.MLPtag.biRNNtag.x1Wn; i///Œj�
xi D �.s; i/ D E tagŒwi�:
(20.6)
pred
pred
pred
pred
pred
DET
ADJ
NN
VB
IN
pred
pred
pred
pred
pred
B-NP
I-NP
I-NP
B-VP
B-PP
xthe
xbrown
xfox
xjumped
xover
xthe
xbrown
xfox
xjumped
xover
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
Figure 20.3: Left: POS tagging network. Right: Chunk tagging network.
Both networks are depicted in Figure 20.3. Diﬀerent colors indicate diﬀerent sets of parameters.
e syntactic chunking task is synergistic with part-of-speech tagging. Information for
predicting chunk boundaries, or the part-of-speech of a word, rely on some shared underlying
syntactic representation. Instead of training a separate network for each task, we can create a single
network with several outputs. e common approach would be to share the biRNN parameters,
but have a dedicated MLP predictor for each task (or have also a shared MLP, in which only
the ﬁnal matrix and bias terms are specialized for a task). is will result in the following, shared
network:
p.chunkTagi D j / D softmax.MLPchunk.biRNNshared.x1Wn; i///Œj�
p.posTagi D j / D softmax.MLPtag.biRNNshared.x1Wn; i///Œj �
xi D �.s; i/ D E sharedŒwi�:
(20.7)
e two networks use the same deep biRNN and embedding layers, but separate ﬁnal output
predictors. is is depicted in Figure 20.4.

20.2. MULTI-TASK LEARNING
241
pred
pred
pred
pred
pred
pred
pred
pred
pred
pred
DET
ADJ
NN
VB
IN
xthe
xbrown
xfox
xjumped
xover
B-NP
I-NP
I-NP
B-VP
B-PP
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
Figure 20.4: A joint POS-tagging and Chunking network. e biRNN parameters are shared, and
the biRNN component is specialized for both tasks. e ﬁnal predictors are separate.
Most of the parameters of the network are shared between the diﬀerent tasks. Useful in-
formation learned from one task can then help to disambiguate other tasks.
20.2.1 TRAINING IN A MULTI-TASK SETUP
e computation graph abstraction makes it very easy to construct such networks and compute
the gradients for them, by computing a separate loss for each available supervision signals, and
then summing the losses into a single loss that is used for computing the gradients. In case we
have several corpora, each with diﬀerent kind of supervision signal (e.g., we have one corpus for
POS and another for chunking), the preferred training protocol would be to choose a corpus
at random, pass the example through the relevant part of the computation graph, compute the
loss, backpropagate the error, and update the parameters. en, on the next step, again choose a
corpus at random and so on. In practice, this is often achieved by shuﬄing all the available training
examples and going through them in order. e important part is that we potentially compute
the gradients with respect to a diﬀerent loss (and using a diﬀerent sub-network) for each training
example.
In some cases, we may have several tasks, but care more about one of them. at is, we have
one or more main tasks, and a few other supporting task which we believe can help the main task,
but whose predictions we do not care about. In such cases, we may want to scale the loss of the
supporting task to be smaller than the loss of the main tasks. Another option is to ﬁrst pre-train

242
20. CASCADED, MULTI-TASK AND SEMI-SUPERVISED LEARNING
a network on the supporting tasks, and then take the shared components of this network and
continue training it only on the main task.
20.2.2 SELECTIVE SHARING
Going back to the POS-tagging and Chunking example, we could argue that while the tasks
share information, the POS-tagging task is in fact somewhat more low level than the chunking
task: the information needed for performing chunking is more reﬁned than that needed for POS-
tagging. In such cases, we may prefer to not share the entire deep biRNN between the two tasks,
but rather have the lower layer of the biRNN be shared, and the upper layers be dedicated to the
chunking task (Figure 20.5).
pred
pred
pred
pred
pred
pred
pred
pred
pred
pred
DET
ADJ
NN
VB
IN
xthe
xbrown
xfox
xjumped
xover
B-NP
I-NP
I-NP
B-VP
B-PP
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
BI
Figure 20.5: A selectively shared POS-tagging and Chunking network. e lower layer of the biRNN
is shared between the two tasks, but the upper layers are dedicated to chunking.
e lower layer in the biRNN is shared between the two tasks. It is primarily supervised by
the POS task, but also receives gradients from the chunking supervision. e upper layers of the
network are dedicated to the chunking task—but are trained to work well with the representation
of the lower layers.
is selective sharing suggestion follows the work of Søgaard and Goldberg [2016]. A
similar approach, using feed-forward rather than recurrent networks, is taken by Zhang and Weiss
[2016] under the name stack propagation.
e selectively shared MTL network in Figure 20.5 is very similar in spirit to the cascaded
setup in discussed in the previous section (Figure 20.1). Indeed, it is often hard to properly draw
the boundary between the two frameworks.

20.2. MULTI-TASK LEARNING
243
Input-output Inversion
Another view of multi-task and cascaded learning is one of input-
output inversion. Instead of thinking of some signal (say POS-tags) as inputs to a higher level
task (say parsing), we can think of them as outputs of intermediate layers in the network for the
higher level tasks. at is, instead of using the parts-of-speech tags as inputs, they are used instead
as a supervision signal to intermediate layers of the network.
20.2.3 WORD-EMBEDDINGS PRE-TRAINING AS MULTI-TASK LEARNING
e chunking and POS-tagging tasks (and indeed, many others) are also synergistic with the
language modeling task. Information for predicting the chunk boundary are the part-of-speech
tag of a word is intimately connected with the ability to predict the identity of the next word, or
the previous one: the tasks share a common syntactic-semantic backbone.
Viewed this way, the use of pre-trained word vectors for initializing the embedding layer of
a task-speciﬁc network is an instance of MTL, with language modeling as a supporting task. e
word embedding algorithms are trained with an distributional objective that is a generalization
of language modeling, and the word embedding layer of the embedding algorithms is then shared
with the other task.
e kind of supervision for the pre-training algorithm (i.e., the choice of contexts) should
be matched to the task the specialized network is trying to solve. Closer tasks results in larger
beneﬁts from MTL.
20.2.4 MULTI-TASK LEARNING IN CONDITIONED GENERATION
MTL can be seamlessly integrated into the conditioned generation framework discussed in Chap-
ter 17. is is done by having a shared encoder feeding into diﬀerent decoders, each attempting to
perform a diﬀerent task. is will force the encoder to encode information that is relevant to each
of the tasks. Not only can this information then be shared by the diﬀerent decoders, it also will
potentially allow for training diﬀerent decoders on diﬀerent training data, enlarging the overall
number of examples available for training. We discuss a concrete example in Section 20.4.4.
20.2.5 MULTI-TASK LEARNING AS REGULARIZATION
Another view of multi-task learning is one of a regularizer. e supervision from the supporting
tasks prevent the network from overﬁtting on the main task, by forcing the shared representa-
tion to be more general, and useful for prediction beyond the training instances of the main task.
Viewed this way, and when the supporting tasks are meant to be used as regularizers, one should
not perform the MTL in a sequence where the supporting tasks are tuned ﬁrst followed by adapt-
ing the representation to the main task (as suggested in Section 20.2.1). Rather, all tasks should
be learned in parallel.

244
20. CASCADED, MULTI-TASK AND SEMI-SUPERVISED LEARNING
20.2.6 CAVEATS
While the prospect of MTL is very appealing, some caveats are in order. MTL often does not
work well. For example, if the tasks are not closely related, you may not see gains from MTL, and
most tasks are indeed not related. Choosing the related tasks for performing MTL can be more
of an art than a science.
Even if the tasks are related, but the shared network doesn’t have the capacity to support
all the tasks, the performance of all of them can degrade. When taking the regularization view,
this means that the regularization is too strong, and prevents the model from ﬁtting the individ-
ual tasks. In such cases, it is better to increase the model capacity (i.e., increase the number of
dimensions in the shared components of the network). If an MTL network with k tasks needs a
k-fold increase in capacity (or close to it) in order to support all tasks, it means that there is likely
no sharing of predictive structure at all between the tasks, and one should forgo the MTL idea.
When the tasks are very closely related, such as the POS tagging and chunking tasks, the
beneﬁts from MTL could be very small. is is especially true when the networks are trained
on a single dataset in which each sentence is annotated for both POS-tag and Chunk label. e
chunking network can learn the representation it needs without the help of the intermediate
POS supervision. We do start to see the beneﬁts of MTL when the POS-training data and the
chunking data are disjoint (but share sizable portions of the vocabulary), or when the POS-tag
data is a superset of the Chunk data. In this situation, the MTL allows to eﬀectively enlarge
the amount of supervision for the chunking task by training on data with related labels for the
POS-tagging task. is lets the Chunk part of the network leverage on and inﬂuence the shared
representation that was learned based on the POS annotations on the additional data.
20.3
SEMI-SUPERVISED LEARNING
A related framework to both multi-task and cascaded learning is semi-supervised learning, in
which we have a small amount of training data for a task we care about, and additional training
data for other tasks. e other tasks can be either supervised, or unsupervised (i.e., where the
supervision can be generated from unannotated corpora, such as in language modeling, word
embeddings, or sentence encodings, as discussed in Section 9.6, Chapters 10 and 17.3).
We would like to use the supervision for the additional tasks (or to invent suitable addi-
tional tasks) in order to improve the prediction accuracy on the main task. is is a very common
scenario, which is an active and important research area: we never have enough supervision for
the tasks we care about.
For an overview of non-neural networks semi-supervised learning methods in NLP, see the
book of Søgaard [2013] in this series.
Within the deep-learning framework, semi-supervised learning can be performed, much
like MTL, by learning a representation based on the additional tasks, that can then be used as
supplement input or as initialization to the main task. Concretely, one can pre-train word em-

20.4. EXAMPLES
245
beddings or sentence representations on unannotated data, and use these to initialize or feed into
a POS-tagger, parser or a document summarization system.
In a sense, we have been doing semi-supervised learning ever since we introduced distri-
butional representations pre-trained word embeddings in Chapter 10. Sometimes, problems lend
themselves to more specialized solutions, as we explore in Section 20.4.3. e similarities and
connections to multi-task learning are also clear: we are using supervision data from one task to
improve performance on another. e main diﬀerence seem to be in how the diﬀerent tasks are
integrated into the ﬁnal model, and in the source of the annotated data for the diﬀerent tasks, but
the border between the approaches is rather blurry. In general, it is probably best not to debate
about the boundaries of cascaded learning, multi-task learning and semi-supervised learning, but
rather see them as a set of complimentary and overlapping techniques.
Other approaches to semi-supervised learning include various regimes in which one or
more models are trained on the small labeled data, and are then used to assign labels to large
amounts of unlabeled data. e automatically annotated data (possibly following some quality
ﬁltering stage based on agreement between the models are other conﬁdence measures) is then used
to train a new model, or provide additional features to an existing on. ese approaches can be
grouped under the collective term self-training. Other methods specify constraints on the solution,
that should help guide the model (i.e., specifying that some words can only be tagged with certain
tags, or that each sentence must contain at least one word tagged as X). Such methods are not
(yet) specialized for neural networks, and are beyond the scope of this book. For an overview, see
the book by Søgaard [2013].
20.4
EXAMPLES
We now describe a few examples in which we MTL was shown to be eﬀective.
20.4.1 GAZE-PREDICTION AND SENTENCE COMPRESSION
In the sentence compression by deletion task, we are given a sentence such as “Alan Turing, known
as the father of computer science, the codebreaker that helped win World War II, and the man tortured
by the state for being gay, is to receive a pardon nearly 60 years after his death” and are required to
produce a shorter (“compressed”) version containing the main information in the sentence by
deleting words from the original sentence. An example compression would be “Alan Turing is to
receive a pardon.” is can be modeled as a deep biRNN followed by an MLP in which the inputs
to the biRNN are the words of the sentence, and the outputs of the MLPs are K or D
decisions for each word.
In work with Klerke et al. [2016], we showed that the performance on the sentence deletion
by compression task can be improved by using two additional sequence prediction tasks: CCG
supertagging and Gaze prediction. e two tasks are added in a selective-sharing architecture, as
individual MLPs that feed from the lower layer of the biRNN.

246
20. CASCADED, MULTI-TASK AND SEMI-SUPERVISED LEARNING
e CCG supertagging task assigns each work with a CCG supertag, which is a complex
syntactic tag such as (S[dcl]nNP)/PP, indicating its syntactic role with respect to the rest of the
sentence.²
e Gaze prediction task is a cognitive task that relates to the way people read written
language. When reading, our eyes move across the page, ﬁxating on some words, skipping others,
and often jumping back to previous words. It is widely believed that eye movement when reading
reﬂects on the sentence processing mechanisms in the brain, which in turn reﬂects on the sentence
structure. Eye-trackers are machines that can accurately track eye-movement while reading, and
some eye-tracked corpora are available in which sentences are paired with exact eye-movement
measurements of several human subjects. In the gaze-prediction task, the network was trained
to predict aspects of the eye-movement behavior on the text (how long of a ﬁxation each word
would receive, or which words will trigger back movements). e intuition being that parts of the
sentence which are less important are more likely to be skipped or glossed over, and parts that are
more important are likely to be ﬁxated more upon when processing the sentence.
e compression data, the syntactic CCG tagging data, and the eye-movement data were
completely disjoint from each other, but we observed clear improvements to the compression
accuracy when including the additional tasks as supervision.
20.4.2 ARC LABELING AND SYNTACTIC PARSING
roughout the book, we described an architecture for arc-standard dependency parsing. In par-
ticular, in Section 16.2.3 we described biRNN based features, and in Section 19.4.1 a structured
prediction learning framework. e parser we described was an unlabeld parser—the model as-
signed a score to each possible head-modiﬁer pair, and the ﬁnal prediction by the parser was a
collection of arcs, representing the best tree over the sentence. However, the scoring function,
and the resulting arcs, only took into consideration which words are syntactically connected to
each other, and not the nature of the relation between the words.
Recall from Section 6.2.2 that a dependency parse-tree usually contains also the relation
information, in term of a dependency label on each arc, i.e., the det, prep, pobj, nsubj, etc. label
annotations in Figure 20.6.
Given an unlabeled parsing, the arc-labels can be assigned using an architecture in which a
biRNN is used to read the words of the sentence, and then, for arc .h; m/ in the tree, concatenate
the corresponding biRNN encodings and feed them into an MLP for predicting the arc’s label.
Rather than training a separate network for the label prediction, we can treat Unlabeled
Parsing and Arc-Labeling as related tasks in a multi-task setting. We then have a single biRNN
for the arc-labeler and the parser, and use the encoded biRNN states as inputs both to the arc-
scorer and to the arc-labeler. In training, the arc-labeler will only see gold arcs (because we do not
²CCG and CCG supertags are beyond the scope of this book. A good pointer to start learning about CCG in NLP is the Ph.D.
thesis of Julia Hockenmaier [Hockenmaier, 2003]. e concept of supertagging is introduced by Joshi and Srinivas [1994].

20.4. EXAMPLES
247
the   boy  with   the  black   shirt   opened  the  door   with   a  key
det
prep
prep
amod
det
pobj
nsubj
root
dobj
pobj
det
det
Figure 20.6: Labeled dependency tree.
have label information for the other, hypothetical arcs), while the arc-scorer will see all possible
arcs.
Indeed, in the work of Kiperwasser and Goldberg [2016b] we observe that the tasks are
indeed closely related. Training the joint network for performing both unlabeled arc-scoring and
arc-labeling, using the same shared biRNN encoder, not only results in accurate arc labeling, but
also substantially improves the accuracy of the unlabeled parse trees.
20.4.3 PREPOSITION SENSE DISAMBIGUATION AND PREPOSITION
TRANSLATION PREDICTION
Consider the preposition-sense disambiguation task discussed in Section 7.6. To recall, this is a
word-in-context problem, in which we need to assign each preposition with one of K possible
sense labels (M, P, L, D, etc.). Annotated corpora for the task
exist [Litkowski and Hargraves, 2007, Schneider et al., 2016], but are small.
In Section 7.6, we discussed a rich set of core features that can be used for training a
preposition-sense disambiguator. Let’s denote the feature extractor taking a preposition instance
and returning an encoding of these features as a vector as �sup.s; i/, where s is the input sentence
(including words, part-of-speech tags, lemmas, and syntactic parse-tree information), and i is the
index of the preposition within the sentence. e feature extractor �sup based on features similar to
those in features in Section 7.6, without the WordNet-based features but with pre-trained word-
embeddings, is a strong one. Feeding it into an MLP for prediction performs reasonably well
(albeit still disappointingly low, below 80% accuracy), and attempts to replace or to supplement
it with a biRNN-based feature extractor does not improve the accuracies.
Here, we show how the accuracy of the sense prediction can be improved further using a
semi-supervised approach, which is based on learning a useful representation from large amounts
of unannotated data, that we transform into related and useful prediction tasks.
Speciﬁcally, we will be using tasks derived from sentence-aligned multilingual data. ese
are pairs of English sentences and their translation into other languages.³ When translating from
³Such resources are readily available from, e.g., proceedings of the European Union (the Europarl corpus, [Koehn, 2005]), or
can be mined from the web [Uszkoreit et al., 2010]. ese are the resources that drive statistical machine translation.

248
20. CASCADED, MULTI-TASK AND SEMI-SUPERVISED LEARNING
English to a diﬀerent language, a preposition can be translated into one of several possible al-
ternatives. e choice of the foreign preposition will be based on the English preposition sense,
as reﬂected by the sentential context in which it appears. While prepositions are ambiguous in
all languages, the ambiguity patterns diﬀer across languages. us, predicting the foreign prepo-
sition into which a given English preposition will be translated based on the English sentential
context is a good auxiliary task for preposition sense disambiguation. is is the approach taken
by Gonen and Goldberg [2016]. We provide here a high-level overview. For details, refer to the
original paper.
Training data is created based on a multilingual sentence-aligned parallel corpus. e corpus
is word-aligned using a word-alignment algorithm [Dyer et al., 2013], and tuples of hsentence,
preposition-position, foreign-language, foreign-prepositionsi are extracted as training examples.
Given such a tuple hs D w1Wn; i; L; f i, the prediction task is to predict the translation of the
preposition wi in the context of the sentence s. e possible outputs are taken from a set of
language speciﬁc options pL, and the correct output is f .
e hope is that a representation of the context of wi that is good at predicting the foreign
preposition f will also be helpful for predicting the preposition sense. We model the task as an
encoder E.s; i/ that encodes the sentential context of wi into a vector, and a predictor, which
attempts to predict the right preposition. e encoder is very similar to a biRNN, but does not
include the preposition itself in order to force the network to pay more attention to the context,
while the predictor is a language speciﬁc MLP.
p.foreign D f js; i; L/ D softmax.MLPL
foreign.E.s; i///Œf �
E.s; i/ D ŒRNNf .w1Wi�1/I RNNb.wnWiC1/�:
(20.8)
e encoder is shared across the diﬀerent languages. After training the network on several million
hEnglish sentence, foreign-prepositioni pairs, we are left with a pre-trained context-encoder that
can then be used in the preposition-sense disambiguation network by concatenating it to the
supervised feature representation. Our semi-supervised disambiguator is then:
p.sense D j js; i/ D softmax.MLPsup.Œ�sup.s; i/I E.s; i/�/Œj �;
(20.9)
where E is the pre-trained encoder that is further trained by the sense prediction network, and
�sup is the supervised feature extractor. e approach substantially and consistently improve the
accuracy of the sense prediction by about 1–2 accuracy points, depending on details of the setup.⁴
⁴While an increase of 1–2 accuracy points may not seem very impressive, it is unfortunately in the upper range of what one
could realistically expect in semi-supervised scenarios in which the baseline supervised system is already relatively strong, using
current semi-supervision techniques. Improvements over weaker baselines are larger.

20.4. EXAMPLES
249
20.4.4 CONDITIONED GENERATION: MULTILINGUAL MACHINE
TRANSLATION, PARSING, AND IMAGE CAPTIONING
MTL can also be easily performed in an encoder-decoder framework. e work of Luong et al.
[2016] demonstrated this in the context of machine translation. eir translation system follows
the sequence-to-sequence architecture (Section 17.2.1), without attention. While better transla-
tion systems exist (notably systems that make use of attention), the focus of the work was to show
that improvements from the multi-task setup are possible.
Luong et al explore diﬀerent setups of multi-task learning under this system. In the
ﬁrst setup (one-to-many), the Encoder component (encoding English sentences into vectors)
is shared, and is used with two diﬀerent decoders: one decoder is generating German transla-
tions, and the other decoder is generating linearized parse-trees for the English sentences (i.e.,
the predicted sequence for the boy opened the door should be (S (NP DT NN ) (VP VBD (NP
DT NN ) ) ) ). e system is trained on a parallel corpus of hEnglish,Germani translation pairs,
and on gold parse trees from the Penn Treebank [Marcus et al., 1993]. e translation data and
the parsing data are disjoint. rough the multi-task setting, the shared encoder learns to produce
vectors that are informative for both tasks. e multi-task encoder-decoder network is eﬀective:
the network that is trained for both tasks (one encoder, two decoders) works better than the
individual networks consisting of a single encode-decoder pair. is setup likely works because
encoding basic elements of the syntactic structure of a sentence are informative for selecting the
word order and syntactic structures in the resulting translation, and vice versa. e translation
and parsing tasks are indeed synergistic.
In the second setup (many-to-one), there is a single decoder, but several diﬀerent encoders.
e tasks here are machine translation (German to English translation) and image captioning
(Image to English description). e decoder is tasked at producing English sentences. One en-
coder is encoding German sentences, while the other is encoding images. Like before, the datasets
for the translation and for the image captioning are disjoint. Again, with some tuning of param-
eters, training the joint system improves over the individual ones, though the gains are somewhat
smaller. Here, there is no real connection between the task of encoding German sentences (which
express elaborate predications and complicated syntactic structures) and encoding image contents
(which encodes the main components of simple scenes). e beneﬁt seem to be from the fact that
both tasks provide supervision for the language-modeling part of the decoder network, allowing
it to produce better sounding English sentences. Additionally, the improvement may stem from a
regularization eﬀect, in which one hencoder,decoderi pair prevents the other pair from overﬁtting
to its training data.
Despite the rather low baseline, the results of Luong et al. [2016] are encouraging, suggest-
ing there are gains to be had from multi-task learning in the conditional generation framework,
when suitable synergistic tasks are chosen.

250
20. CASCADED, MULTI-TASK AND SEMI-SUPERVISED LEARNING
20.5
OUTLOOK
Cascaded, multi-task, and semi-supervised learning are exciting techniques. e neural networks
framework, driven by gradients-based training over a computation graph, provide many seamless
opportunities for using these techniques. In many cases, such approaches bring real and consistent
gains in accuracy. Unfortunately, as of the time of this writing, the gains are often relatively mod-
est compared to the baseline performance, especially when the baselines are high. is should
not discourage you from using the techniques, as the gains often times are real. It should also
encourage you to actively work on improving and reﬁning the techniques, so that we see could
expect to see greater gains in the future.

251
C H A P T E R
21
Conclusion
21.1
WHAT HAVE WE SEEN?
e introduction of neural networks methods has been transformative for NLP. It prompted a
move from linear-models with heavy feature engineering (and in particular the engineering of
backoﬀ and combination features) to multi-layer perceptrons that learn the feature combinations
(as discussed in the ﬁrst part of the book); to architectures like convolutional neural networks
that can identify generalizable ngrams and gappy-ngrams (as discussed in Chapter 13); to archi-
tectures like RNNs and bidirectional RNNs (Chapters 14–16) that can identify subtle patterns
and regularities in sequences of arbitrary lengths; and to recursive neural networks (Chapter 18)
that can represent trees. ey also brought about methods for encoding words as vectors based
on distributional similarity, which can be eﬀective for semi-supervised learning (Chapters 10–
11); and methods for non-markovian language modeling, which in turn pave the way to ﬂexible
conditioned language generation models (Chapter 17), and revolutionized machine translation.
e neural methods also present many opportunities for multi-task learning (Chapter 20). More-
over, established pre-neural structured-prediction techniques can be readily adapted to incorpo-
rate neural network based feature extractors and predictors (Chapter 19).
21.2
THE CHALLENGES AHEAD
All in all, the ﬁeld is progressing very quickly, and it is hard to predict what the future will hold.
One thing is clear though, at least in my view—with all their impressive advantages, neural net-
works are not a silver bullet for natural-language understanding and generation. While they pro-
vide many improvements over the previous generation of statistical NLP techniques, the core
challenges remain: language is discrete and ambiguous, we do not have a good understanding of
how it works, and it is not likely that a neural network will learn all the subtleties on its own with-
out careful human guidance. e challenges mentioned in the introduction are ever-present also
with the neural techniques, and familiarity with the linguistic concepts and resources presented
in Chapter 6 is still as important as ever for designing good language processing systems. e
actual performance on many natural language tasks, even low-level and seemingly simple ones
such as pronominal coreference resolution [Clark and Manning, 2016, Wiseman et al., 2016] or
coordination boundary disambiguation [Ficler and Goldberg, 2016] is still very far from being
perfect. Designing learning systems to target such low-level language understanding tasks is as
important a research challenge as it was before the introduction of neural NLP methods.

252
21. CONCLUSION
Another important challenge is the opaqueness of the learned representations, and the
lack of rigorous theory behind the architectures and the learning algorithms. Research into the
interpretability of neural network representations, as well as into better understanding of the
learning capacity and training dynamics of various architectures, is crucially needed in order to
progress even further.
As of the time of this writing, neural networks are in essence still supervised learning meth-
ods, and require relatively large amounts of labeled training data. While the use of pre-trained
word-embeddings provides a convenient platform for semi-supervised learning, we are still in very
preliminary stages of eﬀectively utilizing unlabeled data and reducing the reliance on annotated
examples. Remember that humans can often generalize from a handful of examples, while neural
networks usually require at least hundreds of labeled examples in order to perform well, even in
the most simple language tasks. Finding eﬀective ways of leveraging small amounts of labeled
data together with large amounts of un-annotated data, as well as generalizing across domains,
will likely result in another transformation of the ﬁeld.
Finally, an aspect which was only very brieﬂy glossed over in this book is that language is
not an isolated phenomena. When people learn, perceive, and produce language, they do it with
a reference to the real world, and language utterances are more often than not grounded in real
world entities or experiences. Learning language in a grounded setting, either coupled with some
other modality such as images, videos, or robot movement control, or as part of an agent that
interacts with the world in order to achieve concrete goals, is another promising research frontier.

253
Bibliography
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,
Greg S. Corrado, Andy Davis, Jeﬀrey Dean, Matthieu Devin, et al. TensorFlow: Large-scale
machine learning on heterogeneous systems, 2015. http://tensorflow.org/
Heike Adel, Ngoc ang Vu, and Tanja Schultz. Combination of recurrent neural networks and
factored language models for code-switching language modeling. In Proc. of the 51st Annual
Meeting of the Association for Computational Linguistics—(Volume 2: Short Papers), pages 206–
211, Soﬁa, Bulgaria, August 2013.
Roee Aharoni, Yoav Goldberg, and Yonatan Belinkov. Proc. of the 14th SIGMORPHON Work-
shop on Computational Research in Phonetics, Phonology, and Morphology, chapter improving se-
quence to sequence learning for morphological inﬂection generation: e BIU-MIT systems
for the SIGMORPHON 2016 shared task for morphological reinﬂection, pages 41–48. Asso-
ciation for Computational Linguistics, 2016. http://aclweb.org/anthology/W16-2007 DOI:
10.18653/v1/W16-2007.
Roee Aharoni and Yoav Goldberg. Towards string-to-tree neural machine translation. Proc. of
ACL, 2017.
M. A. Aizerman, E. A. Braverman, and L. Rozonoer. eoretical foundations of the potential
function method in pattern recognition learning. In Automation and Remote Control, number 25
in Automation and Remote Control, pages 821–837, 1964.
Erin L. Allwein, Robert E. Schapire, and Yoram Singer. Reducing multiclass to binary: A uni-
fying approach for margin classiﬁers. Journal of Machine Learning Research, 1:113–141, 2000.
Rie Ando and Tong Zhang.
A high-performance semi-supervised learning method for text
chunking. In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics
(ACL’05), pages 1–9, Ann Arbor, Michigan, June 2005a. DOI: 10.3115/1219840.1219841.
Rie Kubota Ando and Tong Zhang. A framework for learning predictive structures from multiple
tasks and unlabeled data. e Journal of Machine Learning Research, 6:1817–1853, 2005b.
Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman
Ganchev, Slav Petrov, and Michael Collins. Globally normalized transition-based neural net-
works. In Proc. of the 54th Annual Meeting of the Association for Computational Linguistics—
(Volume 1: Long Papers), pages 2442–2452, 2016. http://aclweb.org/anthology/P16-1231
DOI: 10.18653/v1/P16-1231.

254
BIBLIOGRAPHY
Michael Auli and Jianfeng Gao. Decoder integration and expected BLEU training for recur-
rent neural network language models. In Proc. of the 52nd Annual Meeting of the Association
for Computational Linguistics—(Volume 2: Short Papers), pages 136–142, Baltimore, Maryland,
June 2014. DOI: 10.3115/v1/p14-2023.
Michael Auli, Michel Galley, Chris Quirk, and Geoﬀrey Zweig. Joint language and transla-
tion modeling with recurrent neural networks. In Proc. of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages 1044–1054, Seattle, Washington. Association
for Computational Linguistics, October 2013.
Oded Avraham and Yoav Goldberg. e interplay of semantics and morphology in word embed-
dings. EACL, 2017.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv:1409.0473 [cs, stat], September 2014.
Miguel Ballesteros, Chris Dyer, and Noah A. Smith.
Improved transition-based parsing by
modeling characters instead of words with LSTMs. In Proc. of the 2015 Conference on Em-
pirical Methods in Natural Language Processing, pages 349–359, Lisbon, Portugal. Association
for Computational Linguistics, September 2015. DOI: 10.18653/v1/d15-1041.
Miguel Ballesteros, Yoav Goldberg, Chris Dyer, and Noah A. Smith. Training with exploration
improves a greedy stack-LSTM parser, EMNLP 2016. arXiv:1603.03793 [cs], March 2016.
DOI: 10.18653/v1/d16-1211.
Mohit Bansal, Kevin Gimpel, and Karen Livescu. Tailoring continuous word representations for
dependency parsing. In Proc. of the 52nd Annual Meeting of the Association for Computational
Linguistics—(Volume 2: Short Papers), pages 809–815, Baltimore, Maryland, June 2014. DOI:
10.3115/v1/p14-2131.
Marco Baroni and Alessandro Lenci.
Distributional memory: A general framework
for corpus-based semantics.
Computational Linguistics, 36(4):673–721, 2010. DOI:
10.1162/coli_a_00016.
Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeﬀrey Mark
Siskind. Automatic diﬀerentiation in machine learning: A survey. arXiv:1502.05767 [cs],
February 2015.
Emily M. Bender. Linguistic Fundamentals for Natural Language Processing: 100 Essentials from
Morphology and Syntax. Synthesis Lectures on Human Language Technologies. Morgan &
Claypool Publishers, 2013.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for se-
quence prediction with recurrent neural networks.
CoRR, abs/1506.03099, 2015.
http:
//arxiv.org/abs/1506.03099

BIBLIOGRAPHY
255
Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures.
arXiv:1206.5533 [cs], June 2012. DOI: 10.1007/978-3-642-35289-8_26.
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic
language model. Journal of Machine Learning Research, 3:1137–1155, March 2003. ISSN
1532-4435. DOI: 10.1007/10985687_6.
Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning.
In Proc. of the 26th Annual International Conference on Machine Learning, pages 41–48. ACM,
2009. DOI: 10.1145/1553374.1553380.
Yoshua Bengio, Ian J. Goodfellow, and Aaron Courville. Deep Learning. MIT Press, 2016.
James Bergstra, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume
Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. eano: a CPU and GPU
math expression compiler. In Proc. of the Python for Scientiﬁc Computing Conference (SciPy), June
2010.
Jeﬀ A. Bilmes and Katrin Kirchhoﬀ.
Factored language models and generalized parallel
backoﬀ.
In Companion Volume of the Proc. of HLT-NAACL—Short Papers, 2003. DOI:
10.3115/1073483.1073485.
Zsolt Bitvai and Trevor Cohn. Non-linear text regression with a deep convolutional neural net-
work. In Proc. of the 53rd Annual Meeting of the Association for Computational Linguistics and
the 7th International Joint Conference on Natural Language Processing—(Volume 2: Short Papers),
pages 180–185, Beijing, China, July 2015. DOI: 10.3115/v1/p15-2030.
Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam Tauman Kalai.
Quantifying and reducing stereotypes in word embeddings. CoRR, abs/1606.06121, 2016.
http://arxiv.org/abs/1606.06121
Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N. Vapnik. A training algorithm for optimal
margin classiﬁers. In Proc. of the 5th Annual ACM Workshop on Computational Learning eory,
pages 144–152. ACM Press, 1992. DOI: 10.1145/130385.130401.
Jan A. Botha and Phil Blunsom. Compositional morphology for word representations and lan-
guage modelling. In Proc. of the 31st International Conference on Machine Learning (ICML),
Beijing, China, June 2014.
Léon Bottou.
Stochastic gradient descent tricks.
In Neural Networks: Tricks of the Trade,
pages 421–436. Springer, 2012. DOI: 10.1007/978-3-642-35289-8_25.
R. Samuel Bowman, Gabor Angeli, Christopher Potts, and D. Christopher Manning. A large
annotated corpus for learning natural language inference. In Proc. of the 2015 Conference on Em-
pirical Methods in Natural Language Processing, pages 632–642. Association for Computational
Linguistics, 2015. http://aclweb.org/anthology/D15-1075 DOI: 10.18653/v1/D15-1075.

256
BIBLIOGRAPHY
Peter Brown, Peter deSouza, Robert Mercer, T. Watson, Vincent Della Pietra, and Jenifer Lai.
Class-based n-gram models of natural language. Computational Linguistics, 18(4), December
1992. http://aclweb.org/anthology/J92-4003
John A. Bullinaria and Joseph P. Levy.
Extracting semantic representations from word co-
occurrence statistics: A computational study. Behavior Research Methods, 39(3):510–526, 2007.
DOI: 10.3758/bf03193020.
A. Caliskan-Islam, J. J. Bryson, and A. Narayanan. Semantics derived automatically from lan-
guage corpora necessarily contain human biases. CoRR, abs/1608.07187, 2016.
Rich Caruana. Multitask learning. Machine Learning, 28:41–75, 1997. DOI: 10.1007/978-1-
4615-5529-2_5.
Eugene Charniak and Mark Johnson.
Coarse-to-ﬁne n-best parsing and MaxEnt dis-
criminative reranking.
In Proc. of the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL’05), pages 173–180, Ann Arbor, Michigan, June 2005. DOI:
10.3115/1219840.1219862.
Danqi Chen and Christopher Manning. A fast and accurate dependency parser using neural
networks. In Proc. of the 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pages 740–750, Doha, Qatar. Association for Computational Linguistics, October
2014. DOI: 10.3115/v1/d14-1082.
Stanley F. Chen and Joshua Goodman. An empirical study of smoothing techniques for language
modeling. In 34th Annual Meeting of the Association for Computational Linguistics, 1996. http:
//aclweb.org/anthology/P96-1041 DOI: 10.1006/csla.1999.0128.
Stanley F. Chen and Joshua Goodman. An empirical study of smoothing techniques for lan-
guage modeling.
Computer Speech and Language, 13(4):359–394, 1999. DOI: 10.1006/c-
sla.1999.0128.
Wenlin Chen, David Grangier, and Michael Auli. Strategies for training large vocabulary neu-
ral language models. In Proc. of the 54th Annual Meeting of the Association for Computational
Linguistics—(Volume 1: Long Papers), pages 1975–1985, 2016. http://aclweb.org/anthology
/P16-1186 DOI: 10.18653/v1/P16-1186.
Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, and Jun Zhao. Event extraction via dynamic
multi-pooling convolutional neural networks. In Proc. of the 53rd Annual Meeting of the Asso-
ciation for Computational Linguistics and the 7th International Joint Conference on Natural Lan-
guage Processing—(Volume 1: Long Papers), pages 167–176, Beijing, China, July 2015. DOI:
10.3115/v1/p15-1017.

BIBLIOGRAPHY
257
Kyunghyun
Cho.
Natural
language
understanding
with
distributed
representation.
arXiv:1511.07916 [cs, stat], November 2015.
Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the proper-
ties of neural machine translation: Encoder-decoder approaches. In Proc. of SSST-8, 8th Work-
shop on Syntax, Semantics and Structure in Statistical Translation, pages 103–111, Doha, Qatar.
Association for Computational Linguistics, October 2014a. DOI: 10.3115/v1/w14-4012.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-
decoder for statistical machine translation. In Proc. of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 1724–1734, Doha, Qatar. Association for
Computational Linguistics, October 2014b. DOI: 10.3115/v1/d14-1179.
Do Kook Choe and Eugene Charniak. Parsing as language modeling. In Proc. of the Conference
on Empirical Methods in Natural Language Processing, pages 2331–2336, Austin, Texas. Associ-
ation for Computational Linguistics, November 2016. https://aclweb.org/anthology/D16-
1257 DOI: 10.18653/v1/d16-1257.
Grzegorz Chrupala. Normalizing tweets with edit scripts and recurrent neural embeddings. In
Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics—(Volume 2:
Short Papers), pages 680–686, Baltimore, Maryland, June 2014. DOI: 10.3115/v1/p14-2111.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation
of gated recurrent neural networks on sequence modeling. arXiv:1412.3555 [cs], December
2014.
Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio. A character-level decoder without ex-
plicit segmentation for neural machine translation. In Proc. of the 54th Annual Meeting of the
Association for Computational Linguistics—(Volume 1: Long Papers), pages 1693–1703, 2016.
http://aclweb.org/anthology/P16-1160 DOI: 10.18653/v1/P16-1160.
Kenneth Ward Church and Patrick Hanks. Word association norms, mutual information, and
lexicography. Computational Linguistics, 16(1):22–29, 1990. DOI: 10.3115/981623.981633.
Kevin Clark and Christopher D. Manning. Improving coreference resolution by learning entity-
level distributed representations.
In Association for Computational Linguistics (ACL), 2016.
/u/apache/htdocs/static/pubs/clark2016improving.pdf DOI: 10.18653/v1/p16-1061.
Michael Collins. Discriminative training methods for hidden Markov models: eory and exper-
iments with perceptron algorithms. In Proc. of the Conference on Empirical Methods in Natural
Language Processing, pages 1–8. Association for Computational Linguistics, July 2002. DOI:
10.3115/1118693.1118694.

258
BIBLIOGRAPHY
Michael Collins and Terry Koo.
Discriminative reranking for natural language pars-
ing.
Computational Linguistics, 31(1):25–70, March 2005.
ISSN 0891-2017. DOI:
10.1162/0891201053630273.
Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep
neural networks with multitask learning. In Proc. of the 25th International Conference on Machine
Learning, pages 160–167. ACM, 2008. DOI: 10.1145/1390156.1390177.
Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel
Kuksa. Natural language processing (almost) from scratch. e Journal of Machine Learning
Research, 12:2493–2537, 2011.
Alexis Conneau, Holger Schwenk, Loïc Barrault, and Yann LeCun. Very deep convolutional
networks for natural language processing. CoRR, abs/1606.01781, 2016. http://arxiv.org/
abs/1606.01781
Ryan Cotterell and Hinrich Schutze. Morphological word embeddings. NAACL, 2015.
Ryan Cotterell, Christo Kirov, John Sylak-Glassman, David Yarowsky, Jason Eisner, and Mans
Hulden. Proc. of the 14th SIGMORPHON Workshop on Computational Research in Phonetics,
Phonology, and Morphology, chapter e SIGMORPHON 2016 Shared Task—Morphological
Reinﬂection, pages 10–22. Association for Computational Linguistics, 2016. http://aclweb
.org/anthology/W16-2002 DOI: 10.18653/v1/W16-2002.
Koby Crammer and Yoram Singer. On the algorithmic implementation of multiclass kernel-
based vector machines. e Journal of Machine Learning Research, 2:265–292, 2002.
Mathias Creutz and Krista Lagus. Unsupervised models for morpheme segmentation and mor-
phology learning. ACM Transactions of Speech and Language Processing, 4(1):3:1–3:34, February
2007. ISSN 1550-4875. DOI: 10.1145/1187415.1187418.
James Cross and Liang Huang. Incremental parsing with minimal features using bi-directional
LSTM. In Proc. of the 54th Annual Meeting of the Association for Computational Linguistics—
(Volume 2: Short Papers), pages 32–37, 2016a. http://aclweb.org/anthology/P16-2006 DOI:
10.18653/v1/P16-2006.
James Cross and Liang Huang.
Span-based constituency parsing with a structure-label sys-
tem and dynamic oracles.
In Proc. of the 2016 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP). Association for Computational Linguistics, 2016b. DOI:
10.18653/v1/d16-1001.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Con-
trol, Signals and Systems, 2(4):303–314, December 1989. ISSN 0932-4194, 1435-568X. DOI:
10.1007/BF02551274.

BIBLIOGRAPHY
259
Ido Dagan and Oren Glickman. Probabilistic textual entailment: Generic applied modeling of
language variability. In PASCAL Workshop on Learning Methods for Text Understanding and
Mining, 2004.
Ido Dagan, Fernando Pereira, and Lillian Lee. Similarity-based estimation of word cooccurrence
probabilities. In ACL, 1994. DOI: 10.3115/981732.981770.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
e PASCAL recognising textual en-
tailment challenge. In Machine Learning Challenges, Evaluating Predictive Uncertainty, Vi-
sual Object Classiﬁcation and Recognizing Textual Entailment, First PASCAL Machine Learning
Challenges Workshop, MLCW, pages 177–190, Southampton, UK, April 11–13, 2005. (revised
selected papers). DOI: 10.1007/11736790_9.
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. Recognizing Textual
Entailment: Models and Applications. Synthesis Lectures on Human Language Technologies.
Morgan & Claypool Publishers, 2013. DOI: 10.2200/s00509ed1v01y201305hlt023.
G. E. Dahl, T. N. Sainath, and G. E. Hinton. Improving deep neural networks for LVCSR
using rectiﬁed linear units and dropout.
In 2013 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pages 8609–8613, May 2013. DOI: 10.1109/I-
CASSP.2013.6639346.
Hal Daumé III, John Langford, and Daniel Marcu. Search-based structured prediction. Machine
Learning Journal (MLJ), 2009. DOI: 10.1007/s10994-009-5106-x.
Hal Daumé III. A Course In Machine Learning. Self Published, 2015.
Yann N. Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and
Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-
convex optimization. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q.
Weinberger, Eds., Advances in Neural Information Processing Systems 27, pages 2933–2941.
Curran Associates, Inc., 2014.
Adrià de Gispert, Gonzalo Iglesias, and Bill Byrne. Fast and accurate preordering for SMT using
neural networks. In Proc. of the 2015 Conference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language Technologies, pages 1012–1017, Denver,
Colorado, 2015. DOI: 10.3115/v1/n15-1105.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, omas Lamar, Richard Schwartz, and John
Makhoul. Fast and robust neural network joint models for statistical machine translation.
In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics—(Volume 1:
Long Papers), pages 1370–1380, Baltimore, Maryland, June 2014. DOI: 10.3115/v1/p14-1129.

260
BIBLIOGRAPHY
Trinh Do, ierry Arti, and others. Neural conditional random ﬁelds. In International Conference
on Artiﬁcial Intelligence and Statistics, pages 177–184, 2010.
Pedro Domingos. e Master Algorithm. Basic Books, 2015.
Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. Adaptive recursive neural
network for target-dependent twitter sentiment classiﬁcation.
In Proc. of the 52nd Annual
Meeting of the Association for Computational Linguistics—(Volume 2: Short Papers), pages 49–54,
Baltimore, Maryland, June 2014. DOI: 10.3115/v1/p14-2009.
Li Dong, Furu Wei, Ming Zhou, and Ke Xu. Question answering over freebase with multi-
column convolutional neural networks.
In Proc. of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics and the 7th International Joint Conference on Natural Lan-
guage Processing—(Volume 1: Long Papers), pages 260–269, Beijing, China, July 2015. DOI:
10.3115/v1/p15-1026.
Cicero dos Santos and Maira Gatti. Deep convolutional neural networks for sentiment analysis
of short texts. In Proc. of COLING, the 25th International Conference on Computational Lin-
guistics: Technical Papers, pages 69–78, Dublin City University, Dublin, Ireland. Association
for Computational Linguistics, August 2014.
Cicero dos Santos and Bianca Zadrozny. Learning character-level representations for part-of-
speech tagging. In Proc. of the 31st International Conference on Machine Learning (ICML),
pages 1818–1826, 2014.
Cicero dos Santos, Bing Xiang, and Bowen Zhou. Classifying relations by ranking with con-
volutional neural networks. In Proc. of the 53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint Conference on Natural Language Processing—
(Volume 1: Long Papers), pages 626–634, Beijing, China, July 2015. DOI: 10.3115/v1/p15-
1061.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. e Journal of Machine Learning Research, 12:2121–2159, 2011.
Kevin Duh, Graham Neubig, Katsuhito Sudoh, and Hajime Tsukada. Adaptation data selection
using neural language models: experiments in machine translation. In Proc. of the 51st Annual
Meeting of the Association for Computational Linguistics—(Volume 2: Short Papers), pages 678–
683, Soﬁa, Bulgaria, August 2013.
Greg Durrett and Dan Klein. Neural CRF parsing. In Proc. of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing—(Volume 1: Long Papers), pages 302–312, Beijing, China, July 2015. DOI:
10.3115/v1/p15-1030.

BIBLIOGRAPHY
261
Chris Dyer, Victor Chahuneau, and A. Noah Smith. A simple, fast, and eﬀective reparameter-
ization of IBM model 2. In Proc. of the 2013 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, pages 644–648, 2013.
http://aclweb.org/anthology/N13-1073
Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. Transition-
based dependency parsing with stack long short-term memory. In Proc. of the 53rd Annual
Meeting of the Association for Computational Linguistics and the 7th International Joint Conference
on Natural Language Processing—(Volume 1: Long Papers), pages 334–343, Beijing, China, July
2015. DOI: 10.3115/v1/p15-1033.
C. Eckart and G. Young. e approximation of one matrix by another of lower rank. Psychome-
trika, 1:211–218, 1936. DOI: 10.1007/bf02288367.
Jason Eisner and Giorgio Satta. Eﬃcient parsing for bilexical context-free grammars and head au-
tomaton grammars. In Proc. of the 37th Annual Meeting of the Association for Computational Lin-
guistics, 1999. http://aclweb.org/anthology/P99-1059 DOI: 10.3115/1034678.1034748.
Jeﬀrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179–211, March 1990.
ISSN 1551-6709. DOI: 10.1207/s15516709cog1402_1.
Martin B. H. Everaert, Marinus A. C. Huybregts, Noam Chomsky, Robert C. Berwick, and
Johan J. Bolhuis. Structures, not strings: Linguistics as part of the cognitive sciences. Trends
in Cognitive Sciences, 19(12):729–743, 2015. DOI: 10.1016/j.tics.2015.09.008.
Manaal Faruqui and Chris Dyer. Improving vector space word representations using multilingual
correlation. In Proc. of the 14th Conference of the European Chapter of the Association for Compu-
tational Linguistics, pages 462–471, Gothenburg, Sweden, April 2014. DOI: 10.3115/v1/e14-
1049.
Manaal Faruqui, Jesse Dodge, Kumar Sujay Jauhar, Chris Dyer, Eduard Hovy, and A. Noah
Smith.
Retroﬁtting word vectors to semantic lexicons.
In Proc. of the 2015 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies, pages 1606–1615, 2015. http://aclweb.org/anthology/N15-1184 DOI:
10.3115/v1/N15-1184.
Manaal Faruqui, Yulia Tsvetkov, Graham Neubig, and Chris Dyer. Morphological inﬂection
generation using character sequence to sequence learning.
In Proc. of the 2016 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies, pages 634–643, 2016.
http://aclweb.org/anthology/N16-1077 DOI:
10.18653/v1/N16-1077.
Christiane Fellbaum. WordNet: An Electronic Lexical Database. Bradford Books, 1998.

262
BIBLIOGRAPHY
Jessica Ficler and Yoav Goldberg. A neural network for coordination boundary prediction. In
Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23–32,
Austin, Texas. Association for Computational Linguistics, November 2016. https://aclweb
.org/anthology/D16-1003 DOI: 10.18653/v1/d16-1003.
Katja Filippova and Yasemin Altun. Overcoming the lack of parallel data in sentence compres-
sion. In Proc. of the 2013 Conference on Empirical Methods in Natural Language Processing,
pages 1481–1491. Association for Computational Linguistics, 2013. http://aclweb.org/ant
hology/D13-1155
Katja Filippova, Enrique Alfonseca, Carlos A. Colmenares, Lukasz Kaiser, and Oriol Vinyals.
Sentence compression by deletion with LSTMs. In Proc. of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages 360–368, Lisbon, Portugal. Association for
Computational Linguistics, September 2015. DOI: 10.18653/v1/d15-1042.
Charles J. Fillmore, Josef Ruppenhofer, and Collin F. Baker. FrameNet and representing the
link between semantic and syntactic relations. Language and Linguistics Monographs Series B,
pages 19–62, Institute of Linguistics, Academia Sinica, Taipei, 2004.
John R. Firth. A synopsis of linguistic theory 1930–1955. In Studies in Linguistic Analysis, Special
volume of the Philological Society, pages 1–32. Firth, John Rupert, Haas William, Halliday,
Michael A. K., Oxford, Blackwell Ed., 1957.
John R. Firth. e technique of semantics. Transactions of the Philological Society, 34(1):36–73,
1935. ISSN 1467-968X. DOI: 10.1111/j.1467-968X.1935.tb01254.x.
Mikel L. Forcada and Ramón P. Ñeco. Recursive hetero-associative memories for translation. In
Biological and Artiﬁcial Computation: From Neuroscience to Technology, pages 453–462. Springer,
1997. DOI: 10.1007/bfb0032504.
Philip Gage. A new algorithm for data compression. C Users Journal, 12(2):23–38, February
1994. ISSN 0898-9788. http://dl.acm.org/citation.cfm?id=177910.177914
Yarin Gal. A theoretically grounded application of dropout in recurrent neural networks. CoRR,
abs/1512.05287, December 2015.
Kuzman Ganchev and Mark Dredze. Proc. of the ACL-08: HLT Workshop on Mobile Language
Processing, chapter Small Statistical Models by Random Feature Mixing, pages 19–20. Asso-
ciation for Computational Linguistics, 2008. http://aclweb.org/anthology/W08-0804
Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. PPDB: e paraphrase
database. In Proc. of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 758–764, 2013. http://aclw
eb.org/anthology/N13-1092

BIBLIOGRAPHY
263
Jianfeng Gao, Patrick Pantel, Michael Gamon, Xiaodong He, and Li Deng. Modeling interest-
ingness with deep neural networks. In Proc. of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 2–13, Doha, Qatar. Association for Computational Lin-
guistics, October 2014. DOI: 10.3115/v1/d14-1002.
Dan Gillick, Cliﬀ Brunk, Oriol Vinyals, and Amarnag Subramanya.
Multilingual language
processing from bytes. In Proc. of the Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Language Technologies, pages 1296–1306, 2016.
http://aclweb.org/anthology/N16-1155 DOI: 10.18653/v1/N16-1155.
Jesús Giménez and Lluis Màrquez. SVMTool: A general POS tagger generator based on support
vector machines. In Proc. of the 4th LREC, Lisbon, Portugal, 2004.
Xavier Glorot and Yoshua Bengio. Understanding the diﬃculty of training deep feedforward
neural networks. In International Conference on Artiﬁcial Intelligence and Statistics, pages 249–
256, 2010.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In
International Conference on Artiﬁcial Intelligence and Statistics, pages 315–323, 2011.
Yoav Goldberg. A primer on neural network models for natural language processing. Journal of
Artiﬁcial Intelligence Research, 57:345–420, 2016.
Yoav Goldberg and Michael Elhadad. An eﬃcient algorithm for easy-ﬁrst non-directional depen-
dency parsing. In Human Language Technologies: e Annual Conference of the North American
Chapter of the Association for Computational Linguistics, pages 742–750, Los Angeles, Califor-
nia, June 2010.
Yoav Goldberg and Joakim Nivre. Training deterministic parsers with non-deterministic oracles.
Transactions of the Association for Computational Linguistics, 1(0):403–414, October 2013. ISSN
2307-387X.
Yoav Goldberg, Kai Zhao, and Liang Huang.
Eﬃcient implementation of beam-search in-
cremental parsers.
In Proc. of the 51st Annual Meeting of the Association for Computational
Linguistics—(Volume 2: Short Papers), pages 628–633, Soﬁa, Bulgaria, August 2013.
Christoph Goller and Andreas Küchler. Learning task-dependent distributed representations by
backpropagation through structure. In In Proc. of the ICNN-96, pages 347–352. IEEE, 1996.
Hila Gonen and Yoav Goldberg. Semi supervised preposition-sense disambiguation using multi-
lingual data. In Proc. of COLING, the 26th International Conference on Computational Linguis-
tics: Technical Papers, pages 2718–2729, Osaka, Japan, December 2016. e COLING 2016
Organizing Committee. http://aclweb.org/anthology/C16-1256

264
BIBLIOGRAPHY
Joshua Goodman. A bit of progress in language modeling. CoRR, cs.CL/0108005, 2001. http:
//arxiv.org/abs/cs.CL/0108005 DOI: 10.1006/csla.2001.0174.
Stephan Gouws, Yoshua Bengio, and Greg Corrado. BilBOWA: Fast bilingual distributed rep-
resentations without word alignments. In Proc. of the 32nd International Conference on Machine
Learning, pages 748–756, 2015.
A. Graves. Supervised Sequence Labelling with Recurrent Neural Networks. Ph.D. thesis, Technis-
che Universität München, 2008. DOI: 10.1007/978-3-642-24797-2.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401,
2014. http://arxiv.org/abs/1410.5401
Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to
transduce with unbounded memory. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,
and R. Garnett, Eds., Advances in Neural Information Processing Systems 28, pages 1828–1836.
Curran Associates, Inc., 2015. http://papers.nips.cc/paper/5648-learning-to-transduce-
with-unbounded-memory.pdf
Klaus Greﬀ, Rupesh Kumar Srivastava, Jan Koutník, Bas R. Steunebrink, and Jürgen Schmid-
huber.
LSTM: A search space odyssey.
arXiv:1503.04069 [cs], March 2015. DOI:
10.1109/tnnls.2016.2582924.
Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation prin-
ciple for unnormalized statistical models. In International Conference on Artiﬁcial Intelligence
and Statistics, pages 297–304, 2010.
Zellig
Harris.
Distributional
structure.
Word,
10(23):146–162,
1954.
DOI:
10.1080/00437956.1954.11659520.
Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsuruoka, and Takashi Chikayama. Simple cus-
tomization of recursive neural networks for semantic relation classiﬁcation. In Proc. of the
Conference on Empirical Methods in Natural Language Processing, pages 1372–1376, Seattle,
Washington. Association for Computational Linguistics, October 2013.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Sur-
passing human-level performance on ImageNet classiﬁcation. arXiv:1502.01852 [cs], February
2015. DOI: 10.1109/iccv.2015.123.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In e IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016. DOI: 10.1109/cvpr.2016.90.

BIBLIOGRAPHY
265
Matthew Henderson, Blaise omson, and Steve Young. Deep neural network approach for the
dialog state tracking challenge. In Proc. of the SIGDIAL Conference, pages 467–471, Metz,
France. Association for Computational Linguistics, August 2013.
Karl Moritz Hermann and Phil Blunsom. e role of syntax in vector space models of com-
positional semantics. In Proc. of the 51st Annual Meeting of the Association for Computational
Linguistics—(Volume 1: Long Papers), pages 894–904, Soﬁa, Bulgaria, August 2013.
Karl Moritz Hermann and Phil Blunsom. Multilingual models for compositional distributed
semantics. In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics—
(Volume 1: Long Papers), pages 58–68, Baltimore, Maryland, June 2014. DOI: 10.3115/v1/p14-
1006.
Salah El Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term depen-
dencies. In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, Eds., Advances in Neural
Information Processing Systems 8, pages 493–499. MIT Press, 1996.
Felix Hill, Kyunghyun Cho, Sebastien Jean, Coline Devin, and Yoshua Bengio. Embedding word
similarity with neural machine translation. arXiv:1412.6448 [cs], December 2014.
Geoﬀrey E. Hinton, J. L. McClelland, and D. E. Rumelhart. Distributed representations. In
D. E. Rumelhart, J. L. McClelland, et al., Eds., Parallel Distributed Processing: Volume 1: Foun-
dations, pages 77–109. MIT Press, Cambridge, 1987.
Geoﬀrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R.
Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors.
arXiv:1207.0580 [cs], July 2012.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735–1780, 1997. DOI: 10.1162/neco.1997.9.8.1735.
Julia Hockenmaier. Data and Models for Statistical Parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh, 2003. DOI: 10.3115/1073083.1073139.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White.
Multilayer feedforward networks
are universal approximators. Neural Networks, 2(5):359–366, 1989. ISSN 0893-6080. DOI:
10.1016/0893-6080(89)90020-8.
Dirk Hovy, Stephen Tratz, and Eduard Hovy. What’s in a preposition? dimensions of sense
disambiguation for an interesting word class. In Coling Posters, pages 454–462, Beijing, China,
August 2010. Coling 2010 Organizing Committee. http://www.aclweb.org/anthology/C
10-2052

266
BIBLIOGRAPHY
(Kenneth) Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aish-
warya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra,
Lawrence C. Zitnick, Devi Parikh, Lucy Vanderwende, Michel Galley, and Margaret Mitchell.
Visual storytelling. In Proc. of the 2016 Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Language Technologies, pages 1233–1239, 2016.
http://aclweb.org/anthology/N16-1147 DOI: 10.18653/v1/N16-1147.
Liang Huang, Suphan Fayong, and Yang Guo. Structured perceptron with inexact search. In
Proc. of the Conference of the North American Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 142–151, 2012. http://aclweb.org/anthology/N12-
1015
Sergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv:1502.03167 [cs], February 2015.
Ozan Irsoy and Claire Cardie.
Opinion mining with deep recurrent neural networks.
In
Proc. of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),
pages 720–728, Doha, Qatar. Association for Computational Linguistics, October 2014. DOI:
10.3115/v1/d14-1080.
Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daumé III. A
neural network for factoid question answering over paragraphs. In Proc. of the Conference on
Empirical Methods in Natural Language Processing (EMNLP), pages 633–644, Doha, Qatar.
Association for Computational Linguistics, October 2014a. DOI: 10.3115/v1/d14-1070.
Mohit Iyyer, Peter Enns, Jordan Boyd-Graber, and Philip Resnik. Political ideology detection
using recursive neural networks. In Proc. of the 52nd Annual Meeting of the Association for Com-
putational Linguistics—(Volume 1: Long Papers), pages 1113–1122, Baltimore, Maryland, June
2014b. DOI: 10.3115/v1/p14-1105.
Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daumé III. Deep unordered
composition rivals syntactic methods for text classiﬁcation. In Proc. of the 53rd Annual Meet-
ing of the Association for Computational Linguistics and the 7th International Joint Conference on
Natural Language Processing—(Volume 1: Long Papers), pages 1681–1691, Beijing, China, July
2015. DOI: 10.3115/v1/p15-1162.
Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large
target vocabulary for neural machine translation. In Proc. of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing—(Volume 1: Long Papers), pages 1–10, 2015. http://aclweb.org/antho
logy/P15-1001 DOI: 10.3115/v1/P15-1001.
Frederick Jelinek and Robert Mercer. Interpolated estimation of Markov source parameters from
sparse data. In Workshop on Pattern Recognition in Practice, 1980.

BIBLIOGRAPHY
267
Rie Johnson and Tong Zhang. Eﬀective use of word order for text categorization with convo-
lutional neural networks. In Proc. of the 2015 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, pages 103–112, Den-
ver, Colorado, 2015. DOI: 10.3115/v1/n15-1011.
Aravind K. Joshi and Bangalore Srinivas. Disambiguation of super parts of speech (or supertags):
Allnost parsing. In COLING Volume 1: e 15th International Conference on Computational
Linguistics, 1994. http://aclweb.org/anthology/C94-1024 DOI: 10.3115/991886.991912.
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for eﬃcient
text classiﬁcation. CoRR, abs/1607.01759, 2016. http://arxiv.org/abs/1607.01759
Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recur-
rent network architectures. In Proc. of the 32nd International Conference on Machine Learning
(ICML-15), pages 2342–2350, 2015.
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the
limits of language modeling. arXiv:1602.02410 [cs], February 2016.
Daniel Jurafsky and James H. Martin. Speech and Language Processing, 2nd ed. Prentice Hall,
2008.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for
modelling sentences. In Proc. of the 52nd Annual Meeting of the Association for Computational
Linguistics—(Volume 1: Long Papers), pages 655–665, Baltimore, Maryland, June 2014. DOI:
10.3115/v1/p14-1062.
Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aäron van den Oord, Alex Graves, and
Koray Kavukcuoglu. Neural machine translation in linear time. CoRR, abs/1610.10099, 2016.
http://arxiv.org/abs/1610.10099
Katharina Kann and Hinrich Schütze. Proc. of the 14th SIGMORPHON Workshop on Computa-
tional Research in Phonetics, Phonology, and Morphology, chapter MED: e LMU System for
the SIGMORPHON 2016 Shared Task on Morphological Reinﬂection, pages 62–70. Associ-
ation for Computational Linguistics, 2016. http://aclweb.org/anthology/W16-2010 DOI:
10.18653/v1/W16-2010.
Anjuli Kannan, Karol Kurach, Sujith Ravi, Tobias Kaufmann, Andrew Tomkins, Balint Miklos,
Greg Corrado, Laszlo Lukacs, Marina Ganea, Peter Young, and Vivek Ramavajjala. Smart
reply: Automated response suggestion for email. In Proc. of the ACM SIGKDD Conference on
Knowledge Discovery and Data Mining (KDD), 2016. https://arxiv.org/pdf/1606.04870.pdf
DOI: 10.1145/2939672.2939801.

268
BIBLIOGRAPHY
Andrej Karpathy and Fei-Fei Li. Deep visual-semantic alignments for generating image descrip-
tions. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 3128–
3137, Boston, MA, June 7–12, 2015. DOI: 10.1109/cvpr.2015.7298932.
Andrej Karpathy, Justin Johnson, and Fei-Fei Li. Visualizing and understanding recurrent net-
works. arXiv:1506.02078 [cs], June 2015.
Douwe Kiela and Stephen Clark. A systematic study of semantic vector space model param-
eters. In Workshop on Continuous Vector Space Models and their Compositionality, 2014. DOI:
10.3115/v1/w14-1503.
Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proc. of the Conference on
Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar.
Association for Computational Linguistics, October 2014. DOI: 10.3115/v1/d14-1181.
Yoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush. Character-aware neural lan-
guage models. arXiv:1508.06615 [cs, stat], August 2015.
Diederik Kingma and Jimmy Ba.
ADAM: A method for stochastic optimization.
arXiv:1412.6980 [cs], December 2014.
Eliyahu Kiperwasser and Yoav Goldberg. Easy-ﬁrst dependency parsing with hierarchical tree
LSTMs.
Transactions of the Association of Computational Linguistics—(Volume 4, Issue 1),
pages 445–461, 2016a. http://aclweb.org/anthology/Q16-1032
Eliyahu Kiperwasser and Yoav Goldberg. Simple and accurate dependency parsing using bidi-
rectional LSTM feature representations.
Transactions of the Association of Computational
Linguistics—(Volume 4, Issue 1), pages 313–327, 2016b. http://aclweb.org/anthology/Q16-
1023
Karin Kipper, Hoa T. Dang, and Martha Palmer. Class-based construction of a verb lexicon. In
AAAI/IAAI, pages 691–696, 2000.
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio
Torralba, and Sanja Fidler. Skip-thought vectors. In C. Cortes, N. D. Lawrence, D. D. Lee,
M. Sugiyama, and R. Garnett, Eds., Advances in Neural Information Processing Systems 28,
pages 3294–3302. Curran Associates, Inc., 2015. http://papers.nips.cc/paper/5950-skip-
thought-vectors.pdf
Sigrid Klerke, Yoav Goldberg, and Anders Søgaard. Improving sentence compression by learning
to predict gaze. In Proc. of the Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 1528–1533, 2016. http://ac
lweb.org/anthology/N16-1179 DOI: 10.18653/v1/N16-1179.

BIBLIOGRAPHY
269
Reinhard Kneser and Hermann Ney. Improved backing-oﬀ for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, ICASSP-95, International Conference on, volume 1,
pages 181–184, May 1995. DOI: 10.1109/ICASSP.1995.479394.
Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proc. of MT
Summit, volume 5, pages 79–86, 2005.
Philipp Koehn.
Statistical Machine Translation.
Cambridge University Press, 2010. DOI:
10.1017/cbo9780511815829.
Terry Koo and Michael Collins. Eﬃcient third-order dependency parsers. In Proc. of the 48th
Annual Meeting of the Association for Computational Linguistics, pages 1–11, 2010. http://ac
lweb.org/anthology/P10-1001
Moshe Koppel, Jonathan Schler, and Shlomo Argamon. Computational methods in authorship
attribution. Journal of the American Society for information Science and Technology, 60(1):9–26,
2009. DOI: 10.1002/asi.20961.
Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E. Hinton. ImageNet classiﬁcation with deep
convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Wein-
berger, Eds., Advances in Neural Information Processing Systems 25, pages 1097–1105. Curran
Associates, Inc., 2012. DOI: 10.1007/978-3-319-46654-5_20.
R. A. Kronmal and A. V. Peterson, Jr. On the alias method for generating random variables from
a discrete distribution. e American Statistician, 33:214–218, 1979. DOI: 10.2307/2683739.
Sandra Kübler, Ryan McDonald, and Joakim Nivre.
Dependency Parsing.
Synthesis Lec-
tures on Human Language Technologies. Morgan & Claypool Publishers, 2008. DOI:
10.2200/s00169ed1v01y200901hlt002.
Taku Kudo and Yuji Matsumoto. Fast methods for Kernel-based text analysis. In Proc. of the
41st Annual Meeting on Association for Computational Linguistics—(Volume 1), pages 24–31,
Stroudsburg, PA, 2003. DOI: 10.3115/1075096.1075100.
John Laﬀerty, Andrew McCallum, and Fernando CN Pereira. Conditional random ﬁelds: Prob-
abilistic models for segmenting and labeling sequence data. In Proc. of ICML, 2001.
Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris
Dyer. Neural architectures for named entity recognition. In Proc. of the Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies,
pages 260–270, 2016.
http://aclweb.org/anthology/N16-1030 DOI: 10.18653/v1/N16-
1030.

270
BIBLIOGRAPHY
Phong Le and Willem Zuidema. e inside-outside recursive neural network model for depen-
dency parsing. In Proc. of the Conference on Empirical Methods in Natural Language Processing
(EMNLP), pages 729–739, Doha, Qatar. Association for Computational Linguistics, October
2014. DOI: 10.3115/v1/d14-1081.
Phong Le and Willem Zuidema. e forest convolutional network: Compositional distributional
semantics with a neural chart and without binarization. In Proc. of the Conference on Empirical
Methods in Natural Language Processing, pages 1155–1164, Lisbon, Portugal. Association for
Computational Linguistics, September 2015. DOI: 10.18653/v1/d15-1137.
Quoc V. Le, Navdeep Jaitly, and Geoﬀrey E. Hinton. A simple way to initialize recurrent net-
works of rectiﬁed linear units. arXiv:1504.00941 [cs], April 2015.
Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time-series.
In M. A. Arbib, Ed., e Handbook of Brain eory and Neural Networks. MIT Press, 1995.
Yann LeCun, Leon Bottou, G. Orr, and K. Muller. Eﬃcient BackProp. In G. Orr and Muller K,
Eds., Neural Networks: Tricks of the Trade. Springer, 1998a. DOI: 10.1007/3-540-49430-8_2.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haﬀner. Gradient based learning applied
to pattern recognition. Proc. of the IEEE, 86(11):2278–2324, November 1998b.
Yann LeCun and F. Huang. Loss functions for discriminative training of energy-based models.
In Proc. of AISTATS, 2005.
Yann LeCun, Sumit Chopra, Raia Hadsell, M. Ranzato, and F. Huang. A tutorial on energy-
based learning. Predicting Structured Data, 1:0, 2006.
Geunbae Lee, Margot Flowers, and Michael G. Dyer. Learning distributed representations of
conceptual knowledge and their application to script-based story processing. In Connection-
ist Natural Language Processing, pages 215–247. Springer, 1992. DOI: 10.1007/978-94-011-
2624-3_11.
Moshe Leshno, Vladimir Ya. Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward
networks with a nonpolynomial activation function can approximate any function.
Neural
Networks, 6(6):861–867, 1993. ISSN 0893-6080. http://www.sciencedirect.com/science/
article/pii/S0893608005801315 DOI: 10.1016/S0893-6080(05)80131-5.
Omer Levy and Yoav Goldberg. Dependency-based word embeddings. In Proc. of the 52nd
Annual Meeting of the Association for Computational Linguistics—(Volume 2: Short Papers),
pages 302–308, Baltimore, Maryland, June 2014. DOI: 10.3115/v1/p14-2050.
Omer Levy and Yoav Goldberg. Linguistic regularities in sparse and explicit word representations.
In Proc. of the 18th Conference on Computational Natural Language Learning, pages 171–180.

BIBLIOGRAPHY
271
Association for Computational Linguistics, 2014. http://aclweb.org/anthology/W14-1618
DOI: 10.3115/v1/W14-1618.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In
Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, Eds., Ad-
vances in Neural Information Processing Systems 27, pages 2177–2185. Curran Associates, Inc.,
2014.
Omer Levy, Yoav Goldberg, and Ido Dagan. Improving distributional similarity with lessons
learned from word embeddings. Transactions of the Association for Computational Linguistics, 3
(0):211–225, May 2015. ISSN 2307-387X.
Omer Levy, Anders Søgaard, and Yoav Goldberg. A strong baseline for learning cross-lingual
word embeddings from sentence alignments. In Proc. of the 15th Conference of the European
Chapter of the Association for Computational Linguistics, 2017.
Mike Lewis and Mark Steedman. Improved CCG parsing with semi-supervised supertagging.
Transactions of the Association for Computational Linguistics, 2(0):327–338, October 2014. ISSN
2307-387X.
Mike Lewis, Kenton Lee, and Luke Zettlemoyer. LSTM CCG parsing. In Proc. of the Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 221–231, 2016. http://aclweb.org/anthology/N16-1026 DOI:
10.18653/v1/N16-1026.
Jiwei Li, Rumeng Li, and Eduard Hovy.
Recursive deep models for discourse parsing.
In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP),
pages 2061–2069, Doha, Qatar. Association for Computational Linguistics, October 2014.
DOI: 10.3115/v1/d14-1220.
Jiwei Li, ang Luong, Dan Jurafsky, and Eduard Hovy. When are tree structures necessary
for deep learning of representations? In Proc. of the Conference on Empirical Methods in Natural
Language Processing, pages 2304–2314. Association for Computational Linguistics, 2015. http:
//aclweb.org/anthology/D15-1278 DOI: 10.18653/v1/D15-1278.
Jiwei Li, Michel Galley, Chris Brockett, Georgios Spithourakis, Jianfeng Gao, and Bill Dolan. A
persona-based neural conversation model. In Proc. of the 54th Annual Meeting of the Association
for Computational Linguistics—(Volume 1: Long Papers), pages 994–1003, 2016. http://aclw
eb.org/anthology/P16-1094 DOI: 10.18653/v1/P16-1094.
G. J. Lidstone. Note on the general case of the Bayes-Laplace formula for inductive or a posteriori
probabilities. Transactions of the Faculty of Actuaries, 8:182–192, 1920.

272
BIBLIOGRAPHY
Wang Ling, Chris Dyer, Alan W. Black, and Isabel Trancoso. Two/too simple adaptations of
Word2Vec for syntax problems. In Proc. of the Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies, pages 1299–1304,
Denver, Colorado, 2015a. DOI: 10.3115/v1/n15-1142.
Wang Ling, Chris Dyer, Alan W. Black, Isabel Trancoso, Ramon Fermandez, Silvio Amir, Luis
Marujo, and Tiago Luis. Finding function in form: Compositional character models for open
vocabulary word representation. In Proc. of the Conference on Empirical Methods in Natural
Language Processing, pages 1520–1530, Lisbon, Portugal. Association for Computational Lin-
guistics, September 2015b. DOI: 10.18653/v1/d15-1176.
Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of LSTMs to learn
syntax-sensitive dependencies. Transactions of the Association for Computational Linguistics, 4:
521–535, 2016. ISSN 2307-387X. https://www.transacl.org/ojs/index.php/tacl/article/
view/972
Ken Litkowski and Orin Hargraves. e preposition project. In Proc. of the 2nd ACL-SIGSEM
Workshop on the Linguistic Dimensions of Prepositions and eir Use in Computational Linguistics
Formalisms and Applications, pages 171–179, 2005.
Ken Litkowski and Orin Hargraves.
SemEval-2007 task 06: Word-sense disambiguation of
prepositions. In Proc. of the 4th International Workshop on Semantic Evaluations, pages 24–29,
2007. DOI: 10.3115/1621474.1621479.
Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou, and Houfeng Wang. A dependency-
based neural network for relation classiﬁcation. In Proc. of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing—(Volume 2: Short Papers), pages 285–290, Beijing, China, July 2015. DOI:
10.3115/v1/p15-2047.
Minh-ang Luong, Hieu Pham, and Christopher D. Manning.
Eﬀective approaches to
attention-based neural machine translation. arXiv:1508.04025 [cs], August 2015.
Minh-ang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task
sequence to sequence learning. In Proc. of ICLR, 2016.
Ji Ma, Yue Zhang, and Jingbo Zhu.
Tagging the web: Building a robust web tagger with
neural network.
In Proc. of the 52nd Annual Meeting of the Association for Computational
Linguistics—(Volume 1: Long Papers), pages 144–154, Baltimore, Maryland, June 2014. DOI:
10.3115/v1/p14-1014.
Mingbo Ma, Liang Huang, Bowen Zhou, and Bing Xiang. Dependency-based convolutional
neural networks for sentence embedding. In Proc. of the 53rd Annual Meeting of the Associ-

BIBLIOGRAPHY
273
ation for Computational Linguistics and the 7th International Joint Conference on Natural Lan-
guage Processing—(Volume 2: Short Papers), pages 174–179, Beijing, China, July 2015. DOI:
10.3115/v1/p15-2029.
Xuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional LSTM-CNNs-
CRF. In Proc. of the 54th Annual Meeting of the Association for Computational Linguistics—
(Volume 1: Long Papers), pages 1064–1074, Berlin, Germany, August 2016. http://www.aclw
eb.org/anthology/P16-1101 DOI: 10.18653/v1/p16-1101.
Christopher Manning and Hinrich Schütze. Foundations of Statistical Natural Language Process-
ing. MIT Press, 1999.
Christopher Manning, Prabhakar Raghavan, and Hinrich Schütze. Introduction to Information
Retrieval. Cambridge University Press, 2008. DOI: 10.1017/cbo9780511809071.
Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan L. Yuille. Explain images with multimodal
recurrent neural networks. CoRR, abs/1410.1090, 2014. http://arxiv.org/abs/1410.1090
Ryan McDonald, Koby Crammer, and Fernando Pereira. Online large-margin training of de-
pendency parsers.
In Proc. of the 43rd Annual Meeting of the Association for Computational
Linguistics (ACL’05), pages 91–98, 2005.
http://aclweb.org/anthology/P05-1012 DOI:
10.3115/1219840.1219852.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das,
Kuzman Ganchev, Keith B. Hall, Slav Petrov, Hao Zhang, Oscar Täckström, Claudia Bedini,
Núria Bertomeu Castelló, and Jungmee Lee. Universal dependency annotation for multilingual
parsing. In ACL (2), pages 92–97, 2013.
Tomáš Mikolov. Statistical language models based on neural networks. Ph.D. thesis, Brno University
of Technology, 2012.
Tomáš Mikolov. Martin Karaﬁát, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Re-
current neural network based language model. In INTERSPEECH, 11th Annual Conference of
the International Speech Communication Association, pages 1045–1048, Makuhari, Chiba, Japan,
September 26–30, 2010.
Tomáš Mikolov, Stefan Kombrink, Lukáš Burget, Jan Honza Černocky, and Sanjeev Khudanpur.
Extensions of recurrent neural network language model. In Acoustics, Speech and Signal Pro-
cessing (ICASSP), IEEE International Conference on, pages 5528–5531, 2011. DOI: 10.1109/i-
cassp.2011.5947611.
Tomáš Mikolov. Kai Chen, Greg Corrado, and Jeﬀrey Dean. Eﬃcient estimation of word rep-
resentations in vector space. arXiv:1301.3781 [cs], January 2013.

274
BIBLIOGRAPHY
Tomáš Mikolov. Quoc V. Le, and Ilya Sutskever. Exploiting similarities among languages for
machine translation. CoRR, abs/1309.4168, 2013. http://arxiv.org/abs/1309.4168
Tomáš Mikolov. Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeﬀ Dean. Distributed repre-
sentations of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Weinberger, Eds., Advances in Neural Information
Processing Systems 26, pages 3111–3119. Curran Associates, Inc., 2013.
Tomáš Mikolov. Wen-tau Yih, and Geoﬀrey Zweig. Linguistic regularities in continuous space
word representations. In Proc. of the Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pages 746–751, 2013. http://ac
lweb.org/anthology/N13-1090
Tomáš Mikolov. Armand Joulin, Sumit Chopra, Michael Mathieu, and Marc’Aurelio Ranzato.
Learning longer memory in recurrent neural networks. arXiv:1412.7753 [cs], December 2014.
Scott Miller, Jethran Guinness, and Alex Zamanian.
Name tagging with word clusters and
discriminative training. In Proc. of the Human Language Technology Conference of the North
American Chapter of the Association for Computational Linguistics: HLT-NAACL, 2004. http:
//aclweb.org/anthology/N04-1043
Andriy Mnih and Koray Kavukcuoglu.
Learning word embeddings eﬃciently with noise-
contrastive estimation.
In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and
K. Q. Weinberger, Eds., Advances in Neural Information Processing Systems 26, pages 2265–
2273. Curran Associates, Inc., 2013.
Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic
language models. In John Langford and Joelle Pineau, Eds., Proc. of the 29th International
Conference on Machine Learning (ICML-12), pages 1751–1758, New York, NY, July 2012.
Omnipress.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
MIT Press, 2012.
Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model.
In Robert G. Cowell and Zoubin Ghahramani, Eds., Proc. of the 10th International Workshop
on Artiﬁcial Intelligence and Statistics, pages 246–252, 2005. http://www.iro.umontreal.ca/~
lisa/pointeurs/hierarchical-nnlm-aistats05.pdf
Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise omson, Milica Gasic, Pei-Hao Su, David
Vandyke, Tsung-Hsien Wen, and Steve Young.
Multi-domain dialog state tracking using

BIBLIOGRAPHY
275
recurrent neural networks. In Proc. of the 53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint Conference on Natural Language Processing—
(Volume 2: Short Papers), pages 794–799, Beijing, China. Association for Computational Lin-
guistics, July 2015. DOI: 10.3115/v1/p15-2130.
Masami Nakamura and Kiyohiro Shikano. A study of English word category prediction based on
neural networks. e Journal of the Acoustical Society of America, 84(S1):S60–S61, 1988. DOI:
10.1121/1.2026400.
R. Neidinger.
Introduction to automatic diﬀerentiation and MATLAB object-oriented
programming.
SIAM Review, 52(3):545–563, January 2010.
ISSN 0036-1445. DOI:
10.1137/080743627.
Y. Nesterov. A method of solving a convex programming problem with convergence rate O (1/k2).
In Soviet Mathematics Doklady, 27:372–376, 1983.
Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer Academic Publishers, 2004.
DOI: 10.1007/978-1-4419-8853-9.
Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed Ammar, Antonios
Anastasopoulos, Miguel Ballesteros, David Chiang, Daniel Clothiaux, Trevor Cohn, Kevin
Duh, Manaal Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji, Lingpeng Kong, Adhiguna
Kuncoro, Gaurav Kumar, Chaitanya Malaviya, Paul Michel, Yusuke Oda, Matthew Richard-
son, Naomi Saphra, Swabha Swayamdipta, and Pengcheng Yin. DyNet: e dynamic neural
network toolkit. CoRR, abs/1701.03980, 2017. http://arxiv.org/abs/1701.03980
ien Huu Nguyen and Ralph Grishman. Event detection and domain adaptation with convo-
lutional neural networks. In Proc. of the 53rd Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint Conference on Natural Language Processing—
(Volume 2: Short Papers), pages 365–371, Beijing, China, July 2015. DOI: 10.3115/v1/p15-
2060.
Joakim Nivre.
Algorithms for deterministic incremental dependency parsing.
Computa-
tional Linguistics, 34(4):513–553, December 2008.
ISSN 0891-2017, 1530-9312. DOI:
10.1162/coli.07-056-R1-07-027.
Joakim Nivre, Željko Agić, Maria Jesus Aranzabe, Masayuki Asahara, Aitziber Atutxa, Miguel
Ballesteros, John Bauer, Kepa Bengoetxea, Riyaz Ahmad Bhat, Cristina Bosco, Sam Bowman,
Giuseppe G. A. Celano, Miriam Connor, Marie-Catherine de Marneﬀe, Arantza Diaz de
Ilarraza, Kaja Dobrovoljc, Timothy Dozat, Tomaž Erjavec, Richárd Farkas, Jennifer Foster,
Daniel Galbraith, Filip Ginter, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg, Berta Gon-
zales, Bruno Guillaume, Jan Hajič, Dag Haug, Radu Ion, Elena Irimia, Anders Johannsen, Hi-
roshi Kanayama, Jenna Kanerva, Simon Krek, Veronika Laippala, Alessandro Lenci, Nikola

276
BIBLIOGRAPHY
Ljubešić, Teresa Lynn, Christopher Manning, Cătălina Mărănduc, David Mareček, Héctor
Martínez Alonso, Jan Mašek, Yuji Matsumoto, Ryan McDonald, Anna Missilä, Verginica
Mititelu, Yusuke Miyao, Simonetta Montemagni, Shunsuke Mori, Hanna Nurmi, Petya
Osenova, Lilja Øvrelid, Elena Pascual, Marco Passarotti, Cenel-Augusto Perez, Slav Petrov,
Jussi Piitulainen, Barbara Plank, Martin Popel, Prokopis Prokopidis, Sampo Pyysalo, Lo-
ganathan Ramasamy, Rudolf Rosa, Shadi Saleh, Sebastian Schuster, Wolfgang Seeker, Moj-
gan Seraji, Natalia Silveira, Maria Simi, Radu Simionescu, Katalin Simkó, Kiril Simov, Aaron
Smith, Jan Štěpánek, Alane Suhr, Zsolt Szántó, Takaaki Tanaka, Reut Tsarfaty, Sumire Ue-
matsu, Larraitz Uria, Viktor Varga, Veronika Vincze, Zdeněk Žabokrtský, Daniel Zeman,
and Hanzhi Zhu. Universal dependencies 1.2, 2015. http://hdl.handle.net/11234/1-1548
LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles
University in Prague.
Chris Okasaki. Purely Functional Data Structures. Cambridge University Press, Cambridge, UK,
June 1999. DOI: 10.1017/cbo9780511530104.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of English: e Penn Treebank. Computational Linguistics, 19(2), June 1993, Special
Issue on Using Large Corpora: II, 1993. http://aclweb.org/anthology/J93-2004
Martha Palmer, Daniel Gildea, and Nianwen Xue. Semantic Role Labeling. Synthesis Lectures
on Human Language Technologies. Morgan & Claypool Publishers, 2010. DOI: 10.1093/ox-
fordhb/9780199573691.013.023.
Bo Pang and Lillian Lee. Opinion mining and sentiment analysis. Foundation and Trends in
Information Retrieval, 2:1–135, 2008. DOI: 10.1561/1500000011.
Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable atten-
tion model for natural language inference. In Proc. of EMNLP, 2016. DOI: 10.18653/v1/d16-
1244.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the diﬃculty of training recurrent
neural networks. arXiv:1211.5063 [cs], November 2012.
Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-
Burch. PPDB 2.0: Better paraphrase ranking, ﬁne-grained entailment relations, word embed-
dings, and style classiﬁcation. In Proc. of the 53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint Conference on Natural Language Processing—
(Volume 2: Short Papers), pages 425–430. Association for Computational Linguistics, 2015.
http://aclweb.org/anthology/P15-2070 DOI: 10.3115/v1/P15-2070.
Wenzhe Pei, Tao Ge, and Baobao Chang. An eﬀective neural network model for graph-based de-
pendency parsing. In Proc. of the 53rd Annual Meeting of the Association for Computational Lin-

BIBLIOGRAPHY
277
guistics and the 7th International Joint Conference on Natural Language Processing—(Volume 1:
Long Papers), pages 313–322, Beijing, China, July 2015. DOI: 10.3115/v1/p15-1031.
Joris Pelemans, Noam Shazeer, and Ciprian Chelba.
Sparse non-negative matrix language
modeling.
Transactions of the Association of Computational Linguistics, 4(1):329–342, 2016.
http://aclweb.org/anthology/Q16-1024
Jian Peng, Liefeng Bo, and Jinbo Xu. Conditional neural ﬁelds. In Y. Bengio, D. Schuurmans,
J. D. Laﬀerty, C. K. I. Williams, and A. Culotta, Eds., Advances in Neural Information Pro-
cessing Systems 22, pages 1419–1427. Curran Associates, Inc., 2009.
Jeﬀrey Pennington, Richard Socher, and Christopher Manning. GloVe: global vectors for word
representation. In Proc. of the Conference on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 1532–1543, Doha, Qatar. Association for Computational Linguistics,
October 2014. DOI: 10.3115/v1/d14-1162.
Vu Pham, Christopher Kermorvant, and Jérôme Louradour. Dropout improves recurrent neural
networks for handwriting recognition. CoRR, abs/1312.4569, 2013. http://arxiv.org/abs/
1312.4569 DOI: 10.1109/icfhr.2014.55.
Barbara Plank, Anders Søgaard, and Yoav Goldberg. Multilingual part-of-speech tagging with
bidirectional long short-term memory models and auxiliary loss. In Proc. of the 54th Annual
Meeting of the Association for Computational Linguistics—(Volume 2: Short Papers), pages 412–
418. Association for Computational Linguistics, 2016. http://aclweb.org/anthology/P16-
2067 DOI: 10.18653/v1/P16-2067.
Jordan B. Pollack. Recursive distributed representations. Artiﬁcial Intelligence, 46:77–105, 1990.
DOI: 10.1016/0004-3702(90)90005-k.
B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-
putational Mathematics and Mathematical Physics, 4(5):1–17, 1964. ISSN 0041-5553. DOI:
10.1016/0041-5553(64)90137-5.
Qiao Qian, Bo Tian, Minlie Huang, Yang Liu, Xuan Zhu, and Xiaoyan Zhu. Learning tag
embeddings and tag-speciﬁc composition functions in recursive neural network. In Proc. of the
53rd Annual Meeting of the Association for Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing—(Volume 1: Long Papers), pages 1365–1374,
Beijing, China, July 2015. DOI: 10.3115/v1/p15-1132.
Lev Ratinov and Dan Roth. Proc. of the 13th Conference on Computational Natural Language
Learning (CoNLL-2009), chapter Design Challenges and Misconceptions in Named Entity
Recognition, pages 147–155. Association for Computational Linguistics, 2009. http://aclw
eb.org/anthology/W09-1119

278
BIBLIOGRAPHY
Ronald Rosenfeld. A maximum entropy approach to adaptive statistical language modeling. Com-
puter, Speech and Language, 10:187–228, 1996. Longe version: Carnegie Mellon Technical
Report CMU-CS-94-138. DOI: 10.1006/csla.1996.0011.
Stéphane Ross and J. Andrew Bagnell. Eﬃcient reductions for imitation learning. In Proc. of the
13th International Conference on Artiﬁcial Intelligence and Statistics, pages 661–668, 2010.
Stéphane Ross, Geoﬀrey J. Gordon, and J. Andrew Bagnell. A reduction of imitation learning and
structured prediction to no-regret online learning. In Proc. of the 14th International Conference
on Artiﬁcial Intelligence and Statistics, pages 627–635, 2011.
David E. Rumelhart, Geoﬀrey E. Hinton, and Ronald J. Williams.
Learning represen-
tations by back-propagating errors.
Nature, 323(6088):533–536, October 1986. DOI:
10.1038/323533a0.
Ivan A. Sag, omas Wasow, and Emily M. Bender. Syntactic eory, 2nd ed., CSLI Lecture
Note 152, 2003.
Magnus Sahlgren. e distributional hypothesis. Italian Journal of Linguistics, 20(1):33–54, 2008.
Nathan Schneider, Vivek Srikumar, Jena D. Hwang, and Martha Palmer. A hierarchy with, of,
and for preposition supersenses. In Proc. of the 9th Linguistic Annotation Workshop, pages 112–
123, 2015. DOI: 10.3115/v1/w15-1612.
Nathan Schneider, Jena D. Hwang, Vivek Srikumar, Meredith Green, Abhijit Suresh, Kathryn
Conger, Tim O’Gorman, and Martha Palmer. A corpus of preposition supersenses. In Proc.
of the 10th Linguistic Annotation Workshop, 2016. DOI: 10.18653/v1/w16-1712.
Bernhard Schölkopf. e kernel trick for distances. In T. K. Leen, T. G. Dietterich, and V. Tresp,
Eds., Advances in Neural Information Processing Systems 13, pages 301–307. MIT Press, 2001.
http://papers.nips.cc/paper/1862-the-kernel-trick-for-distances.pdf
M. Schuster and Kuldip K. Paliwal.
Bidirectional recurrent neural networks.
IEEE Trans-
actions on Signal Processing, 45(11):2673–2681, November 1997.
ISSN 1053-587X. DOI:
10.1109/78.650093.
Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gauvain. Continuous space language mod-
els for statistical machine translation.
In Proc. of the COLING/ACL on Main Confer-
ence Poster Sessions, pages 723–730. Association for Computational Linguistics, 2006. DOI:
10.3115/1273073.1273166.
Rico Sennrich and Barry Haddow.
Proc. of the 1st Conference on Machine Translation: Vol-
ume 1, Research Papers, chapter Linguistic Input Features Improve Neural Machine Translation,
pages 83–91. Association for Computational Linguistics, 2016. http://aclweb.org/antholo
gy/W16-2209 DOI: 10.18653/v1/W16-2209.

BIBLIOGRAPHY
279
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words
with subword units. In Proc. of the 54th Annual Meeting of the Association for Computational
Linguistics—(Volume 1: Long Papers), pages 1715–1725, 2016a. http://aclweb.org/antholo
gy/P16-1162 DOI: 10.18653/v1/P16-1162.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
Improving neural machine translation
models with monolingual data. In Proc. of the 54th Annual Meeting of the Association for Com-
putational Linguistics—(Volume 1: Long Papers), pages 86–96. Association for Computational
Linguistics, 2016b. http://aclweb.org/anthology/P16-1009 DOI: 10.18653/v1/P16-1009.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From eory to
Algorithms. Cambridge University Press, 2014. DOI: 10.1017/cbo9781107298019.
John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge Uni-
versity Press, Cambridge, UK, June 2004. DOI: 10.4018/9781599040424.ch001.
Q. Shi, J. Petterson, G. Dror, J. Langford, A. J. Smola, A. Strehl, and V. Vishwanathan. Hash
kernels. In Artiﬁcial Intelligence and Statistics AISTATS’09, Florida, April 2009.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR, 2015.
Noah A. Smith. Linguistic Structure Prediction. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool, May 2011. DOI: 10.2200/s00361ed1v01y201105hlt013.
Richard Socher. Recursive Deep Learning For Natural Language Processing and Computer Vision.
Ph.D. thesis, Stanford University, August 2014.
Richard Socher, Christopher Manning, and Andrew Ng. Learning continuous phrase represen-
tations and syntactic parsing with recursive neural networks. In Proc. of the Deep Learning and
Unsupervised Feature Learning Workshop of {NIPS}, pages 1–9, 2010.
Richard Socher, Cliﬀ Chiung-Yu Lin, Andrew Y. Ng, and Christopher D. Manning. Parsing
natural scenes and natural language with recursive neural networks. In Lise Getoor and To-
bias Scheﬀer, Eds., Proc. of the 28th International Conference on Machine Learning, ICML ,
pages 129–136, Bellevue, Washington, June 28–July 2, Omnipress, 2011.
Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic com-
positionality through recursive matrix-vector spaces. In Proc. of the Joint Conference on Em-
pirical Methods in Natural Language Processing and Computational Natural Language Learning,
pages 1201–1211, Jeju Island, Korea. Association for Computational Linguistics, July 2012.
Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. Parsing with compo-
sitional vector grammars. In Proc. of the 51st Annual Meeting of the Association for Computational
Linguistics—(Volume 1: Long Papers), pages 455–465, Soﬁa, Bulgaria, August 2013a.

280
BIBLIOGRAPHY
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proc. of the 2013 Conference on Empirical Methods in Natural Language Processing,
pages 1631–1642, Seattle, Washington. Association for Computational Linguistics, October
2013b.
Anders Søgaard. Semi-Supervised Learning and Domain Adaptation in Natural Language Process-
ing. Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers,
2013. DOI: 10.2200/s00497ed1v01y201304hlt021.
Anders Søgaard and Yoav Goldberg.
Deep multi-task learning with low level tasks super-
vised at lower layers. In Proc. of the 54th Annual Meeting of the Association for Computational
Linguistics—(Volume 2: Short Papers), pages 231–235, 2016. http://aclweb.org/anthology/P
16-2038 DOI: 10.18653/v1/P16-2038.
Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret
Mitchell, Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. A neural network approach to context-
sensitive generation of conversational responses. In Proc. of the Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Language Technologies,
pages 196–205, Denver, Colorado, 2015. DOI: 10.3115/v1/n15-1020.
Vivek Srikumar and Dan Roth. An inventory of preposition relations. arXiv:1305.5785, 2013a.
Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. Dropout: A simple way to prevent neural networks from overﬁtting. Journal of Machine
Learning Research, 15:1929–1958, 2014. http://jmlr.org/papers/v15/srivastava14a.html
E. Strubell, P. Verga, D. Belanger, and A. McCallum. Fast and accurate sequence labeling with
iterated dilated convolutions. ArXiv e-prints, February 2017.
Martin Sundermeyer, Ralf Schlüter, and Hermann Ney. LSTM neural networks for language
modeling. In INTERSPEECH, 2012.
Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker, and Hermann Ney. Translation model-
ing with bidirectional recurrent neural networks. In Proc. of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 14–25, Doha, Qatar. Association for Com-
putational Linguistics, October 2014. DOI: 10.3115/v1/d14-1003.
Ilya Sutskever, James Martens, and Geoﬀrey E. Hinton. Generating text with recurrent neu-
ral networks. In Proc. of the 28th International Conference on Machine Learning (ICML-11),
pages 1017–1024, 2011. DOI: 10.1109/icnn.1993.298658.
Ilya Sutskever, James Martens, George Dahl, and Geoﬀrey Hinton. On the importance of ini-
tialization and momentum in deep learning. In Proc. of the 30th International Conference on
Machine Learning (ICML-13), pages 1139–1147, 2013.

BIBLIOGRAPHY
281
Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le. Sequence to sequence learning with neural
networks. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Wein-
berger, Eds., Advances in Neural Information Processing Systems 27, pages 3104–3112. Curran
Associates, Inc., 2014.
Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic represen-
tations from tree-structured long short-term memory networks. In Proc. of the 53rd Annual
Meeting of the Association for Computational Linguistics and the 7th International Joint Conference
on Natural Language Processing—(Volume 1: Long Papers), pages 1556–1566, Beijing, China,
July 2015. DOI: 10.3115/v1/p15-1150.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
Recurrent neural networks for word
alignment model. In Proc. of the 52nd Annual Meeting of the Association for Computational
Linguistics—(Volume 1: Long Papers), pages 1470–1480, Baltimore, Maryland, June 2014.
DOI: 10.3115/v1/p14-1138.
Duyu Tang, Bing Qin, and Ting Liu. Document modeling with gated recurrent neural network
for sentiment classiﬁcation. In Proc. of the Conference on Empirical Methods in Natural Language
Processing, pages 1422–1432. Association for Computational Linguistics, 2015. http://aclw
eb.org/anthology/D15-1167 DOI: 10.18653/v1/D15-1167.
Matus Telgarsky. Beneﬁts of depth in neural networks. arXiv:1602.04485 [cs, stat], February
2016.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society, Series B, 58:267–288, 1994. DOI: 10.1111/j.1467-9868.2011.00771.x.
T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average
of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. Word representations: A simple and gen-
eral method for semi-supervised learning. In Proc. of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384–394, 2010. http://aclweb.org/anthology/P10-1040
Peter D. Turney. Mining the web for synonyms: PMI-IR vs. LSA on TOEFL. In ECML, 2001.
DOI: 10.1007/3-540-44795-4_42.
Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of seman-
tics. Journal of Artiﬁcial Intelligence Research, 37(1):141–188, 2010.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Dubiner. Large scale parallel document
mining for machine translation. In Proc. of the 23rd International Conference on Computational
Linguistics (Coling 2010), pages 1101–1109, Organizing Committee, 2010. http://aclweb.o
rg/anthology/C10-1124

282
BIBLIOGRAPHY
Tim Van de Cruys.
A neural network approach to selectional preference acquisition.
In
Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP),
pages 26–35, Doha, Qatar. Association for Computational Linguistics, October 2014. DOI:
10.3115/v1/d14-1004.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. Decoding with large-
scale neural language models improves translation. In Proc. of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1387–1392, Seattle, Washington. Association for
Computational Linguistics, October 2013.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. Decoding with large-scale
neural language models improves translation. In Proc. of the Conference on Empirical Methods
in Natural Language Processing, pages 1387–1392. Association for Computational Linguistics,
2013. http://aclweb.org/anthology/D13-1140
Ashish Vaswani, Yonatan Bisk, Kenji Sagae, and Ryan Musa. Supertagging with LSTMs. In
Proc. of the Conference of the North American Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 232–237. Association for Computational Linguistics,
2016. http://aclweb.org/anthology/N16-1027 DOI: 10.18653/v1/N16-1027.
Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoﬀrey Hinton.
Grammar as a foreign language. arXiv:1412.7449 [cs, stat], December 2014.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural
image caption generator.
In IEEE Conference on Computer Vision and Pattern Recognition,
CVPR, pages 3156–3164, Boston, MA, June 7–12, 2015. DOI: 10.1109/cvpr.2015.7298935.
Stefan Wager, Sida Wang, and Percy S Liang. Dropout training as adaptive regularization. In
C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, Eds., Advances
in Neural Information Processing Systems 26, pages 351–359. Curran Associates, Inc., 2013.
Mengqiu Wang and Christopher D. Manning. Eﬀect of non-linear deep architecture in sequence
labeling. In IJCNLP, pages 1285–1291, 2013.
Peng Wang, Jiaming Xu, Bo Xu, Chenglin Liu, Heng Zhang, Fangyuan Wang, and Hongwei
Hao. Semantic clustering and convolutional neural network for short text categorization. In
Proc. of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Inter-
national Joint Conference on Natural Language Processing—(Volume 2: Short Papers), pages 352–
357, Beijing, China, July 2015a. DOI: 10.3115/v1/p15-2058.
Xin Wang, Yuanchao Liu, Chengjie Sun, Baoxun Wang, and Xiaolong Wang. Predicting polar-
ities of tweets by composing word embeddings with long short-term memory. In Proc. of the
53rd Annual Meeting of the Association for Computational Linguistics and the 7th International

BIBLIOGRAPHY
283
Joint Conference on Natural Language Processing—(Volume 1: Long Papers), pages 1343–1353,
Beijing, China, July 2015b. DOI: 10.3115/v1/p15-1130.
Taro Watanabe and Eiichiro Sumita. Transition-based neural constituent parsing. In Proc. of the
53rd Annual Meeting of the Association for Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing—(Volume 1: Long Papers), pages 1169–1179,
Beijing, China, July 2015. DOI: 10.3115/v1/p15-1113.
K. Weinberger, A. Dasgupta, J. Attenberg, J. Langford, and A. J. Smola. Feature hashing for
large scale multitask learning. In International Conference on Machine Learning, 2009. DOI:
10.1145/1553374.1553516.
David Weiss, Chris Alberti, Michael Collins, and Slav Petrov. Structured training for neural net-
work transition-based parsing. In Proc. of the 53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint Conference on Natural Language Processing—
(Volume 1: Long Papers), pages 323–333, Beijing, China, July 2015. DOI: 10.3115/v1/p15-
1032.
P. J. Werbos. Backpropagation through time: What it does and how to do it. Proc. of the IEEE,
78(10):1550–1560, 1990. ISSN 0018-9219. DOI: 10.1109/5.58337.
Jason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier. Connecting language
and knowledge bases with embedding models for relation extraction. In Proc. of the Conference
on Empirical Methods in Natural Language Processing, pages 1366–1371, Seattle, Washington.
Association for Computational Linguistics, October 2013.
Philip Williams, Rico Sennrich, Matt Post, and Philipp Koehn. Syntax-based Statistical Machine
Translation. Synthesis Lectures on Human Language Technologies. Morgan & Claypool Pub-
lishers, 2016. DOI: 10.2200/s00716ed1v04y201604hlt033.
Sam Wiseman and Alexander M. Rush. Sequence-to-sequence learning as beam-search op-
timization.
In Proc. of the Conference on Empirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguistics, 2016. DOI: 10.18653/v1/d16-1137.
Sam Wiseman, M. Alexander Rush, and M. Stuart Shieber.
Learning global features for
coreference resolution. In Proc. of the Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Language Technologies, pages 994–1004, 2016.
http://aclweb.org/anthology/N16-1114 DOI: 10.18653/v1/N16-1114.
Yijun Xiao and Kyunghyun Cho. Eﬃcient character-level document classiﬁcation by combining
convolution and recurrent layers. CoRR, abs/1602.00367, 2016. http://arxiv.org/abs/1602.
00367

284
BIBLIOGRAPHY
Wenduan Xu, Michael Auli, and Stephen Clark. CCG supertagging with a recurrent neural
network. In Proc. of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing—(Volume 2: Short
Papers), pages 250–255, Beijing, China, July 2015. DOI: 10.3115/v1/p15-2041.
Wenpeng Yin and Hinrich Schütze. Convolutional neural network for paraphrase identiﬁca-
tion. In Proc. of the Conference of the North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies, pages 901–911, Denver, Colorado, 2015.
DOI: 10.3115/v1/n15-1091.
Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In ICLR,
2016.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.
arXiv:1409.2329 [cs], September 2014.
Matthew D. Zeiler. ADADELTA: An adaptive learning rate method. arXiv:1212.5701 [cs],
December 2012.
Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. Relation classiﬁcation via
convolutional deep neural network. In Proc. of COLING, the 25th International Conference on
Computational Linguistics: Technical Papers, pages 2335–2344, Dublin, Ireland, Dublin City
University and Association for Computational Linguistics, August 2014.
Hao Zhang and Ryan McDonald.
Generalized higher-order dependency parsing with cube
pruning. In Proc. of the Joint Conference on Empirical Methods in Natural Language Processing
and Computational Natural Language Learning, pages 320–331. Association for Computational
Linguistics, 2012. http://aclweb.org/anthology/D12-1030
Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk
minimization. e Annals of Statistics, 32:56–85, 2004. DOI: 10.1214/aos/1079120130.
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text
classiﬁcation.
In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,
Eds., Advances in Neural Information Processing Systems 28, pages 649–657. Curran Associates,
Inc., 2015. http://papers.nips.cc/paper/5782-character-level-convolutional-networks-fo
r-text-classification.pdf
Xingxing Zhang, Jianpeng Cheng, and Mirella Lapata. Dependency parsing as head selection.
CoRR, abs/1606.01280, 2016. http://arxiv.org/abs/1606.01280
Yuan Zhang and David Weiss. Stack-propagation: Improved representation learning for syn-
tax.
In Proc. of the 54th Annual Meeting of the Association for Computational Linguistics—
(Volume 1: Long Papers), pages 1557–1566, 2016. http://aclweb.org/anthology/P16-1147
DOI: 10.18653/v1/P16-1147.

BIBLIOGRAPHY
285
Hao Zhou, Yue Zhang, Shujian Huang, and Jiajun Chen. A neural probabilistic structured-
prediction model for transition-based dependency parsing. In Proc. of the 53rd Annual Meet-
ing of the Association for Computational Linguistics and the 7th International Joint Conference on
Natural Language Processing—(Volume 1: Long Papers), pages 1213–1222, Beijing, China, July
2015. DOI: 10.3115/v1/p15-1117.
Chenxi Zhu, Xipeng Qiu, Xinchi Chen, and Xuanjing Huang. A re-ranking model for depen-
dency parser with recursive convolutional neural network. In Proc. of the 53rd Annual Meeting of
the Association for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing—(Volume 1: Long Papers), pages 1159–1168, Beijing, China, July 2015a.
DOI: 10.3115/v1/p15-1112.
Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. Long short-term memory over tree structures.
March 2015b.
Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the
Royal Statistical Society, Series B, 67:301–320, 2005. DOI: 10.1111/j.1467-9868.2005.00503.x.

287
Author’s Biography
YOAV GOLDBERG
Yoav Goldberg has been working in natural language processing for over a decade. He is a Senior
Lecturer at the Computer Science Department at Bar-Ilan University, Israel. Prior to that, he
was a researcher at Google Research, New York. He received his Ph.D. in Computer Science
and Natural Language Processing from Ben Gurion University (2011). He regularly reviews for
NLP and machine learning venues, and serves at the editorial board of Computational Linguistics.
He published over 50 research papers and received best paper and outstanding paper awards at
major natural language processing conferences. His research interests include machine learning
for natural language, structured prediction, syntactic parsing, processing of morphologically rich
languages, and, in the past two years, neural network models with a focus on recurrent neural
networks.

